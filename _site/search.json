[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics for Life Sciences",
    "section": "",
    "text": "Announcements\n\n\n\nFinal grades posted; have a great summer!\n\n\n\nCourse information\nRead the [course syllabus] for detailed information on content, materials, learning outcomes, assessments, and course policies.\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment]\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class.\n\nCompleting assignments:\nOne set of practice problems is included at the end of each lab; these problem sets are your homework assignments. You will often have some time to work on them during class, and they will be due by the following class period. To complete these assignments:\n\nReview the prompts included with the lab.\nDo your work (calculations, making plots, etc.) in the lab script provided in Posit Cloud.\nFollow the link that appears as [problem set N] with the class meeting outline for the period in which the problem set was assigned. This will direct you to a form where you’ll fill out select answers. Refer to your work in Posit Cloud as you complete the form.\n\nSome general remarks:\n\nproblem sets are due one hour before the next class meeting\nlate submissions are accepted until 5pm two days after the due date\nscore summaries will be posted once all deadlines pass\nonce scores are posted, you can see your individual responses using the link that you used to access the form\n\n\n\nWeek 1 (4/1/24)\nAcademic holiday 4/1/24\nIntroduction to statistical thinking and study designs\nWednesday class meeting\n\n[reading] Vu and Harrington 1.1\n[lecture] course introduction; study designs\n[activity] distinguishing types of studies\n[problem set 1] due Monday 4/8; late submissions until Wednesday 4/10 5pm\n[problem set 1 corrections] due by 5pm Friday 4/12\n\nResponse summary [PS1] [PS1 corrections]\n\n\nWeek 2 (4/8/24)\nData types and descriptive statistics\nMonday class meeting\n\nreading quiz [12pm section] [2pm section]\n[reading] Vu and Harrington 1.2\n[lecture] data types\n[lab] R basics\n[problem set 2] due Wednesday 4/10; late submissions until Friday 4/12 5pm\n\nResponse summary [PS2]\nWednesday class meeting\n\n[reading] Vu and Harrington 1.4 - 1.5\n[lecture] descriptive statistics\n[lab] descriptive statistics in R\n[problem set 3] due Monday 4/15; late submissions until Wednesday 4/17 5pm\n\nResponse summary [PS3]\n\n\nWeek 3 (4/15/24)\nDescriptive statistics and graphical summaries\nMonday class meeting\n\n[reading quiz] Vu and Harrington 1.6\n[lecture] descriptive statistics for relationships between two variables\n[lab] bivariate summaries in R\n[problem set 4] due Wednesday 4/17; late submissions until Friday 4/19 5pm\n\nResponse summary [PS4]\nWednesday class meeting\n\n[reading] review course notes and PS1, PS2, PS3 in detail\n[review] recap and Q&A\n[test 1 practice problems] in groups with short solution presentations\n[R cheatsheet] for easy reference\n\nTest 1 available Wednesday 4/17 5pm and due Friday 4/19 5:00pm PDT [prompts] [submission] [upload R script]\n\n\nWeek 4 (4/22/24)\nFoundations for inference\nMonday class meeting\n\n[reading] Vu and Harrington 4.1\n[lecture] point estimation, sampling variability, and interval estimation\n[lab] point and interval estimation for a population mean\n[problem set 5] due Wednesday 4/24; late submissions until Friday 4/26 5pm\n\nResponse summary [PS5]\nWednesday class meeting\n\n[reading quiz] Vu and Harrington 3.3.1, 3.3.2, and 3.3.3; and 4.2\n[lecture] constructing and interpreting confidence intervals\n[lab] computing confidence intervals\n\nTest 1 corrections due Friday 4/26 5pm [submit corrections]\n\n\nWeek 5 (4/29/24)\nOne-sample inference for numerical data\nMonday class meeting\n\n[reading] Vu and Harrington 4.3.1 & 4.3.2\n[lecture] the \\(t\\)-test for a population mean\n[lab] computing test statistics, critical values, and \\(p\\)-values\n[problem set 6] due Wednesday 5/1; late submissions until Friday 5/3 5pm\n\nResponse summary [PS6]\nWednesday class meeting\n\n[reading] Vu and Harrington 4.3.3 & 4.3.4\n[lecture] directional tests\n[lab] directional tests\n[problem set 7] due Monday 5/6; late submissions until Wednesday 5/8 5pm\n\nResponse summary [PS7]\n\n\nWeek 6 (5/6/24)\nTwo-sample inference for numerical data\nPlease complete this short [midquarter feedback survey] by Friday 5/10. Responses are anonymous.\nMonday class meeting\n\n[reading] Vu and Harrington 5.3\n[lecture] two-sample inference\n[lab] two-sample t tests in R\n[problem set 8] due Wednesday 5/8; late submissions until Friday 5/10\n\nResponse summary [PS8]\nWednesday class meeting\n\n[reading] Vu and Harrington 5.4\n[lecture] decision errors; statistical power\n[test 2 practice problems] test 2 prep\n[R cheatsheet] for easy reference\n\nTest 2 available Wednesday 5/8 5pm and due Friday 5/10 5pm [prompts] [submission form] [upload R script] [submit corrections]\n\n\nWeek 7 (5/13/24)\nAnalysis of variance\nMonday class meeting\n\n[reading] Vu and Harrington 5.5.1 & 5.5.2\n[lecture] Introduction to analysis of variance\n[lab] fitting ANOVA models in R\n\nNO Wednesday class meeting\n\n[reading] van Belle et al. 8.4 and 8.5 up to 8.5.4\n[self-paced activity] nonparametric inferences for one- and two-sample problems\n[problem set 9] due Monday 5/20; late submissions until Wednesday 5/22 5pm\n\nResponse summary [PS9]\n\n\nWeek 8 (5/20/24)\nPost hoc inference in ANOVA; inference for a population proportion\nMonday class meeting\n\n[reading] Vu and Harrington 5.5.3 & 5.5.4\n[lecture] post hoc inference in ANOVA\n[lab] pairwise comparisons in R\n[problem set 10] due Wednesday 5/22; late submissions until Friday 5/24 5pm\n\nResponse summary [PS10]\nWednesday class meeting\n\n[reading] Vu and Harrington 8.1 & 8.2\n[lecture] inference for population proportions\n[lab] tests and intervals for proportions in R\n\nTest 3 available Wednesday 5/22 5pm PDT and due Friday 5/24 5:00pm PDT [prompts] [submission] [upload R script] [submit corrections]\n\n\nWeek 9 (5/27/24)\nAcademic holiday 5/27/24; Tuesday follows Monday schedule\nAnalysis of two-way contingency tables\nTuesday class meeting\n\n[reading] Vu and Harrington 8.3 (excluding 8.3.5)\n[lecture] tests of association in two-way tables\n[lab] \\(\\chi^2\\) tests in R\n[problem set 11] due Wednesday 5/29; late submissions until Friday 5/31 5pm\n\nResponse summary [PS11]\nWednesday class meeting\n\n[reading] Vu and Harrington 8.5\n[lecture] inference for odds ratios and relative risk\n[lab] measures of association in R\n[problem set 12] due Monday 6/3; late submissions until Wednesday 6/5 5pm\n\nResponse summary [PS12]\n\n\nWeek 10 (6/3/24)\nSimple linear regression\nNO Monday class meeting\n\n[reading] Vu and Harrington 6.1 & 6.2\n[activity] warm-up for simple linear regression: line fitting\n[problem set 13] due Wednesday 6/5; late submissions until Friday 6/7 5pm\n\nWednesday class meeting\n\n[reading] Vu and Harrington 6.4\n[lecture] inference in simple linear regression\n[lab] estimating the age of the universe\n[miscellany] scheduling oral exam times; course evals\n\nTest 4 due Friday 6/7 5:00pm PDT [prompts] [submission form] [upload R script]\n\n\nFinals week (6/10/24)\nOral exams to be held during scheduled exam time\nScheduled exam times:\n\n[12pm section] Wednesday 6/12 10:10am – 1:00pm\n[2pm section] Monday 6/10 1:10pm – 4:00pm\n\n[project guidelines] [schedule] [upload deliverable]"
  },
  {
    "objectID": "content/lab3-bivariate.html",
    "href": "content/lab3-bivariate.html",
    "title": "Lab 3: Bivariate summaries",
    "section": "",
    "text": "This lab covers descriptive summaries and graphics for two variables. The goal of the activity is to learn to produce joint summaries of two variables for identifying relationships, in particular:\n\ncontingency tables\nproportional stacked bar plots\nscatterplots\nside-by-side boxplots\n\nWe will use the FAMuSS dataset again.\n\nlibrary(tidyverse)\nload('data/famuss.RData')\n\n\nContingency tables\n\nCounts\nA contingency table is a summary of two categorical variables. Specifically, it is a two-way table in which:\n\nrows correspond to the values of one variable\ncolumns correspond to the values of the other variable\nentries are counts of the numbers of observations for each combination of values\n\nIn fact, a contingency table is a frequency distribution for two variables. It can be constructed in R by retrieving the two variables of interest and providing them as arguments to table().\n\n# retrieve variables of interest\ngenotype &lt;- famuss$genotype\nsex &lt;- famuss$sex\n\n# make a contingency table\ntable(sex, genotype)\n\n        genotype\nsex       CC  CT  TT\n  Female 106 149  98\n  Male    67 112  63\n\n\nThe order of the arguments determines the row/column orientation of the table.\nThe values show the observation counts for each combination, so, for example, the number 106 in the upper-leftmost cell indicates that 106 participants were women with genotype CC.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a contingency table of race and genotype, with race as the row variable and genotype as the column variable.\n\n# retrieve variables of interest\n\n# make a contingency table\n\n\n\n\n\nProportions\nTo convert a contingency table to proportions, one can use either the grand total, row totals, or column totals. The resulting proportions have different meanings:\n\ngrand total – combination frequencies\nrow totals – column variable frequencies by row variable value\ncolumn totals – row variable frequencies by column variable value\n\nIn the sex/genotype contingency table, these proportions would be computed and interpreted as follows:\n\n# combination frequencies (using grand total as denominator)\ntable(sex, genotype) |&gt; proportions(margin = NULL)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.1781513 0.2504202 0.1647059\n  Male   0.1126050 0.1882353 0.1058824\n\n# genotype frequencies by sex (using row margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 1)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.3002833 0.4220963 0.2776204\n  Male   0.2768595 0.4628099 0.2603306\n\n# sex frequencies by genotype (using column margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 2)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.6127168 0.5708812 0.6086957\n  Male   0.3872832 0.4291188 0.3913043\n\n\nNotice that the margin = ... argument controls which totals are used as denominators in computing proportions. The syntax is:\n\nmargin = NULL specifies grand total\nmargin = 1 uses row totals\nmargin = 2 uses column totals\n\nThe interpretation of entries depends on which type of proportion is comupted. For instance, looking at only the upper-leftmost entry in each of the three tables above:\n\n[first table] 17.82% of participants were women with genotype CC\n[second table] 30.03% of female participants had genotype CC\n[third table] 61.27% of participants with genotype CC were women\n\nIf you lose track of which margin was which, you can always check yourself by looking at which values sum to one: if all the values add up to one, grand totals were used; if rows sum to one, row totals were used; and if columns add up to one, column totals were used.\n\n\n\n\n\n\nYour turn\n\n\n\nConvert your contingency table of genotype and race from the previous ‘your turn’ to a table showing genotype frequencies by race.\n\n# genotype frequencies by race\n\nThen, interpret the entry for Hispanic/CT in context.\n\n\n\n\nGraphics\nThe latter two proportion tables – those constructed using row or column totals – can be visualized graphically using barplot(...):\n\n# stacked bar plot showing genotype frequencies by sex\ntable(genotype, sex) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nThis function is a little finnicky about the table orientation, so the most straightforward way to approach making the plot is to adhere to this rule of thumb:\n\nalways use margin = 2\nspecify the order of the variables as they are input to table(...) so that the grouping variable appears second\n\nSo, for instance, to visualize the sex frequencies by genotype, we would do the following:\n\n# stacked bar plot showing sex frequencies by genotype\ntable(sex, genotype) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nIf you see uneven bar heights, that means you have not configured the table correctly before passing it to barplot(...).\n\n\n\n\n\n\nYour turn\n\n\n\nMake a plot showing the genotype frequencies by race.\n\n# stacked bar plot showing genotype frequencies by race\n\nCheck your understanding:\n\nFor which group is the CC frequency highest?\nFor which group is the CC frequency lowest?\n\n\n\n\n\n\nScatterplots and correlation\nTaller people tend to be heavier. We should expect a relationship between weight and height. The example below shows a scatterplot of the two variables, and computes the correlation (measure of linear relationship).\n\n# retrieve height and weight columns\nheight &lt;- famuss$height\nweight &lt;- famuss$weight\n\n# basic scatterplot\nplot(height, weight)\n\n\n\n\n\n\n\n# correlation\ncor(weight, height)\n\n[1] 0.5308787\n\n\nA rough rule of thumb for interpreting correlations is as follows:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case, the correlation of 0.53 indicates a moderate positive relationship.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there a relationship between nondominant and dominant percent change in arm strength? Make a scatterplot and compute the correlation.\n\n# retrieve the percent change variables\n\n# construct a scatterplot\n\n# compute the correlation\n\n\n\n\n\nSide-by-side boxplots\nConsider one of the main questions for the study: were differences on the ACTN gene region associated with differential change in arm strength after resistance training?\nThis is a comparison between a categorical variable (genotype) and numeric variable (percent change in arm strength). For this type of comparison, a set of side-by-side boxplots is best:\n\n# side-by-side boxplots for non-dominant arm\nboxplot(ndrm.ch ~ genotype, data = famuss)\n\n\n\n\n\n\n\n# change the orientation \nboxplot(ndrm.ch ~ genotype, data = famuss, horizontal = T)\n\n\n\n\n\n\n\n\nThis graphics summarizes the frequency distribution of percent change in non-dominant arm strength separately for each genotype. It is essentially a grouped summary.\n\n\n\n\n\n\nYour turn\n\n\n\nConstruct side-by-side boxplots of percent change in dominant arm strength by genotype.\n\n# make side-by-side boxplots of percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\n\nTrait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease (CHD); 12,986 participants were recruited for a study examining this hypothesis and followed for five years. The anger dataset includes data for the 8557 participants identified as having normal blood pressure (normotensives) and records, for each participant, whether they are classified as having high, moderate, or low trait anger, and whether they experienced a CHD event during the study period.\n\nMake a contingency table showing trait anger and CHD event occurrence.\nFind the proportion of participants who experienced a CHD event in each trait anger group.\nConstruct a stacked bar plot to visualize the frequencies you found in (b).\nFind the ratio \\(\\frac{\\text{CHD frequency in the high anger group}}{\\text{CHD frequency in the low anger group}}\\). This is called the “relative risk” of a CHD event. (You can calculate this quantity by hand based on your result in (b).)\n\n\n\nUse the nhanes data from earlier to answer the following questions.\n\nMake a scatterplot of systolic blood pressure and diastolic blood pressure. Put systolic blood pressure on the y axis. Characterize the association, if any, as positive/negative/non-associated and linear/nonlinear.\nCompute and interpret the correlation between systolic blood pressure and diastolic blood pressure.\nDoes there appear to be a substantial difference in the distribution of total cholesterol by sex? Construct side-by-side boxplots to answer this question."
  },
  {
    "objectID": "content/week3-bivariate.html#todays-agenda",
    "href": "content/week3-bivariate.html#todays-agenda",
    "title": "Bivariate summaries",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\nLoose end: robustness\nBivariate numeric and graphical summaries\nLab: bivariate graphics in R"
  },
  {
    "objectID": "content/week3-bivariate.html#robustness",
    "href": "content/week3-bivariate.html#robustness",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\n\nPercentile-based measures of location and spread are less sensitive to outliers\n\nConsider adding an observation of 94 to our 12 ages from last time. This is an outlier.\n\n\n\n# append an outlier\nages_add &lt;- c(ages, 94)\n\n# means\nc(original = mean(ages), with.outlier = mean(ages_add))\n\n    original with.outlier \n    24.00000     29.38462 \n\n# medians\nc(original = median(ages), with.outlier = median(ages_add))\n\n    original with.outlier \n        23.5         25.0 \n\n# IQR\nc(original = IQR(ages), with.outlier = IQR(ages_add))\n\n    original with.outlier \n         8.5          9.0 \n\n# SD\nc(original = sd(ages), with.outlier = sd(ages_add))\n\n    original with.outlier \n    5.526794    20.122701 \n\n\n\nThe effect of this outlier on each statistic is:\n\nmean increases by 22.44%\nmedian increases by 6.38%\nIQR increases by 5.88%\nSD increases by 264.09%\n\nRobustness refers to sensitivity to outliers. Mean and SD are less robust than median and IQR."
  },
  {
    "objectID": "content/week3-bivariate.html#choosing-appropriate-measures",
    "href": "content/week3-bivariate.html#choosing-appropriate-measures",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures",
    "text": "Choosing appropriate measures\n\nWhen outliers are present, use percentile-based measures; otherwise, use mean and standard deviation or variance\n\n\nCheck your understanding: which measures are most appropriate for each variable above?"
  },
  {
    "objectID": "content/week3-bivariate.html#limitations-of-univariate-summaries",
    "href": "content/week3-bivariate.html#limitations-of-univariate-summaries",
    "title": "Bivariate summaries",
    "section": "Limitations of univariate summaries",
    "text": "Limitations of univariate summaries\n\nUnivariate summaries aim to capture the distribution of values of a single variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nboth unimodal, no obvious outliers\nheights symmetric\nweights right-skewed\nbut these observations actually come in pairs\n\n\n\nUnivariate summaries don’t reflect how the variables might be related."
  },
  {
    "objectID": "content/week3-bivariate.html#bivariate-summaries",
    "href": "content/week3-bivariate.html#bivariate-summaries",
    "title": "Bivariate summaries",
    "section": "Bivariate summaries",
    "text": "Bivariate summaries\n\nBivariate summaries aim to capture a relationship between two variables.\n\n\n\nA simple example is a scatterplot:\n\n\n\n\n\n\n\n\n\n\nEach point represents a pair of values \\((h, w)\\) for one study participant.\n\nReveals a relationship: taller participants tend to be heavier\nBut no longer shows individual distributions clearly\n\nNotice, though, that the marginal means (dashed red lines) still capture the center well."
  },
  {
    "objectID": "content/week3-bivariate.html#summary-types",
    "href": "content/week3-bivariate.html#summary-types",
    "title": "Bivariate summaries",
    "section": "Summary types",
    "text": "Summary types\n\nBivariate summary techniques differ depending on the data types of the variables.\n\n\n\n\n\n\n\n\nQuestion\nComparison type\n\n\n\n\nDid genotype frequencies differ by race or sex among study participants?\ncategorical/categorical\n\n\nWere differential changes in arm strength observed according to genotype?\nnumeric/categorical\n\n\nDid change in arm strength appear related in any way to body size among study participants?\nnumeric/numeric\n\n\nDid study participants experience similar or different changes in arm strength depending on arm dominance?\n??"
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical",
    "href": "content/week3-bivariate.html#categoricalcategorical",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\n\nA contingency table is a bivariate tabular summary of two categorical variables; it shows the frequency of each pair of values. Usually the marginal totals are also shown.\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n106\n149\n98\n353\n\n\nMale\n67\n112\n63\n242\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\nThere are multiple ways to convert to proportions by using different denominators, and these yield proportions with distinct interpretations:\n\ngrand total – frequency of genotype/sex combination\nrow total – genotype frequency by sex\ncolumn total – sex frequency by genotype"
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical-1",
    "href": "content/week3-bivariate.html#categoricalcategorical-1",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid genotype frequencies differ by sex among study participants?\nFor this question, the row totals should be used to convert to proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n0.3003\n0.4221\n0.2776\n1\n\n\nMale\n0.2769\n0.4628\n0.2603\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are quite close, suggesting minimal sex differences."
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical-2",
    "href": "content/week3-bivariate.html#categoricalcategorical-2",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid sex frequencies differ by genotype among study participants?\nFor this question, the column totals should be used to compute proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.6127\n0.5709\n0.6087\n\n\nMale\n0.3873\n0.4291\n0.3913\n\n\ntotal\n1\n1\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are close, suggesting minimal genotype differences."
  },
  {
    "objectID": "content/week3-bivariate.html#numericcategorical",
    "href": "content/week3-bivariate.html#numericcategorical",
    "title": "Bivariate summaries",
    "section": "Numeric/categorical",
    "text": "Numeric/categorical\nSide-by-side boxplots are usually a good option. Avoid stacked histograms.\nWere differential changes in arm strength observed according to genotype?\n\n\n\n\n\n\n\n\n\n\n\n\nLook for differences:\n\nlocation shift\nspread\ncenter\n\nWhat do you think? Any notable relationships?"
  },
  {
    "objectID": "content/week3-bivariate.html#numericnumeric",
    "href": "content/week3-bivariate.html#numericnumeric",
    "title": "Bivariate summaries",
    "section": "Numeric/numeric",
    "text": "Numeric/numeric\nDid change in arm strength appear related in any way to body size among study participants?\n\nPairwise scatterplots indicate no apparent relationships."
  },
  {
    "objectID": "content/week3-bivariate.html#interpreting-scatterplots",
    "href": "content/week3-bivariate.html#interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots\n\nScatterplots show the presence or absence of an association.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf there is an association (i.e., discernible pattern), it can be:\n\nlinear or nonlinear\n\nlinear if scatter roughly follows a straight line, nonlinear otherwise\n\npositive or negative\n\npositive if scatter is increasing from left to right, negative otherwise\n\n\nThe plot at left is an example of a positive and (slightly) nonlinear relationship."
  },
  {
    "objectID": "content/week3-bivariate.html#practice-interpreting-scatterplots",
    "href": "content/week3-bivariate.html#practice-interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Practice interpreting scatterplots",
    "text": "Practice interpreting scatterplots"
  },
  {
    "objectID": "content/week3-bivariate.html#correlation",
    "href": "content/week3-bivariate.html#correlation",
    "title": "Bivariate summaries",
    "section": "Correlation",
    "text": "Correlation\nIn addition to graphical techniques, for numeric/numeric comparisons, there are also quantiative measures of relationship.\n\n\n\n\nCorrelation measures the strength of linear relationship, and is defined as: \\[r_{xy} = \\frac{1}{n - 1}\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\\]\n\n\\(r \\rightarrow 1\\): positive relationship\n\\(r \\rightarrow -1\\): negative relationship\n\\(r \\rightarrow 0\\): no relationship"
  },
  {
    "objectID": "content/week3-bivariate.html#interpreting-correlations",
    "href": "content/week3-bivariate.html#interpreting-correlations",
    "title": "Bivariate summaries",
    "section": "Interpreting correlations",
    "text": "Interpreting correlations\nDid change in arm strength appear related in any way to body size among study participants?\nHere are the correlations corresponding to the plots we checked earlier.\n\n\n\n\n\n\n\n\n\n\n\n \nheight\nweight\nbmi\n\n\n\n\ndrm.ch\n-0.1104\n-0.1159\n-0.07267\n\n\nndrm.ch\n-0.265\n-0.2529\n-0.1436\n\n\n\n\n\nSo there aren’t any linear relationships here. A rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\\(|r| = 1\\): either a mistake or not real data"
  },
  {
    "objectID": "content/week3-bivariate.html#data-transformations",
    "href": "content/week3-bivariate.html#data-transformations",
    "title": "Bivariate summaries",
    "section": "Data transformations",
    "text": "Data transformations\nSometimes a simple transformation can reveal a linear relationship on an alternate scale.\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, weight)\n\n[1] 0.5308787\n\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356"
  },
  {
    "objectID": "content/week3-bivariate.html#interpretation",
    "href": "content/week3-bivariate.html#interpretation",
    "title": "Bivariate summaries",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356\n\n\n\nApplying these rules of thumb:\n\n\\(|r| &lt; 0.3\\): minimal association\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong\n\nAnd:\n\n\\(r &gt; 0\\): positive association\n\\(r &lt; 0\\): negative association\n\nThe interpretation is:\nThere is a moderately strong positive linear relationship between height and log weight.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/r-cheatsheet.html",
    "href": "content/r-cheatsheet.html",
    "title": "R Cheatsheet",
    "section": "",
    "text": "Note\n\n\n\nThis document is a work in progress and will be updated prior to each test.\n\n\n\nBasics\nLoading data:\n\n[.RData file] load('&lt;FILEPATH&gt;')\n[from package] data(&lt;DATASET NAME&gt;, package = '&lt;PACKAGE NAME&gt;')\n[reading a CSV file] read.csv('&lt;FILEPATH&gt;')\n\nViewing dataframes:\n\n[preview] head(&lt;NAME&gt;)\n[data viewer] view(&lt;NAME&gt;)\n[check structure] str(&lt;NAME&gt;)\n\nExtracting a variable from a dataframe:\n\nDATAFRAME$VARIABLE\n\n\n\nSummary statistics\nIf x is a vector of values of a numeric variable…\n\nmean(x) computes the average\nmedian(x) computes the median\nmin(x) and max(x) compute the minimum and maximum\nquantile(x, probs = &lt;PERCENTILE&gt;) computes the percentile\nsummary(x) computes the five-number summary, plus the mean\nrange(x) computes the range (min, max)\nIQR(x) computes the interquartile range\nvar(x) computes the variance\nsd(x) computes the standard deviation\n\nIf df is a dataframe with a numeric variable y and a categorical variable x…\n\ndf |&gt; group_by(x) |&gt; summarize(&lt;OUTPUT.NAME&gt; = &lt;FUNCTION&gt;(y)) computes the statistic specified by &lt;FUNCTION&gt; separately for each category of the variable x (requires tidyverse package)\n\nSee especially Lab 2: Descriptive statistics.\n\n\nTables\nIf x and y are a vectors of values of two categorical variables…\n\ntable(x) computes the frequency distribution (counts)\ntable(x) |&gt; proportions() computes the frequency distribution (proportions)\ntable(x, y) computes a contingency table\ntable(x, y) |&gt; proportions(margin = NULL) computes proportions using grand total\ntable(x, y) |&gt; proportions(margin = 1) computes proportions using row total (group by x)\ntable(x, y) |&gt; proportions(margin = 2) computes proportions using column total (group by y)\n\nSee especially Lab 1: R basics and Lab 3: Bivariate summaries.\n\n\nGraphics\nIf x and y are vectors of values of two numeric variables…\n\nhist(x, breaks = &lt;NUMBER OF BINS&gt;) generates a histogram\nboxplot(x) generates a boxplot\nplot(x, y) generates a scatterplot\n\nIf x and y are vectors of values of two categorical variables…\n\ntable(x) |&gt; barplot() generates a bar plot (counts)\ntable(x) |&gt; proportions() |&gt; barplot() generates a bar plot (proportions)\ntable(x, y) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by y\ntable(y, x) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by x\n\nIf x is a vector of values of a categorical variable and y is a vector of values of a numeric variable…\n\nboxplot(y ~ x) generates a boxplot with x on the x axis (vertical orientation)\nboxplot(y ~ x, horizontal = T) generates a boxplot with y on the x axis (horizontal orientation)\n\nSee especially Lab 2: Descriptive statistics and Lab 3: Bivariate summaries.\n\n\nOne- and two-sample inference\n\nDirect calculations for one-sample inference\nIf x is a vector of n values of a numeric variable…\n\nmean(x) + c(-1, 1)*qt(1 - alpha/2, df = n - 1)*sd(x)/sqrt(n) computes a \\((1 - \\alpha)\\times 100\\)% confidence interval\n\nsd(x)/sqrt(n) is the standard error for the estimate\nqt(1 - alpha/2, df = n - 1) is the critical value\nfor a 99% interval, use \\(\\alpha = 0.005\\): qt(0.995, df = n - 1)\nfor a 95% interval, use \\(\\alpha = 0.025\\): qt(0.975, df = n - 1)\nfor a 90% interval, use \\(\\alpha = 0.05\\): qt(0.95, df = n - 1)\n\ntstat &lt;- (mean(x) - mu_0)/(sd(x)/sqrt(n)) computes the T statistic for a hypothesis test of \\(H_0: \\mu = \\mu_0\\) against any of the three alternatives\n\nfor \\(H_A: \\mu \\neq \\mu_0\\): 2*pt(abs(tstat), lower.tail = F) computes a two-sided p-value\nfor \\(H_A: \\mu \\neq \\mu_0\\): qt(1 - alpha/2, df = n - 1) computes the critical value for a level \\(\\alpha\\) two-sided test\nfor \\(H_A: \\mu &gt; \\mu_0\\): pt(tstat, lower.tail = F) computes an upper-sided p-value\nfor \\(H_A: \\mu &gt; \\mu_0\\): qt(1 - alpha, df = n - 1) computes the critical value for a level \\(\\alpha\\) upper-sided test\nfor \\(H_A: \\mu &lt; \\mu_0\\): pt(tstat, lower.tail = T) computes a lower-sided p-value\nfor \\(H_A: \\mu &lt; \\mu_0\\): qt(alpha, df = n - 1) computes the critical value for a level \\(\\alpha\\) lower-sided test\n\nto compute quantiles or frequencies directly using the \\(t_{df}\\) model:\n\npt(q = &lt;QUANTILE&gt;, df = &lt;DEGREES OF FREEDOM&gt;) computes the frequency of values less than q for the \\(t_{df}\\) model\npt(q = &lt;QUANTILE&gt;, df = &lt;DEGREES OF FREEDOM&gt;, lower.tail = F) computes the frequency of values greater than q for the \\(t_{df}\\) model\nqt(p = &lt;PROPORTION&gt;, df = &lt;DEGREES OF FREEDOM&gt;) computes the \\(p\\)th quantile for the \\(t_{df}\\) model\n\n\nSee especially Lab 4: Point estimation, Lab 5: Intervals, and Lab 6: Hypothesis testing basics.\n\n\nUsing the t.test(...) function for one-sample inference\nIf x is a vector of values of a numeric variable then\nt.test(x, mu = mu_0, alternative = &lt;DIRECTION&gt;, conf.level = &lt;COVERAGE&gt;)\nperforms a one-sample t test at significance level 1 - &lt;COVERAGE&gt; where:\n\nmu_0 is the hypothetical value for the mean\ndirection can be 'less', 'greater', or 'two.sided' (in quotes)\ncoverage should be \\(1 - \\alpha\\): the ‘complement’ of the intended significance level \\(\\alpha\\) for the test\n\nOutputs are the test statistic, p-value, confidence interval, and point estimate.\nSee especially Lab 7: Directional tests.\n\n\nUsing the t.test(...) function for two-sample inference\nIf DATA is a dataframe with variables VARIABLE (numeric) and GROUP (categorical with two categories), where GROUP distinguishes two independent samples then\nt.test(VARIABLE ~ GROUP, data = DATA, mu = delta_0, \n       alternative = &lt;DIRECTION&gt;, conf.level = &lt;COVERAGE&gt;)\nperforms a two-sample t test at significance level 1 - &lt;COVERAGE&gt; where:\n\ndelta_0 is the hypothetical difference in means (often 0 but not always)\ndirection can be 'less', 'greater', or 'two.sided' (in quotes)\ncoverage should be \\(1 - \\alpha\\): the ‘complement’ of the intended significance level \\(\\alpha\\) for the test\n\nReturns test statistic, p-value, confidence interval for the difference, and point estimates.\nSee especially Lab 8: Two sample inference."
  },
  {
    "objectID": "content/week4-intervals.html#todays-agenda",
    "href": "content/week4-intervals.html#todays-agenda",
    "title": "Confidence intervals",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] \\(t\\) confidence intervals for the mean\n[lab] computing and interpreting confidence intervals"
  },
  {
    "objectID": "content/week4-intervals.html#from-last-time",
    "href": "content/week4-intervals.html#from-last-time",
    "title": "Confidence intervals",
    "section": "From last time",
    "text": "From last time\n\n\nUnder simple random sampling:\n\nthe sample mean \\(\\bar{x}\\) provides a good point estimate of the population mean \\(\\mu\\)\nits estimated sampling variability is given by the standard error \\(SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}} = \\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\nse\n\n\n\n\n5.043\n1.075\n3179\n0.01906\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean total HDL cholesterol among the U.S. adult population is estimated to be 5.043 mmol/L (SE 0.0191)."
  },
  {
    "objectID": "content/week4-intervals.html#interval-estimation",
    "href": "content/week4-intervals.html#interval-estimation",
    "title": "Confidence intervals",
    "section": "Interval estimation",
    "text": "Interval estimation\nA common interval estimate for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\nA range of plausible values for the mean total cholesterol among U.S. adults is 5.005 to 5.081 mmol/L.\n\nTwo related questions:\n\nWhat do we mean by “plausible”?\nWhere did the number 2 come from?"
  },
  {
    "objectID": "content/week4-intervals.html#the-t-model",
    "href": "content/week4-intervals.html#the-t-model",
    "title": "Confidence intervals",
    "section": "The \\(t\\) model",
    "text": "The \\(t\\) model\n\n\nConsider the statistic:\n\\[\nT = \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\]\nThe sampling distribution of \\(T\\) is well-approximated by a \\(t_{n - 1}\\) model whenever either:\n\nthe population model is symmetric and unimodal\n\nOR\n\nthe sample size is not too small"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation",
    "href": "content/week4-intervals.html#t-model-interpretation",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 50% of samples, \\(T &lt; 0\\)\n\n\n# area less than 0\npt(0, df = 20 - 1) \n\n[1] 0.5\n\n\n\nwritten as \\(P(T &lt; 0) = 0.5\\)"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-1",
    "href": "content/week4-intervals.html#t-model-interpretation-1",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 83.5% of samples, \\(T &lt; 1\\)\n\n\n# area less than 1\npt(1, df = 20 - 1) \n\n[1] 0.8350616\n\n\n\nwritten as \\(P(T &lt; 1) = 0.835\\)"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-2",
    "href": "content/week4-intervals.html#t-model-interpretation-2",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 97% of samples, \\(T &lt; 2\\)\n\n\n# area less than 2\npt(2, df = 20 - 1) \n\n[1] 0.969999\n\n\n\nwritten as \\(P(T &lt; 2) = 0.97\\)"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-3",
    "href": "content/week4-intervals.html#t-model-interpretation-3",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 3% of samples, \\(T &gt; 2\\)\n\n\n# area greater than 2\npt(2, df = 20 - 1, lower.tail = F) \n\n[1] 0.03000102\n\n\n\nnotice: \\[\n\\begin{align*}\nP(T &gt; 2) &= 1 - P(T &lt; 2) \\\\\n(0.03) &= 1 - (0.97)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-4",
    "href": "content/week4-intervals.html#t-model-interpretation-4",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 13.5% of samples, \\(1 &lt; T &lt; 2\\)\n\n\n# area between 1 and 2\npt(2, df = 20 - 1) - pt(1, df = 20 - 1) \n\n[1] 0.1349374\n\n\n\nnotice: \\[\n\\begin{align*}\nP(1 &lt; T &lt; 2) &= P(T &lt; 2) - P(T &lt; 1) \\\\\n(0.135) &= (0.97) - (0.835)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-5",
    "href": "content/week4-intervals.html#t-model-interpretation-5",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 94% of samples, \\(-2 &lt; T &lt; 2\\)\n\n\n# area between 1 and 2\npt(2, df = 20 - 1) - pt(-2, df = 20 - 1) \n\n[1] 0.939998\n\n\n\nwritten \\(P(-2 &lt; T &lt; 2) = 0.94\\)"
  },
  {
    "objectID": "content/week4-intervals.html#a-closer-look-at-interval-construction",
    "href": "content/week4-intervals.html#a-closer-look-at-interval-construction",
    "title": "Confidence intervals",
    "section": "A closer look at interval construction",
    "text": "A closer look at interval construction\nSo where did that 2 come from in the margin of error for our interval estimate?\n\\[\n\\bar{x} \\pm \\color{blue}{2}\\times SE(\\bar{x})\n\\]\nWell:\n\n\n\\[\n\\begin{align*}\n0.94 &= P(-\\color{blue}{2} &lt; T &lt; \\color{blue}{2}) \\\\\n&= P\\left(-\\color{blue}{2} &lt; \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}} &lt; \\color{blue}{2}\\right) \\\\\n&= P(\\underbrace{\\bar{x} - \\color{blue}{2}\\times SE(\\bar{x}) &lt; \\mu &lt; \\bar{x} + \\color{blue}{2}\\times SE(\\bar{x})}_{\\text{interval covers population mean}})\n\\end{align*}\n\\]\n\n\nFor 94% of all random samples, the interval covers the population mean.\n\n\n\nSo the number 2 determines the proportion of samples for which the interval covers the mean, known as its coverage."
  },
  {
    "objectID": "content/week4-intervals.html#effect-of-sample-size",
    "href": "content/week4-intervals.html#effect-of-sample-size",
    "title": "Confidence intervals",
    "section": "Effect of sample size",
    "text": "Effect of sample size\n\nThe sample size determines the exact shape of the \\(t\\) model through its ‘degrees of freedom’ \\(n - 1\\). This changes the areas slightly.\n\nThe exact coverage quickly converges to just over 95% as the sample size increases.\n\n\n\n\n\n\n\n\n\n\n\nn\ncoverage\n\n\n\n\n4\n0.8607\n\n\n8\n0.9144\n\n\n16\n0.9361\n\n\n32\n0.9457\n\n\n64\n0.9502\n\n\n128\n0.9524\n\n\n256\n0.9534"
  },
  {
    "objectID": "content/week4-intervals.html#changing-the-coverage",
    "href": "content/week4-intervals.html#changing-the-coverage",
    "title": "Confidence intervals",
    "section": "Changing the coverage",
    "text": "Changing the coverage\nConsider a slightly more general expression for an interval for the mean:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nThe number \\(c\\) is called a critical value. It determines the coverage.\n\nlarger \\(c\\) \\(\\longrightarrow\\) higher coverage\nsmaller \\(c\\) \\(\\longrightarrow\\) lower coverage\n\nThe so-called “empirical rule” is that:\n\n\\(c = 1 \\longrightarrow\\) approximately 68% coverage\n\\(c = 2 \\longrightarrow\\) approximately 95% coverage\n\\(c = 3 \\longrightarrow\\) approximately 99.7% coverage"
  },
  {
    "objectID": "content/week4-intervals.html#interpreting-critical-values",
    "href": "content/week4-intervals.html#interpreting-critical-values",
    "title": "Confidence intervals",
    "section": "Interpreting critical values",
    "text": "Interpreting critical values\n\n\n\\[\nP(\\color{#FF6459}{-2 &lt; T &lt; 2}) = 1 - 2\\times P(\\color{blue}{T &gt; 2})\n\\]\nLook at how the areas add up so that: \\[\nP(\\color{blue}{T &gt; 2}) = 0.03\n\\] Moreover: \\[\nP(T &lt; 2) = 1 - 0.03 = 0.97\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSo the critical value 2 is actually the 97th percentile of the sampling distribution of \\(T\\).\n\nalso called the 0.97 “quantile”\n(percentiles expressed in proportions are called quantiles)"
  },
  {
    "objectID": "content/week4-intervals.html#exact-coverage-using-t-quantiles",
    "href": "content/week4-intervals.html#exact-coverage-using-t-quantiles",
    "title": "Confidence intervals",
    "section": "Exact coverage using \\(t\\) quantiles",
    "text": "Exact coverage using \\(t\\) quantiles\nTo engineer an interval with a specific coverage, use the \\(p\\)th quantile where:\n\\[p = \\left[1 - \\left(\\frac{1 - \\text{coverage}}{2}\\right)\\right]\\] In R:\n\n# coverage 95% using t quantile\ncoverage &lt;- 0.95\nq.val &lt;- 1 - (1 - coverage)/2\ncrit.val &lt;- qt(q.val, df = 20 - 1)\ncrit.val\n\n[1] 2.093024\n\n\nThe effect of increasing/decreasing coverage on the quantile is:\n\nincrease coverage \\(\\longrightarrow\\) larger quantile \\(\\longrightarrow\\) wider interval\ndecrease coverage \\(\\longrightarrow\\) smaller quantile \\(\\longrightarrow\\) narrower interval"
  },
  {
    "objectID": "content/week4-intervals.html#contrasting-coverage-with-precision",
    "href": "content/week4-intervals.html#contrasting-coverage-with-precision",
    "title": "Confidence intervals",
    "section": "Contrasting coverage with precision",
    "text": "Contrasting coverage with precision\n\nPrecision refers to how wide or narrow the interval is.\n\nPrecision depends on every component of the margin of error:\n\ncritical value used\nsample size\nvariability of values\n\nBy contrast, coverage depends only on the critical value used."
  },
  {
    "objectID": "content/week4-intervals.html#confidence-intervals",
    "href": "content/week4-intervals.html#confidence-intervals",
    "title": "Confidence intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nInterval estimates constructed to achieve a specified coverage are called “confidence intervals”; the coverage is interpreted and reported as a “confidence level”.\n\n\n\n# ingredients\ncholesterol.mean &lt;- mean(cholesterol)\ncholesterol.sd &lt;- sd(cholesterol)\ncholesterol.n &lt;- length(cholesterol)\ncholesterol.se &lt;- cholesterol.sd/sqrt(cholesterol.n)\ncrit.val &lt;- qt(1 - (1 - 0.95)/2, df = cholesterol.n - 1)\n\n# interval\ncholesterol.mean + c(-1, 1)*crit.val*cholesterol.se\n\n[1] 5.005566 5.080310\n\n\n\n\nWith 95% confidence, the mean total cholesterol among U.S. adults is estimated to be between 5.0056 and 5.0803 mmol/L.\n\n\n\nThe general formula for a confidence interval for the population mean is\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nwhere \\(c\\) is a critical value, obtained as a quantile of the \\(t_{n - 1}\\) model and chosen to ensure a specific coverage."
  },
  {
    "objectID": "content/week4-intervals.html#recap",
    "href": "content/week4-intervals.html#recap",
    "title": "Confidence intervals",
    "section": "Recap",
    "text": "Recap\nThe “common” interval estimate for the mean is actually an approximate 95% confidence interval:\n\\[\n\\bar{x} \\pm 2 \\times SE(\\bar{x})\n\\]\n\ncaptures the population mean \\(\\mu\\) for roughly 95% of random samples\nreplacing 2 with a \\(t_{n - 1}\\) quantile allows the analyst to adjust coverage\nthe \\(t_{n - 1}\\) model is an approximation for the sampling distribution of \\(\\frac{\\bar{x} - \\mu}{SE(\\bar{x})}\\)\n\napproximation improves with increasing sample size or symmetry\nusually good quality except in “extreme” situations\n\n\nInterval interpretation:\n\nWith [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units]."
  },
  {
    "objectID": "content/week4-intervals.html#simulation-of-coverage",
    "href": "content/week4-intervals.html#simulation-of-coverage",
    "title": "Confidence intervals",
    "section": "Simulation of coverage",
    "text": "Simulation of coverage\n\n\nArtificially simulating a large number of intervals provides an empirical approximation of coverage.\n\nat right, 200 intervals\n94% cover the population mean (vertical dashed line)\npretty close to nominal coverage level 95%\n\nThis is also a handy way to remember the proper interpretation:\n\nIf I made a lot of intervals from independent samples, 95% of them would ‘get it right’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week6-power.html#todays-agenda",
    "href": "content/week6-power.html#todays-agenda",
    "title": "Power analyses",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] Statistical power; post-hoc and sample size power analyses.\n[review/lab] Test 2 practice problems."
  },
  {
    "objectID": "content/week6-power.html#p-values-and-false-rejections",
    "href": "content/week6-power.html#p-values-and-false-rejections",
    "title": "Power analyses",
    "section": "\\(p\\)-values and false rejections",
    "text": "\\(p\\)-values and false rejections\n\nA \\(p\\)-value captures how often you’d make a mistake if \\(H_0\\) were true.\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\nIf there is no effect of cloud seeding, then we would see \\(T &gt; 1.9982\\) for 2.689% of samples.\n\nThe test rejects at the 5% significance level (\\(p &lt; 0.05\\)), but that doesn’t completely rule out \\(H_0\\).\n\nwhile unlikely, our sample could have been one of the 26 in 1000 where \\(T\\) exceeds 1.9982 despite no effect\nby rejecting here (when \\(T = 1.9982\\)) we are willing to be wrong 2.689% of the time\n\nBy rejecting when \\(p &lt; \\alpha\\) we are willing to be wrong \\(\\alpha\\times 100\\)% of the time."
  },
  {
    "objectID": "content/week6-power.html#a-different-kind-of-error",
    "href": "content/week6-power.html#a-different-kind-of-error",
    "title": "Power analyses",
    "section": "A different kind of error?",
    "text": "A different kind of error?\n\nBut you can also make a mistake when \\(H_0\\) is false!\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'two.sided')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.05377\nalternative hypothesis: true difference in means between group seeded and group unseeded is not equal to 0\n95 percent confidence interval:\n  -4.764295 559.556603\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\nWe’d see \\(|T| &gt; 1.9982\\) for 5.377% of samples if there’s no effect. But what if there is an effect?\n\nThe two-sided test fails to reject at the 5% significance level (\\(p &gt; 0.05\\)), but that doesn’t completely rule out \\(H_A\\).\n\nthe estimated effect – increase of 277.4 acre-feet – could be too small relative to the variability in rainfall\nhard to say how often we’d make this kind of mistake without knowing the real difference\n\nThe rate of fail-to-reject errors depends on the (unknown) true parameter value."
  },
  {
    "objectID": "content/week6-power.html#decision-errors",
    "href": "content/week6-power.html#decision-errors",
    "title": "Power analyses",
    "section": "Decision errors",
    "text": "Decision errors\n\nThere are two ways to make a mistake in a hypothesis test – two “error types”.\n\n\n\n\n\n\n\n\n\n\nReject \\(H_0\\)\nFail to reject \\(H_0\\)\n\n\n\n\nTrue \\(H_0\\)\ntype I error\ncorrect decision\n\n\nFalse \\(H_0\\)\ncorrect decision\ntype II error\n\n\n\n\n\nAny statistical test will have certain error rates:\n\ntype I error rate is denoted \\(\\alpha\\)\ntype II error rate is denoted \\(1 - \\beta\\)\n\n\nThe significance level of a test is its type I error rate.\n\nreject when \\(p &lt; \\alpha\\) \\(\\Longleftrightarrow\\) mistakenly reject \\(\\alpha\\times 100\\)% of the time\n\nBut we don’t know the type II error rate!\n\ndepends on which alternative parameter value is true"
  },
  {
    "objectID": "content/week6-power.html#simulating-type-ii-errors",
    "href": "content/week6-power.html#simulating-type-ii-errors",
    "title": "Power analyses",
    "section": "Simulating type II errors",
    "text": "Simulating type II errors\n\n\nSummary stats for cloud data:\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nmean\nsd\nn\n\n\n\n\nseeded\n442\n650.8\n26\n\n\nunseeded\n164.6\n278.4\n26\n\n\n\n\n\nWe can approximate the type II error rate by:\n\nsimulating datasets with matching statistics\nperforming two-sided tests of no difference\ncomputing the proportion of fail-to-reject decisions\n\n\n\ntype2sim(delta = 277, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf in fact the effect size is exactly 277, a level 5% test with similar data will fail to reject ~70% of the time!"
  },
  {
    "objectID": "content/week6-power.html#larger-effect-size",
    "href": "content/week6-power.html#larger-effect-size",
    "title": "Power analyses",
    "section": "Larger effect size",
    "text": "Larger effect size\n\n\nSummary stats for cloud data:\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nmean\nsd\nn\n\n\n\n\nseeded\n442\n650.8\n26\n\n\nunseeded\n164.6\n278.4\n26\n\n\n\n\n\nWe can approximate the type II error rate by:\n\nsimulating datasets with matching statistics\nperforming two-sided tests of no difference\ncomputing the proportion of fail-to-reject decisions\n\n\n\ntype2sim(delta = 350, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf in fact the effect size is exactly 400, a level 5% test with similar data will fail to reject ~40% of the time."
  },
  {
    "objectID": "content/week6-power.html#smaller-effect-size",
    "href": "content/week6-power.html#smaller-effect-size",
    "title": "Power analyses",
    "section": "Smaller effect size",
    "text": "Smaller effect size\n\n\nSummary stats for cloud data:\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nmean\nsd\nn\n\n\n\n\nseeded\n442\n650.8\n26\n\n\nunseeded\n164.6\n278.4\n26\n\n\n\n\n\nWe can approximate the type II error rate by:\n\nsimulating datasets with matching statistics\nperforming two-sided tests of no difference\ncomputing the proportion of fail-to-reject decisions\n\n\n\ntype2sim(delta = 100, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf in fact the effect size is exactly 100, a level 5% test with similar data will fail to reject ~90% of the time."
  },
  {
    "objectID": "content/week6-power.html#statistical-power",
    "href": "content/week6-power.html#statistical-power",
    "title": "Power analyses",
    "section": "Statistical power",
    "text": "Statistical power\nThe power of a test refers to its true rejection rate across alternatives and is defined as: \\[\\beta = \\underbrace{(1 - \\text{type II error rate})}_\\text{correct decision rate when null is false}\\]\nPower is often interpreted as a detection rate:\n\nhigh type II error \\(\\longrightarrow\\) low power \\(\\longrightarrow\\) low detection rate\nlow type II error \\(\\longrightarrow\\) high power \\(\\longrightarrow\\) high detection rate\n\n\nIn general tests have low power for alternatives close to the null value (where “close” is relative to sampling variability)."
  },
  {
    "objectID": "content/week6-power.html#power-curves",
    "href": "content/week6-power.html#power-curves",
    "title": "Power analyses",
    "section": "Power curves",
    "text": "Power curves\n\nPower is usually construed as a curve depending on the true difference.\n\n\n\nPower curve for the test exactly as performed with the cloud seeding data:\n\n\n\n\n\n\n\n\n\n\nAll other attributes of the test are fixed to approximate the test performed:\n\nsample size \\(n = 26\\)\nsignificance level \\(\\alpha = 0.05\\)\npopulation standard deviation \\(\\sigma = 650\\) (larger of two group estimates)"
  },
  {
    "objectID": "content/week6-power.html#two-common-power-analyses",
    "href": "content/week6-power.html#two-common-power-analyses",
    "title": "Power analyses",
    "section": "Two common power analyses",
    "text": "Two common power analyses\n\n\nPost hoc analysis: how much power does the test I conducted have if the true difference is exactly equal to my estimate?\nHelps to interpret negative results:\n\nlow power \\(\\rightarrow\\) failure to reject was likely\nhigh power \\(\\rightarrow\\) failure to reject was not likely\n\n\n\n\n\n\n\nDon’t over-interpret post-hoc analyses\n\n\nFailure to reject using a well-powered test does not confirm the null hypothesis.\n\n\n\n\nSample size determination: how much data do I need to collect to detect a difference of \\(\\delta\\) using a particular test?\nHelps avoid two potential issues:\n\ntoo little data \\(\\rightarrow\\) study not likely to yield significant results\ntoo much data \\(\\rightarrow\\) study is too likely to yield significant results"
  },
  {
    "objectID": "content/week6-power.html#post-hoc-analysis",
    "href": "content/week6-power.html#post-hoc-analysis",
    "title": "Power analyses",
    "section": "Post-hoc analysis",
    "text": "Post-hoc analysis\n\nCan we estimate the power of a test we already performed?\n\n\n\nFeasible if we assume (a) a population standard deviation and (b) test conditions are met.\nFor the cloud seeding test:\n\npower.t.test(delta = 250, # magnitude of difference\n             sd = 650, # largest population SD\n             n = 26, # smallest sample size\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n\n\n     Two-sample t test power calculation \n\n              n = 26\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.2743235\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nFor a conservative estimate, use:\n\nsmallest of the two sample sizes\nlargest of the two standard deviations\nsmaller difference than observed\n\n\n\\(\\Longrightarrow\\) our test would only reject in favor of a difference of the observed magnitude about 27% of the time\n\nFailure to reject doesn’t strongly rule out the alternative."
  },
  {
    "objectID": "content/week6-power.html#sample-size-calculation",
    "href": "content/week6-power.html#sample-size-calculation",
    "title": "Power analyses",
    "section": "Sample size calculation",
    "text": "Sample size calculation\n\nIf you were (re)designing the study, how much data should you collect to detect a specified effect size?\n\n\n\nTo detect a difference of 250 or more due to cloud seeding with power 0.9:\n\npower.t.test(power = 0.9, # target power level\n             delta = 250, # smallest difference\n             sd = 650, # largest population SD\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n\n\n     Two-sample t test power calculation \n\n              n = 143.0276\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nFor a conservative estimate, use:\n\noverestimate of the larger of the two standard deviations\nminimum difference of interest\n\n\n\\(\\Longrightarrow\\) we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time"
  },
  {
    "objectID": "content/week6-power.html#practical-constraints",
    "href": "content/week6-power.html#practical-constraints",
    "title": "Power analyses",
    "section": "Practical constraints",
    "text": "Practical constraints\n\n\nMinimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:\n\n\n\n\n\n\n\n\n\nAssumes \\(\\sigma = 650\\) for a conservative estimate.\n\nIt may not be affordable to obtain data for 144 days per treatment group (pilots and planes are expensive). What is achievable within constraints?\n\npower of 0.8 will require n = 59 per group\n\n138 days total\n\ndecreasing to 0.7 will require n = 45 per group\n\n90 days total\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab9-anova.html",
    "href": "content/lab9-anova.html",
    "title": "Lab 9: Analysis of variance",
    "section": "",
    "text": "The goal of this lab is to learn how to implement analysis of variance (ANOVA) in R. In its most basic form, this comprises three steps: a visual check of the data, fitting a model and generating the ANOVA table, and interpreting results.\nRemember throughout that the first goal of an analysis of variance is to test the hypothesis of no difference in means (interpreted as no effect in the case of an experiment with random treatment allocation). In notation:\n\\[\n\\begin{cases}\nH_0: &\\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_A: &\\mu_i \\neq \\mu_j \\text{ for some } i \\neq j\n\\end{cases}\n\\]\nThis is essentially an extension of the two-sample \\(t\\) test to arbitrarily many means. We’ll use the chicks dataset to illustrate these steps and you’ll reproduce using the anorexia dataset.\nlibrary(tidyverse)\nload('data/chicks-20d.RData')\nanorexia &lt;- read_csv('data/anorexia.csv')"
  },
  {
    "objectID": "content/lab9-anova.html#footnotes",
    "href": "content/lab9-anova.html#footnotes",
    "title": "Lab 9: Analysis of variance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWeindruch, R., Walford, R.L., Fligiel, S. and Guthrie D. (1986). The Retardation of Aging in Mice by Dietary Restriction: Longevity, Cancer, Immunity and Lifetime Energy Intake, Journal of Nutrition 116(4):641–54.↩︎"
  },
  {
    "objectID": "content/lab2-descriptive.html",
    "href": "content/lab2-descriptive.html",
    "title": "Lab 2: Descriptive statistics and simple graphics",
    "section": "",
    "text": "The objectives of this lab are to learn to:\n\nmake basic statistical graphics for visualizing frequency distributions\ncompute common measures of location and spread\ndiscern appropriate measures of location and spread based on presence of outliers and skewness\n\nWe’ll use the FAMuSS dataset, as in lecture.\n\nlibrary(tidyverse)\n\n# load famuss dataset \nload('data/famuss.RData')\n\n# inspect data frame\nhead(famuss)\n\n# A tibble: 6 × 9\n  ndrm.ch drm.ch sex      age race      height weight genotype   bmi\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;\n1      40     40 Female    27 Caucasian   65      199 CC        33.1\n2      25      0 Male      36 Caucasian   71.7    189 CT        25.8\n3      40      0 Female    24 Caucasian   65      134 CT        22.3\n4     125      0 Female    40 Caucasian   68      171 CT        26.0\n5      40     20 Female    32 Caucasian   61      118 CC        22.3\n6      75      0 Female    24 Hispanic    62.2    120 CT        21.8\n\n\nAs a quick refresher, you can extract a vector of the observations for any particular variable from the dataframe as follows: famuss$[variable name].\nWhile not strictly necessary, I recommend retrieving and storing the variable(s) you’ll use as separate objects, at least while you’re still a beginner. For example:\n\n# extract the age variable\nfamuss$age\n\n# store the age column as a vector\nage &lt;- famuss$age\n\n\nBasic statistical graphics\n\nCategorical variables\nFor categorical variables, as you saw in the last lab, table(...) will tabulate counts of the number of occurrences of each unique values of a categorical variable. The result can be passed to barplot(...) for a simple barplot to visualize the frequency distribution:\n\n# retrieve genotype\ngenotype &lt;- famuss$genotype\n\n# make a table, generate a barplot\ntable(genotype) |&gt; barplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nMake a barplot to visualize the frequency distribution of racial groups in the FAMuSS study.\n\n# retrieve race\n\n# make a table, generate a barplot\n\n\n\n\n\nNumerical variables\nIf a numeric variable is discrete without too many values, the frequency distribution could be visualized without any binning as a barplot. However, this is not recommended because it will result in a plot that is not scaled properly.\nInstead, it is better to make a histogram with one bin per unique possible value; this will scale the axis properly. However, it is also acceptable to make a histogram with binning that will result in aggregating some values. Both are shown below.\nThe approximate number of bins, and thus the amount of aggregation, is controlled by the argument breaks = ...:\n\n# retrieve age\nage &lt;- famuss$age\n\n# effectively, a bar plot of ages\nhist(age, breaks = 25)\n\n\n\n\n\n\n\n# fewer bins\nhist(age, breaks = 10)\n\n\n\n\n\n\n\n\nFor continuous variables, binning is a necessity. The second plot is better, because it shows the shape more clearly without obscuring too much detail.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a histogram of percent change in dominant arm strength. Experiment to see how the shape of the distribution appears at various binning resolutions; then pick a number of breaks that you feel reflects the data best.\n\n# retrieve dominant arm percent change\n\n# make a histogram; find a binning that captures the shape well\n\n\n\nTo store a graphic as a separate file for use in other documents, find the ‘export’ icon in the plot panel and select the ‘Save as image’ option; then follow prompts. Try this for the plot you just made.\n\n\n\nDescriptive statistics\nIn class we discussed several descriptive statistics for numeric variables. These statistics are so commonly used that they have their own functions in R.\n\nMeasures of location\nThe following functions return common measures of location for numeric variables:\n\nmean(...) returns an average\nmedian(...) returns a median\nquantile(...) returns a quantile\nmin(...) and max(...) return a minimum and a maximum, respectively\n\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age (middle value)\nmedian(age)\n\n[1] 22\n\n# 25th percentile of age (\"quantile\" is another term for percentile)\nquantile(age, probs = 0.25)\n\n25% \n 20 \n\n# 25th *and* 75th percentile of age\nquantile(age, probs = c(0.25, 0.75))\n\n25% 75% \n 20  27 \n\n# minimum age\nmin(age)\n\n[1] 17\n\n# maximum age\nmax(age)\n\n[1] 40\n\n\nNotice how the probs = ... argument to the quantile() function, which specifies which percentile R will calculate, can be used to calculate multiple percentiles at once.\nIf you want to inspect all of the location measures above (the five-number summary plus the mean) the summary(...) function will do just that.\n\n# all common location measures\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   17.0    20.0    22.0    24.4    27.0    40.0 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry computing the location measures above for percent change in dominant arm strength. Compare the mean and median. What does the comparison tell you about the skewness of this variable? Is this consistent with the histogram from the previous ‘your turn’?\n\n# compute the five-number summary for change in dominant arm strength\n\n\n\n\n\nMeasures of spread\nThe following functions return common measures of spread for numeric variables:\n\nrange(...) returns the range (min, max)\nIQR(...) returns the interquartile range (middle 50% of data)\nvar(...) returns the variance (average squared deviations from mean)\nsd(...) returns the standard deviation (variance, on original scale)\n\n\n# age range\nrange(age)\n\n[1] 17 40\n\n# interquartile range of ages (width of interval containing middle 50% of data)\nIQR(age)\n\n[1] 7\n\n# variance of age\nvar(age)\n\n[1] 33.79966\n\n# standard deviation of age\nsd(age)\n\n[1] 5.813748\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCompute the standard deviation of percent change in dominant arm strength.\n\n# standard deviation of percent change in dominant arm strength\n\nInterpret the value in context.\n\n\n\n\nRobustness\nWhen would you use median instead of mean? IQR instead of standard deviation? The answer has to do with robustness, which in statistics means sensitivity to outliers or extreme values.\nTo explore this idea, recall first the actual mean and median ages for the participants in the FAMuSS study as well as the age range:\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age\nmedian(age)\n\n[1] 22\n\n# range\nrange(age)\n\n[1] 17 40\n\n\nNow let’s add an artificial outlier – a few hypothetical participant who are in their 80’s and 90’s – and compute the measures of location again:\n\n# average age\nmean(c(age, 96, 92, 87, 91))\n\n[1] 24.84975\n\n# median age\nmedian(c(age, 96, 92, 87, 91))\n\n[1] 22\n\n\nThe mean increases while the median does not. More broadly, statistics based on percentiles are in general insensitive to outliers, unless there’s a large group of outlying observations. In this sense they are robust statistics.\nA similar difference can be observed between deviation-based measures and interquartile range. The original measures were:\n\n# variance of ages\nvar(age)\n\n[1] 33.79966\n\n# interquartile range of ages\nIQR(age)\n\n[1] 7\n\n\nNow adding in our artificial outliers:\n\n# age variance\nvar(c(age, 96, 92, 87, 91))\n\n[1] 63.55598\n\n# age iqr\nIQR(c(age, 96, 92, 87, 91))\n\n[1] 7\n\n\n\n\n\nGrouped summaries\nWhat if you wish to find the mean percent change in dominant arm strength separately for each genotype?\nThe tidyverse package loaded at the outset has a pair of functions, group_by and summarize, that allow you to do this efficiently. The steps are:\n\nStart with the data frame famuss\ngroup by the genotype variable\nsummarize\n\n\n# average dominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch))\n\n# A tibble: 3 × 2\n  genotype avg.drm.ch\n  &lt;fct&gt;         &lt;dbl&gt;\n1 CC            10.7 \n2 CT             8.49\n3 TT            13.0 \n\n\nThe summarize function can actually compute multiple summaries: each argument should specify a name for the summary and the calculation to perform in the format &lt;NAME&gt; = &lt;FUNCTION&gt;(&lt;COLUMN NAME&gt;):\n\n# average dominant and nondominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch),\n            avg.ndrm.ch = mean(ndrm.ch))\n\n# A tibble: 3 × 3\n  genotype avg.drm.ch avg.ndrm.ch\n  &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 CC            10.7         48.9\n2 CT             8.49        53.2\n3 TT            13.0         58.1\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate the median percent change in dominant arm strength separately for each genotype by modifying the first example above.\n\n# median percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\nThese problems may look lengthy at face value, but the calculations are rather brief. A suggestion is: determine which calculations are required for each part and focus on doing those calculations first; then look back over your results to interpret them and answer the prompts.\n\n[L3] Use the census data again from the previous problem set and carry out the following descriptive summaries.\n\nMake a histogram of total personal income. Choose the binning so as to capture the shape well but not obscure too much detail. Are there outliers?\nCompute the mean and five-number summary of total personal income. Which measure of location is most appropriate and why?\nCompute the interquartile range and standard deviation of total personal income and interpret them in context. Which measure is more appropriate and why?\nCompute the median total personal income separately for men and women.\n\n\n\n[L3] Data from Chen, W., et al., Maternal investment increases with altitude in a frog on the Tibetan Plateau. Journal of Evolutionary Biology 26-12 (2013) includes measurements pertaining to egg clutches of several populations of frog at breeding ponds (sites) in the eastern Tibetan Plateau.\n\nHow many samples were collected at each site?\nCompute the frequency distribution of site altitudes among samples collected in the study.\nMake a barplot of the frequency distribution from (a). Are samples collected more or less uniformily across altitudes? If not, which altitudes are most represented in the sample?\nMake a histogram of clutch volumes. Describe the shape and number of modes.\nCalculate the mean and five-number summary of clutch volume.\nCalculate and interpret the standard deviation, variance, and interquartile range.\nCalculate the average clutch volume separately for each altitude. Does average clutch volume seem to differ by altitude?\n[optional] Devise a way to calculate the average absolute deviation."
  },
  {
    "objectID": "content/lab11-proportions.html",
    "href": "content/lab11-proportions.html",
    "title": "Lab 11: Inference for binomial proportions",
    "section": "",
    "text": "The goal of this lab is to learn how to implement:\n\ntests and intervals for binomial proportions from one sample\ntests and intervals comparing two proportions from independent samples\n\nThe activity represents our first foray into categorical data analysis. You’ll reproduce examples from lecture using the NHANES data to estimate diabetets prevalence and the Vitamin C experiment; you’ll practice on a few additional datasets.\n\nlibrary(tidyverse)\nload('data/vitamin.RData')\nload('data/nhanes500.RData')\nload('data/obesity.RData')\n\n\nInference for one proportion\n\nRefresher: categorical frequency distributions\nInference for proportions – and for that matter, future material on inference for categorical data – will leverage frequency distributions to perform calculations.\nYou learned how to make these descriptive summaries at the beginning of the quarter, but a refresher may be helpful. Recall that a frequency distribution, for a categorical variable, is simply a set of counts of observations of each unique value of the variable. We made these using table(...).\n\n# extract variable of interest\ndia &lt;- nhanes$diabetes\n\n# construct table of counts (frequency distribution)\ntable(dia)\n\ndia\nYes  No \n 57 443 \n\n# render as proportions (still frequency distribution, but normalized)\ntable(dia) |&gt; prop.table()\n\ndia\n  Yes    No \n0.114 0.886 \n\n# barplot\ntable(dia) |&gt; prop.table() |&gt; barplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn 1\n\n\n\nCompute the frequency distribution, in both counts and proportions, for the sleeptrouble variable in the NHANES dataset, which records whether the participant experiences sleep trouble. Also construct a barplot to visualize the frequency distribution\n\n\n\n\nPoint estimation\nThe point estimate for a population proportion is simply the corresponding sample proportion:\n\\[\n\\hat{p} = \\frac{\\#\\text{ observations of category of interest}}{\\text{sample size }n}\n\\] This can be ascertained directly from the frequency distribution. In the diabetes example, \\(\\hat{p}=57\\).\nThe standard error for \\(\\hat{p}\\) is: \\[\nSE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\] This measures the sample-to-sample variability of \\(\\hat{p}\\). To compute the standard error for a binomial proportion, take the product of the two proportions and divide by the sample size, as shown below.\n\n# point estimates *are* sample proportions in the frequency distribution table\ndia.p &lt;- table(dia) |&gt; prop.table()\n\n# point estimate (sample proportion of interest)\ndia.p.hat &lt;- dia.p[1]\ndia.p.hat\n\n  Yes \n0.114 \n\n# standard error\ndia.n &lt;- length(dia)\ndia.p.hat.se &lt;- sqrt(prod(dia.p)/dia.n)\ndia.p.hat.se\n\n[1] 0.01421295\n\n\nThis is interpreted as follows:\n\nThe proportion of U.S. adults with diagnosed diabetes is estimated to be 0.114 (SE = 0.0142).\n\n\n\n\n\n\n\nYour turn 2\n\n\n\nUsing the NHANES data, compute a point estimate and standard error for the proportion of U.S. adults who have sleep trouble. Interpret the estimate in context as shown above.\n\n\n\n\nConfidence intervals (manually)\nThe confidence interval is straightforward to compute by hand: \\[\n\\hat{p} \\pm c\\times SE(\\hat{p})\n\\] Once estimates are in hand, the calculation requires only finding an appropriate critical value \\(c\\). For a \\(1 - \\alpha\\) confidence interval, this is the \\(1 - \\frac{\\alpha}{2}\\) quantile of the normal model, i.e., the value satisfying:\n\\[\nP(Z \\leq c) = 1 - \\frac{\\alpha}{2}\n\\]\nHere are several examples:\n\n# 90% interval\ncval &lt;- qnorm(1 - 0.10/2)\ndia.p.hat + c(-1, 1)*cval*dia.p.hat.se\n\n[1] 0.09062177 0.13737823\n\n# 95% interval\ncval &lt;- qnorm(1 - 0.05/2)\ndia.p.hat + c(-1, 1)*cval*dia.p.hat.se\n\n[1] 0.08614313 0.14185687\n\n# 99% interval\ncval &lt;- qnorm(1 - 0.01/2)\ndia.p.hat + c(-1, 1)*cval*dia.p.hat.se\n\n[1] 0.07738986 0.15061014\n\n# 99.9% interval\ncval &lt;- qnorm(1 - 0.001/2)\ndia.p.hat + c(-1, 1)*cval*dia.p.hat.se\n\n[1] 0.0672319 0.1607681\n\n\nTo interpret the last interval:\n\nWith 99.9% confidence, the proportion of U.S. adults with diagnosed diabetes is estimated to be between 0.067 and 0.161, with a point estimate of 0.114 (SE 0.0142).\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nUsing the NHANES data, construct and interpret a 98% confidence interval for the proportion of U.S. adults who have sleep trouble.\n\n\n\n\nHypothesis tests\nTo test the hypothesis that a population proportion is some null value \\(p_0\\) against a non-directional (two-sided) alternative, i.e.,\n\\[\n\\begin{cases}\nH_0: &p = p_0 \\\\\nH_A: &p \\neq p_0\n\\end{cases}\n\\]\nwe use a test statistic that is almost the estimation error divided by the standard error, but we substitute the hypothetical value for the estimate in the standard error:\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\n\\]\nThe \\(p\\)-value is simply the proportion of the normal model for the sampling distribution of \\(Z\\) that exceeds the observed statistic in either direction: \\(P(|Z| &gt; Z_obs)\\). This can be computed by hand as shown below.\n\n# test stat\ndia.z &lt;- (dia.p.hat - 0.1)/sqrt(0.1*0.9/dia.n)\n\n# two sided p value\ndia.pval &lt;- 2*pnorm(abs(dia.z), lower.tail = F)\ndia.pval\n\n      Yes \n0.2967175 \n\n\nThe result is interpreted as follows in standard narrative style:\n\nThe data provide no evidence that the proportion of U.S. adults with diagnosed diabetes differs from 0.1 (Z = 1.04, p = 0.2967).\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nTest the hypothesis that the proportion of U.S. adults who have sleep trouble is 0.3 against a two-sided alternative at the 5% significance level. Interpret the result in context.\n\n# test stat\n\n# two sided p value\n\n\n\nA more straightforward way to perform the inference – both the test and interval – utilizes prop.test(...). The command below matches our prior results:\n\n# pass table (counts) to prop.test\ntable(dia) |&gt; \n  prop.test(p = 0.1, alternative = 'two.sided', \n            conf.level = 0.95, correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 1.0889, df = 1, p-value = 0.2967\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.0890369 0.1448491\nsample estimates:\n    p \n0.114 \n\n\nThe resulting test and interval would be reported jointly:\n\nThe data provide no evidence that the proportion of U.S. adults with diagnosed diabetes differs from 0.1 (Z = 1.04, p = 0.2967). &gt; With 95% confidence, the proportion of U.S. adults with diagnosed diabetes is estimated to be between 0.089 and 0.145, with a point estimate of 0.114 (SE 0.0142).\n\nThe prop.test(...) function also makes doing directional tests easier. Below are two examples:\n\n# upper sided test/interval: does p exceed 0.09?\ntable(dia) |&gt; \n  prop.test(p = 0.09, alternative = 'greater', \n            conf.level = 0.95, correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.09\nX-squared = 3.5165, df = 1, p-value = 0.03038\nalternative hypothesis: true p is greater than 0.09\n95 percent confidence interval:\n 0.09266984 1.00000000\nsample estimates:\n    p \n0.114 \n\n# upper sided test/interval: is p under 0.14?\ntable(dia) |&gt; \n  prop.test(p = 0.14, alternative = 'less', \n            conf.level = 0.95, correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.14\nX-squared = 2.8073, df = 1, p-value = 0.04692\nalternative hypothesis: true p is less than 0.14\n95 percent confidence interval:\n 0.000000 0.139485\nsample estimates:\n    p \n0.114 \n\n\n\n\n\n\n\n\nYour turn 5\n\n\n\nTest whether the proportion of U.S. adults who experience sleep trouble is under 0.3 at the 1% significance level. Provide an upper confidence bound along with your test.\n\n# test whether the proportion with sleep trouble is under 0.3\n\n\n\nThe default approach is to apply the continuity correction (set correct = T or omit this argument). We omitted it so that results would match the manual calculations above. You should apply the correction in practice.\n\n\n\nInference for two proportions\nInference comparing two proportions proceeds from a two-way table or “contingency” table. You may recall that this is a bivariate frequency distribution of two categorical variables. Take a moment to refresh your memory on how to construct these tables:\n\n# variables of interest\ntrt &lt;- vitamin$vitamin\nout &lt;- vitamin$autism\n\n# construct contingency table\nvitamin.tbl &lt;- table(trt, out)\nvitamin.tbl\n\n            out\ntrt          autism no autism\n  no vitamin    111        70\n  vitamin       143       159\n\n\nFor inference to work appropriately with prop.test(...), it is important that the outcome be shown in the column dimension and the groups be shown in the row dimension. Further, the outcome of interest should be the first column (not the second).\n\n\n\n\n\n\nYour turn 6\n\n\n\nConstruct a contingency table for the obesity data.\n\n# variables of interest\n\n# contingency table\n\n\n\nThe contingency table can be used directly to perform inference on a difference in proportions:\n\n# test for difference in proportions\nprop.test(vitamin.tbl, \n          alternative = 'two.sided', \n          conf.level = 0.95)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  vitamin.tbl\nX-squared = 8.3131, df = 1, p-value = 0.003936\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.04475176 0.23474771\nsample estimates:\n   prop 1    prop 2 \n0.6132597 0.4735099 \n\n\n\n\n\n\n\n\nYour turn 7\n\n\n\nTest whether the rate of CHD deaths differs among obese and non-obese populations."
  },
  {
    "objectID": "content/week7-anova.html#todays-agenda",
    "href": "content/week7-anova.html#todays-agenda",
    "title": "Analysis of Variance",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] inference comparing several population means\n[lab] fitting ANOVA models in R"
  },
  {
    "objectID": "content/week7-anova.html#more-than-two-means",
    "href": "content/week7-anova.html#more-than-two-means",
    "title": "Analysis of Variance",
    "section": "More than two means?",
    "text": "More than two means?\n\n\nYou previously considered this data on chick weights at 20 days of age by diet:\n\n\n\n\n\n\n\n\n\n\nHere we have four means to compare rather than just two.\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nse\nsd\nn\n\n\n\n\n1\n170.4\n13.45\n55.44\n17\n\n\n2\n205.6\n22.22\n70.25\n10\n\n\n3\n258.9\n20.63\n65.24\n10\n\n\n4\n233.9\n12.52\n37.57\n9\n\n\n\n\n\n\n\n\nDoes mean weight at 20 days differ by diet? How do you test this?"
  },
  {
    "objectID": "content/week7-anova.html#hypotheses-for-a-difference-in-means",
    "href": "content/week7-anova.html#hypotheses-for-a-difference-in-means",
    "title": "Analysis of Variance",
    "section": "Hypotheses for a difference in means",
    "text": "Hypotheses for a difference in means\nLet \\(\\mu_i = \\text{mean weight on diet } i = 1, 2, 3, 4\\).\nThe hypothesis that there are no differences in means by diet is:\n\\[\nH_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\quad (\\text{no difference in means})\n\\]\nThe alternative, if this is false, is that there is at least one difference:\n\\[\nH_A: \\mu_i \\neq \\mu_j \\quad (\\text{at least one difference})\n\\]"
  },
  {
    "objectID": "content/week7-anova.html#how-much-difference-is-enough",
    "href": "content/week7-anova.html#how-much-difference-is-enough",
    "title": "Analysis of Variance",
    "section": "How much difference is enough?",
    "text": "How much difference is enough?\nHere are two made-up examples of four sample means.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy does it look like there’s a difference at right but not at left?\n\n\n\nThink about the \\(t\\)-test: we say there’s a difference if \\(T = \\frac{\\text{estimate} - \\text{hypothesis}}{\\text{variability}}\\) is large.\nSame idea here: we see differences if they are big relative to the variability in estimates."
  },
  {
    "objectID": "content/week7-anova.html#partitioning-variation",
    "href": "content/week7-anova.html#partitioning-variation",
    "title": "Analysis of Variance",
    "section": "Partitioning variation",
    "text": "Partitioning variation\n\nPartitioning variation into two or more components is called “analysis of variance”\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the chick data, two sources of variability:\n\ngroup variability between diets\nerror variability among chicks\n\nThe analysis of variance (ANOVA) model:\n\\[\\color{grey}{\\text{total variation}} = \\color{red}{\\text{group variation}} + \\color{blue}{\\text{error variation}}\\]\n\n\nWe’ll base the test on the ratio \\(F = \\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\)."
  },
  {
    "objectID": "content/week7-anova.html#the-f-statistic-a-variance-ratio",
    "href": "content/week7-anova.html#the-f-statistic-a-variance-ratio",
    "title": "Analysis of Variance",
    "section": "The \\(F\\) statistic: a variance ratio",
    "text": "The \\(F\\) statistic: a variance ratio\n\nThe \\(F\\) statistic measures variability attributable to group differences relative to variability attributable to individual differences.\n\n\n\nNotation:\n\n\\(\\bar{x}\\): “grand” mean of all observations\n\\(\\bar{x}_i\\): mean of observations in group \\(i\\)\n\\(s_i\\): SD of observations in group \\(i\\)\n\\(k\\) groups\n\\(n\\) total observations\n\\(n_i\\) observations per group\n\n\nMeasures of variability:\n\\[\\color{red}{MSG} = \\frac{1}{k - 1}\\sum_i n_i(\\bar{x}_i - \\bar{x})^2 \\quad(\\color{red}{\\text{group}})\\] \\[\\color{blue}{MSE} = \\frac{1}{n - k}\\sum_i (n_i - 1)s_i^2 \\quad(\\color{blue}{\\text{error}})\\] Ratio:\n\\[F = \\frac{\\color{red}{MSG}}{\\color{blue}{MSE}} \\quad\\left(\\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\right)\\]"
  },
  {
    "objectID": "content/week7-anova.html#sampling-distribution-for-f",
    "href": "content/week7-anova.html#sampling-distribution-for-f",
    "title": "Analysis of Variance",
    "section": "Sampling distribution for \\(F\\)",
    "text": "Sampling distribution for \\(F\\)\n\n\nIf the data satisfy these conditions:\n\nthe distribution of values is symmetric and unimodal within each group\nthe variability (standard deviation) is roughly the same across groups\n\nThen the \\(F\\) statistic has a sampling distribution well-approximated by an \\(F_{k - 1, n - k}\\) model.\n\nnumerator degrees of freedom \\(k - 1\\)\ndenominator degrees of freedom \\(n - k\\)\n\n\n\n\n\n\n\n\\(F\\) models for several different numerator degrees of freedom \\(k - 1\\) with fixed \\(n = 30\\)."
  },
  {
    "objectID": "content/week7-anova.html#p-values-for-the-f-test",
    "href": "content/week7-anova.html#p-values-for-the-f-test",
    "title": "Analysis of Variance",
    "section": "\\(p\\)-values for the \\(F\\) test",
    "text": "\\(p\\)-values for the \\(F\\) test\n\n\nTo test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\\\\nH_0: &\\mu_i \\neq \\mu_j \\quad\\text{for some}\\quad i \\neq j\n\\end{cases}\n\\] Calculate the \\(F\\) statistic:\n\n# ingredients of mean squares\nk &lt;- nrow(chicks.summary)\nn &lt;- nrow(chicks)\nn.i &lt;- chicks.summary$n\nxbar.i &lt;- chicks.summary$mean\ns.i &lt;- chicks.summary$sd\nxbar &lt;- mean(chicks$weight)\n\n# mean squares\nmsg &lt;- sum(n.i*(xbar.i - xbar)^2)/(k - 1)\nmse &lt;- sum((n.i - 1)*s.i^2)/(n - k)\n\n# f statistic\nfstat &lt;- msg/mse\nfstat\n\n[1] 5.463598\n\n\nAnd reject \\(H_0\\) when \\(F\\) is large.\n\nFor a significance level \\(\\alpha\\) test, reject \\(H_0\\) when \\(\\underbrace{P(F &gt; F_\\text{obs})}_\\text{p-value} &lt; \\alpha\\).\n\n\n\n\n\n\n\n\n\n\npf(fstat, 4 - 1, 46 - 4, lower.tail = F)\n\n[1] 0.002909054"
  },
  {
    "objectID": "content/week7-anova.html#interpreting-f-statistics-and-p-values",
    "href": "content/week7-anova.html#interpreting-f-statistics-and-p-values",
    "title": "Analysis of Variance",
    "section": "Interpreting \\(F\\) statistics and \\(p\\)-values",
    "text": "Interpreting \\(F\\) statistics and \\(p\\)-values\n\n\n\\[\n\\begin{cases}\nH_0: &\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\\\\nH_0: &\\mu_i \\neq \\mu_j \\quad\\text{for some}\\quad i \\neq j\n\\end{cases}\n\\]\n\\(F = \\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}} = \\frac{MSG}{MSE} = 5.4636\\).\n\n\n\n\n\n\n\n\n\n\npf(fstat, 4 - 1, 46 - 4, lower.tail = F)\n\n[1] 0.002909054\n\n\n\n\nF = 5.4636 means the proportion of variation in weight attributable to diets is 5.46 times greater than the proportion of variation attributable to chicks.\n\nThe statistical significance of this result is measured by the \\(p\\)-value:\n\nif there is in fact no difference in means, then only 0.29% of samples (i.e., 2 in 1000) would produce at least as much diet-to-diet variability as we observed.\nso in this case we reject \\(H_0\\) at the 1% level"
  },
  {
    "objectID": "content/week7-anova.html#anova-in-r",
    "href": "content/week7-anova.html#anova-in-r",
    "title": "Analysis of Variance",
    "section": "ANOVA in R",
    "text": "ANOVA in R\nThe aov(...) function fits ANOVA models using a formula/dataframe specification:\n\n# fit anova model\nfit &lt;- aov(weight ~ diet, data = chicks)\n\n# generate table\nsummary(fit)\n\n\n\n\nAnalysis of Variance Model\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ndiet\n3\n55881\n18627\n5.464\n0.002909\n\n\nResiduals\n42\n143190\n3409\nNA\nNA\n\n\n\n\n\nThe typical style for interpretation closely follows that of previous inferences for the mean:\n\nThe data provide strong evidence of an effect of diet on mean weight (F = 5.464 on 3 and 42 df, p = 0.0029)."
  },
  {
    "objectID": "content/week7-anova.html#analysis-of-variance-table",
    "href": "content/week7-anova.html#analysis-of-variance-table",
    "title": "Analysis of Variance",
    "section": "Analysis of variance table",
    "text": "Analysis of variance table\nThe results of an analysis of variance are traditionally displayed in a table.\n\n\n\n\n\n\n\n\n\n\n\nSource\ndegrees of freedom\nSum of squares\nMean square\nF statistic\np-value\n\n\n\n\nGroup\n\\(k - 1\\)\nSSG\n\\(MSG = \\frac{SSG}{k - 1}\\)\n\\(\\frac{MSG}{MSE}\\)\n\\(P(F &gt; F_\\text{obs})\\)\n\n\nError\n\\(n - k\\)\nSSE\n\\(MSE = \\frac{SSE}{n - k}\\)\n\n\n\n\n\n\nthe sum of square terms are ‘raw’ measures of variability\nthe mean square terms are averages adjusted for the amount of data available to estimate variability due to each source\n\nFormally, the ANOVA model says \\((n - 1)s^2 = SSG + SSE\\)."
  },
  {
    "objectID": "content/week7-anova.html#checking-assumptions",
    "href": "content/week7-anova.html#checking-assumptions",
    "title": "Analysis of Variance",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\n\nThe ANOVA test assumes:\n\nthe distribution of values is symmetric and unimodal within each group\nthe variability (standard deviation) is roughly the same across groups\n\nTo check these assumptions:\n\ncompare group standard deviations for similarity\nvisually inspect distributions within each group for approximate symmetry\n\nSimilar to the \\(t\\) test, greater departures from these assumptions are allowable for larger sample sizes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nse\nsd\nn\n\n\n\n\n1\n170.4\n13.45\n55.44\n17\n\n\n2\n205.6\n22.22\n70.25\n10\n\n\n3\n258.9\n20.63\n65.24\n10\n\n\n4\n233.9\n12.52\n37.57\n9"
  },
  {
    "objectID": "content/week7-anova.html#another-example-treating-anorexia",
    "href": "content/week7-anova.html#another-example-treating-anorexia",
    "title": "Analysis of Variance",
    "section": "Another example: treating anorexia",
    "text": "Another example: treating anorexia\n\n\nWeight change was measured for 72 young female anorexia patients randomly allocated to three treatment groups:\n\ncognitive behavioral therapy (CBT)\nfamily treatment (FT)\na control (Cont)\n\nGrouped summary statistics:\n\n\n\n\n\n\n\n\n\n\n\ntreat\npost - pre\nsd\nn\n\n\n\n\nCBT\n3.007\n7.309\n29\n\n\nCont\n-0.45\n7.989\n26\n\n\nFT\n7.265\n7.157\n17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWere any of the treatments more effective than others?"
  },
  {
    "objectID": "content/week7-anova.html#another-example-treating-anorexia-1",
    "href": "content/week7-anova.html#another-example-treating-anorexia-1",
    "title": "Analysis of Variance",
    "section": "Another example: treating anorexia",
    "text": "Another example: treating anorexia\n\n# fit anova model\nfit &lt;- aov(change ~ treat, data = anorexia)\n\n# generate table\nsummary(fit)\n\n\n\n\nAnalysis of Variance Model\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ntreat\n2\n614.6\n307.3\n5.422\n0.006499\n\n\nResiduals\n69\n3911\n56.68\nNA\nNA\n\n\n\n\n\n\nThe data provide strong evidence of an effect of therapeutic treatment on mean weight change among young women with anorexia (F = 5.422 on 2 and 69 degrees of freedom, p = 0.0065).\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-datatypes.html#todays-agenda",
    "href": "content/week2-datatypes.html#todays-agenda",
    "title": "Data semantics and data types",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\n[lecture] data semantics and data types\n[lab] R basics"
  },
  {
    "objectID": "content/week2-datatypes.html#data-semantics",
    "href": "content/week2-datatypes.html#data-semantics",
    "title": "Data semantics and data types",
    "section": "Data semantics",
    "text": "Data semantics\n\nData are a set of measurements.\nA variable is any measured attribute of study units.\nAn observation is a measurement of one or more variables taken on one particular study unit.\n\nIt is usually expedient to arrange data values in a table in which each row is an observation and each column is a variable:"
  },
  {
    "objectID": "content/week2-datatypes.html#leap-example",
    "href": "content/week2-datatypes.html#leap-example",
    "title": "Data semantics and data types",
    "section": "LEAP example",
    "text": "LEAP example\nA table showing the observations and variables for the LEAP study would look like this:\n\n\n\n\n\n\n\n\n\n\nparticipant.ID\ntreatment.group\nofc.test.result\n\n\n\n\nLEAP_100522\nPeanut Consumption\nPASS OFC\n\n\nLEAP_103358\nPeanut Consumption\nPASS OFC\n\n\nLEAP_105069\nPeanut Avoidance\nPASS OFC\n\n\nLEAP_105328\nPeanut Consumption\nPASS OFC\n\n\n\n\n\nThe table you saw in the reading was a summary of the data (not the data itself):\n\n\n\n\n\n\n\n\n\n\n \nFAIL OFC\nPASS OFC\n\n\n\n\nPeanut Avoidance\n36\n227\n\n\nPeanut Consumption\n5\n262"
  },
  {
    "objectID": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "href": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "title": "Data semantics and data types",
    "section": "Numeric and categorical variables",
    "text": "Numeric and categorical variables\nVariables are classified according to their values. Values can be one of two different types:\n\nA variable is numeric if its value is a number\nA variable is categorical if its value is a category, usually recorded as a name or label\n\nFor example:\n\nthe value of sex can be male or female, so it is categorical\nwhereas age (in years) can be any positive integer, so it is numeric"
  },
  {
    "objectID": "content/week2-datatypes.html#variable-subtypes",
    "href": "content/week2-datatypes.html#variable-subtypes",
    "title": "Data semantics and data types",
    "section": "Variable subtypes",
    "text": "Variable subtypes\nFurther distinctions are made based on the type of number or type of category used to measure an attribute. Can you match the subtypes to the variables at right?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nhispanic\ngrade\nweight\n\n\n\n\n15\nnot\n10\n78.02\n\n\n18\nhispanic\n12\n78.47\n\n\n17\nnot\n11\n95.26\n\n\n18\nnot\n12\n95.26\n\n\n\n\n\n\n\n\na numerical variable is discrete if there are ‘gaps’ between its possible values\na numerical variable is continuous if there are no such gaps\na categorical variable is nominal if its levels are not ordered\na categorical variable is ordinal if its levels are ordered"
  },
  {
    "objectID": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "href": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "title": "Data semantics and data types",
    "section": "Many ways to measure attributes",
    "text": "Many ways to measure attributes\nVariable type (or subtype) is not an inherent quality — attributes can often be measured in many different ways.\nFor instance, age might be measured as either a discrete, continuous, or ordinal variable, depending on the situation:\n\n\n\nAge (years)\nAge (minutes)\nAge (brackets)\n\n\n\n\n12\n6307518.45\n10-18\n\n\n8\n4209187.18\n5-10\n\n\n21\n11258103.08\n18-30\n\n\n\n\nNumeric variables can always be represented as categorical, but not the other way around."
  },
  {
    "objectID": "content/week2-datatypes.html#your-turn",
    "href": "content/week2-datatypes.html#your-turn",
    "title": "Data semantics and data types",
    "section": "Your turn",
    "text": "Your turn\nClassify each variable as nominal, ordinal, discrete, or continuous:\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ngenotype\nsex\nage\nrace\nbmi\n\n\n\n\n33.3\nCT\nFemale\n19\nCaucasian\n21.01\n\n\n71.4\nCT\nFemale\n18\nOther\n23.18\n\n\n37.5\nCC\nFemale\n21\nCaucasian\n28.92\n\n\n50\nCC\nFemale\n28\nAsian\n21.16\n\n\n\n\n\nData are from an observational study investigating demographic, physiological, and genetic characteristics associated with muscle strength.\n\nndrm.ch is change in strength in nondominant arm after resistance training\ngenotype indicates genotype at a particular location within the ACTN3 gene"
  },
  {
    "objectID": "content/week2-datatypes.html#common-summary-statistics",
    "href": "content/week2-datatypes.html#common-summary-statistics",
    "title": "Data semantics and data types",
    "section": "Common summary statistics",
    "text": "Common summary statistics\n\nA statistic is a data summary: in mathematical terms, a function of several observations\n\n\n\nFor numeric variables, the most common summary statistic is the average value:\n\\[\\text{average} = \\frac{\\text{sum of values}}{\\text{# observations}}\\]\nFor example, the average percent change in nondominant arm strength was 53.291%.\n\nFor categorical variables, the most common summary statistic is a proportion:\n\\[\\text{proportion}_i = \\frac{\\text{# observations in category } i}{\\text{# observations}}\\]\nFor example:\n\n\n\nGenotype proportions\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n0.2908\n0.4387\n0.2706"
  },
  {
    "objectID": "content/week2-datatypes.html#descriptive-analyses",
    "href": "content/week2-datatypes.html#descriptive-analyses",
    "title": "Data semantics and data types",
    "section": "Descriptive analyses",
    "text": "Descriptive analyses\nSometimes, a few clever summary statistics can be used to answer a research question.\n\nHow much does the average change in arm strength differ by genotype, if at all?\n\nComputing per-genotype averages provides an answer:\n\n\n\n\n\n\n\n\n\n\n\ngenotype\navg.change\nn.obs\nprop.obs\n\n\n\n\nTT\n58.08\n161\n0.2706\n\n\nCT\n53.25\n261\n0.4387\n\n\nCC\n48.89\n173\n0.2908\n\n\n\n\n\nNumber of observations and proportions are included because they provide information about genotype frequencies in the sample.\n\nconveys how many individuals were measured\nalso provides an estimate of genotype frequencies in the population"
  },
  {
    "objectID": "content/week2-datatypes.html#common-mathematical-notation",
    "href": "content/week2-datatypes.html#common-mathematical-notation",
    "title": "Data semantics and data types",
    "section": "Common mathematical notation",
    "text": "Common mathematical notation\nWhile we won’t use mathematical expressions too often in STAT218, it’s useful to be aware of some common notations.\nTypically, a set of observations is written as:\n\\[x_1, x_2, \\dots, x_n\\]\n\n\\(x\\) represents the variable (e.g., genotype, age, percent change, etc.)\nsubscript indexes observations: \\(x_i\\) is the \\(i\\)th observation\n\\(n\\) is the total number of observations\n\nThe sum of the observations is written \\(\\sum_i x_i\\), where the symbol \\(\\sum\\) stands for ‘summation’. This is useful for writing the formula for computing an average:\n\\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]"
  },
  {
    "objectID": "content/week2-datatypes.html#lab-data-basics-in-r",
    "href": "content/week2-datatypes.html#lab-data-basics-in-r",
    "title": "Data semantics and data types",
    "section": "Lab: data basics in R",
    "text": "Lab: data basics in R\nThe variable types we just discussed map pretty neatly (but not perfectly) onto the main “data types” in R:\n\nnumeric ➜ integer, numeric\ncategorical ➜ character, factor, logical\n\nThe primary way data are arranged in R is in a data frame. This lab will show you how to load, inspect, and use data frames.\nYour objectives in this lab are:\n\nlearn to load and inspect datasets\nlearn to recognize data types\nlearn to perform simple calculations (averages, etc.)"
  },
  {
    "objectID": "content/week2-datatypes.html#opening-the-lab-activity",
    "href": "content/week2-datatypes.html#opening-the-lab-activity",
    "title": "Data semantics and data types",
    "section": "Opening the lab activity",
    "text": "Opening the lab activity\nNavigate to posit.cloud. Then:\n\n\n\n\n\n\nMake sure the class workspace “stat218-s24” is highlighted at left. If “Your Workspace” is highlighted, you won’t see the example assignment.\nClick on the lab1-rbasics, then wait.\n\nOnce everyone is ready, we’ll have a look at the example files together.\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab8-twosample.html",
    "href": "content/lab8-twosample.html",
    "title": "Lab 8: Two-sample inference",
    "section": "",
    "text": "This lab focuses on two-sample inference for differences in population means. The main objectives are:\n\nLearn to implement two-sample \\(t\\) tests in R\nPractice distinguishing directional and nondirectional tests and providing appropriate specifications to the t.test(...) function in R\n\nThe lab uses several datasets for which we will consider two-sample comparisons:\n\nfinch: mean finch beak depths in generations before and after a drought on Daphne Major\ntemps: body temperatures and heart rates for men and women\n\n\nlibrary(tidyverse)\nload('data/finch.RData')\nload('data/temps2.RData')\n\nExamples will utilize the finch data; you’ll practice using the temps data. Here are the summary statistics for the finch data for reference:\n\nfinch |&gt;\n  group_by(year) |&gt;\n  summarize(depth.mean = mean(depth),\n            depth.sd = sd(depth),\n            n = n())\n\n# A tibble: 2 × 4\n   year depth.mean depth.sd     n\n  &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1  1976       9.45    0.962    58\n2  1978      10.2     0.807    65\n\n\n\nChecking assumptions for two-sample tests\nA two-sample \\(t\\) test can be used whenever two one-sample tests are appropriate. So, to check assumptions, we need to inspect the frequency distributions of the variable of interest in both samples.\n\nOption A: two histograms\nOne way to do this is to make two separate histograms. To do that, we’ll need to separate the samples. This can be done by ‘filtering’ observations according to whether year is 1978 or 1976.\n\n# separate samples\nfinch.1978 &lt;- finch |&gt; filter(year == 1978)\nfinch.1976 &lt;- finch |&gt; filter(year == 1976)\n\n# extract depths\ndepth.1978 &lt;- finch.1978$depth\ndepth.1976 &lt;- finch.1976$depth\n\n# make histograms\nhist(depth.1978)\n\n\n\n\n\n\n\nhist(depth.1976)\n\n\n\n\n\n\n\n\nBoth distributions are a bit left-skewed, but each sample is large enough that this isn’t a problem for performing the test.\n\n\n\n\n\n\nYour turn 1\n\n\n\nFilter the temps data by sex to separate the samples, and make histograms of the heart rates. Comment on whether assumptions seem to be met.\n\n# separate samples\n\n# extract heart rates\n\n# make histograms\n\n\n\n\n\nOption B: side-by-side boxplots\nA slightly more efficient alternative is to make side-by-side boxplots. This doesn’t involve filtering the data, and will produce just a single graphic.\nHowever, some details of the distribution (such as multiple modes) may not be evident from the boxplots, so it’s not a perfect substitute for checking histograms.\n\n# side-by-side boxplots\nboxplot(depth ~ year, data = finch, horizontal = T)\n\n\n\n\n\n\n\n\nHere we want to see two things:\n\napproximate symmetry of boxes\nfew to no large outliers\n\nWhile there is a bit of left skewness, the sample sizes are large enough that it’s not a concern.\n\n\n\n\n\n\nYour turn 2\n\n\n\nMake side-by-side boxplots for heart rate and reassess test assumptions.\n\n# side-by-side boxplots for heart rate\n\n\n\n\n\n\nTwo-sample \\(t\\)-tests\nGiven that assumptions seem plausible (both samples show little skew and few outliers, and are sufficiently large), we can go ahead with the test. To test whether the drought imposed selection pressure on the finch population, we want to know whether finch beak depth increased after the drought.\nObserve, first, which sample appears first in the dataset: 1976. R will treat this as the first sample; to keep track of directions, we’ll want to formulate the hypotheses as a comparison between 1976 (first sample) and 1978 (second sample).\nWe want to test whether the mean of the first sample is less than the mean of the second:\n\\[\n\\begin{cases}\nH_0: &\\mu_{1976} = \\mu_{1978} \\\\\nH_A: &\\mu_{1976} &lt; \\mu_{1978}\n\\end{cases}\n\\] Let’s carry out the test at the 5% significance level. The inputs to t.test(...) that implement this test are:\n\na formula &lt;VARIABLE&gt; ~ &lt;SAMPLE&gt; as the first argument: formula = depth ~ year\na data frame containing the variable names mentioned in the formula: data = finch\na null value for the difference: mu = 0\nan alternative: alternative = 'less'\na confidence level to complement the significance level of the test: conf.level = 0.95\n\n\n# perform t test (notice which group comes first)\nt.test(formula = depth ~ year, data = finch, mu = 0, alternative = 'less', conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n\n\nTake a moment to inspect the output and identify each number appearing. We’d report the test result as follows:\n\nThe data provide very strong evidence that mean beak depth increased in the generation of finches following the drought (T = -4.5727 on 111.79 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean beak depth is estimated to have increased by at least 0.4699 mm, with a point estiamte of 0.7373 mm (SE 0.1612).\n\nThe point estimate and standard error can be retrieved by storing the output of t.test(...).\n\n# store t test result\ntt.rslt &lt;- t.test(formula = depth ~ year, data = finch, mu = 0, alternative = 'less', conf.level = 0.95)\n\n# estimates\ntt.rslt$estimate\n\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n\n# estimate for difference in means\ntt.rslt$estimate |&gt; diff()\n\nmean in group 1978 \n          0.737321 \n\n# standard error for estimate of difference in means\ntt.rslt$stderr\n\n[1] 0.1612445\n\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nTest whether mean heart rate differs between men and women at the 1% significance level. Report the test result, confidence interval, and point estimate and standard error for the difference in means.\n\n# perform t test\n\n# store t test result\n\n# estimate for difference in means\n\n# standard error\n\n\n\n\n\nPractice problems\n\nUsing the temps2 dataset, test whether mean body temperature is lower for men.\n\nCheck the assumptions for the test by making both a pair of histograms and side-by-side boxplots.\nPerform the test at the 1% significance level.\nReport the test result, confidence interval, and point estimate and standard error for the difference in means.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 2.2854, df = 127.51, p-value = 0.01197\nalternative hypothesis: true difference in means between group female and group male is greater than 0\n99 percent confidence interval:\n -0.008923783          Inf\nsample estimates:\nmean in group female   mean in group male \n            98.39385             98.10462 \n\n\nmean in group female   mean in group male \n            98.39385             98.10462 \n\n\nmean in group male \n        -0.2892308 \n\n\n[1] 0.126554\n\n\n\nUsing the brfss2 data, test whether actual body weight exceeds desired body weight by more for women than for men.\n\nCheck the assumptions for the test by making both a pair of histograms and side-by-side boxplots.\nPerform the test at the 1% significance level.\nReport the test result, confidence interval, and point estimate and standard error for the difference in means.\n\n\n\n\n# A tibble: 6 × 4\n  sex   weight wtdesire weight.diff\n  &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 m        265      225          40\n2 m        150      150           0\n3 m        137      150         -13\n4 f        159      125          34\n5 f        145      125          20\n6 f        125      120           5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  weight.diff by sex\nt = 2.048, df = 38.875, p-value = 0.02368\nalternative hypothesis: true difference in means between group f and group m is greater than 0\n99 percent confidence interval:\n -3.108832       Inf\nsample estimates:\nmean in group f mean in group m \n      26.354839        9.517241 \n\n\nmean in group m \n       -16.8376 \n\n\n[1] 8.22135"
  },
  {
    "objectID": "content/week3-review.html#outcomes",
    "href": "content/week3-review.html#outcomes",
    "title": "Review session 1",
    "section": "Outcomes",
    "text": "Outcomes\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques"
  },
  {
    "objectID": "content/week3-review.html#lecturelab-recap",
    "href": "content/week3-review.html#lecturelab-recap",
    "title": "Review session 1",
    "section": "Lecture/lab recap",
    "text": "Lecture/lab recap\n\n\n\n\n\n\n\n\nWeek/day\nLecture\nLab\n\n\n\n\n1/W\nExperiments and observational studies\nReading abstracts\n\n\n2/M\nData vocabulary, proportions and means, common notation\nLoading data, extracting variables, computing means and proportions\n\n\n2/W\nMeasures of location/center and spread, simple graphics\nHistograms, barplots, summary statistics\n\n\n3/M\nGraphics and tables for two variables, interpreting relationships, correlation\nStacked bar plots, side-by-side boxplots, scatterplots"
  },
  {
    "objectID": "content/week3-review.html#assignment-recap",
    "href": "content/week3-review.html#assignment-recap",
    "title": "Review session 1",
    "section": "Assignment recap",
    "text": "Assignment recap\n\n\n\n\n\n\n\nProblem set\nTopics\n\n\n\n\nPS1\nInterpreting study descriptions\n\n\nPS2\nSummary statistics (mean, proportion) for one variable; identifying variable types\n\n\nPS3\nMeasures of location and spread for one variable; visualizing frequency distributions\n\n\nPS4\nVisualizing relationships between two variables (C/C, C/N, N/N)"
  },
  {
    "objectID": "content/week3-review.html#questions-l1-l2-study-design",
    "href": "content/week3-review.html#questions-l1-l2-study-design",
    "title": "Review session 1",
    "section": "Questions: [L1, L2] study design",
    "text": "Questions: [L1, L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#questions-l3-descriptive-statistics",
    "href": "content/week3-review.html#questions-l3-descriptive-statistics",
    "title": "Review session 1",
    "section": "Questions: [L3] descriptive statistics",
    "text": "Questions: [L3] descriptive statistics\n\nCompute/interpret mean/median/percentiles/IQR/variance/SD\nCompute/interpret a grouped summary with one or more of the above measures\nDetermine appropriate measures of location/center/spread\nMake/interpret a table of proportions for values of a categorical variable\nMake/interpret a contingency table\nMake/interpret a histogram\nMake/interpret two-way tables of proportions\nMake/interpret stacked bar plots representing proportions\nMake/interpret side-by-side boxplots\nMake/interpret scatterplots\nCompute/interpret correlations"
  },
  {
    "objectID": "content/week3-review.html#commonly-missed-question-types",
    "href": "content/week3-review.html#commonly-missed-question-types",
    "title": "Review session 1",
    "section": "Commonly missed question types",
    "text": "Commonly missed question types\nIdentifying appropriate measures of center/spread (i.e., understanding robustness) Grouped summaries (getting syntax right) Determining"
  },
  {
    "objectID": "content/week3-review.html#key-concepts",
    "href": "content/week3-review.html#key-concepts",
    "title": "Review session 1",
    "section": "Key concepts",
    "text": "Key concepts\n\n[L1, L2] Experiments and observational studies\n[L1, L2] Samples and populations\n[L2] Variable types: categorical (nominal/ordinal) and numeric (discrete/continuous)\n[L3] Summary statistics: mean, median, percentile, IQR, SD, variance, correlation\n[L3] Distribution properties: skewness, outliers, modes\n[L3] Robustness of common summary statistics\n[L3] Relationships: positive/negative; linear/nonlinear"
  },
  {
    "objectID": "content/week3-review.html#about-the-test",
    "href": "content/week3-review.html#about-the-test",
    "title": "Review session 1",
    "section": "About the test",
    "text": "About the test\n\n4-5 questions\nExpect one challenge part, but otherwise very similar to homework problems and practice questions\n48 hours, open book, open note\nPosit cloud project + fillable form\nRevisions will be allowed to earn back credit\nNo collaboration\nDo your own analysis/writing (no AI plagiarism)\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-activity-nonparametric.html",
    "href": "content/week7-activity-nonparametric.html",
    "title": "Week 7 activity: Nonparametric inference",
    "section": "",
    "text": "library(tidyverse)\nload('data/brfss.RData')\nload('data/temps.RData')\nddt &lt;- MASS::DDT\n\nIn this activity you’ll learn about nonparametric alternatives to the \\(t\\) test for one and two means. The activity is organized much like a lab, but with extra narrative. You should read through the narrative at your own pace and try the exercises provided as you go. At the end there are two practice problems that you’ll be expected to complete before next class.\n\nBackground: parametric and nonparametric inference\nConsider the basis for the inferences developed so far: under certain conditions (typically regularity of the underlying population distribution, assessed by checking histograms for unimodality and approximate symmetry) and with sufficient sample sizes, a model is specified for the sampling distribution of a test statistic.\n\ninference for one mean: \\(t_{n - 1}\\) model for the sampling distribution of \\(T = \\frac{\\bar{x} - \\mu}{SE(\\bar{x})}\\)\ninference comparing two means: \\(t_{\\nu}\\) model for the sampling distribution of \\(T = \\frac{\\bar{x} - \\bar{y} - \\delta}{SE(\\bar{x} - \\bar{y})}\\)\ninference comparing several means: \\(F_{k - 1, n - k}\\) model for the sampling distribution of \\(F = \\frac{MSG}{MSE}\\)\n\nThese are all what are known as parametric models, because they are specified through one or two parameters that determine their exact shape. The parameters in this case are the degrees of freedom terms – \\(n - 1\\) for the one-sample \\(t\\) test, \\(\\nu\\) (usually estimated) for the two-sample \\(t\\) test, and \\(k - 1\\) and \\(n - k\\) for the \\(F\\) test.\nAs such, these procedures are examples of parametric inference – inferences that utilize a parametric model for the data and/or test statistic.\nWhen assumptions for parametric inference aren’t tenable, or when a parametric model is not available, there are so-called nonparametric methods of inference: methods that don’t depend on a parametric model such as the \\(t\\) or \\(F\\) models we’ve learned about in class.\nWe will consider specifically nonparametric procedures based on ranks, i.e., ordering observations from smallest to largest.\n\n\nMotivating examples\nIn practice, the situation that most often leads an analyst to consider rank-based nonparametric methods is that the assumptions for the \\(t\\) test either don’t hold or are difficult to assess.\nBelow are two such situations you’ve already encountered in this class.\n\nSmall sample sizes\nWhen sample sizes are small, it’s awkward to assess assumptions for parametric inference, because with few observations histograms can lack any discernible shape. For example, the most we can say about the following data on heart rates for 19 women and 20 men is that there are no evident outliers.\n\nheart.m &lt;- temps |&gt; filter(sex == 'male') |&gt; pull(heart.rate)\nheart.f &lt;- temps |&gt; filter(sex == 'female') |&gt; pull(heart.rate)\n\npar(mfrow = c(1, 2))\nhist(heart.m)\nhist(heart.f)\n\n\n\n\n\n\n\n\nIn practical terms, the \\(t\\) test is likely still fine under these circumstances; however, some may wish to consider an inference for comparing heart rates between groups that doesn’t depend on distributional assumptions.\n\n\nAssumptions don’t hold\nOn occasion you may go to check assumptions and find that they’re clearly violated. For example, the pairwise differences between actual and desired weights from BRFSS respondents showed clear skewness and several large outliers. In that case, the sample size was big enough that we overlooked the issue, but it’s not hard to imagine a similar situation cropping up with fewer observations.\nSuppose you only had 12 observations that showed the same skew and had one big outlier:\n\nset.seed(51424)\nweight.diff &lt;- sample(brfss$weight - brfss$wtdesire, 12)\nhist(weight.diff, breaks = 10)\n\n\n\n\n\n\n\n\nHere the \\(t\\) test isn’t appropriate, and using it anyway would likely result in a true significance level, coverage, and power quite different from the nominal levels specified in the test, so it would be hard to trust the result. This is a great situation to use a rank-based nonparametric alternative.\n\n\n\nInference on “location” (not mean)\nThe usual parametric inferences pertain to the population mean; not so with rank-based nonparametrics. Instead, these inferences pertain simply to “location”.\nOften “location” is characterized in terms of the “center” of a distribution so that inferences can be interpreted in a manner similar to parametric tests and intervals.\nWe will follow this convention and consider the hypotheses to be about the center(s) of the population model(s), denoted \\(c\\). For example, the two-sided test of center would test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &c = c_0 \\\\\nH_A: &c \\neq c_0\n\\end{cases}\n\\]\nFor the two-sided test of difference in centers, we will test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &c_1 = c_2 \\\\\nH_A: &c_1 \\neq c_2\n\\end{cases}\n\\]\nHowever, you should keep in mind that “location” is a more general notion that encompasses all measures of location of a distribution.\n\n\nOne-sample inference: signed rank test\nThe basic premise of this test is that, if the population model is symmetric, its center should evenly divide the data.\n\n\n\n\n\n\nWarm up\n\n\n\nConsider the following 15 measurements of DDT in kale in ppm in order from smallest to largest:\n\nsort(ddt)\n\n [1] 2.79 2.93 3.06 3.07 3.08 3.18 3.22 3.22 3.33 3.34 3.34 3.38 3.56 3.78 4.64\n\n\nSuppose you wish to test whether the center \\(c\\) of the population model is \\(c_0 = 3\\) ppm and assume a symmetric population model.\n\nHow many observations would you expect to be smaller than \\(c_0 = 3\\) if 3ppm is in fact the center?\nHow many observations are actually smaller than 3 ppm?\nBased on your answers to 1-2, do you think it is likely that in fact \\(c = 3\\)?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the population is symmetric about \\(c_0\\), you’d expect roughly half of observations (7.5) to be smaller than 3ppm.\nThe actual number of observations smaller than 3ppm is:\n\n# number of observations below 3\nsum(ddt &lt; 3)\n\n[1] 2\n\n\nThis is much less than half, so it seems unlikely that the center is actually 3ppm.\n\n\n\nThe signed rank test is a nonparametric alternative to the one-sample \\(t\\) test applicable to any symmetric population model. The particular form of symmetry and presence of outliers do not affect the test.\n\nHypotheses\nThe test can be directional or two-sided, just like the \\(t\\) test. Thus, the possible hypotheses are:\n\\[\nH_0: c = c_0 \\quad\\text{vs.}\\quad H_A: c \\mathrel{\\substack{&lt;\\\\\\neq\\\\ &gt;}} c_0\n\\]\n\n\nTest procedure\nWhile the intuition of the test is that half of observations should be smaller than the true center under population symmetry, the test statistic is not quite as direct as a tally of how many observations are below the hypothetical value. Instead, the procedure is as follows:\n\n[center] Calculate deviations \\(d_i = x_i - c_0\\)\n[rank] Sort and rank the absolute deviations \\(|d_i|\\)\n\naverage ranks in case of ties\ndrop zeros\n\n[sum] Add up the positive ‘signed ranks’ \\(\\sum_{\\text{sign}(d_i) &gt; 0} r_i\\)\n\nThis produces the test statistic:\n\\[V = \\sum_{i = 1}^n \\text{sign}(d_i) \\times R_i\\]\n\n\n\n\n\n\nCheck your understanding\n\n\n\nTry working out the rank sum procedure manually using the DDT data – it’s small enough that you could jot down the steps on some scratch paper. See if you can calculate \\(V\\).\nTo help, here are the deviations sorted smallest to largest:\n\n# calculate deviations\ndi &lt;- ddt - 3\nsort(di)\n\n [1] -0.21 -0.07  0.06  0.07  0.08  0.18  0.22  0.22  0.33  0.34  0.34  0.38\n[13]  0.56  0.78  1.64\n\n\n\nStart by writing the deviations in order of absolute value in a column.\nThen rank them 1-15 in an adjacent column. If there is a tie – e.g., two absolute deviations of 0.05 – then assign them both the average of the ranks. For example, if 0.05 occurs twice in positions 2 and 3, then give them both rank 2.5.\nWrite down the sign of the deviation in a new column.\nThen write down the “signed rank”, or product of the sign and the rank, in a fourth column.\nAdd up the positive signed ranks. This is the signed rank statistic.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis shows the steps in R, but don’t worry about the codes; the output is meant to illustrate what you would do on paper to find the rank sum statistic.\n\n# this shows the steps, column by column; focus on output\nddt.srank &lt;- tibble(di = di) |&gt;\n  mutate(abs.di = abs(di), \n         rank = rank(abs.di),\n         sign = sign(di),\n         signed.rank = sign*rank) |&gt;\n  arrange(abs.di)\nddt.srank\n\n# A tibble: 15 × 5\n        di abs.di  rank  sign signed.rank\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1  0.0600 0.0600   1       1         1  \n 2 -0.0700 0.0700   2.5    -1        -2.5\n 3  0.0700 0.0700   2.5     1         2.5\n 4  0.0800 0.0800   4       1         4  \n 5  0.180  0.180    5       1         5  \n 6 -0.21   0.21     6      -1        -6  \n 7  0.220  0.220    7.5     1         7.5\n 8  0.220  0.220    7.5     1         7.5\n 9  0.33   0.33     9       1         9  \n10  0.34   0.34    10.5     1        10.5\n11  0.34   0.34    10.5     1        10.5\n12  0.38   0.38    12       1        12  \n13  0.56   0.56    13       1        13  \n14  0.78   0.78    14       1        14  \n15  1.64   1.64    15       1        15  \n\n# signed rank statistic\nvstat &lt;- ddt.srank |&gt; filter(sign &gt; 0) |&gt; pull(signed.rank) |&gt; sum()\nvstat\n\n[1] 111.5\n\n\n\n\n\n\n\n\\(p\\)-values for the test\nJust like other hypothesis tests, the signed rank test rejects if \\(V\\) is sufficiently large in the direction of the alternative.\nA sampling distribution for \\(V\\) can be found exactly using combinatorics, or approximated using probability theory. In this caseWe won’t go into details about either approach, except to indicate that there is a sampling distribution for \\(V\\) that we can use to obtain \\(p\\)-values in the same fashion that the \\(t_{n - 1}\\) model was used to obtain \\(p\\)-values for the \\(t\\) test.\nIn this case, there are 3.2768^{4} possible sign combinations; of these, only about 0.375% give a larger value of \\(V\\). That provides a \\(p\\)-value for the test, and since \\(p = 0.0018 &lt; 0.05\\) we would reject \\(H_0\\) at the 5% significance level. This result is interpreted as:\n\nThe data provide strong evidence that the typical DDT concentration in kale is not 3ppm (signed rank statistic V = 111.5, p = 0.00375).\n\n\n\nImplementation with wilcox.test(...)\nThe implementation in R looks and functions much like t.test:\n\n# signed rank test at 1% level\nwilcox.test(ddt, mu = 3, alternative = 'two.sided', \n            exact = F, conf.int = T, conf.level = 0.99)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  ddt\nV = 111.5, p-value = 0.003751\nalternative hypothesis: true location is not equal to 3\n99 percent confidence interval:\n 3.060070 3.780032\nsample estimates:\n(pseudo)median \n       3.26001 \n\n\nSome remarks: - exact = F produces approximate \\(p\\)-values and confidence intervals; you may see warning messages if this is excluded - pseudo-median is a measure of center, but not the same as a median or mean (check!)\n\n\n\n\n\n\nYour turn 1\n\n\n\nPerform the analogous inference using the \\(t\\) test and compare the results. Do the tests agree at the 1% significance level?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe tests do not agree: the signed rank test rejects at the 1% level, but the \\(t\\) test does not.\n\n# perform t test at 1% level to compare\nt.test(ddt, mu = 3, alternative = 'two.sided', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.01151\nalternative hypothesis: true mean is not equal to 3\n99 percent confidence interval:\n 2.991996 3.664004\nsample estimates:\nmean of x \n    3.328 \n\n\n\n\n\n\n\n\n\n\n\nYour turn 2\n\n\n\nPerform the signed rank test to determine whether actual weight exceeds desired weight by more than 10lbs at the 5% significance level. Report the result in the usual narrative format. Compare your result with the inference obtained from a \\(t\\) test.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# use signed rank test to determine whether actual exceeds desired by at least 10lbs\nwilcox.test(weight.diff, mu = 10, alternative = 'greater', \n            exact = F, conf.int = T, conf.level = 0.95)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  weight.diff\nV = 61, p-value = 0.04559\nalternative hypothesis: true location is greater than 10\n95 percent confidence interval:\n 10.00003      Inf\nsample estimates:\n(pseudo)median \n       22.8617 \n\n# check t test result to compare\nt.test(weight.diff, mu = 10, alternative = 'greater', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  weight.diff\nt = 1.6127, df = 11, p-value = 0.06755\nalternative hypothesis: true mean is greater than 10\n95 percent confidence interval:\n 7.019188      Inf\nsample estimates:\nmean of x \n    36.25 \n\n\nInterpretation of the signed rank test test:\n\nThe data provide evidence that actual weight exceeds desired weight by more than 10lbs (signed rank statistic V = 61, p = 0.0456).\n\nThe signed rank test finds evidence that actual weight exceeds desired weight by more than 10lbs, where the \\(t\\) test does not.\n\n\n\n\n\n\nTwo-sample inference: rank sum test\nThe rank-sum test is a nonparametric alternative to the two-sample \\(t\\) test based on ranks. The key idea for the test is that if observations in both groups come from the same population distribution then they should be exchangeable (i.e., groupings don’t matter).\nThus, the only assumption for this test is that data are independent. The test can be directional, and thus it is possible to test the following hypotheses comparing centers:\n\\[\nH_0: c_1 = c_2\\quad\\text{vs}\\quad H_A: c_1 \\mathrel{\\substack{&lt;\\\\\\neq\\\\&gt;}} c_2\n\\]\n\nThe alternative hypothesis\nThis test is a bit funny in that the null hypothesis is really that the distributions are exactly the same. The alternative to this possibility can come about in a number of ways. The alternative is usually interpreted as a difference in location, primarily because this is the situation that the test has power to detect.\nSo, it is often said that the test also assumes that the samples differ only in location. In other words, the test is most appropriate when the histograms look “shifted”, but not fundamentally different, as illustrated below.\n\n\n\n\n\n\n\n\n\nIn all of these cases, there is a (true) difference in location, but if shape differs too much, the rank sum test should not be used. That said, it can be hard to tell with small samples, so it may not really be practical to to check this assumption.\nExample 1. Out-of-state tuition costs from 26 public and 26 private universities. These data differ primarily in spread, not location; so the rank-sum test might not work well here.\n\n\n\n\n\n\n\n\n\nExample 2. Deviations from expected cancer rates in CT in years with high and low sunspot activity. The shape is a bit hard to discern here, but it seems plausible that the distribution for high sunspot years is shifted to the right of that for low sunspot years. So, the rank sum test would be appropriate here.\n\n\n\n\n\n\n\n\n\n\n\nRank sum test procedure\nThe rank sum procedure, though a bit opaque, is rather simple:\n\n[pool] Combine observations from both groups\n[rank] Sort and rank pooled observations\n[sum] Add up ranks in the first group\n[adjust] Subtract \\(\\frac{n_1(n_1 + 1)}{2}\\), where \\(n_1\\) is the sample size of the first group\n\nThe test rejects if the sum is larger than expected in the direction of the alternative. As in the signed rank test, combinatorics are used to determine a \\(p\\)-value for the test.\nThe rationale for this procedure is that if the distributions are the same, then the ranks should be evenly distributed among the two groups; this induces a particular sampling distribution on the sum of the ranks in each group. The adjustment facilitates computation of \\(p\\)-values.\nLet’s illustrate using data from an experiment in which participants were randomly assigned to receive a fish oil supplement or a regular oil supplement. For each subject, the reduction in blood pressure was measured after a period of time on the treatments.\n\n# load dataset\nfish.oil &lt;- Sleuth3::ex0112 |&gt; rename_with(tolower)\n\n# make boxplot\nboxplot(bp ~ diet, data = fish.oil, horizontal = T)\n\n\n\n\n\n\n\n\nThere’s a bit of difference in spread, but enough of a shift in location that the rank sum test is reasonable to apply.\n\n\n\n\n\n\nCheck your understanding\n\n\n\nCarry out the rank sum procedure outlined above and compute the rank sum statistic by summing up the ranks in the fish oil group.\nTo facilitate calculations, here are the data in order of increasing blood pressure:\n\nfish.oil |&gt; arrange(bp)\n\n   bp       diet\n1  -6 RegularOil\n2  -4 RegularOil\n3  -3 RegularOil\n4   0    FishOil\n5   0    FishOil\n6   0 RegularOil\n7   1 RegularOil\n8   2    FishOil\n9   2 RegularOil\n10  2 RegularOil\n11  8    FishOil\n12 10    FishOil\n13 12    FishOil\n14 14    FishOil\n\n\nAdd a column of ranks by hand (or in R if you can figure out how!), averaging ranks for any ties. Then add up the ranks in the FishOil group, and subtract \\(\\frac{n_1(n_1 + 1)}{2} = \\frac{7\\times 8}{2} = 28\\) to obtain the rank sum statistic.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe output below illustrates the rankings and selection of which ones to add up. Don’t worry about the codes\n\n# again, ignore the codes; look at output\nfish.oil.ranksum &lt;- fish.oil |&gt; \n  mutate(rank = rank(bp),\n         ranks.fish = rank*(diet == 'FishOil')) |&gt;\n  arrange(bp)\nfish.oil.ranksum\n\n   bp       diet rank ranks.fish\n1  -6 RegularOil    1          0\n2  -4 RegularOil    2          0\n3  -3 RegularOil    3          0\n4   0    FishOil    5          5\n5   0    FishOil    5          5\n6   0 RegularOil    5          0\n7   1 RegularOil    7          0\n8   2    FishOil    9          9\n9   2 RegularOil    9          0\n10  2 RegularOil    9          0\n11  8    FishOil   11         11\n12 10    FishOil   12         12\n13 12    FishOil   13         13\n14 14    FishOil   14         14\n\n# rank sum statistic\nn1 &lt;- count(fish.oil, diet)$n[1]\nsum(fish.oil.ranksum$ranks.fish) - n1*(n1 + 1)/2\n\n[1] 41\n\n\n\n\n\n\n\nImplementation using wilcox.test(...)\nThe wilcox.test function also implements the rank sum test, using the same syntax as t.test(...). For example, using the fish oil data, we might test at the 1% significance level whether the fish oil supplement caused a greater reduction in blood pressure:\n\nwilcox.test(bp ~ diet, data = fish.oil, \n            mu = 0, alternative = 'greater', \n            exact = F, conf.int = T, conf.level = 0.99)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bp by diet\nW = 41, p-value = 0.01957\nalternative hypothesis: true location shift is greater than 0\n99 percent confidence interval:\n -0.9999444        Inf\nsample estimates:\ndifference in location \n              7.999962 \n\n\nFor this test, the \\(p\\)-value gives the percentage of possible rank allocations among the groups for which the rank sum is at least as favorable to \\(H_A\\); again this is computed using combinatorics or approximation methods.\nIn this instance, the \\(p\\)-value indicates that only about 1.96% of all possible rank allocations among the two groups would produce a rank sum statistic at least as large. Thus, testing at the 1% significance level:\n\nThe data provide do not provide sufficient evidence at the 1% significance level that the fish oil supplement caused a greater reduction in blood pressure than the regular oil supplement (rank sum statistic W = 41, p = 0.01957).\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nPerform the corresponding \\(t\\) test for comparison at the 1% significance level. Do the tests agree?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe tests do not agree; the \\(t\\) test supports evidence of an effect at the 1% level where the rank sum test does not.\n\n# t test for effect of treatment at 1% level\nt.test(bp ~ diet, data = fish.oil, mu = 0, \n       alternative = 'greater', conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  bp by diet\nt = 3.0621, df = 9.2643, p-value = 0.006542\nalternative hypothesis: true difference in means between group FishOil and group RegularOil is greater than 0\n95 percent confidence interval:\n 3.111056      Inf\nsample estimates:\n   mean in group FishOil mean in group RegularOil \n                6.571429                -1.142857 \n\n\n\n\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nUse the cancer dataset (specifically the delta variable) to test whether the change in cancer rate is higher in years with high sunspot activity. Carry out the test at the 5% level, and report the result in the usual narrative style.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# test whether change in cancer rate is higher in years with high sunspot activity at the 5% level\nwilcox.test(delta ~ sunspot, data = cancer, mu = 0,\n            exact = F, alternative = 'greater', \n            conf.int = T, conf.level = 0.95)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  delta by sunspot\nW = 157.5, p-value = 0.3072\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n -0.1000484        Inf\nsample estimates:\ndifference in location \n            0.09999013 \n\n\nInterpretation:\n\nThe data provide no evidence that the change in cancer rate is higher in years with high sunspot activity (rank sum statistic W = 157.5, p = 0.3072).\n\n\n\n\n\n\n\nPractice problem\n\nIs there a difference in cholesterol associated with consuming oat bran compared with consuming corn flakes? Use the cholesterol dataset to test for a difference at the 5% level using a nonparametric test.\n\nConstruct boxplots to compare the distributions for location shift. Does the nonparametric test seem appropriate?\nCarry out the test.\nReport the test result in the usual narrative style."
  },
  {
    "objectID": "content/lab7-directional.html",
    "href": "content/lab7-directional.html",
    "title": "Lab 7: Directional \\(t\\)-tests",
    "section": "",
    "text": "This lab has two objectives:\n\nLearn to use the t.test(...) function\nLearn to discern the appropriate direction for a \\(t\\) test\n\nWe’ll use two familiar datasets: body temperature and heart rate measurements for 39 individuals; and data on birth weights and weeks at birth for a sample of 100 births in North Carolina in 2004.\n\nlibrary(tidyverse)\nload('data/temps.RData')\nload('data/nhanes.RData')\nncbirths &lt;- read_csv('data/ncbirths.csv')\n\n\nThe t.test(...) function\nThe t.test(...) function produces both a hypothesis test and an confidence interval, and can be used to obtain either or both in practice.\nLet’s demonstrate with the practice problem you completed most recently: inference on the mean nightly hours of sleep among U.S. adults based on NHANES data.\nThe default behavior of t.test(...) if given no arguments besides a vector of data is to test \\(H_0: \\mu = 0\\) against a two-sided alternative (\\(H_A: \\mu \\neq 0\\)) and provide a 95% confidence interval. There are three key arguments that allow you to adjust this behavior:\n\nmu = ... adjusts the value for the mean in the null hypothesis \\(H_0\\)\n\ndefault mu = 0\n\nalternative = ... adjusts the direction of the alternative, with options\n\n'less' for a lower-sided alternative\n'greater' for an upper-sided alternative\n'two.sided' (default) for a two-sided alternative\n\nconf.level = ... adjusts the confidence level for the interval estimate\n\ndefault conf.level = 0.95\n\n\nThe examples below illustrate this usage. Run each command and look at the output closely to determine what changes.\n\n# extract sleep variable\nsleep &lt;- nhanes$sleephrsnight\n\n# default behavior (these are equivalent)\nt.test(sleep)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = 284.36, df = 3178, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\nt.test(sleep, mu = 0, alternative = 'two.sided', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = 284.36, df = 3178, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n# change null value to 7 hours of sleep\nt.test(sleep, mu = 7, alternative = 'two.sided', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n# change the confidence level\nt.test(sleep, mu = 7, alternative = 'two.sided', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n99 percent confidence interval:\n 6.896032 7.022182\nsample estimates:\nmean of x \n 6.959107 \n\n# change the direction of the alternative\nt.test(sleep, mu = 7, alternative = 'less', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.04741\nalternative hypothesis: true mean is less than 7\n99 percent confidence interval:\n     -Inf 7.016067\nsample estimates:\nmean of x \n 6.959107 \n\n\nFocus for a moment on the last example. In detail, this tests, at the 1% significance level, the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu &lt; 7\n\\end{cases}\n\\]\nWhile the conf.level argument doesn’t affect the \\(p\\)-value, it does imply a significance level – in this case, \\(\\alpha = 0.01\\). So, even though the \\(p\\)-value is less than the conventional level (\\(p &lt; 0.05\\)), it is not less than the implied significance level (here \\(p &gt; 0.01\\)), so the test output implies we’d fail to reject the hypothesis that adults sleep less than 7 hours.\nIn general, it’s important to set the confidence level to correspond to the significance level of the test you wish to perform, so that the test interpretation and interval provided match.\n\n\n\n\n\n\nYour turn 1\n\n\n\nAdjust the arguments of the t.test(...) function to achieve the following:\n\nfind a 90% CI for the mean\ntest whether mean sleep is 6.9 at the 5% level\ntest whether mean sleep is 6.9 at the 1% level\ntest whether mean sleep exceeds 6.9 at the 5% level\ntest whether mean sleep exceeds 6.9 at the 1% level\n\n\n# obtain a 90% confidence interval for the mean hours of sleep\n\n# test whether mean sleep is 6.9 at the 5% level\n\n# test whether mean sleep is 6.9 at the 1% level\n\n# test whether mean sleep exceeds 6.9 at the 5% level\n\n# test whether mean sleep exceeds 6.9 at the 1% level\n\nFor extra practice, write a short interpretation of the results of each test following the style introduced in class.\n\n\n\n\nDistinguishing directional alternatives\nHere we’ll use the temperature/heartrate data to illustrate a variety of directional tests based on questions of interest.\nAs you’re looking over the examples, focus on the correspondence between the questions and the direction of the alternative.\n\n# extract body temperature variable\nbodytemps &lt;- temps$body.temp\n\n# is mean temperature different from 98.6 at the 5% significance level?\nt.test(bodytemps, mu = 98.6, alternative = 'two.sided', conf.level = 0.95)\n\n# is mean temperature less than 98.6 at the 5% significance level?\nt.test(bodytemps, mu = 98.6, alternative = 'less', conf.level = 0.95)\n\n# is mean temperature greater than 98.1 at the 5% significance level?\nt.test(bodytemps, mu = 98.1, alternative = 'greater', conf.level = 0.95)\n\n# is mean temperature greater than 98.1 at the 1% significance level?\nt.test(bodytemps, mu = 98.1, alternative = 'greater', conf.level = 0.99)\n\n# is mean temperature less than 98.9 at the 5% significance level?\nt.test(bodytemps, mu = 98.9, alternative = 'less', conf.level = 0.95)\n\nAs an aside (but an important one!), performing all of these tests together is only meant to illustrate how the function works, not how to perform an analysis. Trying out many tests until you obtain significant results is known as “\\(p\\) hacking”, and is not an acceptable practice.\n\n\n\n\n\n\nYour turn 2\n\n\n\nUsing the heart.rate variable, test the following hypotheses:\n\nIs mean heart rate 65bpm at the 5% level?\nIs mean heart rate 70bpm at the 1% level?\nIs mean heart rate greater than 70bpm at the 1% level?\nIs mean heart rate less than 75bpm at the 10% level?\nIs mean heart rate greater than 75bpm at the 10% level?\n\n\n# extract heart rate variable\n\n# is mean heart rate 65bpm at the 5% level?\n\n# is mean heart rate 70bpm at the 1% level?\n\n# is mean heart rate greater than 70bpm at the 1% level?\n\n# is mean heart rate less than 75bpm at the 10% level?\n\n# is mean heart rate greater than 75bpm at the 10% level?\n\n\n\n\n\nA brief analysis\nNow that you’re familiar with using the t.test(...) function, let’s do something a bit more realistic. Suppose that, using the ncbirths data, you want to perform inference on the number of weeks at birth. We’re told that 40 weeks is typical.\n\nAdvance decisions\nIn advance of looking at the data (or perhaps even having data) we should determine:\n\nthe hypotheses to test\nthe level at which we’ll perform the test\n\nTo make these choices, first note that there’s no obvious directional question to ask here. So, we’ll test whether the mean number of weeks at birth is 40. A 5% significance level is conventional, so we’ll stick with that.\n\n\nAssessing assumptions\nBefore going ahead, let’s inspect the data.\n\n# extract variable of interest\nweeks &lt;- ncbirths$weeks\n\n# inspect distribution\nhist(weeks, breaks = 15)\n\n\n\n\n\n\n\n\nThis is an interesting case, because we do have a left-skewed distribution and there are a few outliers below 30 weeks. However, the sample size is large (\\(n = 100\\)), so the test should still work well regardless.\n\n\nPerforming the test\nFor inference we’ll want to report a test result and interval estimate. Both of these are obtained using t.test(...) as above, but of course we only perform one test/interval calculation.\n\n# inference\nt.test(weeks, mu = 40, alternative = 'two.sided', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  weeks\nt = -5.0421, df = 99, p-value = 2.084e-06\nalternative hypothesis: true mean is not equal to 40\n95 percent confidence interval:\n 37.97938 39.12062\nsample estimates:\nmean of x \n    38.55 \n\n\nTake a moment to inspect the results.\n\n\nInterpreting results\nFollowing the format in class, a report of the results should interpret the test and interval in context, providing supporting statistics parenthetically:\n\nData from North Carolina in 2004 provide strong evidence that the mean number of weeks at birth differs from 40 (T = -5.0421 on 99 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean number of weeks at birth is estimated to be between 37.98 and 39.12 weeks, with a point estimate of 38.55 weeks (SE 0.289).\n\n\n\n\nPractice problems\n\nPerform and interpret the results of inference on the mean birth weight to investigate the claim that the typical birth weight is at least 7 lbs. Carry out inference at the 5% significance level.\n\nDetermine your hypotheses.\nCheck test assumptions.\nPerform the calculations.\nWrite a short report of the results.\n\n\n\n[REVISED] Using the BRFSS data, test whether actual body weight exceeds desired body weight and estimate the difference. Perform the test at the 1% level."
  },
  {
    "objectID": "content/week1-studies.html#todays-agenda",
    "href": "content/week1-studies.html#todays-agenda",
    "title": "Welcome to STAT218",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nCourse logistics\n[lecture] Study designs\n[activity, if time] Distinguishing study types"
  },
  {
    "objectID": "content/week1-studies.html#icebreakers",
    "href": "content/week1-studies.html#icebreakers",
    "title": "Welcome to STAT218",
    "section": "Icebreakers",
    "text": "Icebreakers\nBy show of hands…\n\n\nFirst statistics class ever?\nLast statistics class ever?\nExpect to take STAT313?\nExpect to use statistics for your degree coursework or senior project?\nConsidering a statistics or data science minor?"
  },
  {
    "objectID": "content/week1-studies.html#class-composition",
    "href": "content/week1-studies.html#class-composition",
    "title": "Welcome to STAT218",
    "section": "Class composition",
    "text": "Class composition\nBy the numbers…"
  },
  {
    "objectID": "content/week1-studies.html#statistics-and-uncertainty",
    "href": "content/week1-studies.html#statistics-and-uncertainty",
    "title": "Welcome to STAT218",
    "section": "Statistics and uncertainty",
    "text": "Statistics and uncertainty\n\nLife is full of uncertainty, and this can make a lot of questions hard to answer, because similar situations do not always result in the same outcome.\n\nStatistical thinking: uncertainty is measurable.\nWhat statistics can offer:\n\nprinciples for designing studies and collecting data in order to capture outcome variability\ndata analytic tools to distinguish random from systematic variability\nheuristics to make inferences that account for uncertainty"
  },
  {
    "objectID": "content/week1-studies.html#course-goal-and-scope",
    "href": "content/week1-studies.html#course-goal-and-scope",
    "title": "Welcome to STAT218",
    "section": "Course goal and scope",
    "text": "Course goal and scope\nThe overarching goal of 218 is to introduce you to statistics in a hands-on way that is relevant to your major.\nSo we will focus on:\n\nstatistical thinking, study design, and data analysis\nclassical methods, mostly developed 1900-1940\ncase studies from life sciences"
  },
  {
    "objectID": "content/week1-studies.html#materials",
    "href": "content/week1-studies.html#materials",
    "title": "Welcome to STAT218",
    "section": "Materials",
    "text": "Materials\nComputer/tablet. You’ll need a laptop (preferred) or tablet with keyboard (workable).\nCourse website. All materials are hosted/linked on the course website. I won’t be using Canvas.\nTextbook. Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences. I suggest a $5-15 donation.\nStatistical software. R/RStudio hosted online via posit.cloud workspace. You will need to create an account and purchase a $5/month student subscription."
  },
  {
    "objectID": "content/week1-studies.html#class-meetings",
    "href": "content/week1-studies.html#class-meetings",
    "title": "Welcome to STAT218",
    "section": "Class meetings",
    "text": "Class meetings\nClass meetings will usually consist of a reading quiz, a lecture, a break, and a lab.\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class."
  },
  {
    "objectID": "content/week1-studies.html#assignments",
    "href": "content/week1-studies.html#assignments",
    "title": "Welcome to STAT218",
    "section": "Assignments",
    "text": "Assignments\nYou will have three categories of assignments:\n\nhomework problems: two per class due by next class\ntests: every 2-3 weeks, distributed Wednesday, due Friday\na project: find and present a case study\n\nDeadline policies:\n\none-hour grace period on all deadlines\nfour homework problem sets can be turned in up to 48 hours late without notice\nbesides free lates, extensions must be arranged 24 hours in advance of the deadline"
  },
  {
    "objectID": "content/week1-studies.html#grades",
    "href": "content/week1-studies.html#grades",
    "title": "Welcome to STAT218",
    "section": "Grades",
    "text": "Grades\nEvery graded question/problem is matched to one or more of the 11 course learning outcomes.\n\nQuestions/problems are evaluated as satisfactory (S), needs improvement (NI), or missing (M).\nFor each outcome, the percentage of questions/problems awarded a satisfactory mark is used to determine whether that outcome is fully met, partly met, or not met:\n\nfully met: 80% or more of matched questions satisfactory\npartly met: 50% – 80% of matched questions satisfactory\nnot met: less than 50% of matched questions satisfactory\n\n\nYour course grade is based on how many learning outcomes are fully met. To pass, you must partly or fully meet at least 6 outcomes; for a C-, you must fully meet at least 3 outcomes."
  },
  {
    "objectID": "content/week1-studies.html#important-policies",
    "href": "content/week1-studies.html#important-policies",
    "title": "Welcome to STAT218",
    "section": "Important policies",
    "text": "Important policies\n\nextensions must be confirmed (not simply requested) 24 hours in advance\ncollaboration on homework is encouraged, but everyone involved needs to…\n\nmake a contribution\nwrite up their own work\n\nsubmitting AI-generated content in place of your own work is not acceptable\n\nresponsible use is okay, but not recommended (GPT outputs are misleading)\npenalties for AI plagiarism depend on precedent and severity\n\n\n\n\n\nMinor offense\nMajor offense\nPenalty\n\n\n\n\nFirst\n\nloss of credit and warning\n\n\nSecond\nFirst\nloss of credit and OSRR report\n\n\nThird\nSecond\ncourse failure and second OSRR report"
  },
  {
    "objectID": "content/week1-studies.html#what-is-a-study",
    "href": "content/week1-studies.html#what-is-a-study",
    "title": "Welcome to STAT218",
    "section": "What is a study?",
    "text": "What is a study?\nA study is an effort to collect data in order to answer one or more research questions.\n\nstudies must be well-matched to research questions to provide good answers\nhow data are obtained is just as important as how the resulting data are analyzed\nno analysis, no matter how sophisticated will rescue a poorly conceived study\n\nA study unit is the smallest object or entity that is measured in a study; also called experimental unit or observational unit."
  },
  {
    "objectID": "content/week1-studies.html#two-types-of-studies",
    "href": "content/week1-studies.html#two-types-of-studies",
    "title": "Welcome to STAT218",
    "section": "Two types of studies",
    "text": "Two types of studies\nObservational studies collect data from an existing situation without intervention.\n\nAim is to detect associations and patterns\nCan’t be used to establish causal links\n\nExperiments collect data from a situation in which one or more interventions have been introduced by the investigator.\n\nAim is to draw conclusions about the causal effect of interventions\nStronger form of scientific evidence than an observational study\n\nOften, observational studies are used to explore/generate hypotheses prior to designing an experiment."
  },
  {
    "objectID": "content/week1-studies.html#comparing-study-types",
    "href": "content/week1-studies.html#comparing-study-types",
    "title": "Welcome to STAT218",
    "section": "Comparing study types",
    "text": "Comparing study types\nEither type of study can be used to address a question.\n\n\n\n\n\n\n\n\nQuestion\nObservational study\nExperiment\n\n\n\n\nAre diet and mood related?\nConduct surveys on diet, lifestyle, and affect\nRecruit study participants, assign diets, measure affect\n\n\nIs vaping safer than smoking?\nFollow groups of vapers and smokers over time and record health outcomes\nAmong a group of smokers, assign some to switch to vaping; compare health outcomes over time\n\n\nDo insecticide applications affect soil microbes?\nAnalyze soil samples from farms using different insecticides\n[Your turn]\n\n\n\nCan you think of pros and cons for each study type?"
  },
  {
    "objectID": "content/week1-studies.html#why-does-intervention-matter",
    "href": "content/week1-studies.html#why-does-intervention-matter",
    "title": "Welcome to STAT218",
    "section": "Why does intervention matter?",
    "text": "Why does intervention matter?\nControl over conditions allows a researcher to study causal effects resulting from interventions. This is not possible in observational studies due to the potential for confounding.\n\n\nConfounding: an unobserved condition is associated with both the study condition and the outcome.\n\nFailure to measure and account for confounders potentially distorts observed associations\nExample: a study finds that dog owners live longer, but doesn’t measure exercise; so it might just be the daily walks.\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved variable) --- B(study variable) & C(study outcome) \n\n\n\n\n\n\n\n\nThis is very common in observational studies, because you can’t measure every study condition."
  },
  {
    "objectID": "content/week1-studies.html#antidote-randomization",
    "href": "content/week1-studies.html#antidote-randomization",
    "title": "Welcome to STAT218",
    "section": "Antidote: randomization",
    "text": "Antidote: randomization\nThe ability to control study conditions allows researchers to randomly allocate interventions among study subjects.\n\nRandomization eliminates confounding by isolating the condition(s) of interest:\n\ninterventions are independent of extraneous conditions ⟹ no association possible\nif outcomes differ systematically according to the intervention, you can be certain that it is not an artifact\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved condition) x-.-x B(study condition)\n  A --- C(outcome)"
  },
  {
    "objectID": "content/week1-studies.html#practical-consequences",
    "href": "content/week1-studies.html#practical-consequences",
    "title": "Welcome to STAT218",
    "section": "Practical consequences",
    "text": "Practical consequences\nThe ability to randomize interventions in experiments means:\n\nobserved associations are independent of extraneous factors\nresults can support causal inferences\n\nThe absence of randomization in observational studies means:\n\nconfounding is always possible\nresults may be misleading"
  },
  {
    "objectID": "content/week1-studies.html#experimental-designs",
    "href": "content/week1-studies.html#experimental-designs",
    "title": "Welcome to STAT218",
    "section": "Experimental designs",
    "text": "Experimental designs\nA treatment is an experimental intervention; the design of an experiment refers to how treatments are allocated to study units.\nThe most basic design is:\n\n[balanced] each treatment is replicated an equal number of times\n[randomized] treatments are allocated completely at random to study units\n[no crossover] each study unit receives exactly one treatment\n\nWe’ll call this a completely randomized design. It’s the only kind of experimental design we’re going to consider in STAT218.\nThere are many other designs that we won’t discuss (but see STAT313); these are all about improving experimental efficiency by controlling extraneous variation."
  },
  {
    "objectID": "content/week1-studies.html#data-collection",
    "href": "content/week1-studies.html#data-collection",
    "title": "Welcome to STAT218",
    "section": "Data collection",
    "text": "Data collection\nStudy units should be chosen so as to represent a larger collection.\n\n\n\n\nA study population is a collection of all study units of interest.\nA sample is a subcollection from a population:\n\nrandom if study units have a known chance of inclusion in the sample\nnonrandom or convenience otherwise\n\n\n\nThe gold standard is the simple random sample: each study unit in the population has an equal chance of inclusion in the sample."
  },
  {
    "objectID": "content/week1-studies.html#leap-study",
    "href": "content/week1-studies.html#leap-study",
    "title": "Welcome to STAT218",
    "section": "LEAP Study",
    "text": "LEAP Study\n\n\nLearning early about peanut allergy (LEAP) study:\n\n640 infants in UK with eczema or egg allergy but no peanut allergy enrolled\neach infant randomly assigned to peanut consumption and peanut avoidance groups\n\npeanut consumption: fed 6g peanut protein daily until 5 years old\npeanut avoidance: no peanut consumption until 5 years old\n\nat 5 years old, oral food challenge (OFC) allergy test administered\n13.3% of the avoidance group developed allergies, compared with 1.9% of the consumption group\n\n\n\n\n\nStudy characteristics\n\n\nStudy type: experiment\nStudy population: UK infants with eczema or egg allergy but no peanut allergy\nSample: 640 infants from population\nStudy design: completely randomized design\nTreatments: peanut consumption; peanut avoidance\nStudy outcome: development of peanut allergy by 5 years of age\n\n\n\n\n\n\nStudy results\n\n\nModerated peanut consumption causes a reduction in the likelihood of developing an allergy."
  },
  {
    "objectID": "content/week1-studies.html#checklist-for-next-time",
    "href": "content/week1-studies.html#checklist-for-next-time",
    "title": "Welcome to STAT218",
    "section": "Checklist for next time",
    "text": "Checklist for next time\n\nObtain a copy of the textbook.\nCreate a posit.cloud account and purchase a student subscription. Ensure you can access the stat218-s24 workspace.\nComplete practice problems and reading before class.\nWrite down one question about the reading.\nPrint a paper or virtual copy of the slides."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account",
    "href": "content/week1-studies.html#posit-cloud-account",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nGo to: course webpage &gt; syllabus &gt; materials. Then look for the link to join the class workspace:"
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-1",
    "href": "content/week1-studies.html#posit-cloud-account-1",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nFollow prompts to create an account. Use your Cal Poly email."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-2",
    "href": "content/week1-studies.html#posit-cloud-account-2",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nOnce your email is verified, return to posit.cloud (or click the link in the syllabus again), and join the class workspace."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-3",
    "href": "content/week1-studies.html#posit-cloud-account-3",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nUpgrade your account to the student plan. Input payment details."
  },
  {
    "objectID": "content/week1-studies.html#printing-slides",
    "href": "content/week1-studies.html#printing-slides",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nOpen menu from lower left"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-1",
    "href": "content/week1-studies.html#printing-slides-1",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nNavigate to tools"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-2",
    "href": "content/week1-studies.html#printing-slides-2",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nSelect PDF export mode"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-3",
    "href": "content/week1-studies.html#printing-slides-3",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\n\n\nThen print from browser to PDF\n\n\nI suggest landscape layout and either 1, 2 or 4 slides per page\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-inference.html#todays-agenda",
    "href": "content/week4-inference.html#todays-agenda",
    "title": "Introduction to inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nTest 1 discussion\n[lecture] Inference vs. description, point estimation, interval estimation\n[lab] Exploring interval coverage"
  },
  {
    "objectID": "content/week4-inference.html#description-vs.-inference",
    "href": "content/week4-inference.html#description-vs.-inference",
    "title": "Introduction to inference",
    "section": "Description vs. inference",
    "text": "Description vs. inference\n\nStatistical inferences are statements about population statistics based on samples.\n\nTo appreciate the meaning of this, consider the contrast with descriptive findings, which are statements about sample statistics:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nA descriptive finding is:\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nThe corresponding inference would be:\nThe median percent change in strength is highest among adults with genotype TT."
  },
  {
    "objectID": "content/week4-inference.html#inference-or-description",
    "href": "content/week4-inference.html#inference-or-description",
    "title": "Introduction to inference",
    "section": "Inference or description?",
    "text": "Inference or description?\nSee if you can tell the difference:\n\nThe proportion of children who developed a peanut allergy was 0.133 in the avoidance group.\nThe proportion of children who develop peanut allergies is estimated to be 0.133 when peanut protein is avoided in infancy.\nThe average lifetime of mice on normal 85kCal diets is estimated to be 32.7 months.\nThe 57 mice on the normal 85kCal diet lived 32.7 months on average.\nThe relative risk of a CHD event in the high trait anger group compared with the low trait anger group was 2.5.\nThe relative risk of a CHD event among adults with high trait anger compared with adults with low trait anger is estimated to be 2.5."
  },
  {
    "objectID": "content/week4-inference.html#random-sampling",
    "href": "content/week4-inference.html#random-sampling",
    "title": "Introduction to inference",
    "section": "Random sampling",
    "text": "Random sampling\n\nSampling establishes the link (or lack thereof) between a sample and a population.\n\n\n\n\n\nIn a simple random sample, units are chosen in such a way that every individual in the population has an equal probability of inclusion. For the SRS:\n\nsample statistics mirror population statistics (sample is representative)\nsampling variability depends only on population variability and sample size"
  },
  {
    "objectID": "content/week4-inference.html#population-models",
    "href": "content/week4-inference.html#population-models",
    "title": "Introduction to inference",
    "section": "Population models",
    "text": "Population models\n\nInference consists in using statistics of a random sample to ascertain population statistics under a population model.\n\nA population model represents the distribution of values you’d see if you measured every individual in the study population. We think of the sample values as a random draw.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Density is an alternative scale to frequency that is independent of population size.)"
  },
  {
    "objectID": "content/week4-inference.html#point-estimates",
    "href": "content/week4-inference.html#point-estimates",
    "title": "Introduction to inference",
    "section": "Point estimates",
    "text": "Point estimates\n\nSample statistics, viewed as guesses for the values of population statistics, are called ‘point estimates’.\n\nWe’ll focus on inferences involving the following:\n\n\n\nPopulation statistic\nParameter\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)"
  },
  {
    "objectID": "content/week4-inference.html#a-difficulty",
    "href": "content/week4-inference.html#a-difficulty",
    "title": "Introduction to inference",
    "section": "A difficulty",
    "text": "A difficulty\n\nDifferent samples yield different estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample means:\n\n\n\n\n\n\n\n\n\nsample.1\nsample.2\n\n\n\n\n5.093\n5.136\n\n\n\n\n\n\nestimates are close but not identical\nthe population mean can’t be both 5.093 and 5.136\nprobably neither estimate is exactly correct\n\nEstimation error and sample-to-sample variability are inherent to point estimation."
  },
  {
    "objectID": "content/week4-inference.html#simulating-sampling-variability",
    "href": "content/week4-inference.html#simulating-sampling-variability",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\n\n\n\n\n\n\n\n\n\n\n\n\nThese are 20 random samples with the sample mean indicated by the dashed line and the population distribution and mean overlaid in red.\n\nsample size \\(n = 20\\)\nfrequency distributions differ a lot\nsample means differ some\n\nWe can actually measure this variability!"
  },
  {
    "objectID": "content/week4-inference.html#simulating-sampling-variability-1",
    "href": "content/week4-inference.html#simulating-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\nIf we had means calculated from a much larger number of samples, we could make a frequency distribution for the values of the sample mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample\n1\n2\n\\(\\cdots\\)\n10,000\n\n\nmean\n4.957\n5.039\n\\(\\cdots\\)\n5.24\n\n\n\n\nWe could then use the usual measures of center and spread to characterize the distribution of sample means.\n\nmean of \\(\\bar{x}\\): 5.0373228\nstandard deviation of \\(\\bar{x}\\): 0.2387869\n\n\nAcross 10,000 random samples of size 20, the typical sample mean was 5.04 and the root average squared distance of the sample mean from its typical value was 0.239."
  },
  {
    "objectID": "content/week4-inference.html#sampling-distributions",
    "href": "content/week4-inference.html#sampling-distributions",
    "title": "Introduction to inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nWhat we are simulating is known as a sampling distribution: the frequency of values of a statistic across all possible random samples.\n\n\n\n\n\n\n\n\n\n\n\n\nProvided data are from a random sample, the sample mean \\(\\bar{x}\\) has a sampling distribution with\n\nmean \\(\\color{red}{\\mu}\\) (population mean)\nstandard deviation \\(\\color{red}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\nregardless of its exact form.\n\n\nIn other words, across all random samples of a fixed size…\n\nThe average value of the sample mean is the population mean.\nThe average squared error (sample mean - population mean)\\(^2\\) is \\(\\frac{\\sigma^2}{n}\\)"
  },
  {
    "objectID": "content/week4-inference.html#effect-of-sample-size",
    "href": "content/week4-inference.html#effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\) is inversely proportional to sample size.\n\n\n\n\n\n\n\n\n\n\n\n\nAs sample size increases…\n\naccuracy remains the same\nestimates get more precise\nskewness vanishes"
  },
  {
    "objectID": "content/week4-inference.html#measuring-sampling-variability",
    "href": "content/week4-inference.html#measuring-sampling-variability",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nIn practice \\(\\sigma\\) is not known so we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared error of the sample mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-inference.html#reporting-point-estimates",
    "href": "content/week4-inference.html#reporting-point-estimates",
    "title": "Introduction to inference",
    "section": "Reporting point estimates",
    "text": "Reporting point estimates\nIt is common style to report the value of a point estimate with a standard error given parenthetically.\n\n\nStatistics from full NHANES sample:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\n\n\n\n\n5.043\n1.075\n3179\n\n\n\n\n\n\n\nThe mean total cholesterol among the population is estimated to be 5.043 mmol/L (SE 0.019)\n\n\n\nThis style of report communicates:\n\nparameter of interest\nvalue of point estimate\nerror/variability of point estimate"
  },
  {
    "objectID": "content/week4-inference.html#interval-estimation",
    "href": "content/week4-inference.html#interval-estimation",
    "title": "Introduction to inference",
    "section": "Interval estimation",
    "text": "Interval estimation\n\nAn interval estimate is a range of plausible values for a population parameter.\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.043 \\pm 2\\times 0.0191 = (5.005, 5.081)\\]\n\nIn R:\n\navg.totchol &lt;- mean(totchol)\nse.totchol &lt;- sd(totchol)/sqrt(length(totchol))\navg.totchol + c(-2, 2)*se.totchol\n\n[1] 5.004817 5.081059\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab5-intervals.html",
    "href": "content/lab5-intervals.html",
    "title": "Lab 5: Confidence intervals",
    "section": "",
    "text": "The objective of this lab is to learn to compute confidence intervals for a population mean, and more specifically, to learn to adjust interval coverage by calculating appropriate critical values. Since you already learned to calculate an interval in the last lab, the basic mechanics of the arithmetic are familiar.\nWe’ll use data from a sample of 100 births in North Carolina in 2004. To change things up a little, the data is stored as a .csv file (not an .RData file). If you were to download and open this file on your computer, it would likely appear as a spreadsheet (try if you’re curious). Read in the data using the command below.\n\n# read in data and preview\nncbirths &lt;- read_csv('data/ncbirths.csv')\nhead(ncbirths)\n\n# A tibble: 6 × 4\n  mother.age weeks birth.weight sex   \n       &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt; \n1         36    39         7.69 male  \n2         35    40         8.88 male  \n3         40    40         9    female\n4         37    40         7.94 male  \n5         35    28         1.63 female\n6         25    40         8.75 female\n\n\nRecall that the general formula for an interval is:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\] Throughout this lab, you’ll manipulate the coverage by obtaining different values of the critical value \\(c\\). We’ll start the back-of-the-envelope approach following the empirical rule.\n\nIntervals using the empirical rule\nThe empirical rule allows us to construct intervals using whole number multiples of the standard error and obtain the following approximate coverages:\n\n\\(c = 1\\) gives 68% coverage\n\\(c = 2\\) gives 95% coverage\n\\(c = 3\\) gives 99.7% coverage\n\nAn approximate 95% confidence interval for the mean birth weight (lbs) in NC in 2004 is:\n\n# retrieve variable of interest\nbweight &lt;- ncbirths$birth.weight\n\n# interval ingredients\nbweight.mean &lt;- mean(bweight)\nbweight.sd &lt;- sd(bweight)\nbweight.n &lt;- length(bweight)\n\n# standard error\nbweight.se &lt;- bweight.sd/sqrt(bweight.n)\n\n# 95% interval using empirical rule\nbweight.mean + c(-2, 2)*bweight.se\n\n[1] 6.89267 7.46633\n\n\nFollowing class discussion, we’d interpret this as follows:\n\nWith 95% confidence, the mean birth weight of babies born in North Carolina in 2004 is estimated to be between 6.893 and 7.466 lbs.\n\nTo compute a 68% interval, we need only change the critical value. All of the above remains the same except the last command, which we change to:\n\n# 68% interval using empirical rule\nbweight.mean + c(-1, 1)*bweight.se\n\n[1] 7.036085 7.322915\n\n\nNotice that the interval got narrower: a more precise estimate can be given at a reduced coverage rate (which of course means the estimate is wrong more often).\n\nWith 68% confidence, the mean birth weight of babies born in North Carolina in 2004 is estimated to be between 7.036 and 7.323 lbs.\n\n\n\n\n\n\n\nYour turn 1\n\n\n\nCalculate and interpret a 99.7% confidence interval for the mean number of weeks at birth.\n\n# retrieve variable of interest (no. weeks at birth)\n\n# interval ingredients\n\n# standard error\n\n# 99.7% interval for mean number of weeks at birth using empirical rule\n\n\n\nBecause we’re only changing the critical value here, let’s save some work and write a simple function to calculate an interval from a vector of values and a critical value. (You don’t need to understand the syntax or be able to write functions in R, as this is a programming technique, but it may interest you to see how it can be done.)\n\n# run this before continuing, but ignore unless interested\nmake_ci &lt;- function(vec, cval){\n  vec.mean &lt;- mean(vec)\n  vec.mean.se &lt;- sd(vec)/sqrt(length(vec) - 1)\n  interval &lt;- vec.mean + c(-1, 1)*cval*vec.mean.se \n  names(interval) &lt;- c('lwr', 'upr')\n  return(interval)\n}\n\nWe can use this function to compute an interval a bit more efficiently. Check that the following give the same intervals as obtained above using fully manual calculations.\n\n# 95% interval\nmake_ci(bweight, cval = 2)\n\n     lwr      upr \n6.891225 7.467775 \n\n# 68% interval\nmake_ci(bweight, cval = 1)\n\n     lwr      upr \n7.035362 7.323638 \n\n\nTry it yourself to get the hang of using this function.\n\n\n\n\n\n\nYour turn 2\n\n\n\nUse make_ci(...) to compute 95% and 99.7% confidence intervals for the mean number of weeks at birth.\n\n# 99.7% interval for mean number of weeks at birth, using make_ci(...)\n\n# 95% interval for mean number of weeks at birth, using make_ci(...)\n\n\n\n\n\nIntervals using \\(t\\) critical values\nIf you want to construct an interval with a coverage other than 68%, 95%, or 99.7%, you’ll need to use a different critical value. Instead of a whole number, you’ll need the \\(p\\)th quantile from the \\(t_{n-1}\\) model where:\n\\[p = \\left[1 - \\left(\\frac{1 - \\text{coverage}}{2}\\right)\\right]\\] This is perhaps a little more complex than it looks. You could probably determine which quantile to use in your head – the quantile you want is just the midpoint between your coverage level and 1. Consider the following examples:\n\nfor a 95% interval, use \\(p = 0.975\\)\nfor an 80% interval, use \\(p = 0.9\\)\nfor a 99% interval, use \\(p = 0.995\\)\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nWhich quantile would you use for…\n\nA 96% confidence interval?\nAn 85% confidence interval?\nA 98% confidence interval?\n\n\n\n\nCalculating quantiles\nThe qt(...) function in R will calculate quantiles for you. It takes two arguments: which quantile you want (\\(p\\)) and the degrees of freedom for the \\(t_{n - 1}\\) model. The degrees of freedom is one less than the sample size in this case (\\(n - 1\\)). The following commands illustrate the calculation.\n\n# for 80% interval from n = 15 observations, use this quantile\nqt(p = 0.9, df = 14)\n\n[1] 1.34503\n\n# for a 92% interval from n = 30 observations, use this quantile\nqt(p = 0.96, df = 29)\n\n[1] 1.814238\n\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nCalculate \\(t\\) quantiles for the following scenarios:\n\n90% interval from 27 observations\n95% interval from 18 observations\n99% interval from 51 observations\n\n\n# quantile for a 90% interval from 27 observations\n\n# quantile for a 95% interval from 18 observations\n\n# quantile for a 99% interval from 51 observations\n\n\n\nThese quantiles are the critical values you’d use to construct an interval with the specified coverage and number of observations.\n\n\nConstructing intervals\nIf we want a confidence interval for the mean with a specific coverage, first determine which quantile is needed as above and compute it, and then construct the interval as usual using that quantile as the critical value.\nFor example, if we want confidence intervals for the mean birth weight:\n\n# 98% ci for mean birth weight\ncrit.val &lt;- qt(p = 0.99, df = 99)\nmake_ci(bweight, cval = crit.val)\n\n     lwr      upr \n6.838671 7.520329 \n\n# 95% ci for mean birth weight\ncrit.val &lt;- qt(p = 0.975, df = 99)\nmake_ci(bweight, cval = crit.val)\n\n     lwr      upr \n6.893499 7.465501 \n\n# 90% ci for mean birth weight\ncrit.val &lt;- qt(p = 0.95, df = 99)\nmake_ci(bweight, cval = crit.val)\n\n     lwr      upr \n6.940175 7.418825 \n\n\nAs a matter of interest, note that the critical value for the 95% interval is 1.9842. Technically, this is the value that provides an interval with 95% coverage; the approximation of 2 provided by the empirical rule is just that – an approximation.\n\n\n\n\n\n\nYour turn 5\n\n\n\nConstruct and interpret a 99% confidence interval for the mean number of weeks at birth.\n\n# 99% ci for mean number of weeks at birth\n\n\n\nNow that you have a sense of the critical value calculation, it’s helpful to remind yourself how to make the interval fully from scratch.\n\n\n\n\n\n\nYour turn 6\n\n\n\nRepeat the previous calculation, but without using the make_ci(...) function. You should obtain exactly the same numerical result.\n\n## repeat last calculation but fully 'by hand'\n\n# point estimate and standard error\n\n# critical value\n\n# interval\n\n\n\n\n\n\nWorking backwards to determine coverage\nNow let’s try doing the above backwards. If you’re given a confidence interval and you know the summary statistics, you can figure out the interval coverage by solving for the critical value and using the pt(...) function. Really, you only need to know the standard error (or sample size and standard deviation) to do this.\nFirst, find the margin of error by taking half the interval width.\n\\[\n\\text{margin of error} = \\frac{\\text{upr} - \\text{lwr}}{2}\n\\]\nThen, divide by the standard error to solve for the critical value:\n\\[\nc = \\frac{\\text{margin of error}}{SE(\\bar{x})}\n\\] Lastly, find the coverage as the area of the sampling distribution below the critical value: \\[\n\\text{coverage} = P(T &lt; c)\n\\]\nIn R:\n\n# example interval for mean birth weight\nbweight.ci &lt;- c(7.030077, 7.328923)\n\n# margin of error (half the width)\nbweight.ci.me &lt;- diff(bweight.ci)/2\n\n# divide out standard error to get critical value\ncrit.val &lt;- bweight.ci.me/bweight.se\n\n# coverage\npt(q = crit.val, df = bweight.n - 1)\n\n[1] 0.85\n\n\nTake a moment to align the R commands with the calculations shown above and check to make sure you see how this is consistent with the way we formed the interval in the first place. Then try it on your own.\n\n\n\n\n\n\nYour turn 7\n\n\n\nDetermine the coverage for the interval below for the mean number of weeks at birth.\n\n# interval for mean number of weeks at birth\nbweeks.ci &lt;- c(37.79485, 39.30515)\n\n# margin of error (half the width)\n\n# divide out standard error to get critical value\n\n# coverage"
  },
  {
    "objectID": "content/test3.html",
    "href": "content/test3.html",
    "title": "Test 3",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/24. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test3.html#instructions",
    "href": "content/test3.html#instructions",
    "title": "Test 3",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/24. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test3.html#problems",
    "href": "content/test3.html#problems",
    "title": "Test 3",
    "section": "Problems",
    "text": "Problems\n\n[L9] Recall that the FAMuSS study, which we have used for several in-class examples, sought to determine whether the ACTN3 gene is associated with differential physical response to strength training. Here you’ll answer that question.\n\nUse an appropriate graphical summary to check whether assumptions are met for an ANOVA to test for differences in mean change in nondominant arm strength by genotype.\nFit the ANOVA model, construct the ANOVA table, and determine whether the study provides evidence that mean change in nondominant arm strength differs by genotype. Carry out your inference at the 5% significance level.\nIf your test in (b) indicated significant differences at the 5% level, carry out tests for pairwise differences to determine which genotypes differ. Provide interval estimates for any significant differences.\nCheck whether there are any significant differences in mean change in dominant arm strength by genotype at the 5% significance level and use pairwise comparisons to identify any such differences; do your conclusions match the analysis of change in nondominant arm strength?\n\n[L9] The output below shows: (1) an ANOVA model fit to data on the average number of flowers per meadowfoam plant grown in an experimental plot and the light intensity (μmol/m^2/sec) that the plot received during the experiment; (2) tests for contrasts with the lowest intensity level. The intensity levels were randomly allocated among the plots.\n\nConstruct the ANOVA table to test for an effect of light intensity on meadowfoam flowering. Interpret the result of the test in context.\nHow many observations and treatments were there in the expriment?\nWhat is the estimated standard deviation of the average number of flowers per plant across plots?\nBased on the output beneath the model fit summary, what is the effect of increasing intensity on flowering? Explain.\n\n\n\n\nCall:\n   aov(formula = flowers ~ intensity, data = meadow)\n\nTerms:\n                intensity Residuals\nSum of Squares   2683.514  1654.422\nDeg. of Freedom         5        18\n\nResidual standard error: 9.587093\nEstimated effects are balanced\n\n\n contrast                    estimate   SE df t.ratio p.value\n intensity300 - intensity150    -9.12 6.78 18  -1.346  0.5357\n intensity450 - intensity150   -13.38 6.78 18  -1.973  0.2234\n intensity600 - intensity150   -23.23 6.78 18  -3.426  0.0130\n intensity750 - intensity150   -27.75 6.78 18  -4.093  0.0030\n intensity900 - intensity150   -29.35 6.78 18  -4.329  0.0018\n\nP value adjustment: dunnettx method for 5 tests \n\n\n\n[L9] To study the influence of ocean grazers on regeneration rates of seaweed in the intertidal zone, a researcher scraped rock plots free of seaweed and observed the degree of regeneration when certain types of seaweed-grazing animals were denied access. The grazers were limpets (L), small fishes (f) and large fishes (F). Each plot received one of six treatments named by which grazers were allowed access, or a control (C) in which no grazers were allowed access. The grazers dataset contains observations of percent of regenerated seaweed for 96 plots along with which treatment the plot received.\n\nConstruct boxplots to inspect the distributions of percent cover regeneration among plots by treatment group. Assess whether assumptions for ANOVA seem to be met.\nHow many replicates (i.e., plots) per treatment group are there?\nFit an ANOVA model to test for an effect of grazers on seaweed regeneration. Carry out your inference at the 1% significance level.\nTest the appropriate contrasts to determine which grazers have a significant effect (at the 1% level) on seaweed regeneration relative to no grazers.\nEstimate any significant effects identified in (d) with confidence intervals at the appropriate confidence level for the tests you performed.\n\n\n\nExtra credit\n\n[L5] The logging dataset contains measurements on the number of tree seedlings lost per transect in nine logged (L) and seven unlogged (U) plots affected by the Oregon Biscuit Fire.\n\nUse an appropriate nonparametric rank test to assess whether the percentage of seedlings lost differed between logged and unlogged areas.\nWhich type of area saw the lesser impact of the fire? Support your answer quantitatively with the inference from (a)."
  },
  {
    "objectID": "content/week10-activity-linefitting.html",
    "href": "content/week10-activity-linefitting.html",
    "title": "Week 10 activity: line fitting",
    "section": "",
    "text": "This activity is a self-paced warmup in preparation for learning about linear regression. Simply put, linear regression consists of fitting a line to data in order to describe the relationship between two numeric variables.\nBy ‘fitting’ we do mean something a bit more specific than drawing any old line through a scatterplot, but it’s worth emphasizing that the criteria we’ll develop later in class, though formal, do reflect several common intuitions about what a ‘well-fitting’ line looks like. So, the goal of this activity is to explore some of those intuitions.\nThe over-arching question I’d like you to consider as you work through this activity is: what makes a line “good” as a representation of the relationship between two variables?\nWe’ll use two datasets:\n\nprevend, which contains measurements of cognitive assessment score and age for 208 adults\nmammals, which contains measurements of brain weight (g) and body weight (kg) for 62 mammal species\n\n\nlibrary(tidyverse)\nload('data/prevend.RData')\nload('data/mammals.RData')\nhand.fit &lt;- readRDS('fns/handfit.rds')\nsim.by.cor &lt;- readRDS('fns/simbycor.rds')\n\n\nScatterplots\nTo start, let’s review scatterplots. These are straightforward to construct using plot(...); in addition to what you have seen before, the commands below show you an alternate syntax for making scatterplots using a formula specification of the form y ~ x, as well as how to adjust the labels.\n\n# variables of interest\nage &lt;- prevend$age\nrfft &lt;- prevend$rfft\n\n# scatterplot of RFFT score against age\nplot(age, rfft)\n\n\n\n\n\n\n\n# alternate syntax: formula\nplot(rfft ~ age)\n\n\n\n\n\n\n\n# change labels\nplot(rfft ~ age, xlab = 'age', ylab = 'RFFT score')\n\n\n\n\n\n\n\n\n\nThere is a negative linear relationship between RFFT score and age, suggesting cognitive function decreases with age.\n\nScatterplots allow for visual assessment of trend. A trend is linear if it seems to follow a straight line; linear trends are positive if they increase from left to right and negative if they decrease from left to right. Most of the time, you won’t see a perfectly clear straight-line relationship, so what we really mean by this is that (a) there’s a trend and (b) a line would describe it about as well as any other path drawn through the scatter.\n\n\n\n\n\n\nYour turn 1\n\n\n\nUsing the mammals dataset, construct a scatterplot of log.brain (y) against log.body (x) using the formula syntax. Label the axes “log body weight (kg)” and “log brain weight (g)”. Describe the trend you see in the data.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere is a positive linear relationship between log brain weight and log body weight, suggesting that larger mammals in general have larger brains.\n\n# variables of interest\nlog.brain &lt;- mammals$log.brain\nlog.body &lt;- mammals$log.body\n\n# scatterplot of log brain weight against log body weight \nplot(log.brain ~ log.body, \n     xlab = 'log body weight (kg)', \n     ylab = 'log brain weight (g)')\n\n\n\n\n\n\n\n\n\n\n\nIn most contexts one variable is of direct interest, and the other is considered as potentially explaining the variable of interest. For example:\n\nin the prevend data, age might explain cognitive functioning\nin the mammals data, body size might explain brain size\n\nPlots are typically oriented so that the variable of interest, or response, is on the vertical (y) axis and the explanatory variable is on the horizontal (x) axis. Throughout the rest of this activity, you’ll see these terms and the notation \\(x, y\\) used correspondingly.\n\n\nCorrelation\nBoth of the above examples show linear trends of differing strength. The prevend data are not as close to falling on a straight line as the mammals data, so there is a stronger linear relationship between log brain and log body weights than there is between RFFT score and age.\nFurther, the two examples show trends of differing direction: the trend in the prevend data is negative (RFFT declines with age) whereas the trend in the mammals data is positive (brain size increases with body size).\nThe direction and strength of a linear relationship can be measured directly by the correlation coefficient, defined as:\n\\[\nr = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\n\\] The correlation coefficient is always between -1 and 1:\n\n\\(r \\rightarrow 0\\) indicates no relationship\n\\(r \\rightarrow 1\\) indicates a perfect linear relationship\n\\(r &gt; 0\\) indicates a positive linear relationship\n\\(r &lt; 0\\) indicates a negative linear relationship\n\nCorrelations are simple to compute in R:\n\n# correlation between age and rfft\ncor(age, rfft)\n\n[1] -0.635854\n\n\nInterpretations should note the direction and strength. Strength is a little subjective, but as a rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case:\n\nThere is a moderate negative linear relationship between age and RFFT.\n\n\n\n\n\n\n\nYour turn 2\n\n\n\nCompute the correlation between log brain weight and log body weight; interpret the correlation in context.\n\n# correlation between log brain and log body\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere is a strong positive linear relationship between log body weight and log brain weight.\n\n# correlation between log brain and log body\ncor(log.brain, log.body)\n\n[1] 0.9595748\n\n\n\n\n\nGenerally, the stronger the correlation, the easier it will be to visualize a line passing through the data.\n\n\n\n\n\n\nExploration: visualizing correlations\n\n\n\nThe sim.by.cor function simulates observations of two numeric variables that have a specified correlation r. It returns a plot of the simulated data showing the scatter, the \\(y = x\\) line, and the sample correlation. The sample correlation will differ a little bit from the true value, so don’t be surprised if the sample correlation doesn’t exactly match the value of r that you input.\nUse this to explore the correspondence between the sample correlation and the visual appearance of scatterplots:\n\ntry several negative correlations\ntry several positive correlations\ntry both large (near \\(\\pm\\) 1) and small (near 0) values\n\nFor each value of r you try, run the command a few times to see several simulated datasets.\n\n# adjust r to see what different correlations look like in simulated data\nsim.by.cor(r = 0, n = 500)\n\n\n\n\n\n\n\n\n\n\n\n\nHand-fitting lines\nNow we’ll turn to actually drawing lines through data scatter to approximate linear relationships. We’ll do this by specifying a slope and (sometimes) intercept, so a quick reminder of the slope-intercept form for the equation of a line may be handy:\n\\[\ny = a + bx\n\\]\n\nthe intercept \\(a\\) is the value at which the line passes through the vertical axis (the value of \\(y\\) when \\(x = 0\\))\nthe slope \\(b\\) is the amount by which \\(y\\) increases for each increment in \\(x\\) (sometimes called ‘rise over run’)\n\nI’ve written a function called hand.fit that allows you to construct a scatterplot and easily add a line by specifying the slope and intercept.\nFirst we’ll consider “eye-fitting” a line by trying out different slopes and intercepts until a combination is found that looks good. For example, for the prevend dataset a slope of -1.3 and an intercept of 145 look reasonable:\n\n# i adjusted a and b until the line reflected the trend well\nhand.fit(x = age, y = rfft, a = 145, b = -1.3)\n\n\n\n\n\n\n\n\nConsider what this line says about the relationship between RFFT and age:\n\nWith each year of age, RFFT score decreases by 1.3 points.\n\nThe intercept doesn’t have a clear interpretation, because \\(x = 0\\) is not meaningful for this dataset: technically, it says that RFFT for a 0-year-old would be 145, which of course is nonsensical.\n\n\n\n\n\n\nYour turn 3\n\n\n\nFor the log brain weight and log body weight variables in the mammals dataset, experiment with the intercept a and the slope b until you find a pair of values that seem to reflect the trend well. Interpret the relationship described by the line.\nI suggest starting with a = 0 and b = 0. Note that if the values are too extreme, the line may not appear at all on the plot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are many possible lines that visually fit the data well. One possible solution:\n\n# adjust a and b until the line reflects the trend well\nhand.fit(x = log.body, y = log.brain, a = 1.9, b = 0.85)\n\n\n\n\n\n\n\n\nThis line says that each 1-unit increment in the log of a mammal’s body weight is associated with a 0.85-unit increase in log brain weight. (There is a better interpretation, but it’s a bit more mathematically complicated.)\n\n\n\nNow we’ll develop some ideas that will help to assess how good these lines are as representations of the \\(x-y\\) relationship.\n\n\nResiduals\nA residual is something left over; in this context, the difference between the line and a data point. There is one residual for every data point. If we denote the value of the response variable on the line as:\n\\[\n\\hat{y}_i = a + bx\n\\]\nThen the residual for the \\(i\\)th observation is:\n\\[\ne_i = y_i - \\hat{y}_i\n\\]\nResiduals help to capture the fit of the model, because taken together, they convey how close the line is to each data point.\n\nVisualizing residuals\nAdding the argument res = T to the hand.fit(...) function will show the residuals visually as vertical red line segments.\n\n# add residuals to the plot\nhand.fit(x = age, y = rfft, a = 145, b = -1.3, res = T)\n\n\n\n\n\n\n\n\nData points below the line have negative residuals, and data points above the line have positive residuals.\n\n\n\n\n\n\nYour turn 4\n\n\n\nPlot the residuals for the line you identified in the last exercise.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the values I identified, the residuals look like this:\n\n# add residuals to your line from before\nhand.fit(x = log.body, y = log.brain, a = 1.9, b = 0.85, res = T)\n\n\n\n\n\n\n\n\n\n\n\nTake a moment to consider what the residuals in your plot and the example above convey about line fit:\n\nWhat would the residuals look like if fit is good?\nWhat would they look like if fit is good?\n\n\n\nBias\nIn this context, bias refers to how often the line overestimates and underestimates data values:\n\nif the line tends to overestimate and underestimate equally often, it is unbiased\nif the line tends to overestimate more often or underestimate more often, then it is biased\n\nThe average residual captures whether the line is biased:\n\npositive average residual \\(\\longrightarrow\\) underestimates more often \\(\\longrightarrow\\) negative bias\nnegative average residual \\(\\longrightarrow\\) overestimates more often \\(\\longrightarrow\\) positive bias\naverage residual near zero \\(\\longrightarrow\\) unbiased\n\nSo, define as a measure of bias the negative average residual: \\[\n\\text{bias} = -\\frac{1}{n}\\sum_{i = 1}^n e_i\n\\]\nIn the example above, the bias of 4.5 means that on average, the line overestimates RFFT score by 4.5 points. If we increase the intercept, we will overestimate even more often, and thus increase the bias:\n\n# obvious positive bias\nhand.fit(x = age, y = rfft, a = 165, b = -1.3, res = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn 5\n\n\n\nUsing the log brain and log body weights in the mammals dataset, see if you can fit lines with positive, negative, and no bias.\n\nFind a line that has obvious positive bias.\nFind a line that has obvious negative bias.\nFind a line that has low bias.\n\nFor simplicity, keep your slope the same, and just change the intercept.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIndividual results will vary a lot. Here are a few scenarios:\n\n# positive bias\nhand.fit(x = log.body, y = log.brain, a = 3, b = 0.85, res = T)\n\n\n\n\n\n\n\n# negative bias\nhand.fit(x = log.body, y = log.brain, a = 1, b = 0.85, res = T)\n\n\n\n\n\n\n\n# low bias \nhand.fit(x = log.body, y = log.brain, a = 2, b = 0.85, res = T)\n\n\n\n\n\n\n\n\n\n\n\nClearly, low bias is a desirable property of a fitted line. However, it is not the only desirable property.\n\n\nError\nThe total magnitude of the residuals conveys how close the line is to the data scatter in general and thus, error. One way to measure this is by the sum of squared residuals:\n\\[\n\\text{SSE} = \\sum_{i = 1}^n e_i^2\n\\]\nThis quantity will be smallest whenever the line is as close as it can be to as many data points as possible at once. As a result, a good fit will generally have low error.\nWhile high error can arise from bias, it is also possible for a line with no bias to have high error. For example, a horizontal line through the mean RFFT will have zero bias, but still be a bad estimate:\n\n# no bias but high error\nhand.fit(x = age, y = rfft, a = mean(rfft), b = 0, res = T)\n\n\n\n\n\n\n\n\nThe reason the bias is zero is that the residuals balance each other out in the average. It is a mathematical fact that whenever the line passes exactly through the point defined by the sample mean of each variable – i.e., the point \\((\\bar{x}, \\bar{y})\\) – the bias is exactly zero.\n\n\n\n\n\n\nYour turn 6\n\n\n\nMimic the example above using the log brain and log body weights in the mammals dataset: find a line that has low bias but high error.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIndividual results will vary. A horizontal line through the mean log brain weight will work. Here’s a more extreme example:\n\n# low bias but high error\nhand.fit(x = log.body, y = log.brain, \n         a = mean(log.brain) + mean(log.body), \n         b = -1, res = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest unbiased line\nConsidering that there is an easy way to ensure the bias of the line is zero – constrain it to pass through the center \\((\\bar{x}, \\bar{y})\\) – we should concern ourselves with finding the unbiased line with lowest error. Finding the best unbiased line reduces, essentially, to finding a slope.\nOmitting the intercept altogether in the hand.fit(...) function will constrain it to pass through the center, add a point to visualize the center of the data, and print the intercept resulting from the constraint:\n\n# omit intercept to force the line through the center point\nhand.fit(x = age, y = rfft, b = -1.2, res = T)\n\n\n\n\n\n\n\n\nintercept     slope \n 134.6375   -1.2000 \n\n\nOnce we have a slope in hand, the intercept for the fitted line can be found by direct arithmetic:\n\n# calculate intercept by direct arithmetic\nmean(rfft) - (-1.2)*mean(age)\n\n[1] 134.6375\n\n\nSo, finding the best unbiased line amounts to fine-tuning the slope until SSE reaches a minimum. For the prevend data, the best unbiased line is about this:\n\n# fine tune slope to minimize sse by hand\nhand.fit(x = age, y = rfft, b = -1.19, res = T)\n\n\n\n\n\n\n\n\nintercept     slope \n 134.0515   -1.1900 \n\n\nSo the best unbiased line results around \\(a = 134.05\\) and \\(b = -1.19\\).\nThere is an analytic solution for the slope of the best unbiased line:\n\\[\na = r\\times \\frac{s_y}{s_x}\n\\]\n\n# analytic solution \nhand.fit(age, rfft, res = T,\n         b = cor(rfft, age)*sd(rfft)/sd(age))\n\n\n\n\n\n\n\n\n intercept      slope \n134.098052  -1.190794 \n\n\nThe exact best unbiased line is therefore: \\[\ny = 134.0981  + -1.1908x\n\\]\n\n\n\n\n\n\nYour turn 7\n\n\n\nFind the best unbiased line for the mammals data.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# find exact best unbiased line for mammals data\nhand.fit(x = log.body, y = log.brain, res = T,\n         b = cor(log.body, log.brain)*sd(log.brain)/sd(log.body))\n\n\n\n\n\n\n\n\nintercept     slope \n2.1347887 0.7516859 \n\n\n\n\n\n\n\n\nPractice problem\n\n[L10] The kleiber dataset contains observations of log-transformed average mass (kg) and log-transformed metabolic rate (kJ/day). Kleiber’s law refers to the relationship by which metabolism depends on body mass.\n\nConstruct a scatterplot of the data, and be sure to orient the response variable and explanatory variable properly on the plot. Is there a trend, and if so, is it linear?\nCompute the correlation and comment on the strength and direction of linear relationship between log-mass and log-metabolism.\nFind the equation of the best unbiased line in slope-intercept form.\n[extra credit] Find an expression of the form \\(y = a x^b\\) for the relationship in (c) on the original (i.e., not log-transformed) scale. What is the exponent \\(b\\)?"
  },
  {
    "objectID": "content/lab14-slr.html",
    "href": "content/lab14-slr.html",
    "title": "Lab 14: Simple linear regression",
    "section": "",
    "text": "The objective of this lab is to learn to fit simple linear regression models in R and use/interpret relevant output to construct intervals and perform significance tests on the slope parameter.\nSpecifically, the lab covers the following:\n\nuse of lm to fit models\ninterpreting the output of summary.lm\nconstructing confidence intervals using confint\n\nWe’ll use three now-familiar datsets:\n\nprevend: cognitive assessment scores and age from the PREVEND study\nkleiber: observations of metabolic rate and body mass among animals\nmammals: observations of brain size and body size among common species of mammal\n\n\nlibrary(tidyverse)\nload('data/prevend.RData')\nload('data/kleiber.RData')\nload('data/mammals.RData')\n\n\nWarm-up\nConsider constructing a scatterplot using formula-dataframe syntax (rather than vector inputs):\n\n# scatterplot of rfft against age\nplot(rfft ~ age, data = prevend)\n\n\n\n\n\n\n\n\nThe formula should specify the response variable on the left-hand side and the explanatory variable on the right-hand side, e.g.:\n&lt;RESPONSE&gt; ~ &lt;EXPLANATORY&gt;\nThe variable names that appear in the formula must match exactly variables contained in the dataframe supplied as the data = ... argument.\n\n\n\n\n\n\nYour turn 1\n\n\n\nConstruct a scatterplot of log brain size (response) against log body size (explanatory).\n\n# scatterplot of log brain size against log body size\n\n\n\n\n\nFitting SLR models\nThe general form of a simple linear regression model for a response \\(y\\) in terms of an explanatory varaible \\(x\\) is:\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\nAbove, the coefficients \\(\\beta_0, \\beta_1\\) are model parameters. Here you’ll see how to obtain the ‘fitted model’ in which estimates are provided for the model parameters:\n\\[\ny = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\\] Note that the fitted model is written without the error term \\(\\epsilon\\).\n\nComputing estimates\nSLR models are fitted via the function lm in R. The syntax for lm(...) – short for linear model – is identical to constructing scatterplots using the formula-dataframe syntax:\n\n# slr model of rfft (response) by age (explanatory)\nfit.rfft &lt;- lm(rfft ~ age, data = prevend)\n\nThe default output shows only the coefficient estimates. Here, the fitted model equation is: \\[\n\\text{RFFT} = 134.098052187513 + 1.19079380169042\\times\\text{age}\n\\]\nThe slope parameter on age captures the relationship of interest:\n\nEach year of age is associated with an estimated decrease in mean RFFT score of 1.191.\n\n\n\n\n\n\n\nYour turn 2\n\n\n\nFit a simple linear regression model of log brain size with log body size as the explanatory variable. Write the fitted model equation and interpret the coefficient estimate for the slope parameter in context.\n\n# slr model of log brain size by log body size\n\n\n\n\n\nVisualizing a fitted model\nThe model is easy to visualize using abline(...), which adds a line to an existing plot. Conveniently, the function has a method for the output of an lm call:\n\n# reproduce the scatterplot, then add a line using abline\nplot(rfft ~ age, data = prevend)\nabline(reg = fit.rfft, col = 'blue', lwd = 2)\n\n\n\n\n\n\n\n\nThe additional arguments (color col and line width lwd) make the line easier to distinguish from the data scatter.\n\n\n\n\n\n\nYour turn 3\n\n\n\nAdd a line showing the fitted SLR model to your scatterplot from before of log brain size against log body size.\n\n\n\n\n\nFit summary\nThe full set of information about a fitted model – estimates and standard errors, variance explained, significance tests, and the error variability estimate – are obtained using the summary(...) function:\n\n# inspect model summary for estimates, inference, and measures of fit\nsummary(fit.rfft)\n\n\nCall:\nlm(formula = rfft ~ age, data = prevend)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.085 -14.690  -2.937  12.744  77.975 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 134.0981     6.0701   22.09   &lt;2e-16 ***\nage          -1.1908     0.1007  -11.82   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.52 on 206 degrees of freedom\nMultiple R-squared:  0.4043,    Adjusted R-squared:  0.4014 \nF-statistic: 139.8 on 1 and 206 DF,  p-value: &lt; 2.2e-16\n\n\nTake a moment to locate the following among the output above:\n\nage explains 40.43% of total variability in RFFT scores\n\\(\\hat{\\beta}_1 = -1.191\\)\n\\(SE\\left(\\hat{\\beta}_1\\right) = 0.1007\\)\n\\(\\hat{\\sigma} = 20.518\\)\n\nInference for simple linear regression models usually focuses on the slope parameter \\(\\beta_1\\). The \\(p\\)-values you see in the coefficient summary table show the results of partial significance tests, which are t tests of the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\beta_j = 0 \\\\\nH_A: &\\beta_j \\neq 0\n\\end{cases}\n\\] The partial significance test on the slope parameter is typically interpreted as a test of association, since if the slope is zero then, e.g., age and mean RFFT are unrelated. So the small \\(p\\)-value for the slope coefficient indicates:\n\nThe data provide very strong evidence that age is associated with RFFT score (T = -11.82 on 206 degrees of freedom, p &lt; 0.0001).\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nInspect the model summary for your simple linear regression model of log brain and log body size and interpret the inference on the slope parameter in context.\n\n\n\n\nConfidence intervals\nConfidence intervals can be obtained for coefficient estimates using the confint(...) function; this takes a fitted lm object as input together with a confidence level.\n\n# 95% confidence intervals\nconfint(object = fit.rfft, level = 0.95)\n\n                 2.5 %      97.5 %\n(Intercept) 122.130647 146.0654574\nage          -1.389341  -0.9922471\n\n# 99% confidence intervals\nconfint(object = fit.rfft, level = 0.99)\n\n                0.5 %      99.5 %\n(Intercept) 118.31647 149.8796336\nage          -1.45262  -0.9289675\n\n# for slope only...\nconfint(object = fit.rfft, parm = 'age', level = 0.99)\n\n       0.5 %     99.5 %\nage -1.45262 -0.9289675\n\n\nThe interpretation is as follows:\n\nWith 99% confidence, each additional year of age is associated with an estimated decrease in mean RFFT score of between 0.929 and 1.453 points.\n\n\n\n\n\n\n\nYour turn 5\n\n\n\nCompute a 95% confidence interval for the slope parameter from your SLR model of log brain size. Interpret the interval in context.\n\n\n\n\nPractice problem\n\n[L10] The kleiber dataset contains observations of log-transformed average mass (kg) and log-transformed metabolic rate (kJ/day). Kleiber’s law refers to the relationship by which metabolism depends on body mass.\n\nFit an SLR model of log metabolism (response) against log mass (explanatory). Write the fitted model equation and interpret the coefficient estimate for the slope parameter.\nConstruct a scatterplot with a line overlaid to visualize the fitted model.\nProduce the model summary and identify the proportion of variability explained by the model.\nInterpret the partial significance test for the slope parameter.\nConstruct a 99% confidence interval for the slope parameter."
  },
  {
    "objectID": "content/lab1-rbasics.html",
    "href": "content/lab1-rbasics.html",
    "title": "Lab 1: R Basics",
    "section": "",
    "text": "This lab is intended to introduce you to the basics in R that you will need for this class. Most of our analyses will consist of just a few steps:\n\nload a dataset\nidentify and select variable(s) of interest\nperform one or more calculations using variable(s) of interest as inputs\n\nWe will illustrate this process so that you can get used to the mechanics and familiarize yourself with how different data types appear in R.\n\nHow to do this lab\nI’ve provided you with a project on Posit Cloud containing data files and a script (a script is a plain text file containing R commands). The script contains all commands shown in this document, and some blank areas for you to fill in, with comment lines (the ones starting with #) to help you navigate.\nYou should refer back to this document for instructions and context, and fill in the script as you go:\n\nrun the codes provided as you read through the narrative in this document and inspect the results\nin the ‘your turn’ sections, refer to the prompt in this document and use the example commands provided immediately beforehand to determine which command to write\nwrite in your commands in the script the space below the corresponding comment, not in the console (otherwise you’ll have a hard time keeping track of your work)\n\nYour goal is to complete all of the “your turn” parts in the script. Two practice problems are given at the end of the lab as homework for you to complete on your own before next class.\n\n\nHow to use this lab\nThis lab (and the lab activities in general) are designed to provide you with a set of examples to learn initially in class and then follow on your own later when doing the homework problems given at the end of the lab.\nIf you can do the examples and ‘your turn’ activities in class, all you’ll need to do to complete the homeworks is copy commands from those examples and activities and adjust some small details (variable names, dataset names, etc.).\nIf you later need to figure out how to do something in R for a homework problem or test, all you’ll need to do is refer back to the labs.\n\n\nPackages in R\nA “package” is a bundle of functions, datasets, and other objects that can be imported into R for use in your working environment. Many scripts begin by loading packages that will be used throughout the script. Packages are loaded using the command library(&lt;PACKAGE NAME&gt;) where &lt;PACKAGE NAME&gt; is replaced by the actual name of the package. For example:\n\nlibrary(tidyverse)\n\nPackages do need to be installed before they can be loaded. One of the nice things about using Posit Cloud is that I can manage all of these installs for you. However, if you ever wish to install and use a package that’s not available (or if you use R on your own machine), you can install a package using the command install.packages(\"&lt;PACKAGE NAME&gt;\") after replacing &lt;PACKAGE NAME&gt; with the actual name of the package (but keeping the quotation marks!).\n\n\nLoading a dataset\nThere are several ways to load datasets in R. The strategy we’ll use most often is to load an .RData file, but you will encounter a few others here and there.\n\n# load nhanes data\nload('data/nhanes.RData')\n\nThis command looks for a file called nhanes.RData in a directory folder named data and reads the file.\nNotice that once you run the command, an object called nhanes appears in the “Environment” tab in the upper right hand panel of your RStudio window.\nIf you click the little blue carrot next to nhanes in the environment tab, you will then see a list of variables contained in the dataset. You can also see the first few rows of the dataset using head(...).\n\n# first few rows\nhead(nhanes)\n\n# A tibble: 6 × 9\n  subj.id gender   age poverty pulse bpsys1 bpdia1 totchol sleephrsnight\n    &lt;int&gt; &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;         &lt;int&gt;\n1       1 male      34    1.36    70    114     88    3.49             4\n2       2 male      34    1.36    70    114     88    3.49             4\n3       3 male      34    1.36    70    114     88    3.49             4\n4       5 female    49    1.91    86    118     82    6.7              8\n5       8 female    45    5       62    106     62    5.82             8\n6       9 female    45    5       62    106     62    5.82             8\n\n\nThis kind of object in R is called a data frame. Data frames are displayed in a tabular layout, like a spreadsheet. While data frames should be arranged so that observations are shown in rows and variables in columns, this is not guaranteed, so you should be in the habit of checking to make sure the layout is sensible; otherwise, you might accidentally perform bogus calculations and analyses.\nBeyond providing a sanity check, inspecting the data frame will show you three key pieces of information besides the values of the first few observations of each variable.\n\nData dimensions: how many observations (rows) and how many variables (columns)\nVariable names: subj.id, gender, age, etc.\nData types:\n\nint for integer (numerical data type)\nfct for factor (categorical data type)\nnum for numeric (numerical data type)\nchr for character (categorical data type)\n\n\nSo, for example, seeing that pulse is of data type int tells you that pulse is a discrete numerical variable. It also tells you what name to use to refer to the variable in subsequent R commands.\n\n\n\n\n\n\nYour turn\n\n\n\nThere is another data file in the data directory called famuss.RData. Load this into the environment, preview the first few observations, and check the variable names and data types.\n\n# load famuss dataset\n\n# preview first few rows\n\nTo check your understanding:\n\nhow many observations and variables?\nidentify a categorical variable\nwhat kind of variable is bmi?\n\n\n\n\n\nSelecting variables\nThe variable names in a dataset can be used to retrieve or refer to specific variables. For example, try running this command:\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# preview first few values\nhead(total.cholesterol)\n\n[1] 3.49 3.49 3.49 6.70 5.82 5.82\n\n\nThat command did the following:\n\nextracted the totchol column of nhanes (the nhanes$totchol part)\nassigned the result a new name total.cholesterol (the &lt;- part)\n\nAssignment (&lt;-) is a very important concept in R – you can store the result of any calculation as an object with a name of your choosing.\nYou’ll notice that total.cholesterol looks a bit different than the data frame in terms of its appearance. This is because it’s not a data frame but rather a different kind of object called a vector: a collection of values of the same data type.\n\n\n\n\n\n\nYour turn\n\n\n\nExtract the change in nondominant arm strength variable from the FAMuSS dataset, and store it as a vector called strength.\n\n# store the change in nondominant arm strength variable as a vector called 'strength'\n\n# preview the first few values\n\n\n\n\n\nPerforming calculations\nExtracting and storing variables as vectors isn’t strictly necessary, but does make it easier to perform many calculations. While you’re a beginner, I’d recommend using this strategy.\n\nNumeric summaries\nMost simple summary statistics can be calculated using simple functions in R that take a single vector argument. For example, to calculate the average, minimum, and maximum total cholesterol among the respondents in the sample:\n\n# average total cholesterol\nmean(total.cholesterol)\n\n[1] 5.042938\n\n# minimum\nmin(total.cholesterol)\n\n[1] 2.33\n\n# maximum\nmax(total.cholesterol)\n\n[1] 13.65\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nFind the average percent change in nondominant arm strength of participants in the FAMuSS study sample using the strength vector you created before.\n\n# compute mean change in nondominant arm strength\n\n\n\n\n\nCategorical summaries\nMost data summaries for categorical variables proceed from counts of the number of observations in each category. These counts can be obtained by passing a vector of observations to table(...):\n\n# retreive sex variable\nsex &lt;- nhanes$gender\n\n# counts\ntable(sex)\n\nsex\nfemale   male \n  1588   1591 \n\n\nTo obtain the proportion of observations in each category – the counts divided by the total number of observations – pass the table to the proportions(...) function:\n\n# proportions\ntable(sex) |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nThe character string |&gt; is a bit of syntax that you could read verbally as ‘then’: first make a table, then obtain proportions. It’s known as the pipe operator, because it ‘pipes’ the result of the command on its left into the command on its right.\nTo see another example of the pipe operator in action, you could rewrite the previous command as a chain of three steps:\n\n# same as above\nsex |&gt; table() |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nYou could interpret this as follows: start with sex, pass that to table(), then pass the result to proportions.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the FAMuSS dataset, calculate the genotype frequencies in the sample (i.e., find the proportion of observations of each genotype).\n\n# retrieve genotype\n\n# counts\n\n# proportions\n\n\n\nWhile the analyses you’ll learn will get more complex than computing summary statistics, the mechanics of performing the computations in R will be analogous to what you just did: executing a one-line command with a vector input.\n\n\n\nPutting together the pieces\nReflect for a moment on what you just did: you wrote a few lines of code to import a dataset, extract a variable, and compute a statistic. If you filled in the script as instructed, you now have a record of the commands you executed that you can use to retrace your steps.\nIn fact, anyone with your script and the data files (including future you) could easily reproduce your work. Reproducibility is a pillar of data-driven science; by storing analyses in the form of executable scripts, researchers can easily create and share records of their work.\nWe could put the steps above together in just a few lines as if it were a short script. Typical style is to provide line-by-line comments explaining what the commands do.\n\n# import nhanes data\nload('data/nhanes.RData')\n\n# inspect data\nhead(nhanes)\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# compute average total cholesterol\nmean(total.cholesterol)\n\n# extract sex\nsex &lt;- nhanes$gender\n\n# proportions of men and women in sample\ntable(sex) |&gt; proportions()\n\n\n\n\n\n\n\nYour turn\n\n\n\nFollow the example above and combine the previous exercises into a few lines of code with appropriate line comments.\n\n# load famuss dataset\n\n# inspect data\n\n# extract nondominant change in arm strength\n\n# compute average change in strength\n\n# extract genotype\n\n# compute genotype frequencies (proportions)\n\n\n\nIf this was all entirely new to you, congratulations on writing your first lines of code!\n\n\nExtras\n\nReading CSV files\nOften data are stored in spreadsheets, which can be easily converted to comma-separated values or CSV files (extension .csv). These are plain-text files that are a bit more lightweight than an Excel spreadsheet.\nR can read CSV (as well as other) files. The read.csv(...) function will parse the file and produce a data frame. The result can be assigned a name and stored as an object in the environment.\n\n# parse a csv file\nread.csv('data/gss.csv')\n\n# store the result in the environment\ngss &lt;- read.csv('data/gss.csv')\n\nMost of the time in class we’ll load .RData files or obtain datasets through packages (more on this later), but if you use R outside of class you may find it more common to manage data input via .csv files.\n\n\nMore about R\nWhile you will learn new commands going forward, we won’t go much more in depth with R than what you just saw. However, if you’re interested in understanding the above concepts in greater detail, or learning about R as a programming environment, see An Introduction to R.\n\n\n\nPractice problems\nDue before the next class meeting.\n\nThe census dataset contains a sample of data for 377 individuals included in the 2000 U.S. census. Load and inspect the dataset, and determine:\n\nthe youngest and oldest individual in the sample\nthe average total personal income\nthe average total family income\nhow many variables are in the dataset, not including census year and FIPS code\nhow many categorical variables are in the dataset, not including FIPS code\n\n\n\nThe cdc.samp dataset in the oibiostat package contains a sample of data for 60 individuals surveyed by the CDC’s Behavioral Risk Factors Surveillance System (BRFSS). Use the provided commands to load the dataset, and then inspect it the usual way. Notice that several of the variables are 1’s and 0’s. Use the command ?cdc.samp to view the data documentation.\n\nWhat do the values (1’s and 0’s) mean in the exerany variable?\nWhat proportion of the sample are men? What proportion are women?\nFor each general health category, find the proportion of respondents who rated themselves in that category.\nHow many of the respondents have health coverage? (Hint: sum(x) will add up the values in a vector x; adding up a collection of 1’s and 0’s is equivalent to counting the number of 1’s.) What percentage of the respondents have health coverage?"
  },
  {
    "objectID": "content/test4.html",
    "href": "content/test4.html",
    "title": "Test 4",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 6/7. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test4.html#instructions",
    "href": "content/test4.html#instructions",
    "title": "Test 4",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 6/7. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test4.html#problems",
    "href": "content/test4.html#problems",
    "title": "Test 4",
    "section": "Problems",
    "text": "Problems\n\n[L3, L6, L7, L8] In a study examining the association between green tea consumption and esophageal carcinoma, researchers recruited 300 patients with carcinoma and 571 without carcinoma and administered a questionnaire about tea drinking habits. Out of the 47 individuals who reported that they regularly drink green tea, 17 had carcinoma. Out of the 824 individuals who reported they never drink green tea, 283 had carcinoma. The greentea dataset contains the participant-level observations.\n\n[L3] Construct a contingency table of tea consumption by carcinoma status.\n[L6] Estimate the proportion of patients with carcinoma that regularly consume green tea; provide a point estimate and 95% confidence interval and interpret the estimates in context.\n[L6] Estimate the proportion of patients without carcinoma that regularly consume green tea; provide a point estimate and 95% confidence interval and interpret the estimates in context.\n[L7] Check the assumptions for a \\(\\chi^2\\) test of association. If they hold, perform the test at the 5% significance level.\n[L8] Compute a point estimate for the appropriate measure of association. What does the estimate suggest about the direction of association?\n\n[L3, L6, L7] A 2010 Pew Research poll asked 1,306 Americans, “From what you’ve read and heard, is there solid evidence that the average temperature on earth has been getting warmer over the past few decades, or not?” The warming dataset contains the observations from this poll: each participant’s party or ideology and whether they answered affirmatively or negatively.\n\n[L6] Estimate the proportion of Americans in 2010 who believe there is solid evidence of climate change. Provide both a point estimate and a 95% confidence interval and interpret the estimates in context following the narrative style from class.\n[L6] What is the margin of error for this poll?\n[L3] Compute the frequency (proportion) of each answer by party/ideology and construct a stacked barplot showing the frequencies.\n\n[L7] Test for an association between climate change opinion and party/ideology. Verify assumptions and carry out the test at the 1% significance level. Interpret the result in context following the narrative style from class.\n[L7] If the test is significant, conduct a residual analysis to determine which responses were more or less frequent among each party/ideology than would be expected if opinion and ideology were unrelated.\n[LX] Determine the expected proportions of responses by ideology under the assumption that ideology and opinion are unrelated.\n[LX] Using a Bonferroni correction, compute simultaneous 95% confidence intervals for the proportions of Americans in each ideological group in 2010 who believe there is solid evidence of climate change.\n\n[L8] The sulphin dataset contains observations from an experiment studying the efficacy of Sulphinpyrazone for treating patients who have had a heart attack.\n\n[L8] At the 10% significance level, test for an effect of treatment and compute an interval estimate for the relative risk of death in the treatment group compered with the control group at the appropriate confidence level. Be sure to check assumptions and use the appropriate test.\n[LX] (Extra credit) Provide a point estimate and confidence interval for the efficacy of Sulphinpyrazone with respect to reducing the risk of death.\n\n[L3, L10] The galapagos dataset contains observations of (log-transformed) island area (\\(km^2\\)) and (log-transformed) total number of observed species for 30 of the Galapagos islands recorded in 1973.\n\n[L3] Construct a scatterplot of the log total number of species against the log area, and compute the correlation between the two. Based on your results, comment on the apparent linearity, direction, and strength of the relationship.\n[L10] Estimate the relationship between log-species and log-area using a simple linear regression model; write the fitted model equation and add the fitted line to your plot in (a).\n[L10] Following the narrative style from class, report the proprtion of variance explained and significance of the relationship at the 1% level.\n[L10] Provide point and interval estimates at the appropriate confidence level for the model parameter of interest.\n[LX] (Optional extra credit) Write the model equation on the original (i.e., not log-transformed) scale; re-interpret the interval estimate from part (d) in terms of the power law relationship."
  },
  {
    "objectID": "content/test4.html#extra-credit",
    "href": "content/test4.html#extra-credit",
    "title": "Test 4",
    "section": "Extra credit",
    "text": "Extra credit\n\n[L10] The Hubble constant \\(H\\) is a fundamental cosmological constant that relates a galaxy’s relative distance and velocity as \\(H\\times d = v\\). The value of the constant can be used to estimate the age of the universe in years is obtained via the conversion \\(\\frac{1}{H}\\times\\frac{km}{Mpc}\\times\\frac{yr}{s}\\). The hubble dataset comprises observations of relative velocities (km/sec) and distances (Mpc) for 24 galaxies.\n\nUse the data provided to estimate the reciprocal of the Hubble constant by fitting a regression model with distance as the response and velocity as the explanatory variable. Fit the model without an intercept using the model specification formula = &lt;RESPONSE&gt; ~ &lt;EXPLANATORY&gt; - 1.\nObtain a 90% confidence interval for the reciprocal of the Hubble constant \\(\\frac{1}{H}\\).\nMultiply the endpoints of your interval by the conversion factor \\(\\frac{km}{Mpc}\\times\\frac{yr}{s}\\) to obtain an interval estimate for the age of the universe in years.\nReport the interval in billions of years and interpret your interval in context."
  },
  {
    "objectID": "content/lab13-measures.html",
    "href": "content/lab13-measures.html",
    "title": "Lab 13: Odds ratio and relative risk",
    "section": "",
    "text": "This lab has two main objectives:\n\nlearn to estimate odds ratios and relative risk in R using epitools\ndistinguish which measure of association is most appropriate based on desired interpretation and/or study design\n\nIn addition, you’ll get some practice combining inference of association in two-way tables with inference for an appropriate measure of association and interpreting results correctly.\nWe’ll use the datasets from class, along with a few additional examples.\n\nlibrary(tidyverse)\nlibrary(epitools)\nload('data/smoking.RData')\nload('data/asthma.RData')\nload('data/chd.RData')\nload('data/outbreak.RData')\nmalaria &lt;- openintro::malaria\n\n\nOdds ratios\nIt is worth underscoring at the outset that odds ratios can be estimated for any study design; they are not reserved strictly for case-control studies.\nThat said, the smoking dataset is an example of a situation in which we can only estimate odds ratios, because of the case-control study design: 86 lung cancer patients and 86 controls were sampled separately, and the smoking status of each participant was recorded.\nNotice below how adding names when constructing the contingency table adds labels to the row and column dimensions in the output.\n\n# construct table with dimension names group, smoking\nsmoking.tbl &lt;- table(group = smoking$group, smoking = smoking$smoking)\nsmoking.tbl\n\n         smoking\ngroup     Smokers NonSmokers\n  cancer       83          3\n  control      72         14\n\n\nIt is good practice to check whether expected counts are sufficiently large to use the \\(\\chi^2\\) test; if they aren’t, then we’ll know to use the Fisher’s exact \\(p\\)-value later.\n\n# check whether expected counts are at least ten \nchisq.test(smoking.tbl)$expected\n\n         smoking\ngroup     Smokers NonSmokers\n  cancer     77.5        8.5\n  control    77.5        8.5\n\n\nLast time, we bent the rules a little and went ahead with the \\(\\chi^2\\) test anyway because the counts were close enough to the cutoff. So this time, it’ll be interesting to see how much results differ using the exact inference method.\nTo compute odds ratios and inference for association simultaneously, use the oddsratio(...) function:\n\noddsratio(smoking.tbl, rev = 'both', conf.level = 0.95, method = 'wald', correction = T)\n\n$data\n         smoking\ngroup     NonSmokers Smokers Total\n  control         14      72    86\n  cancer           3      83    86\n  Total           17     155   172\n\n$measure\n         odds ratio with 95% C.I.\ngroup     estimate    lower    upper\n  control  1.00000       NA       NA\n  cancer   5.37963 1.486376 19.47045\n\n$p.value\n         two-sided\ngroup      midp.exact fisher.exact chi.square\n  control          NA           NA         NA\n  cancer  0.005116319  0.008822805 0.01062183\n\n$correction\n[1] TRUE\n\nattr(,\"method\")\n[1] \"Unconditional MLE & normal approximation (Wald) CI\"\n\n\nThis command has several moving parts:\n\nrev rearranges the table by reversing rows, columns, or both\nconf.level determines the confidence level of the resulting interval\nmethod determines how the interval is computed (the version we learned in class is a Wald approximation)\ncorrection determines whether a continuity correction is applied (always set to T)\n\nThe most important argument is rev: the contingency table must be arranged so that the outcome of interest is in the second position in the rows/columns.\n\nif the order of columns were opposite what is shown above, we would get instead an estimate of the odds of cancer among nonsmokers compared with smokers\nif the order of rows were opposite what is shown above, we would get instead an estimate of the odds of not having cancer among smokers compared with nonsmokers\nif both were opposite what is shown above, we would get instead an estimate of the odds of not having cancer among nonsmokers compared with smokers\n\nIt may be a good idea to double-check that the odds ratio you obtained is in fact the right one by doing the calculations by hand and checking the point estimate.\n\n\n\n\n\n\nYour turn 1\n\n\n\nCompute the odds ratio using direct arithmetic to double-check that the output above gives you the odds ratio you intend.\n\n\nIn this example, the exact \\(p\\)-value is fairly close to the \\(\\chi^2\\) test; they would produce different conclusions at the 1% significance level, but not otherwise. Either would be appropriate to use here, but if you prefer to stick to the \\(\\hat{n} \\geq 10\\) rule of thumb strictly, use Fisher’s exact test:\n\nThe data provide evidence that smoking is associated with lung cancer (Fisher’s exact test, p = 0.0088). With 95% confidence, the odds of lung cancer are estimated to be between 1.49 and 19.47 times higher among smokers compared with nonsmokers, with a point estimate of 5.38.\n\n\n\n\n\n\n\nYour turn 2\n\n\n\nUsing the outbreak data, which comprise data from a case-control study in which 30 cases and 60 controls were sampled and each subject’s exposure to raspberries was assessed, perform inference on association using the odds ratio:\n\nCheck assumptions for the chi square test\nUse oddsratio(...) to perform calculations for the inference with the odds ratio\nInterpret relevant outputs in context\n\n\n# construct contingency table\n\n# check assumptions for chi square test\n\n# perform inference with odds ratio\n\n# double-check your point estimate to verify data were arranged in correct orientation\n\n\n\n\n\nRelative risk\nThe implementation for inference with relative risk is identical to that for inference with odds ratios, but a bit more care is required to orient the contingency table correctly. To obtain the correct relative risk, it is essential to put the groups in rows and outcomes in columns. This orientation did not matter for the odds ratio implementation.\n\n# put groups in rows and outcome in columns\nasthma.tbl &lt;- table(sex = asthma$sex, asthma = asthma$asthma)\nasthma.tbl\n\n        asthma\nsex      asthma no asthma\n  female     49       781\n  male       30       769\n\n# check assumptions for chi square test\nchisq.test(asthma.tbl)$expected\n\n        asthma\nsex        asthma no asthma\n  female 40.25169  789.7483\n  male   38.74831  760.2517\n\n\nHere, all expected counts are above 10, so when we interpret the output of riskratio(...), we can use the p-value from the \\(\\chi^2\\) test.\n\nriskratio(asthma.tbl, rev = 'both', conf.level = 0.90, method = 'wald', correction = T)\n\n$data\n        asthma\nsex      no asthma asthma Total\n  male         769     30   799\n  female       781     49   830\n  Total       1550     79  1629\n\n$measure\n        risk ratio with 90% C.I.\nsex      estimate    lower    upper\n  male   1.000000       NA       NA\n  female 1.572329 1.083353 2.282007\n\n$p.value\n        two-sided\nsex      midp.exact fisher.exact chi.square\n  male           NA           NA         NA\n  female 0.04412095   0.04961711 0.05703135\n\n$correction\n[1] TRUE\n\nattr(,\"method\")\n[1] \"Unconditional MLE & normal approximation (Wald) CI\"\n\n\nHere, the consequences of reversing the order of rows/columns are as follows:\n\nif rows were arranged opposite of what is shown above, we would obtain the relative risk of asthma among men compared with women (sensible but different)\nif columns were arranged opposite of what is shown above, we would obtain the relative risk of not having asthma among women compared with men (not sensible)\nif both rows and columns were arranged opposite of what is shown above, we would obtain the relative risk of not having asthma amond men compared with women (not sensible)\n\nIn addition, the consequence of orienting the table opposite is:\n\nif columns and rows were swapped, but the order were the same as shown above, we would obtain an estimate of the relative risk of being a woman among asthmatics compared with non-asthmatics (not sensible)\n\nYou can double check that the risk ratio computed is the one intended by direct arithmetic:\n\n# compute proportions\nasthma.tbl |&gt; prop.table(margin = 1)\n\n        asthma\nsex          asthma  no asthma\n  female 0.05903614 0.94096386\n  male   0.03754693 0.96245307\n\n# risk ratio by hand\n0.05903614/0.03754693\n\n[1] 1.572329\n\n\nSince assumptions for the \\(\\chi^2\\) test were met, we can combine the inference from this test with the interval estimate for the relative risk:\n\nThe data provide evidence at the 10% significance level of an association between asthma and sex (\\(\\chi^2\\) = 3.62 on 1 degree of freedom, p = 0.057). With 90% confidence, the risk of asthma is estimated to be betwen 1.08 and 2.28 times greater for women than for men, with a point estimate of 1.57.\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nThe chd data contain observations on the incidence of coronary heart disease from a cohort study of 3000 smokers and 5000 nonsmokers.\n\nConstruct the contingency table with the groups you wish to compare shown in the row dimension and the outcome of interest in the column dimension.\nCheck assumptions for the \\(\\chi^2\\) test.\nPerform inference on association with relative risk at the 1% significance level.\nDouble-check the relative risk point estimate to make sure you obtained the comparison you intended.\n\n\n# construct contingency table in proper orientation for inference with relative risk\n\n# check assumptions for chi square test\n\n# carry out inference at 1% significance level\n\n# double check point estimate by manual calculation\n\n\n\n\n\nPractice problems\n\n[L8] For the diabetes_meds dataset from last time comparing rates of cardiovascular problems between two diabetes medications among ~200K medicare beneficiaries, determine an appropriate measure of association to add to the inference you performed previously. Provide a narrative interpretation of the result (both the test and estimates) following the style introduced in class.\n[L8] The Learning Early About Peanut allergy (LEAP) study recruited 530 children with risk factors for developing peanut allergies and randomly allocated peanut exposure and peanut avoidance regimens to each participant. At 5 years of age, an oral food challenge (OFC) test was administered to determine whether participants had developed allergies. Data are contained in the leap dataset.\n\nConstruct the contingency table for this data.\nCheck the assumptions for the \\(\\chi^2\\) test of association.\nTest for association at the 1% significance level and provide point and interval estimates for an appropriate measure of association.\n\n[L8] Researchers studying the link between prenatal vitamin use and autism surveyed the mothers of a random sample of children aged 24 - 60 months with autism and seperately surveyed the mothers of a random sample of children with typical development. The vitamin dataset contains observations of whether mothers in each group did or did not use prenatal vitamins during the three months before pregnancy (periconceptional period).\n\nWhich proportions are possible to estimate? Based on the study design, is it possible to estimate the relative risk of autism?\nConstruct the contingency table and check assumptions for the \\(\\chi^2\\) test.\nTest for an association between taking prenatal vitamins and autism at the 1% significance level; include an appropriate meausre of association and provide point and interval estimates.\nInterpret the result in context following the narrative style introduced in class."
  },
  {
    "objectID": "content/week9-measures.html#todays-agenda",
    "href": "content/week9-measures.html#todays-agenda",
    "title": "Measures of association",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] relative risk and odds ratios\n[lab] using epitools to estimate risk and odds ratios in R\n[discussion] final project guidelines"
  },
  {
    "objectID": "content/week9-measures.html#from-last-time-smoking",
    "href": "content/week9-measures.html#from-last-time-smoking",
    "title": "Measures of association",
    "section": "From last time: smoking",
    "text": "From last time: smoking\n\n\nResearchers sampled 86 lung cancer patients (cases) and 86 healthy individuals (controls) and recorded smoking status:\n\n\n\n\n\n\n\n\n\n\n \nSmokers\nNonSmokers\n\n\n\n\ncancer\n83\n3\n\n\ncontrol\n72\n14\n\n\n\n\n\nAt the 5% level, association is significant:\n\nsmoking.tbl &lt;- table(smoking$group, \n                     smoking$smoking) \nsmoking.tbl |&gt; chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  smoking.tbl\nX-squared = 6.5275, df = 1, p-value = 0.01062\n\n\n\nBut there is a difficulty for measuring the association:\n\ndata are really two independent samples\nso can’t estimate cancer prevalence"
  },
  {
    "objectID": "content/week9-measures.html#case-control-studies",
    "href": "content/week9-measures.html#case-control-studies",
    "title": "Measures of association",
    "section": "Case-control studies",
    "text": "Case-control studies\n\nOutcome-based sampling limits which proportions are estimable\n\n\n\ncases and controls are two independent samples\nexposure prevalence is estimable (within case and control populations)\ncase prevalence is not estimable"
  },
  {
    "objectID": "content/week9-measures.html#smoking-example",
    "href": "content/week9-measures.html#smoking-example",
    "title": "Measures of association",
    "section": "Smoking example",
    "text": "Smoking example\n\n\nSo due to study design, the only estimable difference in proportions is:\n\nsmoking.prop.test &lt;- smoking.tbl |&gt; prop.test()\nsmoking.prop.test$conf\n\n[1] 0.029149 0.226665\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\nWith 95% confidence, the proportion of smokers among cancer patients is estimated to be betwteen 2.9 and 22.7 percentage points higher than among controls.\n\n\n\nThis makes for an awkward conclusion:\n\nwe really want to know how smoking affects cancer risk\n…not how cancer affects smoking risk\n\nSo for this kind of study, we need a different measure of association: odds ratios."
  },
  {
    "objectID": "content/week9-measures.html#odds",
    "href": "content/week9-measures.html#odds",
    "title": "Measures of association",
    "section": "Odds",
    "text": "Odds\n\nThe odds of an outcome measure its relative likelihood\n\nIf \\(p\\) is the true cancer prevalence (a population proportion), then the odds of cancer are defined as the ratio: \\[\n\\text{odds} = \\frac{p}{1 - p}\n\\] The odds represent the factor by which cancer is more likely than not, e.g.:\n\n\\(\\text{odds} = 2\\) indicates the outcome (cancer) is twice as likely as not\n\\(\\text{odds} = 1/2\\) indicates the outcome (cancer) is half as likely as not"
  },
  {
    "objectID": "content/week9-measures.html#odds-ratios",
    "href": "content/week9-measures.html#odds-ratios",
    "title": "Measures of association",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nAn odds ratio compares the relative likelihood of an outcome between two groups\n\n\n\nIf \\(a, b, c, d\\) are population proportions:\n\n\n\n\n\n\n\n\n\\(\\;\\)\noutcome 1 (O1)\noutcome 2 (O2)\n\n\n\n\ngroup 1 (G1)\na\nb\n\n\ngroup 2 (G2)\nc\nd\n\n\n\n\nthe odds of outcome 1 in group 1 is \\(\\frac{\\textcolor{red}{a}}{\\textcolor{blue}{b}}\\)\nthe odds of outcome 1 in group 2 is \\(\\frac{\\textcolor{orange}{c}}{\\textcolor{purple}{d}}\\)\n\n\nThe odds ratio of outcome 1 comparing group 1 with group 2 is:\n\\[\n\\frac{\\text{odds}_{G1}(O1)}{\\text{odds}_{G2}(O1)}\n= \\frac{\\textcolor{red}{a}/\\textcolor{blue}{b}}{\\textcolor{orange}{c}/\\textcolor{purple}{d}}\n= \\frac{\\textcolor{red}{a}\\textcolor{purple}{d}}{\\textcolor{blue}{b}\\textcolor{orange}{c}}\n\\] A surprising algebraic fact is that:\n\\[\n\\frac{\\text{odds}_{G1}(O1)}{\\text{odds}_{G2}(O1)}\n=\\frac{\\text{odds}_{O1}(G1)}{\\text{odds}_{O2}(G1)}\n\\]\n\n\nThis means that the ratio of odds of cancer between smokers and nonsmokers can be estimated from the ratio of odds of smoking between cases and controls."
  },
  {
    "objectID": "content/week9-measures.html#estimating-odds-ratios",
    "href": "content/week9-measures.html#estimating-odds-ratios",
    "title": "Measures of association",
    "section": "Estimating odds ratios",
    "text": "Estimating odds ratios\n\n\nFor notation let:\n\n\\(\\hat{p}_\\text{case}\\) : proportion of smokers among cases\n\\(\\hat{p}_\\text{control}\\) : proportion of smokers among controls\n\n\n\n\n\n\n\n\n\n\n\n\n \nSmokers\nNonSmokers\n\n\n\n\ncancer\n83\n3\n\n\ncontrol\n72\n14\n\n\n\n\n\n\n\nAn estimate of ratio of odds of cancer among smokers compared with nonsmokers (\\(\\omega\\)) is the estimated ratio of odds of smoking among cancer patients compared with controls: \\[\n\\hat{\\omega}\n=\\left(\\frac{\\hat{p}_\\text{case}}{1 - \\hat{p}_\\text{case}}\\right)\\Bigg/\\left(\\frac{\\hat{p}_\\text{control}}{1 - \\hat{p}_\\text{control}}\\right)\n= \\frac{83/3}{72/14} = 5.38\n\\]\n\nIt is estimated that the odds of lung cancer are 5.38 times greater for smokers compared with nonsmokers."
  },
  {
    "objectID": "content/week9-measures.html#confidence-intervals-for-odds-ratios",
    "href": "content/week9-measures.html#confidence-intervals-for-odds-ratios",
    "title": "Measures of association",
    "section": "Confidence intervals for odds ratios",
    "text": "Confidence intervals for odds ratios\nThe sampling distribution of the log odds ratio can be approximated by a normal model.\n\\[\n\\log\\left(\\hat{\\omega}\\right) \\pm c \\times SE\\left(\\log\\left(\\hat{\\omega}\\right)\\right)\n\\quad\\text{where}\\quad\nSE\\left(\\log\\left(\\hat{\\omega}\\right)\\right) = \\sqrt{\\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}}\n\\]\n\n\nThe oddsratio(...) function in the epitools package will compute and back-transform the interval for you.\n\noddsratio(smoking.tbl, rev = 'both', \n          method = 'wald', correction = T)\n\n\n\n         odds ratio with 95% C.I.\n          estimate    lower    upper\n  control  1.00000       NA       NA\n  cancer   5.37963 1.486376 19.47045\n\n\n\n\ncritical value \\(c\\) from the normal model\nexponentiate to obtain an interval for \\(\\omega\\)\n\n\nWith 95% confidence, the odds of lung cancer are estimated to be between 1.49 and 19.47 times greater for smokers compared with nonsmokers."
  },
  {
    "objectID": "content/week9-measures.html#implementation-details",
    "href": "content/week9-measures.html#implementation-details",
    "title": "Measures of association",
    "section": "Implementation details",
    "text": "Implementation details\n\n\n\noddsratio(smoking.tbl, rev = 'both',\n          method = 'wald', correction = T)\n\n\n\n$data\n         \n          NonSmokers Smokers Total\n  control         14      72    86\n  cancer           3      83    86\n  Total           17     155   172\n\n$measure\n         odds ratio with 95% C.I.\n          estimate    lower    upper\n  control  1.00000       NA       NA\n  cancer   5.37963 1.486376 19.47045\n\n$p.value\n         two-sided\n           midp.exact fisher.exact chi.square\n  control          NA           NA         NA\n  cancer  0.005116319  0.008822805 0.01062183\n\n\n\noddsratio is picky about data inputs:\n\noutcome of interest should be second column\ngroup of interest should be second row\nthe rev argument will reverse orders\n\nrev = neither keeps original orientation (default)\nrev = rows reverses order of rows\nrev = columns reverses order of columns\nrev = both reverses both"
  },
  {
    "objectID": "content/week9-measures.html#interpretation",
    "href": "content/week9-measures.html#interpretation",
    "title": "Measures of association",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\noddsratio(smoking.tbl, rev = 'both',\n          method = 'wald', correction = T)\n\n\n\n$data\n         \n          NonSmokers Smokers Total\n  control         14      72    86\n  cancer           3      83    86\n  Total           17     155   172\n\n$measure\n         odds ratio with 95% C.I.\n          estimate    lower    upper\n  control  1.00000       NA       NA\n  cancer   5.37963 1.486376 19.47045\n\n$p.value\n         two-sided\n           midp.exact fisher.exact chi.square\n  control          NA           NA         NA\n  cancer  0.005116319  0.008822805 0.01062183\n\n\n\nFirst report the test result, then the measure of association:\n\nThe data provide evidence of an association between smoking and lung cancer (\\(\\chi^2\\) = 6.53 on 1 degree of freedom, p = 0.1062). With 95% confidence, the odds of cancer are estimated to be between 1.49 amd 19.47 times greater among smokers compared with nonsmokers, with a point estimate of 5.38.\n\n\n\nComments:\n\nthe chi.square \\(p\\)-value is from the test of association/independence\nif assumptions aren’t met, can use the fisher.exact instead (see V&H 8.3.5)"
  },
  {
    "objectID": "content/week9-measures.html#asthma-data",
    "href": "content/week9-measures.html#asthma-data",
    "title": "Measures of association",
    "section": "Asthma data",
    "text": "Asthma data\n\n\nConsider estimating the difference in proportions:\n\n\n\n\n\n\n\n\n\n\n \nasthma\nno asthma\n\n\n\n\nfemale\n49\n781\n\n\nmale\n30\n769\n\n\n\n\n\n\n\nasthma.tbl &lt;- table(asthma$sex, asthma$asthma)\nprop.test(asthma.tbl, conf.level = 0.9)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  asthma.tbl\nX-squared = 3.6217, df = 1, p-value = 0.05703\nalternative hypothesis: two.sided\n90 percent confidence interval:\n 0.002841347 0.040137075\nsample estimates:\n    prop 1     prop 2 \n0.05903614 0.03754693 \n\n\n\n\n\nWith 90% confidence, asthma prevalence is estimated to be between 0.28 and 4.01 percentage points higher among women than among men.\n\nIs a difference of up to 4 percentage points practically meaningful? Well, it depends:\n\nyes if prevalence is very low\nno if prevalence is very high"
  },
  {
    "objectID": "content/week9-measures.html#relative-risk",
    "href": "content/week9-measures.html#relative-risk",
    "title": "Measures of association",
    "section": "Relative risk",
    "text": "Relative risk\nIf \\(p_F, p_M\\) are the (population) proportions of women and men with asthma, then the relative risk of asthma among women compared with men is defined as:\n\\[\nRR = \\frac{p_F}{p_M}\n\\qquad\n\\left(\\frac{\\text{risk among women}}{\\text{risk among men}}\\right)\n\\]\nAn estimate of the relative risk is simply the ratio of estimated proportions. For the asthma data, an estimate is: \\[\n\\widehat{RR} = \\frac{\\hat{p}_F}{\\hat{p}_M} = \\frac{0.059}{0.038} = 1.57\n\\]\n\nIt is estimated that the risk of asthma among women is 1.57 times greater than among men."
  },
  {
    "objectID": "content/week9-measures.html#confidence-intervals-for-relative-risk",
    "href": "content/week9-measures.html#confidence-intervals-for-relative-risk",
    "title": "Measures of association",
    "section": "Confidence intervals for relative risk",
    "text": "Confidence intervals for relative risk\nA normal model can be used to approximate the sampling distribution of \\(\\log(RR)\\) and construct a confidence interval. If \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\) are the two estimated proportions:\n\\[\\log\\left(\\widehat{RR}\\right) \\pm c \\times SE\\left(\\log\\left(\\widehat{RR}\\right)\\right)\n\\quad\\text{where}\\quad SE\\left(\\log\\left(\\widehat{RR}\\right)\\right) = \\sqrt{\\frac{1 - p_1}{p_1n_1} + \\frac{1 - p_2}{p_2n_2}}\\]\n\n\nThe riskratio(...) function from the epitools package will compute and back-transform the interval for you:\n\nriskratio(asthma.tbl, \n          rev = 'both', conf.level = 0.9,\n          method = 'wald', correction = T)\n\n\n\n        risk ratio with 90% C.I.\n         estimate    lower    upper\n  male   1.000000       NA       NA\n  female 1.572329 1.083353 2.282007\n\n\n\n\ncritical value \\(c\\) from normal model\nexponentiate for an interval for \\(RR\\)\n\n\nWith 90% confidence, the risk of asthma is estimated to be betwen 1.08 and 2.28 times greater for women than for men, with a point estimate of 1.57."
  },
  {
    "objectID": "content/week9-measures.html#implementation-details-1",
    "href": "content/week9-measures.html#implementation-details-1",
    "title": "Measures of association",
    "section": "Implementation details",
    "text": "Implementation details\n\n\n\nriskratio(asthma.tbl, \n          rev = 'both', conf.level = 0.9,\n          method = 'wald', correction = T)\n\n\n\n$data\n        \n         no asthma asthma Total\n  male         769     30   799\n  female       781     49   830\n  Total       1550     79  1629\n\n$measure\n        risk ratio with 90% C.I.\n         estimate    lower    upper\n  male   1.000000       NA       NA\n  female 1.572329 1.083353 2.282007\n\n$p.value\n        two-sided\n         midp.exact fisher.exact chi.square\n  male           NA           NA         NA\n  female 0.04412095   0.04961711 0.05703135\n\n\n\nImplementation is identical to oddsratio:\n\noutcome of interest should be second column\ngroup of interest should be second row\nrearrange the input data using rev\n\nAlso similarly:\n\nchi.square gives the \\(p\\)-value for the \\(\\chi^2\\) test of association\nfisher.exact gives an exact p-value you can use if assumptions aren’t met"
  },
  {
    "objectID": "content/week9-measures.html#interpretation-1",
    "href": "content/week9-measures.html#interpretation-1",
    "title": "Measures of association",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\nriskratio(asthma.tbl, \n          rev = 'both', conf.level = 0.9,\n          method = 'wald', correction = T)\n\n\n\n$data\n        \n         no asthma asthma Total\n  male         769     30   799\n  female       781     49   830\n  Total       1550     79  1629\n\n$measure\n        risk ratio with 90% C.I.\n         estimate    lower    upper\n  male   1.000000       NA       NA\n  female 1.572329 1.083353 2.282007\n\n$p.value\n        two-sided\n         midp.exact fisher.exact chi.square\n  male           NA           NA         NA\n  female 0.04412095   0.04961711 0.05703135\n\n\n\nFirst report the test result, then the measure of association:\n\nThe data provide evidence at the 10% significance level of an association between asthma and sex (\\(\\chi^2\\) = 3.62 on 1 degree of freedom, p = 0.057). With 90% confidence, the risk of asthma is estimated to be betwen 1.08 and 2.28 times greater for women than for men, with a point estimate of 1.57.\n\n\n\nComments:\n\nthe chi.square \\(p\\)-value is from the test of association/independence\nif assumptions aren’t met, can use the fisher.exact instead"
  },
  {
    "objectID": "content/week9-measures.html#further-examples-cyclosporiasis",
    "href": "content/week9-measures.html#further-examples-cyclosporiasis",
    "title": "Measures of association",
    "section": "Further examples: cyclosporiasis",
    "text": "Further examples: cyclosporiasis\n\n\nAn outbreak of cyclosporiasis was detected among residents of New Jersey. In a case-control study, investigators found that 21 of 30 case-patients and 4 of 60 controls had eaten raspberries.\n\n\n\n\n\n\n\n\n\n\n \nraspberries\nno raspberries\n\n\n\n\ncase\n21\n9\n\n\ncontrol\n4\n56\n\n\n\n\n\n\noutcome-based sampling\nodds ratio should be used\n\n\n\n# check test assumptions\nchisq.test(cyclo.tbl)$expected\n\n                cyclosporiasis\nraspberries           case  control\n  raspberries     8.333333 16.66667\n  no raspberries 21.666667 43.33333\n\n\n\n# compute measure of association and p value\noddsratio(cyclo.tbl, rev = 'both',\n          method = 'wald', correction = T)\n\n\n\n$measure\n                odds ratio with 95% C.I.\nraspberries      estimate    lower    upper\n  no raspberries  1.00000       NA       NA\n  raspberries    32.66667 9.081425 117.5048\n\n$p.value\n                two-sided\nraspberries        midp.exact fisher.exact   chi.square\n  no raspberries           NA           NA           NA\n  raspberries    6.358611e-10 6.183017e-10 1.247883e-09\n\n\n\n\n\nThe data provide very strong evidence of an association between eating raspberries during the outbreak and case incidence (\\(\\chi^2\\) = 36.89 on 1 degree of freedom, p &lt; 0.0001). With 95% confidence, the odds of indcidence are estimated to be between 9.08 and 117.5 times higher among NJ residents who ate raspberries during the outbreak."
  },
  {
    "objectID": "content/week9-measures.html#further-examples-smoking-and-chd",
    "href": "content/week9-measures.html#further-examples-smoking-and-chd",
    "title": "Measures of association",
    "section": "Further examples: smoking and CHD",
    "text": "Further examples: smoking and CHD\n\n\nA cohort study of 3,000 smokers and 5,000 nonsmokers investigated the link between smoking and the development of coronary heart disease (CHD) over 1 year.\n\n\n\n\n\n\n\n\n\n\n \nCHD\nno CHD\n\n\n\n\nsmoker\n84\n2916\n\n\nnonsmoker\n87\n4913\n\n\n\n\n\n\ntwo independent samples, but not outcome-based sampling\n\n\n\n# check test assumptions\nchisq.test(chd.tbl)$expected\n\n           chd\nsmoker          CHD   no CHD\n  smoker     64.125 2935.875\n  nonsmoker 106.875 4893.125\n\n\n\nriskratio(chd.tbl, rev = 'both',\n          method = 'wald', correction = T)\n\n\n\n$measure\n           risk ratio with 95% C.I.\nsmoker      estimate    lower    upper\n  nonsmoker 1.000000       NA       NA\n  smoker    1.609195 1.196452 2.164325\n\n$p.value\n           two-sided\nsmoker       midp.exact fisher.exact  chi.square\n  nonsmoker          NA           NA          NA\n  smoker    0.001799736  0.001800482 0.001976694\n\n\n\n\n\nThe data provide very strong evidence that smoking is associated with coronary heart disease (\\(\\chi^2\\) = 9.5711 on 1 degree of freedom, p = 0.00198). With 95% confidence, the risk of CHD is estimated to be between 1.196 and 2.164 times greater among smokers compared with nonsmokers, with a point estimate of 1.609."
  },
  {
    "objectID": "content/week9-measures.html#further-examples-malaria-vaccine",
    "href": "content/week9-measures.html#further-examples-malaria-vaccine",
    "title": "Measures of association",
    "section": "Further examples: malaria vaccine",
    "text": "Further examples: malaria vaccine\n\n\nIn a randomized trial for a malaria vaccine, 20 individuals were randomly allocated to receive a dose of the vaccine or a placebo.\n\n\n\n\n\n\n\n\n\n\n \ninfection\nno infection\n\n\n\n\nplacebo\n6\n0\n\n\nvaccine\n5\n9\n\n\n\n\n\n\nsmall sample size, so expected counts are likely too small to meet \\(\\chi^2\\) test assumptions\n\n\n\n# check test assumptions\nchisq.test(malaria.tbl)$expected\n\n         outcome\ntreatment infection no infection\n  placebo       3.3          2.7\n  vaccine       7.7          6.3\n\n\n\nriskratio(malaria.tbl, rev = 'columns',\n          method = 'wald', correction = T)\n\n\n\n$measure\n         risk ratio with 95% C.I.\ntreatment  estimate     lower     upper\n  placebo 1.0000000        NA        NA\n  vaccine 0.3571429 0.1768593 0.7212006\n\n$p.value\n         two-sided\ntreatment midp.exact fisher.exact chi.square\n  placebo         NA           NA         NA\n  vaccine  0.0119195   0.01408669 0.03094368\n\n\n\n\n\nThe data provide moderate evidence that the malaria vaccine is effective (Fisher’s exact test, p = 0.0141).\n\nA twist: how to interpret the CI? Answer: \\(\\text{efficacy = 1 - RR}\\)\n\nWith 95% confidence, the risk of infection is estimated to be between 27.88% and 82.31% lower among vaccinated individuals, with a point estimate of 64.29% efficacy.\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/test2.html",
    "href": "content/test2.html",
    "title": "Test 2",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/10. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test2.html#instructions",
    "href": "content/test2.html#instructions",
    "title": "Test 2",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/10. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test2.html#problems",
    "href": "content/test2.html#problems",
    "title": "Test 2",
    "section": "Problems",
    "text": "Problems\n\nProblem 1: gifted children\n\n[L3, L4, L5] The gifted dataset contains data on 36 children identified as gifted in a large city. Assume for the purpose of the problem that the data are from a random sample of gifted children in the metropolitan region where the data were collected.\n\n[L3] Is there a relationship between the mother’s IQ and the child’s test score? Construct a scatterplot and compute and interpret the correlation.\n[L3] Repeat but with the father’s IQ.\n[L5] Given your results in (a)-(b), formulate a hypothesis about which parent’s IQ is higher. Explain your reasoning.\n[L5] Construct a histogram of the pairwise differences between the mother’s IQ and father’s IQ for each child in the dataset and check the assumptions for inference using the \\(t\\) model.\n[L4, L5] Test the hypothesis you proposed in (c) at the 1% level and provide a corresponding interval estimate. Report your results in the narrative style introduced in class.\n[L4, L5] It’s thought that the mean age by which infants can count to 10 is around two years old. Test the hypothesis at the 5% level that gifted children do this sooner and provide a corresponding interval estimate. Interpret the test results and interval estimate in context following the narrative style introduced in class.\n\n\n\n\nProblem 2: lizard running speeds\n\n[L3, L4, L5] The dataset lizards contains running speeds measured in a laboratory race track for two species of lizards, Western Fence (WF) and Sagebrush (S) lizards. Speeds are recorded in meters per second.\n\n[L3] Construct side-by-side boxplots of top speed by species. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Compute point estimates and standard errors for the mean top speed for each species.\n[L4] Compute and interpret 99.5% confidence intervals for the mean top speed for each species.\n[L4, L5] Test for a difference in mean top speed between species at the 1% significance level and provide an interval estimate at the appropriate confidence level. Interpret the test and estimate following the narrative style introduced in class.\n\n\n\n\nProblem 3: self- and cross-fertilization and plant vigor\n\n[L3, L4, L5] Does self-fertilization produce less vigorous plants than cross-fertilization? The dataset plants contains measurements of plant heights in inches for 15 pairs of plants of the same age; one plant in each pair was grown from a seed from a cross-fertilized flower, and the other was grown from a seed from a self-fertilized flower.\n\n[L3] Visualize the distribution of differences in plant heights between the cross-fertilized and self-fertilized individuals. Does the plot alone suggest an answer to the question of interest?\n[L4, L5] Test, at the 2% level, whether mean height of plants grown from cross-fertilized seeds exceeds that of plants grown from self-fertilized seeds and provide a confidence bound for the difference at the level corresponding to your test. Report the results of your analysis in context following the narrative style introduced in class.\n\n\n\n\nProblem 4: creativity and motivation\n\n[L1, L2, L3, L4, L5] The creativity dataset contains data from an experiment on the effect of intrinsic vs. extrinsic motivation on creativity. A random sample of 47 creative writing students at an unnamed university were randomly assigned to one of two groups, extrinsic and intrinsic; each subject was instructed to write two short poems, but those in the extrinsic motivation group were primed on the task in a way that oriented them to external motivations for writing, and those in the intrinsic group were primed on the task in a way that oriented them to internal motivations for writing. Poems were scored by judges for creativity on a 40-point scale, and each subject received an average score.\n\n[L1] What is the study population? Based on the study description, is the sample representative, and if so, why?\n[L2] What type of study is this? Based on the study description, can the data support causal inferences about motivation and creativity, and if so, why?\n[L3] Construct an appropriate graphical summary comparing the distributions of average scores by treatment group.\n[L4] Provide point estimates and standard errors for the mean creativity score in each group.\n[L5] Test the hypothesis that motivational framing has no effect on creativity at the 1% significance level. Compute an interval estimate for the difference in mean scores at the level corresponding to your test. Use your results from (c)-(d) to check assumptions. Write a short narrative summary of your results following the style introduced in class. (Don’t forget to include a point estimate and standard error.)"
  },
  {
    "objectID": "content/test2-practice.html",
    "href": "content/test2-practice.html",
    "title": "Test 2 practice problems",
    "section": "",
    "text": "Test 2 information\nThe test will comprise four problems focused on point estimation, interval estimation, and hypothesis tests for means (learning outcomes L4-L5). Each problem will have multiple parts, some of which may require skills from earlier (especially summary statistics and statistical graphics).\nYou will have 48 hours to complete the test; a Posit cloud project will be provided with comment outlines to help you organize your calculations. You’ll submit your work via an online form, and will be expected to also upload your R script from your Posit cloud project.\nThe problems below are intended to help you practice the skills and concepts that will be assessed in the test. An expandable “solution” is provided below each prompt that shows the calculations needed to answer the prompts; of course, resolving the problems satisfactorily also requires interpreting results accurately. You’re encouraged to ask about interpretations in class.\n\n\nPractice problems\n\n[L3, L4] The tuition dataset contains in-state and out-of-state tuition at a random sample of 25 public universities from 2011-2012.\n\n[L3] Visualize the distribution of differences between in-state and out-of-state tuition. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Calculate and interpret a 95% confidence interval for the mean difference between in-state and out-of-state tuition.\n[L4] Interpret your interval in context following the style introduced in class.\n\n\n\n\nSolution\n# load data\nload('data/tuition.RData')\n\n# part a: visualize distribution of differences; are assumptions for use of t model met?\ntuition.diffs &lt;- tuition$out.of.state - tuition$in.state\nhist(tuition.diffs, breaks = 10)\n\n# part b: 95% interval estimate for differences\nt.test(tuition.diffs)$conf.int\n\n\n\n[L3, L4, L5] The dataset cancer contains skin cancer rates per 100,000 people in Connecticut each year from 1938 to 1972. Each year is also classified as following a period of higher than average or lower than average sunspot activity. The delta variable is the change in cancer rate relative to the previous year. In this problem, you’ll perform inference on the mean delta by sunspot activity level to determine whether higher than average sunspot activity is associated with an increase in mean skin cancer rates from the prior year.\n\n[L4] Estimate the mean delta (irrespective of sunspot activity level). Provide both a point estimate and standard error, and interpret the estimate in context. Does the estimate suggest that the cancer rate is increasing or decreasing? Explain.\n[L4, L5] Perform a test for mean delta to determine whether the mean cancer rate is increasing. Use a 5% significance level, and report your test result together with an interval estimate following the narrative style introduced in class.\n[L3] Plot the ‘raw’ cancer rate (i.e., not the delta) against year. (Add the argument type = 'b' to draw a path connecting the observations.) Is your answer in (b) consistent with any trend(s) you see?\n[L3] Make a side-by-side boxplot of the delta variable for each level of sunspot activity. Comment on the plot: does there seem to be a difference?\n[L5] Test whether the mean change in cancer rate is higher in years with higher than average sunspot activity. Carry out inference at the 5% significance level.\n\n\n\n\nSolution\n# load data\ncancer &lt;- read_csv('data/cancer.csv')\n\n# part a: point estimate of mean delta and standard error\nmean(cancer$delta)\nsd(cancer$delta)/sqrt(length(cancer))\n\n# part b: is delta increasing? test at 5% level\nt.test(cancer$delta, mu = 0, alternative = 'greater', conf.level = 0.95)\n\n# part c: plot \nplot(cancer$year, cancer$rate, type = 'b')\n\n# part d: boxplots by activity level; different?\nboxplot(delta ~ sunspot, data = cancer, horizontal = T)\n\n# part e: test for a difference in mean delta by sunspot activity at the 5% level\nt.test(delta ~ sunspot, alternative = 'greater', data = cancer, conf.level = 0.95)\n\n\n\n[L4, L5] Studies have provided evidence that the hippocampus is smaller in schizophrenic patients on average. The dataset hippocampus contains data on volumes of the left hippocampus in cubic centimeters for 15 pairs of monozygotic twins; one twin in each pair was affected by schizophrenia and the other was not.\n\n[L5] Compute the pairwise differences in hippocampal volume by twin pair and inspect the distribution. Do assumptions for inference using the \\(t\\) model seem plausible?\n[L5] Formulate a hypothesis to test whether hippocampal volume is smaller among the affected twin on average. Write the hypotheses in notation.\n[L4, L5] Carry out the test in (b) at the 1% significance level. Report your test result along with a corresponding interval estimate following the narrative style introduced in class.\n\n\n\n\nSolution\n# load and inspect data\ntwins &lt;- read_csv('data/hippocampus.csv')\nhead(twins)\n\n# part a: compute differences; check distribution for t inference assumptions\nhvolume.diff &lt;- twins$affected - twins$unaffected\nhist(hvolume.diff, breaks = 5)\n\n# part c: carry out test at 1% significance level\nt.test(hvolume.diff, mu = 0, alternative = 'less', conf.level = 0.99)"
  },
  {
    "objectID": "content/test1.html",
    "href": "content/test1.html",
    "title": "Test 1",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 1 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 4/19. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test1.html#instructions",
    "href": "content/test1.html#instructions",
    "title": "Test 1",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 1 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 4/19. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test1.html#question-prompts",
    "href": "content/test1.html#question-prompts",
    "title": "Test 1",
    "section": "Question prompts",
    "text": "Question prompts\n\n[L2, L3] The yrbss dataset contains measurements on a small collection of variables from 10,587 survey responses collected as part of the CDC’s Youth Risk Behavior Surveillance System (YRBSS) from 1991-2013. The objective of the survey program is to track behaviors with potential negative physical and mental health impacts among adolescents. In this problem you’ll explore the amount of sleep that respondents get on school nights and the number of days per week respondents are physically active.\n\n[L2] Read briefly about the YRBSS on the CDC website: https://www.cdc.gov/healthyyouth/data/yrbs/overview.htm. Based on the overview and the description above, are the data observational or experimental?\n[L1] Based on the overview (link in part (a)), identify the study population.\n[LX] Load the dataset and identify the type of each variable. What kind of variable is sleep.hours?\n[LX] Examine the frequency distributions of age and grade level. Do any grades or ages seem over-represented in the sample?\n[L3] Make a bar plot showing the frequency distribution of hours of sleep on school nights. Based on the summary, what is the typical amount of sleep respondents get on school nights?\n[L3] Make a stacked bar plot showing levels of sleep by grade. Do older students sleep more on school nights than younger students?\n[L3] Visualize the frequency distribution of the number of days per week that survey participants are physically active. Describe the distribution and interpret any patterns.\n\n\n\n[L2, L3] Diet restriction and longevity. The longevity dataset contains data from a study in which 237 mice were randomly allocated to one of four diets at different levels of restriction: no restriction (NP), normal 85kCal diet before and after weaning (N/N85), normal diet before weaning and restricted 50kCal diet after weaning (N/R50), and normal diet before weaning and restricted 40kCal diet after weaning (N/R40). Researchers recorded the lifetime in months of each mouse in the study.\n\n[L2] Was this an experiment or observational study and why?\n[L3] Find the average lifetime of mice in each diet group.\n[L3] Find the standard deviation of lifetimes in each diet group.\n[L3] Make a plot comparing lifetimes by diet group.\n[L2] Write a short summary of the study results based on your work in (b)-(d). Indicate specifically whether there appears to be a relationship between dietary caloric intake and lifetime.\n\n\n\n[L3] Brain and body size. The mammals dataset contains average body weights (kg) and average brain weights (g) for 62 common species of mammal, as well as log-transformed versions of those weights.\n\n[L3] Make a scatterplot of log brain weights against log body weights and describe the apparent relationship, if any.\n[L3] Compute and interpret the correlation between log brain weight and log body weight.\n[L3] Make a histogram of brain weights (not on the log scale) with an appropriate number of bins. Describe the distribution. Are there outliers?\n[L3] Based on (c), compute an appropriate measure of center and spread for brain weight.\n[L3] Which species of mammal has the largest average brain weight? Inspect the data directly using view(...) to answer this question.\n[L3] Which species of mammal has the smallest \\(\\frac{\\text{brain weight}}{\\text{body weight}}\\) ratio? The largest? Inspect the data directly using view(...) to answer this question.\n\n\n\n[L1, L2] The following is an excerpt from the abstract of the study that reported the results of the Moderna Covid vaccine phase three clinical trial1: “Vaccines are needed to prevent coronavirus disease 2019 (Covid-19) and to protect persons who are at high risk for complications. The mRNA-1273 vaccine is a lipid nanoparticle–encapsulated mRNA-based vaccine that encodes the prefusion stabilized full-length spike protein of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes Covid-19. This [study] was conducted at 99 centers across the United States … The trial enrolled 30,420 [adult volunteers with no known history of SARS-CoV-2 infection and no circumstances that put them at high risk of infection or severe Covid-19 or both,] who were randomly assigned in a 1:1 ratio to receive either vaccine or placebo (15,210 participants in each group) … Symptomatic Covid-19 illness was confirmed in 185 participants in the placebo group and in 11 participants in the mRNA-1273 group; vaccine efficacy was 94.1%.”\n\n[L2] Is this an experiment or observational study? Explain.\n[L1] Identify the study population.\n[L1] Describe the study sample.\n[L1] What outcome(s) were measured in the study?\n[L3] The moderna dataset contains simulated observations according to the study description. Make a contingency table and use it to construct a table showing the proportions of volunteers infected and not infected in each group.\n[L3] Optional. Find the relative risk of illness in the vaccine group compared with the placebo group. Can you determine how efficacy is defined?\n\n\n\n[L1, L2, L3] The temps dataset contains physical data collected on a number of individuals. Explore the dataset and write a brief summary of your descriptive analysis. While open-ended, your analysis should include descriptions of the variables and their statistical properties, and descriptions of relationships between the variables. Include at least one graphic related to your summary. Your summary need not be exhaustive – in fact, it is better to pick 1-2 interesting findings and report those, rather than describe everything you tried."
  },
  {
    "objectID": "content/test1.html#footnotes",
    "href": "content/test1.html#footnotes",
    "title": "Test 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBaden, L. R., et al. (2021). Efficacy and safety of the mRNA-1273 SARS-CoV-2 vaccine. New England journal of medicine, 384(5), 403-416.↩︎"
  },
  {
    "objectID": "content/week6-twosample.html#todays-agenda",
    "href": "content/week6-twosample.html#todays-agenda",
    "title": "Two sample inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] two sample inference for means\n[lab] two-sample \\(t\\) tests in R\n[test prep] practice problems"
  },
  {
    "objectID": "content/week6-twosample.html#from-last-time",
    "href": "content/week6-twosample.html#from-last-time",
    "title": "Two sample inference",
    "section": "From last time",
    "text": "From last time\nPractice problem: test whether actual body weight exceeds desired body weight.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubject\nactual\ndesired\ndifference\n\n\n\n\n1\n265\n225\n40\n\n\n2\n150\n150\n0\n\n\n3\n137\n150\n-13\n\n\n4\n159\n125\n34\n\n\n5\n145\n125\n20\n\n\n\n\n\n\n\nweight.diffs &lt;- brfss$weight - brfss$wtdesire\nt.test(weight.diffs, \n       mu = 0, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  weight.diffs\nt = 4.2172, df = 59, p-value = 4.311e-05\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 10.99824      Inf\nsample estimates:\nmean of x \n 18.21667 \n\n\n\n\n\nThe data provide very strong evidence that the average U.S. adult’s actual weight exceeds their desired weight (T = 4.2172 on 59 degrees of freedom, p &lt; 0.0001).\n\nInference is on the mean difference: \\(H_0: \\delta = 0\\) vs. \\(H_A: \\delta &gt; 0\\).\nCan we also do inference on a difference in means?"
  },
  {
    "objectID": "content/week6-twosample.html#evolution-of-darwins-finches",
    "href": "content/week6-twosample.html#evolution-of-darwins-finches",
    "title": "Two sample inference",
    "section": "Evolution of Darwin’s finches",
    "text": "Evolution of Darwin’s finches\n\n\nPeter and Rosemary Grant caught and measured birds from more than 20 generations of finches on Daphne Major.\n\nsevere drought in 1977 limited food to large tough seeds\nselection pressure favoring larger and stronger beaks\nhypothesis: beak depth increased in 1978 relative to 1976\n\n\n\n\n\n\n\n\n\n\n\nyear\ndepth\n\n\n\n\n1976\n10.8\n\n\n1976\n7.4\n\n\n1978\n11.4\n\n\n1978\n10.6\n\n\n\n\n\n\n\nTo answer this, we need to test a hypothesis involving two means:\n\\[\n\\begin{cases}\nH_0: &\\mu_{1976} = \\mu_{1978} \\\\\nH_A: &\\mu_{1976} &lt; \\mu_{1978}\n\\end{cases}\n\\]\n\ncan’t do inference on a mean difference here (no pairing of observations)\ntreat each year as an independent sample"
  },
  {
    "objectID": "content/week6-twosample.html#two-sample-inference",
    "href": "content/week6-twosample.html#two-sample-inference",
    "title": "Two sample inference",
    "section": "Two-sample inference",
    "text": "Two-sample inference\nIf \\(x_1, \\dots, x_{58}\\) are the 1976 observations and \\(y_1, \\dots, y_{65}\\) are the 1978 observations:\n\n\\(\\bar{x}\\) is a point estimate for \\(\\mu_{1976}\\) with standard error \\(SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\)\n\\(\\bar{y}\\) is a point estimate for \\(\\mu_{1978}\\) with standard error \\(SE(\\bar{y}) = \\frac{s_y}{\\sqrt{n}}\\)\n\n\n\nInference uses a new \\(T\\) statistic:\n\\[\nT = \\frac{\\bar{x} - \\bar{y} - \\delta_0}{SE(\\bar{x} - \\bar{y})}\n\\]\n\n\\(\\delta_0\\) is the hypothesized difference in means\n\\(SE(\\bar{x} - \\bar{y}) = \\sqrt{SE(\\bar{x})^2 + SE(\\bar{y})^2}\\)\n\\(t_\\nu\\) model approximates the sampling distribution when each sample meets assumptions for one-sample inference"
  },
  {
    "objectID": "content/week6-twosample.html#checking-assumptions",
    "href": "content/week6-twosample.html#checking-assumptions",
    "title": "Two sample inference",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\nThe two-sample test is appropriate whenever two one-sample tests would be.\n\n\n\nIn other words, the test assumes that both samples are either:\n\nsufficiently large; or\nhave little skew and few outliers\n\nTo check, simply inspect each histogram.\n\nboth distributions unimodal\nboth a bit left skewed\nno extreme outliers\nlarge sample sizes (58, 65)"
  },
  {
    "objectID": "content/week6-twosample.html#checking-assumptions-alternative",
    "href": "content/week6-twosample.html#checking-assumptions-alternative",
    "title": "Two sample inference",
    "section": "Checking assumptions (alternative)",
    "text": "Checking assumptions (alternative)\n\nThe two-sample test is appropriate whenever two one-sample tests would be.\n\n\n\nIn other words, the test assumes that both samples are either:\n\nsufficiently large; or\nhave little skew and few outliers\n\nCould also check side-by-side boxplots for:\n\napproximate symmetry of boxes\noutliers far from whiskers\n\nThis is also a nice visualization of differences between samples."
  },
  {
    "objectID": "content/week6-twosample.html#interpreting-outputs-and-results",
    "href": "content/week6-twosample.html#interpreting-outputs-and-results",
    "title": "Two sample inference",
    "section": "Interpreting outputs and results",
    "text": "Interpreting outputs and results\n\n\n\nt.test(depth ~ year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n\n\n\n\nThe data provide very strong evidence that mean beak depth increased following the drought (T = -4.5727 on 111.79 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean increase is estimated to be at least 0.4699 mm, with a point estimate of 0.7373 (SE 0.1612).\n\n\n\nHighly similar, but notice:\n\ninput is a formula depth ~ year (“depth depends on year”) and data frame finch\nmu now indicates hypothesized difference in means\ndecimal degrees of freedom\nalternative is relative to the order in which groups appear"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data",
    "href": "content/week6-twosample.html#cloud-data",
    "title": "Two sample inference",
    "section": "Cloud data",
    "text": "Cloud data\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\nData are rainfall measurements in a target area from 26 days when clouds were seeded and 26 days when clouds were not seeded.\n\nrainfall gives volume of rainfall in acre-feet\ntreatment indicates whether clouds were seeded\n\nHypotheses to test: \\[\n\\begin{cases}\nH_0: &\\mu_\\text{seeded} = \\mu_\\text{unseeded} \\\\\nH_A: &\\mu_\\text{seeded} &gt; \\mu_\\text{unseeded}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\nrainfall\ntreatment\n\n\n\n\n334.1\nseeded\n\n\n489.1\nseeded\n\n\n200.7\nseeded\n\n\n40.6\nseeded\n\n\n21.7\nunseeded\n\n\n17.3\nunseeded\n\n\n68.5\nunseeded\n\n\n830.1\nunseeded"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-which-alternative",
    "href": "content/week6-twosample.html#cloud-data-which-alternative",
    "title": "Two sample inference",
    "section": "Cloud data: which alternative?",
    "text": "Cloud data: which alternative?\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.9731\nalternative hypothesis: true difference in means between group seeded and group unseeded is less than 0\n95 percent confidence interval:\n     -Inf 512.1582\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\n\nYou can tell which group R considers first based on which estimate is printed first.\n\n'greater' is interpreted as [FIRST GROUP] &gt; [SECOND GROUP]\n'less' is interpreted as [FIRST GROUP] &lt; [SECOND GROUP]"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-interpretation",
    "href": "content/week6-twosample.html#cloud-data-interpretation",
    "title": "Two sample inference",
    "section": "Cloud data: interpretation",
    "text": "Cloud data: interpretation\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\n\nThe data provide moderate evidence that cloud seeding increases mean rainfall (T = 1.9982 on 33.855 degrees of freedom, p = 0.02689). With 95% confidence, seeding is estimated to increase mean rainfall by at least 42.63 acre-feet, with a point estimate of 277.4 (SE 138.8199)."
  },
  {
    "objectID": "content/week6-twosample.html#body-temperatures-again",
    "href": "content/week6-twosample.html#body-temperatures-again",
    "title": "Two sample inference",
    "section": "Body temperatures (again)",
    "text": "Body temperatures (again)\n\nDoes mean body temperature differ between men and women?\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest \\(H_0: \\mu_F = \\mu_M\\) against \\(H_A: \\mu_F \\neq \\mu_M\\)\n\nt.test(body.temp ~ sex, data = temps, \n       mu = 0, alternative = 'two.sided')\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 1.7118, df = 34.329, p-value = 0.09595\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.09204497  1.07783444\nsample estimates:\nmean in group female   mean in group male \n            98.65789             98.16500 \n\n\n\n\nSuggestive but insufficient evidence that mean body temperature differs by sex.\nNotice: estimated difference (F - M) is 0.493 °F (SE 0.2879)"
  },
  {
    "objectID": "content/week6-twosample.html#what-if-we-had-more-data",
    "href": "content/week6-twosample.html#what-if-we-had-more-data",
    "title": "Two sample inference",
    "section": "What if we had more data?",
    "text": "What if we had more data?\nHere are estimates from two larger samples of 65 individuals each (compared with 19, 20):\n\n\n\n\n\n\n\n\n\n\n\nsex\nmean.temp\nse\nn\n\n\n\n\nfemale\n98.39\n0.09222\n65\n\n\nmale\n98.1\n0.08667\n65\n\n\n\n\n\n\nestimated difference (F - M) is smaller 0.2892 °F\nbut so is the standard error SE 0.1266 (recall more data \\(\\longleftrightarrow\\) better precision)\n\n\n\n\nt.test(body.temp ~ sex, data = temps.aug, \n       mu = 0, alternative = 'two.sided')\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 2.2854, df = 127.51, p-value = 0.02394\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n 0.03881298 0.53964856\nsample estimates:\nmean in group female   mean in group male \n            98.39385             98.10462 \n\n\n\n\nThe data provide moderate evidence that mean body temperature differs by sex (T = 2.29 on 127.51 degrees of freedom, p = 0.02394)."
  },
  {
    "objectID": "content/week6-twosample.html#power-calculations",
    "href": "content/week6-twosample.html#power-calculations",
    "title": "Two sample inference",
    "section": "Power calculations",
    "text": "Power calculations\n\nHow much data do you need to collect in order to detect a difference of \\(\\delta\\)?\n\n\n\nThe statistical power of a test captures how often it detects a specified alternative.\n\nmeasures how often the test correctly rejects (proportion of samples)\nvalue depends on…\n\nmagnitude of difference between null value and true value of parameter\nsignificance level\nsample size\n\n\n\n\npower.t.test(power = 0.95, \n             delta = 0.5, \n             sig.level = 0.05, \n             type = 'two.sample',\n             alternative = 'two.sided')\n\n\n     Two-sample t test power calculation \n\n              n = 104.928\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\\(\\Rightarrow\\) need 105 observations in each group to detect a difference of 0.5 standard deviations for 95% of samples with a 5% significance level test"
  },
  {
    "objectID": "content/week6-twosample.html#a-statistical-trap",
    "href": "content/week6-twosample.html#a-statistical-trap",
    "title": "Two sample inference",
    "section": "A statistical trap",
    "text": "A statistical trap\n\nIf you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo keep in mind:\n\nstatistical significance \\(\\neq\\) practical significance\nalways check your point estimates"
  },
  {
    "objectID": "content/week6-twosample.html#the-equal-variance-t-test",
    "href": "content/week6-twosample.html#the-equal-variance-t-test",
    "title": "Two sample inference",
    "section": "The equal-variance \\(t\\)-test",
    "text": "The equal-variance \\(t\\)-test\nIf it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power by using a different standard error:\n\\[SE_\\text{pooled}(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{\\color{red}{s_p^2}}{n_x} + \\frac{\\color{red}{s_p^2}}{n_y}}\n\\quad\\text{where}\\quad \\color{red}{s_p} = \\underbrace{\\sqrt{\\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\\text{weighted average of } s_x^2 \\;\\&\\; s_y^2}\\]\nImplement by adding var.equal = T as an argument to t.test().\n\nlarger df is used, hence more frequent rejections\navoid unless you have a small sample\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab4-estimation.html",
    "href": "content/lab4-estimation.html",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "",
    "text": "library(tidyverse)\nload('data/nhanes.RData')\nload('data/temps.RData')\n\nThe goal of this lab is to learn to compute a point estimate, standard error, and interval estimate for a population mean “by hand” by performing the arithmetic directly in R. You will have an opportunity to practice interpreting these quantities as you go.\nWe will also use this lab for a short class activity to explore how often the interval estimate we introduced is “correct”.\n\nPoint estimation\n\nEstimate for the population mean\nSince the point estimate for the population mean of a numeric variable is the sample mean, you already know how to perform the calculation in R. We’ll store this for later use:\n\n# retrieve total cholesterol variable\ntotchol &lt;- nhanes$totchol\n\n# compute and store sample mean\ntotchol.mean &lt;- mean(totchol)\ntotchol.mean\n\n[1] 5.042938\n\n\nThe only novelty here is that we now interpret this as a point estimate of the population mean total cholesterol:\n\nThe mean total cholesterol of U.S. adults is estimated to be 5.043 mmol/L.\n\nThis is in contrast to the interpretation as a descriptive summary:\n\nThe average total cholesterol among the respondents in the NHANES survey was 5.043 mmol/L.\n\nBoth interpretations are valid; just different. By interpreting the sample mean as a point estimate, we are implicitly assuming that the data are a random sample from the U.S. adult population.\n\n\n\n\n\n\nYour turn\n\n\n\nUse the temps data to estimate mean body temperature.\n\n# retrieve variable of interest\n\n# compute and store sample mean\n\nCheck your understanding:\n\ninterpret the result as a descriptive summary\ninterpret the result as a point estimate\n\n\n\n\n\nStandard error for the sample mean\nA standard error is a measure of the sampling variability of a point estimate. Technically, it’s an estimate of the point estimate’s standard deviation across all possible random samples of a fixed size.\nThe standard error for the sample mean is calculated according to the formula: \\[SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\] Where:\n\n\\(s_x\\) is the sample standard deviation\n\\(n\\) is the sample size\n\nTo calculate this in R, we perform the arithmetic by hand (for now):\n\n# store sample sd and sample size\ntotchol.sd &lt;- sd(totchol)\ntotchol.n &lt;- length(totchol)\n\n# compute standard error\ntotchol.se &lt;- totchol.sd/sqrt(totchol.n)\ntotchol.se\n\n[1] 0.01906042\n\n\nThis result is interpreted as follows:\n\nThe root average deviation of the sample mean from the population mean is estimated to be 0.0191 mmol/L.\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error\n\n\n\n\n\n\nInterval estimation\n\nInterval estimate for the mean\nA common interval for the population mean is:\n\\[\\bar{x} \\pm \\underbrace{2\\times SE(\\bar{x})}_{\\text{margin of error}}\\]\nFor now, we’ll calculate this by directly performing the arithmetic. Later, you’ll use commands that return interval estimates by default.\n\n# add/subtract two standard errors from the mean\ntotchol.mean + c(-2, 2)*totchol.se\n\n[1] 5.004817 5.081059\n\n\nWe’ll talk more about the exact interpretation later; for now, you should think of this as a range of plausible values for the population mean.\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate an interval estimate for the mean body temperature using the body temperature data.\n\n# interval estimate for mean body temp\n\n\n\n\n\nHow often is the interval correct?\nAn interval “covers” the population mean if the true value is between the interval endpoints.\nWe can explore how often the interval covers the parameter by having everyone in the class simulate their own sample from a population with a known mean and check whether the interval they obtain from the sample covers the population mean or not.\nThe commands below simulate a sample and then compute an interval.\n\n# function to simulate body temp data from a population with mean 98.6\nsample.bodytemps &lt;- function(n){\n  rnorm(n, mean = 98.6, sd = 1)\n}\n\n# simulate a sample of body temperatures\nbodytemp &lt;- sample.bodytemps(n = 150)\n\n# compute interval 'ingredients'\nbodytemp.mean &lt;- mean(bodytemp)\nbodytemp.sd &lt;- sd(bodytemp)\nbodytemp.n &lt;- length(bodytemp)\nbodytemp.se &lt;- bodytemp.sd/sqrt(bodytemp.n)\n\n# compute interval estimate\nbodytemp.mean + c(-2, 2)*bodytemp.se\n\n[1] 98.43491 98.77259\n\n# margin of error\n2*bodytemp.se\n\n[1] 0.1688409\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nUse the example above to generate a sample of size 20 and compute an interval estimate for the mean body temperature.\nThen:\n\nDetermine whether your interval covers the population mean.\nCompute the margin of error used in your interval (\\(2\\times SE(\\bar{x})\\)).\n\nRepeat with \\(n = 150\\). Then fill out this form.\n\n\n\n\n\nPractice problems\n\nVu and Harrington exercise 4.1. Additionally:\n\nCompute an interval estimate for the mean BGC of nests.\nSupposing a sample of 30 nests returned exactly the same summary statistics, recompute your interval in (e). Is the margin of error smaller or larger?\n\n\n\nThe brfss dataset contains a measurement of body weight, weight, as well as a variable, wtdesire, that is the desired weight reported by respondents.\n\nEstimate the mean difference between actual and desired weight. Report the point estimate and standard error.\nDoes the point estimate suggest that the average U.S. adult would prefer to lose or gain weight?\nCompute an interval estimate for the mean difference between actual and desired weight."
  },
  {
    "objectID": "content/week2-descriptive.html#todays-agenda",
    "href": "content/week2-descriptive.html#todays-agenda",
    "title": "Descriptive statistics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] frequency distributions; measures of spread and center\n[lab] descriptive statistics and simple graphics in R"
  },
  {
    "objectID": "content/week2-descriptive.html#last-time",
    "href": "content/week2-descriptive.html#last-time",
    "title": "Descriptive statistics",
    "section": "Last time",
    "text": "Last time\n\n\n\nData semantics\n\n\ncategorical data: ordinal (ordered) or nominal (unordered)\nnumeric data: continuous (no ‘gaps’) or discrete (‘gaps’)\n\n\nData types and data structures in R\n\n\nbasic types: numeric, character, logical, integer\na vector is a collection of values of one type\na data frame is a type-heterogeneous list of vectors of equal length\n\n\nVectors can store observations of one variable:\n\n# 4 observations of age\nages &lt;- c(18, 22, 18, 12)\nages\n\n[1] 18 22 18 12\n\n\nData frames can store observations of many variables:\n\n# 3 observations of 2 variables\ndata.frame(subject.id = c(11, 2, 31),\n           age = c(24, 31, 17),\n           sex = c('m', 'm', 'f'))\n\n  subject.id age sex\n1         11  24   m\n2          2  31   m\n3         31  17   f\n\n\n\n\nTechniques for summarizing data depend on the data type"
  },
  {
    "objectID": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "href": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "title": "Descriptive statistics",
    "section": "What are descriptive statistics?",
    "text": "What are descriptive statistics?\nWe learned last time that a statistic is a data summary, i.e., any function of a set of observations.\nDescriptive statistics refers to analysis of sample characteristics using summary statistics.\n\nthese are data analyses that uses statistics interpreted on face value\nin contrast to inferential statistics, which uses statistics interpreted relative to a broader population\n\nDescriptive statistics can be either numerical or graphical; we’ll discuss both."
  },
  {
    "objectID": "content/week2-descriptive.html#dataset-famuss-study",
    "href": "content/week2-descriptive.html#dataset-famuss-study",
    "title": "Descriptive statistics",
    "section": "Dataset: FAMuSS study",
    "text": "Dataset: FAMuSS study\nObservational study of 595 individuals comparing change in arm strength before and after resistance training between genotypes for a region of interest on the ACTN3 gene.\n\nPescatello, L. S., et al. (2013). Highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or FAMuSS study. BioMed research international.\n\n\n\n\nExample data rows\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n125\n0\nFemale\n40\nCaucasian\n68\n171\nCT\n26"
  },
  {
    "objectID": "content/week2-descriptive.html#categorical-frequency-distributions",
    "href": "content/week2-descriptive.html#categorical-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Categorical frequency distributions",
    "text": "Categorical frequency distributions\nFor categorical variables, the frequency distribution is simply an observation count by category. For example:\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\ngenotype\n\n\n\n\n494\nTT\n\n\n510\nTT\n\n\n216\nCT\n\n\n19\nTT\n\n\n278\nCT\n\n\n86\nTT\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n173\n261\n161"
  },
  {
    "objectID": "content/week2-descriptive.html#numeric-frequency-distributions",
    "href": "content/week2-descriptive.html#numeric-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Numeric frequency distributions",
    "text": "Numeric frequency distributions\nFrequency distributions of numeric variables are observation counts by range; a plot of a numeric frequency distribution is called a histogram.\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\nbmi\n\n\n\n\n194\n22.3\n\n\n141\n20.76\n\n\n313\n23.48\n\n\n522\n29.29\n\n\n504\n42.28\n\n\n273\n20.34\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\n\n(10,20]\n(20,30]\n(30,40]\n(40,50]\n\n\n\n\n69\n461\n58\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe operation of dividing a numeric variable into interval ranges is called binning."
  },
  {
    "objectID": "content/week2-descriptive.html#histograms-and-binning",
    "href": "content/week2-descriptive.html#histograms-and-binning",
    "title": "Descriptive statistics",
    "section": "Histograms and binning",
    "text": "Histograms and binning\nBinning has a big effect on the visual impression. Which one captures the shape best?"
  },
  {
    "objectID": "content/week2-descriptive.html#shapes",
    "href": "content/week2-descriptive.html#shapes",
    "title": "Descriptive statistics",
    "section": "Shapes",
    "text": "Shapes\nFor numeric variables, the histogram reveals the shape of the distribution:\n\nsymmetric if it shows left-right symmetry about a central value\nskewed if it stretches farther in one direction from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#modes",
    "href": "content/week2-descriptive.html#modes",
    "title": "Descriptive statistics",
    "section": "Modes",
    "text": "Modes\nHistograms also reveal the number of modes or local peaks of frequency distributions.\n\nuniform if there are zero peaks\nunimodal if there is one peak\nbimodal if there are two peaks\nmultimodal if there are two or more peaks"
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nConsider four variables from the FAMuSS study. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nHere are some made-up data. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#descriptive-measures",
    "href": "content/week2-descriptive.html#descriptive-measures",
    "title": "Descriptive statistics",
    "section": "Descriptive measures",
    "text": "Descriptive measures\nA descriptive measure is a summary statistic that captures a particular feature of the frequency distribution of a numeric variable.\nCommonly, measures capture either location or spread.\n\n\nMeasures of location:\n\nmean\nmedian\nmode\npercentiles/quantiles\n\n\nMeasures of spread:\n\nrange (min and max)\ninterquartile range\naverage deviation\nvariance\nstandard deviation\n\n\n\nIt is common practice to report multiple measures."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-location",
    "href": "content/week2-descriptive.html#measures-of-location",
    "title": "Descriptive statistics",
    "section": "Measures of location",
    "text": "Measures of location\nOften location is specified by the “center” of a frequency distribution.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23"
  },
  {
    "objectID": "content/week2-descriptive.html#quick-example",
    "href": "content/week2-descriptive.html#quick-example",
    "title": "Descriptive statistics",
    "section": "Quick example",
    "text": "Quick example\nConsider the first 8 observations of change in nondominant arm strength from the FAMuSS study data:\n\n\n40, 25, 40, 125, 40, 75, 100 and 57.1\n\n\nCompute the mean, median, and mode."
  },
  {
    "objectID": "content/week2-descriptive.html#comparing-measures-of-center",
    "href": "content/week2-descriptive.html#comparing-measures-of-center",
    "title": "Descriptive statistics",
    "section": "Comparing measures of center",
    "text": "Comparing measures of center\nEach statistic is a little different, but often they roughly agree; for example, all are between 20 and 25, which seems to capture the typical BMI well enough.\n\nHow do you think the frequency distribution affects which one is “best”?"
  },
  {
    "objectID": "content/week2-descriptive.html#means-medians-and-skewness",
    "href": "content/week2-descriptive.html#means-medians-and-skewness",
    "title": "Descriptive statistics",
    "section": "Means, medians, and skewness",
    "text": "Means, medians, and skewness\nThe mean and median both get ‘pulled’ in the direction of skewness, but the mean is more sensitive:\n\nComparing means and medians captures information about skewness present since:\n\nmean \\(&gt;\\) median: right skew\nmean \\(&lt;\\) median: left skew\nmean \\(\\approx\\) median: symmetric"
  },
  {
    "objectID": "content/week2-descriptive.html#when-to-use-modes",
    "href": "content/week2-descriptive.html#when-to-use-modes",
    "title": "Descriptive statistics",
    "section": "When to use mode(s)",
    "text": "When to use mode(s)\nMode is rarely used unless extreme skewness or multiple modes are present; below are two examples."
  },
  {
    "objectID": "content/week2-descriptive.html#percentiles",
    "href": "content/week2-descriptive.html#percentiles",
    "title": "Descriptive statistics",
    "section": "Percentiles",
    "text": "Percentiles\nA percentile is a value with specified proportions of data lying both above and below that value.\n\nmeasure of location (but not center)\ndefined with reference to the percentage of data below\n\nFor example, the 20th percentile is the value with 20% of observations below and 80% of observations above. Suppose we have 5 observations:\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n19\n20\n21\n25\n31\n\n\nrank\n1\n2\n3\n4\n5\n\n\n\n\n\nThe 20th percentile is not unique! In fact any number between 19 and 20 is a 20th percentile since it would satisfy:\n\n20% below (19)\n80% above (20, 21, 25, 31)"
  },
  {
    "objectID": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "href": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "title": "Descriptive statistics",
    "section": "Cumulative frequency distribution",
    "text": "Cumulative frequency distribution\nThe cumulative frequency distribution is a data summary showing percentiles. Think of it as percentile (y) against value (x).\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of some specific values:\n\nabout 40% of the subjects are 20 or younger\nabout 80% of the subjects are 24 or younger\n\nYour turn:\n\nRoughly what percentage of subjects are 22 or younger?\nAbout what age is the 10th percentile?"
  },
  {
    "objectID": "content/week2-descriptive.html#common-percentiles",
    "href": "content/week2-descriptive.html#common-percentiles",
    "title": "Descriptive statistics",
    "section": "Common percentiles",
    "text": "Common percentiles\n\n\nThe five-number summary is a collection of five percentiles that succinctly describe the frequency distribution:\n\n\n\nStatistic name\nMeaning\n\n\n\n\nminimum\n0th percentile\n\n\nfirst quartile\n25th percentile\n\n\nmedian\n50th percentile\n\n\nthird quartile\n75th percentile\n\n\nmaximum\n100th percentile\n\n\n\n\nBoxplots provide a graphical display of the five-number summary."
  },
  {
    "objectID": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "href": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "title": "Descriptive statistics",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\nNotice how the two displays align, and also how they differ. The histogram shows shape in greater detail, but the boxplot is much more compact."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-spread",
    "href": "content/week2-descriptive.html#measures-of-spread",
    "title": "Descriptive statistics",
    "section": "Measures of spread",
    "text": "Measures of spread\nThe spread of observations refers to how concentrated or diffuse the values are.\n\nTwo ways to understand and measure spread:\n\nranges of values capturing much of the distribution\ndeviations of values from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#range-based-measures",
    "href": "content/week2-descriptive.html#range-based-measures",
    "title": "Descriptive statistics",
    "section": "Range-based measures",
    "text": "Range-based measures\nA simple way to understand and measure spread is based on ranges. Consider more ages, sorted and ranked:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\nrank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\nThe range is the minimum and maximum values: \\[\\text{range} = (\\text{min}, \\text{max}) = (16, 34)\\]\nThe interquartile range (IQR) is the difference [75th percentile] - [25th percentile] \\[\\text{IQR} = 29 - 19 = 10\\] When might you prefer IQR to range? Can you think of an example?"
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures",
    "href": "content/week2-descriptive.html#deviation-based-measures",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe average deviation is defined as the average of the absolute values of the deviations from the mean: \\[\\frac{8 + 6 + 5 + 4 + 3 + 2 + 1 + 2 + 4 + 5 + 6 + 10}{12}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures-1",
    "href": "content/week2-descriptive.html#deviation-based-measures-1",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe variance is the average squared deviation from the mean (but divided by one less than the sample size): \\[\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}\\]\nThe standard deviation is the square root of the variance: \\[\\sqrt{\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#mathematical-notations",
    "href": "content/week2-descriptive.html#mathematical-notations",
    "title": "Descriptive statistics",
    "section": "Mathematical notations",
    "text": "Mathematical notations\nFollowing the convention from before, write a set of \\(n\\) observations as \\(x_1, x_2, \\dots, x_n\\).\n\n\nThe mean of the observations is written: \\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]\nThe average deviation is: \\[\\frac{1}{n} \\sum_i |x_i - \\bar{x}|\\]\n\nThe variance is: \\[s_x^2 = \\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2\\]\nThe standard deviation is: \\[s_x = \\sqrt{\\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#interpretations",
    "href": "content/week2-descriptive.html#interpretations",
    "title": "Descriptive statistics",
    "section": "Interpretations",
    "text": "Interpretations\nListed from largest to smallest, here are each of the measures of spread for the 12 ages:\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin\nmax\niqr\nvariance\nst.dev\navg.dev\n\n\n\n\n16\n34\n8.5\n30.55\n5.527\n4.667\n\n\n\n\n\nThe interpretations differ between these statistics:\n\n[range] all of the data lies on an between 16 and 34 years old on an interval 18 years in width\n[IQR] the middle half of the data lies on an interval 8.5 years in width\n[average deviation] the average distance from the mean is 4.67 years\n[variance] the average squared distance from the mean is 30.55 years\\(^2\\)\n[standard deviation] the average squared distance from the mean, rescaled to years, is 5.53 years\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab10-posthoc.html",
    "href": "content/lab10-posthoc.html",
    "title": "Lab 10: Post-hoc inference in ANVOA",
    "section": "",
    "text": "The objective of this lab is to learn how to perform post-hoc inference for group means and contrasts in R. These procedures are called post-hoc because they are typically performed after detecting a treatment effect or difference in means using the ANOVA \\(F\\) test.\nIn other words, suppose you tested the hypotheses: \\[\n\\begin{cases}\nH_0: &\\mu_i = \\mu_j \\quad\\text{for all}\\quad &i, j \\\\\nH_A: &\\mu_i \\neq \\mu_j \\quad\\text{for some}\\quad &i\\neq j \\\\\n\\end{cases}\n\\] And found evidence favoring \\(H_A\\). This raises the question, “which means differ and by how much?”\nWe will cover:\n\nSimultaneous interval estimates for \\(\\mu_i\\)\nIntervals for pairwise contrasts \\(\\mu_i - \\mu_j\\)\nSignificance tests for pairwise contrasts \\(\\mu_i - \\mu_j\\)\nInference for contrasts with a control group \\(\\mu_i - \\mu_\\text{ctrl}\\)\n\nWe will use the \\(\\texttt{emmeans}\\) package in R. However, you should be advised that there are other common implementations of these procedures that you might encounter if you search on your own.\nExamples will use the longevity dataset, which contain observations of lifetimes of mice randomly allocated to four different diet restriction groups. You’ll practice using the anorexia dataset, which contains observations of percent change in body weight after a treatment period for young women randomly allocated to two treatment groups and a control group.\n\nlibrary(tidyverse)\nlibrary(emmeans)\nload('data/longevity.RData')\nload('data/anorexia.RData')\n\n\nRefresher: fitting ANOVA models\nPost-hoc inferences all utilize fitted ANOVA models. We will skip the step of making a graphical check on model assumptions, and proceed directly with fitting models. However, if you’re not sure what that step consists of, you should take a moment to look at the previous lab to remind yourself.\nFor the longevity data, inference compares the mean lifetime in months for four levels of dietary restriction.\n\n# fit the model\nfit.longevity &lt;- aov(lifetime ~ diet, data = longevity)\n\n# generate the ANOVA table\nsummary(fit.longevity)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndiet          3  11426    3809   87.41 &lt;2e-16 ***\nResiduals   233  10152      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA \\(F\\) test is significant, indicating an effect of diet restriction on mean lifetime.\n\nThe data provide evidence of an effect of diet restriction on mean lifetime among mice (F = 87.41 on 3 & 233 degrees of freedom, p &lt; 0.0001).\n\n\n\n\n\n\n\nYour turn 1\n\n\n\nFit an ANOVA model to the anorexia data. Here, inference is on the mean percent change in body weight. Interpret the result of the test in context.\n\n\ntibble [72 × 2] (S3: tbl_df/tbl/data.frame)\n $ pct.change: num [1:72] 0.994 0.896 0.941 1.166 0.974 ...\n $ treatment : Factor w/ 3 levels \"Cont\",\"CBT\",\"FT\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nThe emmeans(...) function (and other related functions) work directly with fitted ANOVA models to produce inferences for group means and contrasts. emmeans(...) is short for “estimated marginal means”. The function takes as its arguments a fitted ANOVA model, and a “specification” in the form of a formula that determines its precise behavior:\nemmeans(object = &lt;FITTED MODEL&gt;, spec = &lt;SPECIFICATION&gt;)\nFor us, the specification is always a one-sided formula simply reiterating the grouping variable.\n\n# default behavior is to produce unadjusted 95% confidence intervals for group means\nemmeans(object = fit.longevity, specs = ~ diet)\n\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.5     29.3\n N/N85   32.7 0.874 233     31.0     34.4\n N/R50   42.3 0.783 233     40.8     43.8\n N/R40   45.1 0.852 233     43.4     46.8\n\nConfidence level used: 0.95 \n\n\nThe result can be piped to helper functions to obtain estimates of group means and contrasts.\n\n\nEstimating group means\nInitially we might like to estimate the group means. All we need to do is implement the adjustment for multiple inference and specify the confidence level. This is done using the confint(...) helper function:\n\n# simultaneous 95% confidence intervals for group means with bonferroni adjustment (correct)\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.0     29.8\n N/N85   32.7 0.874 233     30.5     34.9\n N/R50   42.3 0.783 233     40.3     44.3\n N/R40   45.1 0.852 233     43.0     47.3\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \n\n\nThe results can be interpreted in the usual way, for instance:\n\nWith 95% confidence, the mean lifetime for mice on a normal 85kcal diet is estimated to be between 30.5 and 34.9 months, with a point estimate of 32.7 months (SE 0.874).\n\nBecause of the Bonferroni adjustment, the confidence level is simultaneous for all intervals, meaning that all four intervals cover the mean at the same time 95% of the time.\n\n\n\n\n\n\nYour turn 2\n\n\n\nUsing the anorexia data, compute simultaneous 99% confidence intervals for the mean percent change in body weight in each treatment group and the control group. Interpret the interval for the family therapy (FT) group.\n\n\nSometimes a plot is preferable to a table of estimates. This is accomplished by adding one more pipe to plot(...) and specifying labels:\n\n# plot via: emmeans(...) |&gt; confint(...) |&gt; plot(...)\nemmeans(object = fit.longevity, specs = ~ diet) |&gt; \n  confint(level = 0.95, adjust = 'bonferroni') |&gt; \n  plot(xlab = 'mean lifetime (months)', ylab = 'diet')\n\n\n\n\n\n\n\n\nIf you’re curious, remove the label arguments and see what the default looks like.\n\n\n\n\n\n\nYour turn 3\n\n\n\nMake a plot showing the simultaneous 99% interval estimates for the mean percent change in body weight that you computed in the previous “your turn”.\n\n\n\n\nEstimating contrasts\nA difference in means is an example of a “contrast”. Inferences for contrasts allow us to determine which groups differ and by how much. There are several types of contrasts, but the most common are pairwise differences in means and differences between treatments and a control group.\n\nInference for pairwise contrasts\nFirst we’ll consider computing intervals and tests for all pairwise contrasts. This is accomplished by simply passing the result of emmeans(...) to contrast(...) and specifying the type of contrast you wish to obtain. The result can be passed to test() to obtain tests and confint() to obtain intervals. In the context of the longevity example:\n\n# test for pairwise differences at the 5% significance level\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  test(adjust = 'bonferroni')\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n\n# simultaneous 95% intervals for all pairwise contrasts\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n\n\nThe tests indicate which means differ significantly; the intervals indicate by how much. For example:\n\nThe data provide strong evidence that mean lifespan differs significantly between mice on a normal 85kcal compared with mice on an unrestricted diet (p = 0.0003). With 95% confidence, the difference in mean lifespan (normal - unrestricted) is estimated to be between 1.87 and 8.71 months, with a point estimate of 5.29 (SE 1.29).\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nUsing the data from the anorexia study…\n\nCompute simultaneous 90% confidence intervals for all pairwise contrasts.\nCompute adjusted \\(p\\)-values for all pairwise contrasts and determine which groups differ significantly at the 10% level.\nInterpret the test and interval for the contrast between family therapy and the control group.\n\n\n\n\n\nContrasts with a control\nMany studies involve a control group; naturally, it is of interest to compare treatments to controls. This is a special category of contrasts because all contrasts involve the same group; as such, there is a special adjustment method for multiple inference that achieves better power for this particular setting.\nTo perform inference for contrasts with a control, change the contrast type from 'pairwise' to 'trt.vs.ctrl'. R will assume that your control group is the first level of the grouping variable. The datasets for this class are organized in just this way, so you don’t have to worry about this detail for now.\n\n# tests for contrasts with a control group\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('trt.vs.ctrl') |&gt;\n  test(adjust = 'dunnett')\n\n contrast     estimate   SE  df t.ratio p.value\n (N/N85) - NP     5.29 1.29 233   4.113  0.0002\n (N/R50) - NP    14.90 1.23 233  12.150  &lt;.0001\n (N/R40) - NP    17.71 1.27 233  13.938  &lt;.0001\n\nP value adjustment: dunnettx method for 3 tests \n\n# simultaneous 95% intervals for contrasts with a control group\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('trt.vs.ctrl') |&gt;\n  confint(level = 0.95, adjust = 'dunnett')\n\n contrast     estimate   SE  df lower.CL upper.CL\n (N/N85) - NP     5.29 1.29 233     2.23     8.34\n (N/R50) - NP    14.90 1.23 233    11.98    17.81\n (N/R40) - NP    17.71 1.27 233    14.70    20.73\n\nConfidence level used: 0.95 \nConf-level adjustment: dunnettx method for 3 estimates \n\n\nThe results indicate that all levels of diet restriction have an effect on mean lifetime that differs from the control group. Moreover, the intervals indicate that mean lifetimes are longer in every treatment group than in the control group; so diet restriction at every level causes an increase in lifetime. In particular, for instance:\n\nThe data provide very strong evidence that mean lifespan differs significantly between mice on a restricted 40kcal diet compared with mice on an unrestricted diet (p &lt; 0.0001). With 95% confidence, the difference (restricted - unrestricted) in mean lifetime is estimated to be between 11.98 and 17.81 months, with a point estimate of 14.90 months (SE 1.23).\n\n\n\n\n\n\n\nYour turn 5\n\n\n\nUsing the data from the anorexia study…\n\nTest, at the 1% significance level, for significant differences in mean percent change in body weight for each treatment compared with the control group. Are significant differences improvements relative to the control?\nEstimate the efficacy (difference relative to control) of each treatment at the confidence level appropriate for the test you performed. Interpret the result in context.\n\n\n# test for differences relative to control at 1% level\n\n# estimate differences at the appropriate confidence level\n\n\n\n\n\nExtra: testing a minimum difference\nWhile not especially common, sometimes you might wish to test whether group means differ by at least a certain amount. The usual hypothesis tests for pairwise differences are: \\[\n\\begin{cases}\nH_0: &\\mu_i - \\mu_j = 0 \\\\\nH_A: &\\mu_i - \\mu_j \\neq 0 \\\\\n\\end{cases}\n\\] We could instead test for a minimum difference of \\(c\\) by testing: \\[\n\\begin{cases}\nH_0: &|\\mu_i - \\mu_j| = c \\\\\nH_A: &|\\mu_i - \\mu_j| &gt; c \\\\\n\\end{cases}\n\\]\nThis looks tricky on face value because of the absolute value. However, if the groups are ordered in R monotonically by means (i.e., in increasing/decreasing order of group mean), the signs for pairwise contrasts will all match, as they do in the example provided. In this case, the hypothesis above reduce, for \\(i &gt; j\\), to: \\[\n\\begin{cases}\nH_0: &\\mu_i - \\mu_j = c \\\\\nH_A: &\\mu_i - \\mu_j &gt; c \\\\\n\\end{cases}\n\\]\nSo, to test for a minimum difference, we simply do a directional test with a nonzero null value: add null = ... and side = ... arguments to test(...). In the context of comparisons with the control:\n\n# test whether mean lifetime exceeds control by more than 1 year\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('trt.vs.ctrl') |&gt;\n  test(null = 12, side = '&gt;')\n\n contrast     estimate   SE  df null t.ratio p.value\n (N/N85) - NP     5.29 1.29 233   12  -5.219  1.0000\n (N/R50) - NP    14.90 1.23 233   12   2.362  0.0283\n (N/R40) - NP    17.71 1.27 233   12   4.496  &lt;.0001\n\nP value adjustment: sidak method for 3 tests \nP values are right-tailed \n\n# interval estimate\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('trt.vs.ctrl') |&gt;\n  confint(level = 0.95, side = '&gt;')\n\n contrast     estimate   SE  df lower.CL upper.CL\n (N/N85) - NP     5.29 1.29 233     2.55      Inf\n (N/R50) - NP    14.90 1.23 233    12.28      Inf\n (N/R40) - NP    17.71 1.27 233    15.00      Inf\n\nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 3 estimates \n\n\nNotice that a different adjustment method is used. This has a slightly more precise interpretation:\n\nThe data provide moderate evidence that mean lifespan increases by at least one year when intake is restricted to 50kcal/day compared with an unrestricted diet (T = 2.362 on 233 degrees of freedom, p = 0.0283). With 95% confidence, the mean increase is estimated to be at least 12.28 months, with a point estimate of 14.9 months (SE 1.23).\n\n\n\n\nPractice problems\n\n[L9] The mussels dataset includes observations of a shell measurement, anterior adductor muscle (AAM) scar length, for mytilus trossulus mussels from five populations. A plot of the data is provided below.\n\nFit an ANOVA model and test for significant differences among the populations at the 1% significance level.\nEstimate mean AAM length for each population.\nTest for pairwise differences to determine which populations differ at the 1% significance level.\nProvide simultaneous interval estimates at an appropriate confidence level for each significant difference.\n\n\n\n\n\n\n\n\n\n\n\n\n[L9] The plantgrowth dataset includes measurements of dry weight of plants grown using one of two fertilizer treatments or no fertilizer (control); treatments were randomly allocated to plants. A plot of the data is provided below.\n\nAssess assumptions for ANOVA based on the plot.\nFit an ANOVA model and test for a difference in mean dry weight at the 5% significance level.\nTest for significant differences in mean dry weight between each treatment compared with the control at the 5% level.\nInterpret your results. Do treatments cause a change in growth, as measured by dry weight, compared with the control?\n\n\n\n\n\n\n\n\n\n\n\n\n[Extra credit] Using the longevity data from lecture, compute interval estimates for log-contrasts and back-transform to obtain estimates for the percent change in median lifespan relative to the control group. Report the comparison between the normal (N/N85) diet and the unrestricted (NP) diet."
  },
  {
    "objectID": "content/lab12-association.html",
    "href": "content/lab12-association.html",
    "title": "Lab 12: Chi-square tests of association",
    "section": "",
    "text": "The purpose of this lab is to learn to implement \\(\\chi^2\\) tests of independence/association for two-way contingency tables:\n\ninference and residual analysis in \\(2\\times 2\\) tables\nextension to \\(I\\times J\\) tables\n\nMuch of the focus of the lab activity is on the \\(2\\times 2\\) setting. You’ll replicate the examples from class using the asthma data from an NHANES subsample, and practice using the diabetes_meds data.\nThe diabetes_meds data comes from an observational study of 227,571 Medicare beneficiaries who initiated treatment with one of two diabetes medications. In the study, it was recorded whether each patient reported cardiovascular problems.\nApplications in the \\(I\\times J\\) case will both use the famuss dataset.\n\nlibrary(tidyverse)\nload('data/asthma.RData')\nload('data/diabetes_meds.RData')\nload('data/famuss.RData')\n\n\nInference for \\(2\\times 2\\) tables\n\nBasic implementation and checking assumptions\nThe test of association/independence in two-way tables pertains to the hypotheses: \\[\n\\begin{cases}\nH_0: &\\text{row variable} \\perp \\text{column variable} \\\\\nH_A: &\\neg(\\text{row variable} \\perp \\text{column variable})\n\\end{cases}\n\\]\nIn words, the hypotheses are: (null) the row variable and column variable are independent; (alternative) there is an association between the row and column variables. This is fairly straightforward to implement in R using prop.test(...), which takes a contingency table as input:\n\n# make a two-way table\ntable(asthma$sex, asthma$asthma) \n\n        \n         asthma no asthma\n  female     49       781\n  male       30       769\n\n# pass to chisq.test\ntable(asthma$sex, asthma$asthma) |&gt;\n  chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 3.6217, df = 1, p-value = 0.05703\n\n\nThe test (with Yates’ continuity correction) produces \\(p &gt; 0.05\\), so we’d fail to reject \\(H_0\\). The typical narrative style for the report is:\n\nThe data provide suggestive but insufficient evidence at the 5% significance level that sex and asthma prevalence are associated (\\(\\chi^2\\) = 3.62 on 1 degree of freedom, p = 0.057).\n\n\n\n\n\n\n\nYour turn 1\n\n\n\nTest whether diabetes medication is associated with cardiovascular problems. Carry out inference at the 5% significance level and report the result of the test in the narrative style above.\n\n# test whether diabetes medication is associated with cardiovascular problems\n\n\n\nThese inferences rely on the assumption that expected counts are all over 10. The expected counts can be inspected directly by storing the result of the test:\n\n# store test result\nasthma.rslt &lt;- table(asthma$sex, asthma$asthma) |&gt;\n  chisq.test()\n\n# view expected counts to check assumptions\nasthma.rslt$expected\n\n        \n           asthma no asthma\n  female 40.25169  789.7483\n  male   38.74831  760.2517\n\n\nEach expected count exceeds ten, so the test is appropriate to use.\n\n\n\n\n\n\nYour turn 2\n\n\n\nCheck the expected counts for the inference you performed above to determine whether conditions for the Chi-square test are met.\n\n# store test result\n\n# view expected counts to check assumptions\n\n\n\nLuckily, prop.test(...) will print a warning if the counts are too small. For instance:\n\n# example using a dataset with low counts\nopenintro::malaria |&gt; table() |&gt; chisq.test()\n\nWarning in chisq.test(table(openintro::malaria)): Chi-squared approximation may\nbe incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(openintro::malaria)\nX-squared = 4.6561, df = 1, p-value = 0.03094\n\n\nWe can check that indeed the assumptions are not met by inspecting expected counts:\n\n# all expected counts are under ten\nmalaria.rslt &lt;- openintro::malaria |&gt; table() |&gt; chisq.test()\nmalaria.rslt$expected\n\n         outcome\ntreatment infection no infection\n  placebo       3.3          2.7\n  vaccine       7.7          6.3\n\n\n\n\nResidual analysis\nIf significant results (or suggestive evidence) is obtained from the Chi-square test, it is useful to be able to say which categories account for the association (or possible association).\nIn this context, residuals are standardized differences in expected and observed counts:\n\\[\nr_{ij} = \\frac{n_{ij} - \\hat{n}_{ij}}{\\sqrt{\\hat{n}_{ij}}}\n\\]\nThese convey information about which value combinations are ‘unusual’ under the assumption of independence between row and column variables.\n\npositive residual: larger-than-expected count\nnegative residual: smaller-than-expected count\n\nYou can retrieve the residuals from the test and inspect for the largest (absolute) values:\n\n# view residuals\nasthma.rslt$residual\n\n        \n             asthma  no asthma\n  female  1.3788982 -0.3113006\n  male   -1.4053932  0.3172821\n\n\nHere, the residuals suggest that asthma rates are higher than expected among women and lower than expected among men.\n\n\n\n\n\n\nYour turn 3\n\n\n\nCheck the residuals from your inference of whether diabetes medication is associated with cardiovascular problems. See if you can explain any inferred association.\n\n# check residuals from your inference above; what explains the association?\n\n\n\n\n\nMeasuring association by a difference in proportions\nOnce you’ve inferred an association between two categorical variables, it’s helpful to be able to provide a quantitative measure.\nA difference in proportions is one potential measure, though not always possible to compute (e.g., for outcome-based sampling). Here, since data are a random sample from the target population (U.S. adults), we can estimate the difference in the proportion of individuals with asthma between men and women:\n\n# use prop.test to get estimates and confidence interval\ntable(asthma$sex, asthma$asthma) |&gt;\n  prop.test(conf.level = 0.90)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 3.6217, df = 1, p-value = 0.05703\nalternative hypothesis: two.sided\n90 percent confidence interval:\n 0.002841347 0.040137075\nsample estimates:\n    prop 1     prop 2 \n0.05903614 0.03754693 \n\n\nFor a report, we’d combine this with the inferred association as follows:\n\nThe data provide suggestive but insufficient evidence at the 5% significance level that sex and asthma prevalence are associated (\\(\\chi^2\\) = 3.62 on 1 degree of freedom, p = 0.057). With 90% confidence, asthma prevalence is estimated to be between 0.28 and 4.01 percentage points higher among women compared with men, with a point estimate of 2.15 percentage points.\n\n(A 90% interval is reported, not consistent with the significance level of the test, to point to the direction of possible association.)\n\n\n\n\n\n\nYour turn 4\n\n\n\nProvide point and interval estimates for the difference in proportions of patients experiencing cardiovascular problems on each diabetes medication as a measure of association.\n(Unlike the example above, your confidence level for the interval should match the significance level of your test, as usual.)\n\n# estimate the difference in proportions of patients experiencing cardiovascular problems\n\n\n\n\n\n\nInference for \\(I \\times J\\) tables\nThe implementation of the \\(\\chi^2\\) test is identical for larger tables, as are the means of obtaining expected counts and residuals from the test. The only difference is the check on assumptions. For \\(I\\times J\\) tables, the following two conditions should be met:\n\nAll expected counts are at least 1\nRoughly 80% or more of expected counts are at least 5\n\nThe commands below illustrate the implementation, assumption check, and residual analysis.\n\n# perform test\nfamuss.rslt &lt;- table(famuss$race, famuss$genotype) |&gt;\n  chisq.test()\n\n# check expected counts to assess assumptions\nfamuss.rslt$expected\n\n            \n                     CC        CT         TT\n  African Am   7.850420  11.84370   7.305882\n  Asian       15.991597  24.12605  14.882353\n  Caucasian  135.783193 204.85210 126.364706\n  Hispanic     6.687395  10.08908   6.223529\n  Other        6.687395  10.08908   6.223529\n\n# interpret test result\nfamuss.rslt\n\n\n    Pearson's Chi-squared test\n\ndata:  table(famuss$race, famuss$genotype)\nX-squared = 19.4, df = 8, p-value = 0.01286\n\n# residual analysis\nfamuss.rslt$residuals\n\n            \n                      CC          CT          TT\n  African Am  2.90863193 -1.69802497 -0.85310170\n  Asian       1.25242978 -1.24720387  0.28971360\n  Caucasian  -0.92538910  0.77888407 -0.03244366\n  Hispanic   -1.03920927 -0.02804356  1.11294757\n  Other       0.12088363  0.28678513 -0.49045147\n\n\nThe interpretation of this result would be:\n\nThe data provide moderate evidence of an association between genotype and race for the ACTN3 gene (\\(\\chi^2\\) = 19.4 on 8 degrees of freedom, p = 0.01286).\n\nWhile there isn’t a standard style or language for the residual analysis, and appropriate interpretation would be:\n\nThe data further suggest that African American and Asian populations have higher-than-expected and lower-than-expected CC and CT genotype frequencies, respectively; and Hispanic populations have higher-than-expected and lower-than-expected TT and CC genotype frequencies, respectively.\n\nAs an aside, measures of association are trickier for \\(I\\times J\\) tables; typically one would calculate a set of measures that capture pairwise comparisons.\n\n\n\n\n\n\nYour turn 5\n\n\n\nTest for an association between genotype and sex.\n\ncheck the test assumptions\nassuming they are met, interpret the test\nif the test is significant, carry out a residual analysis to explain the inferred association\n\n\n# perform test\n\n# check expected counts to assess assumptions\n\n# interpret test result\n\n# residual analysis\n\n\n\n\n\nPractice problems\n\n[L7] The mammogram dataset contains observations from a 30-year study to investigate the effectiveness of mammograms versus a standard non-mammogram breast cancer exam on survival. The study was conducted in Canada with 89,835 female participants: during a 5-year screening period, each woman was randomized to either receive annual mammograms or standard physical exams for breast cancer; the study recorded the number of breast cancer deaths during a 25-year follow-up period in each group.\n\nExplain the hypotheses for the Chi-square test of association in words.\nCheck assumptions for the test.\nIf assumptions are met, test for association at the 5% significance level and report the result; if the test is significant, perform a residual analysis to explain the inferred association.\n\n[L7] The smoking dataset contains observations from a retrospective case-control study in which smoking status was recorded for 86 lung cancer patients and 86 healthy patients.\n\nConstruct a contingency table of the data.\nWhich proportions are possible to estimate using data from this study?\nCheck assumptions for the \\(\\chi^2\\) test of association.\nIf assumptions are met, perform the test at the 5% level. Interpret the result in the usual narrative style.\n(Extra credit) Make a plot of the residuals that allows you to identify which group/outcome combinations differ from expectations under independence.\n\n[L6] The gss data contains several demographic measurements for a random sample of 500 U.S. adults.\n\nProvide a point estimate for the proportion of U.S. adults with a college degree.\nProvide a 99% confidence interval for the proportion. Interpret the interval in context.\nTest whether at least three in ten U.S. adults have a college degree at the 5% significance level. If the result is significant, provide a corresponding lower confidence bound.\n\n[LX] (Extra credit) Again using the gss data, test whether political party is associated with socioeconomic class.\n\nCheck assumptions for the test.\nIf appropriate, perform the test at the 1% significance level. Interpret the result in context, and if a significant association is found, perform a residual analysis to explain the inferred association."
  },
  {
    "objectID": "content/week5-directional.html#todays-agenda",
    "href": "content/week5-directional.html#todays-agenda",
    "title": "Tests for directional hypotheses",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nQuick review of decision criteria for the \\(t\\) test\n[lecture] test-interval relationship; directional \\(t\\) tests\n[lab] upper-sided, lower-sided, and two-sided tests for the population mean"
  },
  {
    "objectID": "content/week5-directional.html#recap-decision-criteria",
    "href": "content/week5-directional.html#recap-decision-criteria",
    "title": "Tests for directional hypotheses",
    "section": "Recap: decision criteria",
    "text": "Recap: decision criteria\n\nA hypothesis test boils down to deciding whether your estimate is too far from a hypothetical value for that hypothesis to be plausible.\n\n\n\nTo test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = \\mu_0 \\\\\nH_A: &\\mu \\neq \\mu_0\n\\end{cases}\n\\]\nWe use the test statistic:\n\\[\nT = \\frac{\\bar{x} - \\mu_0}{SE(\\bar{x})}\n\\quad\\left(\\frac{\\text{estimation error under } H_0}{\\text{standard error}}\\right)\n\\]\n\nWe say \\(H_0\\) is implausible at level \\(\\alpha\\) if either:\n\n\\(|T| &gt; q\\) for the \\(\\alpha\\)-critical value \\(q\\)\n\n\\(q\\) is the \\(1 - \\frac{\\alpha}{2}\\) quantile of the \\(t_{n - 1}\\) model\n\n\\(\\underbrace{P(|T| &gt; |T_\\text{observed}|)}_\\text{p-value} &lt; \\alpha\\)\n\n\n\nThis procedure controls the error rate \\(\\alpha\\): the proportion of samples for which we’d make a false rejection."
  },
  {
    "objectID": "content/week5-directional.html#from-last-time",
    "href": "content/week5-directional.html#from-last-time",
    "title": "Tests for directional hypotheses",
    "section": "From last time",
    "text": "From last time\nPractice problem: test the hypothesis that the average U.S. adult sleeps 8 hours.\n\n\n\n\n\n\n\n\n\n\n\n\n# calculations\nsleep.mean &lt;- mean(sleep) \nsleep.mean.se &lt;- sd(sleep)/sqrt(length(sleep))\ntstat &lt;- (sleep.mean - 8)/sleep.mean.se \ncrit.val &lt;- qt(0.975, df = 3178) \np.val &lt;- 2*pt(abs(tstat), df = 3178, lower.tail = F)\nci &lt;- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstd.err\ntstat\ncval\npval\n\n\n\n\n6.959\n0.02447\n-42.53\n1.961\n2.622e-313\n\n\n\n\n\n95% confidence interval: (6.91, 7.01)\n\nA complete narrative summary:\n\nThe data provide evidence that the average U.S. adult does not sleep 8 hours per night (T = -42.53 on 3178 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean nightly hours of sleep among U.S. adults is estimated to be between 6.91 and 7.01 hours, with a point estimate of 6.59 hours (SE: 0.0245)."
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals",
    "href": "content/week5-directional.html#tests-and-intervals",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\n\nTests and intervals are usually reported together\n\nConsider how the test and interval provide complementary information:\n\nthe test tells you U.S. adults don’t sleep 8 hours\nthe interval tells you how much they do sleep\n\nIt is reasonable that they should be consistent, and in fact they are.\n\n\n\n# calculations\nsleep.mean &lt;- mean(sleep) \nsleep.mean.se &lt;- sd(sleep)/sqrt(length(sleep))\ntstat &lt;- (sleep.mean - 8)/sleep.mean.se \ncrit.val &lt;- qt(0.975, df = 3178) \np.val &lt;- 2*pt(abs(tstat), df = 3178, lower.tail = F)\nci &lt;- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se\n\n\n\nNotice that the critical value in the 5% significance level test is exactly the same as that used in the 95% confidence interval."
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals-1",
    "href": "content/week5-directional.html#tests-and-intervals-1",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\nThe critical value used in a \\(\\alpha\\) significance level test is identical to the critical value used in a \\((1 - \\alpha)\\) interval. Consequently:\n\\[\n\\underbrace{\\bar{x} - c\\times SE(\\bar{x}) &lt; \\mu_0 &lt; \\bar{x} + c\\times SE(\\bar{x})}_\\text{hypothesized value is in the interval}\n\\quad\\Longleftrightarrow\\quad\n\\underbrace{-c &lt; \\frac{\\bar{x} - \\mu_{0}}{SE(\\bar{x})} &lt; c}_{|T| &lt; c}\n\\]\nMeaning: the interval includes exactly those values that the test fails to reject.\n\nSensible considering both use the same information: the distance between the point estimate and population mean, relative to the variability of the estimate"
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals-2",
    "href": "content/week5-directional.html#tests-and-intervals-2",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\n\nThe level-\\(\\alpha\\) test rejects \\(H_0: \\mu = \\mu_0\\) exactly when \\(\\mu_0\\) is outside the \\((1 - \\alpha)\\times 100\\)% confidence interval for \\(\\mu\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft, \\(p\\)-values for a sequence of tests:\n\n\\(p &gt; 0.05\\) precisely for \\(\\mu_0\\) in 95% CI\n\\(p &gt; 0.01\\) precisely for \\(\\mu_0\\) in 99% CI\n\nIn other words:\n\\[\\text{level $\\alpha$ test rejects} \\Longleftrightarrow \\text{$1 - \\alpha$ CI excludes}\\]"
  },
  {
    "objectID": "content/week5-directional.html#the-t.test...-function",
    "href": "content/week5-directional.html#the-t.test...-function",
    "title": "Tests for directional hypotheses",
    "section": "The t.test(...) function",
    "text": "The t.test(...) function\nSince tests and intervals go together, there is a single R function that computes both.\n\n\n\nt.test(sleep, mu = 8, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -42.533, df = 3178, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 8\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nNo critical value is reported, so you have to make the decision using the \\(p\\) value:\n\n\\(p &lt; \\alpha\\): reject\n\\(p &gt; \\alpha\\): fail to reject\n\n\n\nTake a moment to locate each component of the test and estimates from the output."
  },
  {
    "objectID": "content/week5-directional.html#interpreting-p-values",
    "href": "content/week5-directional.html#interpreting-p-values",
    "title": "Tests for directional hypotheses",
    "section": "Interpreting \\(p\\)-values",
    "text": "Interpreting \\(p\\)-values\n\n\\(p\\)-values measure the strength of evidence against \\(H_0\\) and favoring \\(H_A\\): smaller \\(p\\)-values indicate stronger evidence; larger \\(p\\)-values indicate weaker evidence.\n\n\n\nThe mathematical definition is: \\[p = P(|T| &gt; |T_\\text{observed}|)\\]\n\ntechnically, the probability under \\(H_0\\) that \\(T\\) exceeds the observed value in magnitude\ninformally, how unusual/rare your data are\n\n\nAs a measure of the strength of evidence favoring the alternative:\n\n\n\nvalue\nstrength of evidence\n\n\n\n\n\\(p &lt; 0.001\\)\nvery strong\n\n\n\\(0.001 &lt; p &lt; 0.01\\)\nstrong\n\n\n\\(0.01 &lt; p &lt; 0.05\\)\nmoderate\n\n\n\\(0.05 &lt; p &lt; 0.1\\)\nsuggestive\n\n\n\\(0.1 &lt; p\\)\nno evidence"
  },
  {
    "objectID": "content/week5-directional.html#your-turn-interpret-these-p-values",
    "href": "content/week5-directional.html#your-turn-interpret-these-p-values",
    "title": "Tests for directional hypotheses",
    "section": "Your turn: interpret these \\(p\\)-values",
    "text": "Your turn: interpret these \\(p\\)-values\n\nDon’t just match the value to the table; add context, and state the test outcome.\n\n\n\n\n# example 1\nt.test(sleep, mu = 6.8)$p.value\n\n[1] 9.205865e-11\n\n# example 2\nt.test(sleep, mu = 6.9)$p.value\n\n[1] 0.01578191\n\n# example 3\nt.test(sleep, mu = 7)$p.value\n\n[1] 0.09482291\n\n# example 4\nt.test(sleep, mu = 7.1)$p.value\n\n[1] 9.366935e-09\n\n# example 5\nt.test(sleep, mu = 7.2)$p.value\n\n[1] 1.532371e-22\n\n\n\n\n\n\nvalue\nstrength of evidence\n\n\n\n\n\\(p &lt; 0.001\\)\nvery strong\n\n\n\\(0.001 &lt; p &lt; 0.01\\)\nstrong\n\n\n\\(0.01 &lt; p &lt; 0.05\\)\nmoderate\n\n\n\\(0.05 &lt; p &lt; 0.1\\)\nsuggestive\n\n\n\\(0.1 &lt; p\\)\nno evidence\n\n\n\n\n\nExample: “the data [DO/DO NOT] provide [STRENGTH] evidence that [ALTERNATIVE]”"
  },
  {
    "objectID": "content/week5-directional.html#a-directional-test",
    "href": "content/week5-directional.html#a-directional-test",
    "title": "Tests for directional hypotheses",
    "section": "A directional test",
    "text": "A directional test\n\nDoes the average U.S. adult sleep less than 7 hours?\n\n\n\nThis example leads to a directional test:\n\\[\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu &lt; 7\n\\end{cases}\n\\]\nThe test statistic is the same as before:\n\\[\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n\\]\n\nThe lower-sided \\(p\\)-value is 0.0474:\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(p\\)-value, we look at how often \\(T\\) is larger in the direction of the alternative.\n\nin this case, how often \\(T\\) is smaller (underestimate by more)"
  },
  {
    "objectID": "content/week5-directional.html#the-other-direction",
    "href": "content/week5-directional.html#the-other-direction",
    "title": "Tests for directional hypotheses",
    "section": "The other direction",
    "text": "The other direction\n\nDoes the average U.S. adult sleep more than 7 hours?\n\n\n\nNow the alternative is the opposite direction:\n\\[\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu &gt; 7\n\\end{cases}\n\\]\nThe test statistic is the same as before:\n\\[\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n\\]\n\nThe upper-sided \\(p\\)-value is 0.9526:\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(p\\)-value, we look at how often \\(T\\) is larger in the direction of the alternative.\n\nin this case, how often \\(T\\) is larger (overestimate by more)"
  },
  {
    "objectID": "content/week5-directional.html#directional-hypotheses",
    "href": "content/week5-directional.html#directional-hypotheses",
    "title": "Tests for directional hypotheses",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nTests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.\n\n\n\nTest type\nAlternative\nDirection favoring alternative\n\n\n\n\nUpper-sided\n\\(\\mu &gt; \\mu_0\\)\nlarger \\(T\\)\n\n\nLower-sided\n\\(\\mu &lt; \\mu_0\\)\nsmaller \\(T\\)\n\n\nTwo-sided\n\\(\\mu \\neq \\mu_0\\)\nlarger \\(|T|\\)\n\n\n\nThe direction of the test affects the \\(p\\)-value calculation (and thus decision), but won’t change the test statistic.\nConceptually tricky, but easy in R:\n\n# upper-sided\nt.test(ddt, mu = mu_0, alternative = 'greater')\n\n# lower-sided\nt.test(ddt, mu = mu_0, alternative = 'less')\n\n# two-sided (default)\nt.test(ddt, mu = mu_0, alternative = 'two.sided')"
  },
  {
    "objectID": "content/week5-directional.html#three-t-tests",
    "href": "content/week5-directional.html#three-t-tests",
    "title": "Tests for directional hypotheses",
    "section": "Three \\(t\\)-tests",
    "text": "Three \\(t\\)-tests\n\n\nDo U.S. adults sleep 7 hours per night?\n\n# two sided test\nt.test(sleep, \n       mu = 7)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nSuggestive but insufficient evidence that U.S. adults don’t sleep 7 hours\n\n\nDo U.S. adults sleep less than 7 hours per night?\n\n# lower-sided test\nt.test(sleep, \n       mu = 7, \n       alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.04741\nalternative hypothesis: true mean is less than 7\n95 percent confidence interval:\n     -Inf 6.999372\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nModerate evidence that U.S. adults sleep less than 7 hours\n\n\nDo U.S. adults sleep more than 7 hours per night?\n\n# upper-sided test\nt.test(sleep, \n       mu = 7, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.9526\nalternative hypothesis: true mean is greater than 7\n95 percent confidence interval:\n 6.918841      Inf\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nNo evidence that U.S. adults sleep more than 7 hours"
  },
  {
    "objectID": "content/week5-directional.html#another-example-ddt-data",
    "href": "content/week5-directional.html#another-example-ddt-data",
    "title": "Tests for directional hypotheses",
    "section": "Another example: DDT data",
    "text": "Another example: DDT data\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm).\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu = 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]\nWe choose this direction because we’re concerned with evidence that mean DDT exceeds the threshold."
  },
  {
    "objectID": "content/week5-directional.html#another-example-ddt-data-1",
    "href": "content/week5-directional.html#another-example-ddt-data-1",
    "title": "Tests for directional hypotheses",
    "section": "Another example: DDT data",
    "text": "Another example: DDT data\n\n\nIf in fact \\(\\mu = 3\\), then according to the \\(t\\) model 0.58% of samples would produce an error of this magnitude or more in the direction of the alternative:\n\n\n\n\n\n\n\n\n\n\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328 \n\n\n\nThe data provide strong evidence that mean DDT in kale exceeds 3ppm (T = 2.9059 on 14 degrees of freedom, p = 0.0058). With 95% confidence, the mean DDT is estimated to be at least 3.129, with a point estimate of 3.32 (SE: 0.1168).\n\n\n\nNotice the one-sided interval! (Inf = \\(\\infty\\).) This is called a “lower confidence bound”."
  },
  {
    "objectID": "content/week5-directional.html#your-turn-which-alternative",
    "href": "content/week5-directional.html#your-turn-which-alternative",
    "title": "Tests for directional hypotheses",
    "section": "Your turn: which alternative?",
    "text": "Your turn: which alternative?\n\nWrite the hypotheses in notation and identify which test (upper/lower/two sided) should be used.\n\nUsing the temperature/heartrate data:\n\nIs mean body temperature less than 98.6°F?\nIs mean heart rate greater than 60 bpm?\nIs mean heart rate 65 bpm?\n\nUsing the NC births data:\n\nIs the mean number of weeks at birth 40?\nIs the mean birth weight at least 7 lbs?\nIs the mean birth weight under 8 lbs?\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab6-hypothesis.html",
    "href": "content/lab6-hypothesis.html",
    "title": "Lab 6: Hypothesis testing basics",
    "section": "",
    "text": "In class we discussed the \\(t\\) test for the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = \\mu_0 \\\\\nH_A: &\\mu \\neq \\mu_0\n\\end{cases}\n\\]\nThe objective of this lab is to learn to perform the basic calculations involved in this \\(t\\) test “by hand” (without the use of the function that we’ll apply later):\nWe’ll use the temps dataset to illustrate.\nlibrary(tidyverse)\nload('data/temps.RData')\nhead(temps)\n\n  body.temp    sex heart.rate\n1      98.8 female         69\n2      98.6 female         85\n3      98.4   male         68\n4      97.2 female         66\n5      99.5   male         75\n6      97.1   male         82"
  },
  {
    "objectID": "content/lab6-hypothesis.html#footnotes",
    "href": "content/lab6-hypothesis.html#footnotes",
    "title": "Lab 6: Hypothesis testing basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI usually think in the following terms for the \\(t\\)-test:\n\nsmall: \\(n \\leq 20\\)\nmodest: \\(20 &lt; n \\leq 50\\)\nlarge: \\(50 &lt; n\\)\n\nUnless I’m in the ‘small’ regime, I’m not too worried about skew or outliers. In the ‘modest’ regime, I’m not concerned unless I spot very pronounced skew or outliers. In the ‘large’ regime, I’m really only concerned about (strong) multimodality. Interestingly, in the latter case, the \\(t\\) test still works for multimodal populations, but the population mean isn’t meaningful.↩︎"
  },
  {
    "objectID": "content/project-guidelines.html",
    "href": "content/project-guidelines.html",
    "title": "Project guidelines",
    "section": "",
    "text": "In place of a final exam, you will work with a partner to find a case study relevant to one of your major fields of study that employs a method from the class (or a method closely related) to answer a research question. You’ll prepare a short summary and meet with me in person during the final exam time to briefly present your case study and field a few questions."
  },
  {
    "objectID": "content/project-guidelines.html#expectations",
    "href": "content/project-guidelines.html#expectations",
    "title": "Project guidelines",
    "section": "Expectations",
    "text": "Expectations\nYour case study should meet the following criteria:\n\naddress a research question in your field of study\nuse a statistical method we have discussed in class\nbe included in a published paper or report\n\nMost papers report multiple findings. Thus, you can expect to focus for your case study on a small portion of the full publication; you do not need to summarize a published study in its entirety. Rather, you are expected to identify one inferential analysis.\nKeep it simple in terms of the analysis – a few intervals/tests/estimates are fine, but don’t overcomplicate things by trying to summarize too many results at once. Keep in mind that you’ll need to be able to explain your case study in just a few minutes.\nYou have two options for the project deliverable:\n\n[option 1] prepare a written summary of the case study\n[option 2] replicate the analysis for the case study"
  },
  {
    "objectID": "content/project-guidelines.html#evaluation-criteria",
    "href": "content/project-guidelines.html#evaluation-criteria",
    "title": "Project guidelines",
    "section": "Evaluation criteria",
    "text": "Evaluation criteria\nYou will be evaluated on the appropriateness of the example you choose, the clarity of your summary/presentation, and your understanding of the statistical method(s) you discuss. Satisfactory work should:\n\ndemonstrate an understanding of the case study and its relevance\ndemonstrate an adequate understanding of the data and statistical method(s) involved\nbe free of obvious errors/misconceptions"
  },
  {
    "objectID": "content/project-guidelines.html#deliverables",
    "href": "content/project-guidelines.html#deliverables",
    "title": "Project guidelines",
    "section": "Deliverables",
    "text": "Deliverables\nYour deliverable should be submitted via file upload 24 hours in advance of your scheduled exam time.\nChoose ONE of the options below.\n\nOption 1: written summary\nPrepare a short 1-2 page summary addressing the following:\n\nWhat is the research question that the analysis addresses?\nWhat inference(s) are used to answer the question?\nWhat data were utilized for the inference(s)?\nWhat statistical method(s) are used to perform the inference(s)?\nWhat are the results?\n\nYour deliverable for this option should be uploaded as a .docx or .pdf file.\n\n\nOption 2: replicate an analysis\nPrepare an R script, data file, and 1-page summary of results. Your summary should include the following:\n\nA data description\nA data summary (plot or table)\nRelevant R output\nInterpretation of inference(s) in context\n\nYour deliverable for this option should be a .zip file containing one R script, one data file, and your written summary (as a .docx or .pdf file)."
  },
  {
    "objectID": "content/project-guidelines.html#logistics",
    "href": "content/project-guidelines.html#logistics",
    "title": "Project guidelines",
    "section": "Logistics",
    "text": "Logistics\nExam times are held in the usual classroom (186-C100) at:\n\n[12pm section] Wednesday 6/12 10:10am – 1:00pm\n[2pm section] Monday 6/10 1:10pm – 4:00pm\n\nYou can expect to schedule a 10-minute time slot in the window of the scheduled exam. A scheduling link will be provided at the end of week 10. Requests for alternate times must be made by the end of week 10 and include a motivation for the request. We will be on a tight schedule, so once you have your time slot, you should plan on arriving five minutes in advance.\nA few of you may be scheduled outside of the regular times at a location TBD. I will check with you first before scheduling you outside of the exam time."
  },
  {
    "objectID": "content/week9-association.html#todays-agenda",
    "href": "content/week9-association.html#todays-agenda",
    "title": "Tests of association",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] tests of association in contingency tables\n[lab] \\(\\chi^2\\) tests and residual analysis in R"
  },
  {
    "objectID": "content/week9-association.html#do-asthma-rates-differ-by-sex",
    "href": "content/week9-association.html#do-asthma-rates-differ-by-sex",
    "title": "Tests of association",
    "section": "Do asthma rates differ by sex?",
    "text": "Do asthma rates differ by sex?\n\n\nFrom a subsample of NHANES data:\n\n\n\n\n\n\n\n\n\n\n \nasthma\nno asthma\n\n\n\n\nfemale\n49\n781\n\n\nmale\n30\n769\n\n\n\n\n\nInference for the difference in proportions:\n\ntable(asthma$sex, asthma$asthma) |&gt;\n  prop.test(alternative = 'two.sided', \n            conf.level = 0.95,\n            correct = F)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 4.0741, df = 1, p-value = 0.04355\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.0007323913 0.0422460305\nsample estimates:\n    prop 1     prop 2 \n0.05903614 0.03754693 \n\n\n\nInterpretation:\n\nThere is moderate evidence that asthma prevalence differs between men and women (Z = 2.108, p = 0.0436). With 95% confidence, the difference in prevalence (F - M) is estimated to be between 0.07% and 4.22%, with a point estimate of 2.15%.\n\nThis inference relies on a specific measure of association (difference in prevalence) that we can’t always estimate.\nCould we test for association between sex and asthma without relying on a specific measure?"
  },
  {
    "objectID": "content/week9-association.html#association-and-independence",
    "href": "content/week9-association.html#association-and-independence",
    "title": "Tests of association",
    "section": "Association and independence",
    "text": "Association and independence\n\n\nConsider the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\text{asthma}\\perp\\text{sex} \\; &(\\text{independence})\\\\\nH_A: &\\neg(\\text{asthma}\\perp\\text{sex}) \\; &(\\text{association})\n\\end{cases}\n\\]\nConsider also the proportions:\n\n\n\n\n\n\n\n\n\n\n\n \nasthma\nno asthma\ntotal\n\n\n\n\nfemale\n0.03008\n0.4794\n0.5095\n\n\nmale\n0.01842\n0.4721\n0.4905\n\n\ntotal\n0.0485\n0.9515\n1\n\n\n\n\n\n\n\\(p_{ij}\\): proportion of observations in cell \\(ij\\)\n\\(p_i\\), \\(p_j\\): marginal proportions in row \\(i\\) or column \\(j\\)\n\n\nIf sex and asthma are independent:\n\\[\np_{ij} \\approx p_i \\times p_j\n\\]\nFor example, we’d expect:\n\\[\n0.4721 \\approx 0.4905 \\times 0.9515\n\\]\nIn other words:\n\n49% of respondents are men\n95% of respondents don’t have asthma\nso roughly 49% of 95% would be men without asthma"
  },
  {
    "objectID": "content/week9-association.html#basis-for-a-test-expected-counts",
    "href": "content/week9-association.html#basis-for-a-test-expected-counts",
    "title": "Tests of association",
    "section": "Basis for a test: expected counts",
    "text": "Basis for a test: expected counts\nExpected proportions translate directly to expected counts: \\[p_{ij} = p_i \\times p_j\n\\quad\\Longleftrightarrow\\quad n_{ij} = \\frac{n_{i\\cdot} \\times n_{\\cdot j}}{n}\\]\n\n\nActual counts:\n\n\n\n\n\n\n\n\n\n \nO1\nO2\ntotal\n\n\n\n\nG1\n\\(n_{11}\\)\n\\(n_{12}\\)\n\\(\\color{red}{n_{1\\cdot}}\\)\n\n\nG2\n\\(n_{21}\\)\n\\(n_{22}\\)\n\\(\\color{orange}{n_{2\\cdot}}\\)\n\n\ntotal\n\\(\\color{blue}{n_{\\cdot 1}}\\)\n\\(\\color{green}{n_{\\cdot 2}}\\)\n\\(n\\)\n\n\n\n\nExpected counts under independence:\n\n\n\n\n\n\n\n\n\n \nO1\nO2\ntotal\n\n\n\n\nG1\n\\(\\hat{n}_{11} = \\frac{\\color{red}{n_{1\\cdot}} \\color{black}{\\times} \\color{blue}{n_{\\cdot 1}}}{n}\\)\n\\(\\hat{n}_{12} = \\frac{\\color{red}{n_{1\\cdot}} \\color{black}{\\times} \\color{green}{n_{\\cdot 2}}}{n}\\)\n\\(\\color{red}{n_{1\\cdot}}\\)\n\n\nG2\n\\(\\hat{n}_{21} = \\frac{\\color{orange}{n_{2\\cdot}} \\color{black}{\\times} \\color{blue}{n_{\\cdot 1}}}{n}\\)\n\\(\\hat{n}_{22} = \\frac{\\color{orange}{n_{2\\cdot}} \\color{black}{\\times} \\color{green}{n_{\\cdot 2}}}{n}\\)\n\\(\\color{orange}{n_{2\\cdot}}\\)\n\n\ntotal\n\\(\\color{blue}{n_{\\cdot 1}}\\)\n\\(\\color{green}{n_{\\cdot 2}}\\)\n\\(n\\)\n\n\n\n\n\nIdea for a test of independence:\n\nreject \\(H_0\\) if actual and expected counts differ enough across the table\ni.e., reject \\(H_0\\) when \\(n_{ij} - \\hat{n}_{ij}\\) is large across \\(i, j\\)"
  },
  {
    "objectID": "content/week9-association.html#computing-expected-counts",
    "href": "content/week9-association.html#computing-expected-counts",
    "title": "Tests of association",
    "section": "Computing expected counts",
    "text": "Computing expected counts\n\n\nActual counts:\n\n\n\n\n\n\n\n\n\n \nO1\nO2\ntotal\n\n\n\n\nG1\n\\(n_{11}\\)\n\\(n_{12}\\)\n\\(\\color{red}{n_{1\\cdot}}\\)\n\n\nG2\n\\(n_{21}\\)\n\\(n_{22}\\)\n\\(\\color{orange}{n_{2\\cdot}}\\)\n\n\ntotal\n\\(\\color{blue}{n_{\\cdot 1}}\\)\n\\(\\color{green}{n_{\\cdot 2}}\\)\n\\(n\\)\n\n\n\n\nExpected counts under independence:\n\n\n\n\n\n\n\n\n\n \nO1\nO2\ntotal\n\n\n\n\nG1\n\\(\\hat{n}_{11} = \\frac{\\color{red}{n_{1\\cdot}} \\color{black}{\\times} \\color{blue}{n_{\\cdot 1}}}{n}\\)\n\\(\\hat{n}_{12} = \\frac{\\color{red}{n_{1\\cdot}} \\color{black}{\\times} \\color{green}{n_{\\cdot 2}}}{n}\\)\n\\(\\color{red}{n_{1\\cdot}}\\)\n\n\nG2\n\\(\\hat{n}_{21} = \\frac{\\color{orange}{n_{2\\cdot}} \\color{black}{\\times} \\color{blue}{n_{\\cdot 1}}}{n}\\)\n\\(\\hat{n}_{22} = \\frac{\\color{orange}{n_{2\\cdot}} \\color{black}{\\times} \\color{green}{n_{\\cdot 2}}}{n}\\)\n\\(\\color{orange}{n_{2\\cdot}}\\)\n\n\ntotal\n\\(\\color{blue}{n_{\\cdot 1}}\\)\n\\(\\color{green}{n_{\\cdot 2}}\\)\n\\(n\\)\n\n\n\n\n\nFor the asthma example:\n\n\n\n\n\nActual\n\n\n\n\n\n\n\n\n \nasthma\nno asthma\ntotal\n\n\n\n\nfemale\n49\n781\n830\n\n\nmale\n30\n769\n799\n\n\ntotal\n79\n1550\n1629\n\n\n\n\n\n\n\n\n\nExpected\n\n\n\n\n\n\n\n\n \nasthma\nno asthma\ntotal\n\n\n\n\nfemale\n40.25\n789.7\n830\n\n\nmale\n38.75\n760.3\n799\n\n\ntotal\n79\n1550\n1629"
  },
  {
    "objectID": "content/week9-association.html#the-chi-square-chi2-statistic",
    "href": "content/week9-association.html#the-chi-square-chi2-statistic",
    "title": "Tests of association",
    "section": "The chi-square (\\(\\chi^2\\)) statistic",
    "text": "The chi-square (\\(\\chi^2\\)) statistic\nA measure of the amount by which actual counts differ from expected counts under independence is the chi (pronounced /ˈkaɪ ) square statistic:\n\\[\n\\chi^2 = \\sum_{ij} \\frac{\\left(n_{ij} - \\hat{n}_{ij}\\right)^2}{\\hat{n}_{ij}}\n\\qquad\\left(\\sum_\\text{all cells} \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\\right)\n\\]\n\n\nCell-wise calculation:\n\n\n\n\n\n\n\n\n \nasthma\nno asthma\n\n\n\n\nfemale\n\\(\\frac{(49 - 40.25)^2}{40.25}\\)\n\\(\\frac{(781 - 789.7)^2}{789.7}\\)\n\n\nmale\n\\(\\frac{(30 - 38.75)^2}{38.75}\\)\n\\(\\frac{(769 - 760.3)^2}{760.3}\\)\n\n\n\n\nResult:\n\n\n\n\n\n\n\n\n\n\n \nasthma\nno asthma\n\n\n\n\nfemale\n1.901\n0.09691\n\n\nmale\n1.975\n0.1007\n\n\n\n\n\n\n\nChi-square statistic: \\[\n\\chi^2\n= 1.9014 + 1.9751 + 0.0969 + 0.1007\n= 4.0741\n\\]"
  },
  {
    "objectID": "content/week9-association.html#sampling-distribution-for-chi2",
    "href": "content/week9-association.html#sampling-distribution-for-chi2",
    "title": "Tests of association",
    "section": "Sampling distribution for \\(\\chi^2\\)",
    "text": "Sampling distribution for \\(\\chi^2\\)\n\n\nUnder \\(H_0\\), the \\(\\chi^2\\) statistic has a sampling distribution that can be approximated by a \\(\\chi^2_1\\) model.\n\nsubscript indicates degrees of freedom parameter\n\nThe model assumes no expected counts are too small.\n\nrule of thumb: at least 10 (\\(\\hat{n}_{ij} \\geq 10\\))\nconsequences: if \\(\\hat{n}_{ij}\\) are too small, the statistic is inflated relative to the model, leading to a higher type I error rate"
  },
  {
    "objectID": "content/week9-association.html#computing-p-values",
    "href": "content/week9-association.html#computing-p-values",
    "title": "Tests of association",
    "section": "Computing \\(p\\) values",
    "text": "Computing \\(p\\) values\n\n\n\\[\n\\begin{cases}\nH_0: &\\text{asthma}\\perp\\text{sex} \\; &(\\text{independence})\\\\\nH_A: &\\neg(\\text{asthma}\\perp\\text{sex}) \\; &(\\text{association})\n\\end{cases}\n\\]\nTo determine the test outcome, find the \\(p\\)-value:\n\\[\nP(\\chi^2_1 &gt; \\chi^2_\\text{obs}) = P(\\chi^2_1 &gt; 4.074) = 0.0435\n\\]\nSo if asthma and sex were independent, only 4% of random samples would produce a table that deviates from expected counts by more than what we observed.\n\n\n\n\n\n\n\n\n\n\n\npchisq(4.074, df = 1, lower.tail = F)\n\n[1] 0.04354804"
  },
  {
    "objectID": "content/week9-association.html#implementation-in-r",
    "href": "content/week9-association.html#implementation-in-r",
    "title": "Tests of association",
    "section": "Implementation in R",
    "text": "Implementation in R\nThe R implementation is chisq.test(...).\n\ninput: contingency table\nno constraints on row/column arrangement\n\n\n\n\n# construct table and pass to chisq.test\ntable(asthma$sex, asthma$asthma) |&gt; \n  chisq.test(correct = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 4.0741, df = 1, p-value = 0.04355\n\n\n\n\nThe data provide moderate evidence that asthma prevalence is associated with sex (\\(\\chi^2\\) = 4.074 on 1 degree of freedom, p = 0.0435)."
  },
  {
    "objectID": "content/week9-association.html#residuals-in-chi2-tests",
    "href": "content/week9-association.html#residuals-in-chi2-tests",
    "title": "Tests of association",
    "section": "Residuals in \\(\\chi^2\\) tests",
    "text": "Residuals in \\(\\chi^2\\) tests\n\n\nThe residual for each cell is defined as a standardized difference between the observed and expected count:\n\\[r_{ij} = \\frac{n_{ij} - \\hat{n}_{ij}}{\\sqrt{\\hat{n}_{ij}}} \\]\nExamining residuals can indicate the source(s) of an inferred association.\n\n\\(r_{ij} &gt; 0\\): observation exceeds expectation\n\\(r_{ij} &lt; 0\\): observation is under expectation\nlarge \\(|r_{ij}|\\) explain the association\n\n\n\n# store test result\nrslt &lt;- chisq.test(asthma.tbl, correct = F)\n\n# examine residuals\nrslt$residuals\n\n\n\n\n\n\n\n\n\n\n\n \nasthma\nno asthma\n\n\n\n\nfemale\n1.379\n-0.3113\n\n\nmale\n-1.405\n0.3173\n\n\n\n\n\nLook for the largest residuals:\n\nAsthma prevalence is higher-than-expected among women and lower-than-expected among men."
  },
  {
    "objectID": "content/week9-association.html#continuity-correction",
    "href": "content/week9-association.html#continuity-correction",
    "title": "Tests of association",
    "section": "Continuity correction",
    "text": "Continuity correction\nThe \\(\\chi^2\\) test for independence is typically applied with Yates’ continuity correction.\n\n\nThis consists in using a modified version of the test statistic:\n\\[\n\\chi^2_\\text{Yates} = \\sum_{ij} \\frac{\\left(|n_{ij} - \\hat{n}_{ij}| - 0.5\\right)^2}{\\hat{n}_{ij}}\n\\]\n\nevery other detail of the test is the same\ndoesn’t change expected counts\nresiduals are still computed as \\(\\frac{n_{ij} - \\hat{n}_{ij}}{\\sqrt{\\hat{n}_{ij}}}\\)\n\n\nImplementation:\n\n# construct table and pass to chisq.test\ntable(asthma$sex, asthma$asthma) |&gt; \n  chisq.test(correct = T)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 3.6217, df = 1, p-value = 0.05703\n\n\nNote the larger \\(p\\)-value – this changes the conclusion!"
  },
  {
    "objectID": "content/week9-association.html#spot-any-similarities",
    "href": "content/week9-association.html#spot-any-similarities",
    "title": "Tests of association",
    "section": "Spot any similarities?",
    "text": "Spot any similarities?\nCompare the \\(\\chi^2\\) test with inference on the difference in proportions.\n\n\n\n# chi square test\ntable(asthma$sex, asthma$asthma) |&gt; \n  chisq.test(correct = T)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 3.6217, df = 1, p-value = 0.05703\n\n\n\n# difference in proportions\ntable(asthma$sex, asthma$asthma) |&gt; \n  prop.test(alternative = 'two.sided', \n            conf.level = 0.95, \n            correct = T)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 3.6217, df = 1, p-value = 0.05703\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.0004958005  0.0434742223\nsample estimates:\n    prop 1     prop 2 \n0.05903614 0.03754693 \n\n\n\n\nthe tests are identical!\nthe difference in proportions \\(\\hat{p}_F - \\hat{p}_M\\) is one specific measure of association\nnext time we’ll learn about other measures, which also have the same inference attached"
  },
  {
    "objectID": "content/week9-association.html#extending-to-i-times-j-tables",
    "href": "content/week9-association.html#extending-to-i-times-j-tables",
    "title": "Tests of association",
    "section": "Extending to \\(I \\times J\\) tables",
    "text": "Extending to \\(I \\times J\\) tables\n\n\nFAMuSS data:\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nAfrican Am\n16\n6\n5\n27\n\n\nAsian\n21\n18\n16\n55\n\n\nCaucasian\n125\n216\n126\n467\n\n\nHispanic\n4\n10\n9\n23\n\n\nOther\n7\n11\n5\n23\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\n\nExpected counts:\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nAfrican Am\n7.85\n11.84\n7.31\n27\n\n\nAsian\n15.99\n24.13\n14.88\n55\n\n\nCaucasian\n135.8\n204.8\n126.4\n467\n\n\nHispanic\n6.69\n10.09\n6.22\n23\n\n\nOther\n6.69\n10.09\n6.22\n23\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\n\n\n\nexpected counts and chi-square statistic are calculated exactly the same way\ndegrees of freedom are now \\((I - 1)\\times(J - 1)\\)\nappropriate provided all \\(\\hat{n}_{ij} &gt; 1\\) and most (~80%) \\(\\hat{n}_{ij} \\geq 5\\)"
  },
  {
    "objectID": "content/week9-association.html#extending-to-itimes-j-tables",
    "href": "content/week9-association.html#extending-to-itimes-j-tables",
    "title": "Tests of association",
    "section": "Extending to \\(I\\times J\\) tables",
    "text": "Extending to \\(I\\times J\\) tables\nIn detail:\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nAfrican Am\n\\(\\frac{(16 - 7.85)^2}{7.85}\\)\n\\(\\frac{(6 - 11.84)^2}{11.84}\\)\n\\(\\frac{(5 - 7.306)^2}{7.306}\\)\n\n\nAsian\n\\(\\frac{(21 - 15.99)^2}{15.99}\\)\n\\(\\frac{(18 - 24.13)^2}{24.13}\\)\n\\(\\frac{(16 -14.88)^2}{14.88}\\)\n\n\nCaucasian\n\\(\\frac{(125 - 135.8)^2}{135.8}\\)\n\\(\\frac{(216 - 204.9)^2}{204.9}\\)\n\\(\\frac{(126 - 126.4)^2}{126.4}\\)\n\n\nHispanic\n\\(\\frac{(4 - 6.687)^2}{6.687}\\)\n\\(\\frac{(10 - 10.09)^2}{10.09}\\)\n\\(\\frac{(9 - 6.224)^2}{6.224}\\)\n\n\nOther\n\\(\\frac{(7 - 6.687)^2}{6.687}\\)\n\\(\\frac{(11 - 10.09)^2}{10.09}\\)\n\\(\\frac{(5 - 6.224)^2}{6.224}\\)\n\n\n\nThen:\n\\[\\begin{cases} &\\chi^2 = \\sum \\text{all cells above} = 19.4 \\\\\n&P(\\chi^2_{8} &gt; 19.4) = 0.01286 \\end{cases}\n\\quad\\Longrightarrow\\quad \\text{reject hypothesis of no association}\\]"
  },
  {
    "objectID": "content/week9-association.html#inference-for-itimes-j-tables-in-r",
    "href": "content/week9-association.html#inference-for-itimes-j-tables-in-r",
    "title": "Tests of association",
    "section": "Inference for \\(I\\times J\\) tables in R",
    "text": "Inference for \\(I\\times J\\) tables in R\n\n\nThe implementation is the same as for a \\(2\\times 2\\) table:\n\n# construct table and pass to chisq.test\ntable(famuss$race, famuss$actn3.r577x) |&gt;\n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  table(famuss$race, famuss$actn3.r577x)\nX-squared = 19.4, df = 8, p-value = 0.01286\n\n\n\n\nThe data provide evidence of an association between race and genotype (\\(\\chi^2\\) = 19.4 on 8 degrees of freedom, p = 0.01286).\n\n\n\nWhich genotype/race combinations are contributing most to this inferred association?"
  },
  {
    "objectID": "content/week9-association.html#residual-analysis",
    "href": "content/week9-association.html#residual-analysis",
    "title": "Tests of association",
    "section": "Residual analysis",
    "text": "Residual analysis\n\n\n\n# store result of test; display residuals\nrslt &lt;- chisq.test(tbl)\nrslt$residuals\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nAfrican Am\n2.909\n-1.698\n-0.8531\n\n\nAsian\n1.252\n-1.247\n0.2897\n\n\nCaucasian\n-0.9254\n0.7789\n-0.03244\n\n\nHispanic\n-1.039\n-0.02804\n1.113\n\n\nOther\n0.1209\n0.2868\n-0.4905\n\n\n\n\n\n\nAgain look for the largest absolute residuals to explain inferred association.\n\nAfrican American and Asian populations have higher CC and lower CT frequencies than would be expected if genotype were independent of race.\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week8-proportions.html#todays-agenda",
    "href": "content/week8-proportions.html#todays-agenda",
    "title": "Inference for population proportions",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] Inference for binomial proportions\n[lab] Tests for proportions in R"
  },
  {
    "objectID": "content/week8-proportions.html#binomial-proportions",
    "href": "content/week8-proportions.html#binomial-proportions",
    "title": "Inference for population proportions",
    "section": "Binomial proportions",
    "text": "Binomial proportions\n\nA binomial variable is a nominal categorical variable with two unique values.\n\n\n\nUsually, binomial data record the presence/absence of an event, trait, or property of interest.\nInference for binomial data has a different flavor:\n\nnon-numeric values \\(\\Rightarrow\\) can’t compute usual statistics (mean, variance, etc.)\nfocus on proportions instead\n\nExample: prevalence of diabetes among US adults?\n\nestimate and standard error?\nconfidence interval?\nhypothesis test?"
  },
  {
    "objectID": "content/week8-proportions.html#estimating-proportions",
    "href": "content/week8-proportions.html#estimating-proportions",
    "title": "Inference for population proportions",
    "section": "Estimating proportions",
    "text": "Estimating proportions\n\n\n\n\n\nDiabetes data summary\n\n\n\n\n\n\n\n\n \nYes\nNo\ntotal\n\n\n\n\ncount\n57\n443\n500\n\n\nproportion\n0.114\n0.886\n1\n\n\n\n\n\nEstimated diabetes prevalence: 11.4%.\n\nNHANES data are a random sample of the U.S. adult population\nsample statistics should approximate population statistics\n\n\nWe’ll formalize this as estimating the population proportion \\[p = \\frac{\\# \\text{ individuals with diabetes}}{\\text{total population size } N}\\] Using the sample proportion \\[\\hat{p} = \\frac{\\# \\text{ respondents with diabetes}}{\\text{sample size } n}\\]\n\n\nThe first step towards inference is a measure of precision for \\(\\hat{p}\\). What is \\(SE(\\hat{p})\\)?"
  },
  {
    "objectID": "content/week8-proportions.html#se-for-a-sample-proportion",
    "href": "content/week8-proportions.html#se-for-a-sample-proportion",
    "title": "Inference for population proportions",
    "section": "SE for a sample proportion",
    "text": "SE for a sample proportion\n\nBinomial data are most variable when \\(p = 0.5\\) and least variable when \\(p \\approx 0\\) or \\(1\\)\n\n\n\nMeasure of spread for binomial data: \\[\\sqrt{\\hat{p}(1 - \\hat{p})}\\]\n\nhighest when \\(\\hat{p} \\approx 0.5\\)\nlowest when \\(\\hat{p} \\approx 0 \\text{ or } 1\\)\n\nAnalogous to estimating a mean: \\[\nSE\\left(\\hat{p}\\right)\n= \\frac{\\text{spread}}{\\sqrt{\\text{sample size}}}\n= \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]"
  },
  {
    "objectID": "content/week8-proportions.html#sampling-distribution-of-hatp",
    "href": "content/week8-proportions.html#sampling-distribution-of-hatp",
    "title": "Inference for population proportions",
    "section": "Sampling distribution of \\(\\hat{p}\\)",
    "text": "Sampling distribution of \\(\\hat{p}\\)\n\n\nThe sample proportion \\(\\hat{p}\\) has a sampling distribution that can be approximated by a normal model, provided:\n\n\\(\\hat{p}\\) isn’t too close to 0 or 1\n\\(n\\) is sufficiently large\n\nA common condition to check:\n\\[n\\hat{p} \\geq 10\\text{ and }n(1 - \\hat{p}) \\geq 10\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis model can be used to construct hypothesis tests and confidence intervals for \\(p\\)."
  },
  {
    "objectID": "content/week8-proportions.html#confidence-interval-for-p",
    "href": "content/week8-proportions.html#confidence-interval-for-p",
    "title": "Inference for population proportions",
    "section": "Confidence interval for \\(p\\)",
    "text": "Confidence interval for \\(p\\)\nA confidence interval for a binomial proportion \\(p\\) is:\n\\[\\hat{p} \\pm c \\times SE(\\hat{p})\\]\nThe critical value \\(c\\) comes from the normal model.\n\nempirical rule:\n\n\\(c = 1\\) gives a 68% interval\n\\(c = 2\\) gives a 95% interval\n\\(c = 3\\) gives a 99.7% interval\n\nfor a \\((1 - \\alpha)\\times 100 \\%\\) confidence interval use the \\(1 - \\frac{\\alpha}{2}\\) quantile of the normal model\n\n\nqnorm(1 - 0.1/2) # c for 90% interval\nqnorm(1 - 0.05/2) # c for 95% interval\nqnorm(1 - 0.01/2) # c for 99% interval"
  },
  {
    "objectID": "content/week8-proportions.html#example-diabetes-prevalence",
    "href": "content/week8-proportions.html#example-diabetes-prevalence",
    "title": "Inference for population proportions",
    "section": "Example: diabetes prevalence",
    "text": "Example: diabetes prevalence\n\n\n\n\n\nPoint estimate for diabetes prevalence\n\n\n\n\n\n\n\np.hat\nse\nn\n\n\n\n\n0.114\n0.01421\n500\n\n\n\n\n\n\n\nIt is estimated that the proportion of the U.S. adult population with diagnosed diabetes is 11.4% (SE = 1.42%).\n\n\n\nCheck assumptions for the normal model:\n\\[\n500\\times 0.114 = 57 \\geq 10\n\\quad\\text{and}\\quad 500\\times 0.886 = 443 \\geq 10\n\\]\n\n\n95% confidence interval for diabetes prevalence:\n\\[\n0.114 \\pm 2\\times 0.01421 = (0.0881, 0.1459)\n\\]\n\n\nWith 95% confidence, the proportuion of U.S. adults with diagnosed diabetes is estimated to be between 8.81% and 14.59%."
  },
  {
    "objectID": "content/week8-proportions.html#hypothesis-tests-for-p",
    "href": "content/week8-proportions.html#hypothesis-tests-for-p",
    "title": "Inference for population proportions",
    "section": "Hypothesis tests for \\(p\\)",
    "text": "Hypothesis tests for \\(p\\)\n\n\nTo test whether true prevalence is 10%: \\[\n\\begin{cases}\nH_0: &p = 0.1 \\\\\nH_A: &p \\neq 0.1\n\\end{cases}\n\\]\nWe can use the test statistic:\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n= \\frac{\\hat{p} - 0.1}{\\sqrt{\\frac{0.1 (0.9)}{500}}}\n\\] Under \\(H_0\\), the sampling distribution of \\(Z\\) is approximated by a normal model, provided:\n\n\\(np_0 \\geq 10\\)\n\\(n(1 - p_0) \\geq 10\\)\n\n\n\n\n\n\n\n\n\n\n\nHere \\(p = P(|Z| &gt; 1.043) = 0.2967\\), so:\n\nthe data provide no evidence that prevalence differs from 10%."
  },
  {
    "objectID": "content/week8-proportions.html#hypothesis-tests-for-p-1",
    "href": "content/week8-proportions.html#hypothesis-tests-for-p-1",
    "title": "Inference for population proportions",
    "section": "Hypothesis tests for \\(p\\)",
    "text": "Hypothesis tests for \\(p\\)\n\n\nTo test whether true prevalence is 15%: \\[\n\\begin{cases}\nH_0: &p = 0.15 \\\\\nH_A: &p \\neq 0.15\n\\end{cases}\n\\]\nWe can use the test statistic:\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n= \\frac{\\hat{p} - 0.15}{\\sqrt{\\frac{0.15 (0.85)}{500}}}\n\\] Under \\(H_0\\), the sampling distribution of \\(Z\\) is approximated by a normal model, provided:\n\n\\(np_0 \\geq 10\\)\n\\(n(1 - p_0) \\geq 10\\)\n\n\n\n\n\n\n\n\n\n\n\nHere \\(p = P(|Z| &gt; 2.254) = 0.0242\\), so:\n\nthe data provide moderate evidence that prevalence differs from 15%."
  },
  {
    "objectID": "content/week8-proportions.html#hypothesis-tests-for-p-2",
    "href": "content/week8-proportions.html#hypothesis-tests-for-p-2",
    "title": "Inference for population proportions",
    "section": "Hypothesis tests for \\(p\\)",
    "text": "Hypothesis tests for \\(p\\)\n\n\nTo test if prevalence is below 14%: \\[\n\\begin{cases}\nH_0: &p = 0.14 \\\\\nH_A: &p &lt; 0.14\n\\end{cases}\n\\]\nWe can use the test statistic:\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n= \\frac{\\hat{p} - 0.14}{\\sqrt{\\frac{0.14 (0.86)}{500}}}\n\\]\nUnder \\(H_0\\), the sampling distribution of \\(Z\\) is approximated by a normal model, provided:\n\n\\(np_0 \\geq 10\\)\n\\(n(1 - p_0) \\geq 10\\)\n\n\n\n\n\n\n\n\n\n\n\nHere \\(p = P(Z &lt; 1.676) = 0.0469\\), so:\n\nthe data provide moderate evidence that prevalence is less than 14%."
  },
  {
    "objectID": "content/week8-proportions.html#inference-for-a-proportion-in-r",
    "href": "content/week8-proportions.html#inference-for-a-proportion-in-r",
    "title": "Inference for population proportions",
    "section": "Inference for a proportion in R",
    "text": "Inference for a proportion in R\n\n\nInference using the normal model in R:\n\nConstruct a table of the frequency distribution\nPass the table to prop.test()\n\nRemarks about output:\n\nX-squared gives \\(Z^2\\)\ncorrect = F performs the test without continuity correction\n\n\n\n# variable of interest\ndia &lt;- nhanes$diabetes\n\n# pass table to prop.test\ntable(dia) |&gt; \n  prop.test(p = 0.1, alternative = 'two.sided',\n            conf.level = 0.95, correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 1.0889, df = 1, p-value = 0.2967\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.0890369 0.1448491\nsample estimates:\n    p \n0.114 \n\n\n\n\n\nThe data provide no evidence that diabetes prevalence among U.S. adults differs from 10%. With 95% confidence, prevalence is estimated to be between 8.90% and 14.48%, with a point estimate of 11.4% (SE = 1.42%)."
  },
  {
    "objectID": "content/week8-proportions.html#correct-f",
    "href": "content/week8-proportions.html#correct-f",
    "title": "Inference for population proportions",
    "section": "correct = F?",
    "text": "correct = F?\n\nA “continuity correction” reduces approximation error for the normal model.\n\n\n\n\ntable(dia) |&gt; \n  prop.test(p = 0.1, \n            alternative = 'two.sided',\n            conf.level = 0.95, \n            correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 1.0889, df = 1, p-value = 0.2967\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.0890369 0.1448491\nsample estimates:\n    p \n0.114 \n\n\n\n\ntable(dia) |&gt; \n  prop.test(p = 0.1, \n            alternative = 'two.sided',\n            conf.level = 0.95, \n            correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 0.93889, df = 1, p-value = 0.3326\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.08814952 0.14594579\nsample estimates:\n    p \n0.114 \n\n\n\n\nOmitting the correct argument implements the correction by default."
  },
  {
    "objectID": "content/week8-proportions.html#exact-inference-for-a-proportion",
    "href": "content/week8-proportions.html#exact-inference-for-a-proportion",
    "title": "Inference for population proportions",
    "section": "Exact inference for a proportion",
    "text": "Exact inference for a proportion\nThe test can also be performed using the exact sampling distribution obtained from a binomial probability model.\n\nbinom.test(x = 57, n = 500, p = 0.1, alternative = 'two.sided')\n\n\n    Exact binomial test\n\ndata:  57 and 500\nnumber of successes = 57, number of trials = 500, p-value = 0.2964\nalternative hypothesis: true probability of success is not equal to 0.1\n95 percent confidence interval:\n 0.0874949 0.1451685\nsample estimates:\nprobability of success \n                 0.114 \n\n\nInputs:\n\nx gives the number of occurrences of the category of interest\nn gives the sample size"
  },
  {
    "objectID": "content/week8-proportions.html#two-way-tables",
    "href": "content/week8-proportions.html#two-way-tables",
    "title": "Inference for population proportions",
    "section": "Two-way tables",
    "text": "Two-way tables\n\nTwo-way tables or “contingency” tables compare two categorical variables.\n\n\n\n\n\n\nVitamin C experiment\n\n\n\n\n\n\n\n\n \nCold\nNoCold\nn\n\n\n\n\nPlacebo\n335\n76\n411\n\n\nVitC\n302\n105\n407\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvitamin C and placebo treatments were randomly allocated to 818 volunteers\nvolunteers took treatments daily for a cold season\nstudy recorded how many volunteers came down with a cold\n\nIs vitamin C effective at preventing common cold?"
  },
  {
    "objectID": "content/week8-proportions.html#inference-for-two-proportions",
    "href": "content/week8-proportions.html#inference-for-two-proportions",
    "title": "Inference for population proportions",
    "section": "Inference for two proportions",
    "text": "Inference for two proportions\nWe can first consider inferences on the difference in proportions:\n\\[\\delta = p_\\text{placebo} - p_\\text{vitC}\\]\n\nInferences are based on groupwise estimates:\n\npoint estimate: \\(\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC}\\)\nstandard error: \\(\\sqrt{SE^2(\\hat{p}_\\text{placebo}) + SE^2(\\hat{p}_\\text{vitC})}\\)\n\nWhen both groups meet the conditions for inference for one proportion, the statistic\n\\[\nZ = \\frac{\\hat{p}_1 - \\hat{p}_2 - \\delta}{SE(\\hat{p}_1 - \\hat{p}_2)}\n\\] has a sampling distribution well-approximated by a normal model."
  },
  {
    "objectID": "content/week8-proportions.html#confidence-interval-for-the-difference",
    "href": "content/week8-proportions.html#confidence-interval-for-the-difference",
    "title": "Inference for population proportions",
    "section": "Confidence interval for the difference",
    "text": "Confidence interval for the difference\n\\[\n\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC} \\pm c\\times SE(\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC})\n\\] For a \\((1 - \\alpha)\\times 100\\%\\) confidence interval the critical value \\(c\\) is chosen to be the \\(\\left(1 - \\frac{\\alpha}{2}\\right)\\) quantile of the normal model.\n\npoint estimate: \\(\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC} = 0.0731\\)\nstandard error: \\(\\sqrt{SE^2(\\hat{p}_\\text{placebo}) + SE^2(\\hat{p}_\\text{vitC})} = 0.0289\\)\ncritical value for 95% interval: qnorm(1 - 0.05/2) = 1.959964\n\n95% confidence interval: (0.0164, 0.1298)\n\nWith 95% confidence, the prevalence of common cold is estimated to be between 1.64% and 12.98% lower among adults who take daily vitamin C supplements."
  },
  {
    "objectID": "content/week8-proportions.html#tests-for-a-difference-in-proportions",
    "href": "content/week8-proportions.html#tests-for-a-difference-in-proportions",
    "title": "Inference for population proportions",
    "section": "Tests for a difference in proportions",
    "text": "Tests for a difference in proportions\n\n\nWe can also test whether vitamin C prevents common cold:\n\\[\n\\begin{cases}\nH_0: &p_\\text{placebo} - p_\\text{vitC} = 0\\\\\nH_A: &p_\\text{placebo} - p_\\text{vitC} &gt; 0\n\\end{cases}\n\\]\nHypothesis tests use the test statistic:\n\\[Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\\]\nWith a slightly different SE where: \\[\\hat{p} = \\frac{n_1\\hat{p}_1 + n_2\\hat{p}_2}{n_1 + n_2}\\]\n\n\n\n\n\n\n\n\n\n\nHere \\(p = P(Z &gt; 2.517) = 0.0059\\), so:\n\nthe data provide strong evidence that vitamin C prevents common cold."
  },
  {
    "objectID": "content/week8-proportions.html#inference-in-r",
    "href": "content/week8-proportions.html#inference-in-r",
    "title": "Inference for population proportions",
    "section": "Inference in R",
    "text": "Inference in R\n\n\nThree steps:\n\nConstruct a table of the frequency distribution by group\n\noutcomes should be columns\ngroups should be rows\n\nPass to prop.test()\n\nThe alternative reads the same way as in t.test.\n\n\n# variables of interest\ntreatment &lt;- vitamin$treatment\noutcome &lt;- vitamin$outcome\n\n# pass table to prop.test\ntable(treatment, outcome) |&gt;\n  prop.test(alternative = 'greater', \n            correct = F)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  table(treatment, outcome)\nX-squared = 6.3366, df = 1, p-value = 0.005914\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.02548153 1.00000000\nsample estimates:\n   prop 1    prop 2 \n0.8150852 0.7420147 \n\n\n\n\n\nThe data provide strong evidence that vitamin C prevents common cold (Z = 2.517, p = 0.0059). With 95% confidence, the reduction in probability is estimated to be at least 0.0255, with a point estimate of 0.0731 (SE = 0.0289)."
  },
  {
    "objectID": "content/week8-proportions.html#sampling-and-two-way-tables",
    "href": "content/week8-proportions.html#sampling-and-two-way-tables",
    "title": "Inference for population proportions",
    "section": "Sampling and two-way tables",
    "text": "Sampling and two-way tables\n\n\nConsider this case-control study:\n\n\n\n\n\n\n\n\n\n\n\n \nSmokers\nNonSmokers\nn\n\n\n\n\nCancer\n83\n3\n86\n\n\nControl\n72\n14\n86\n\n\n\n\n\nThis is an example of outcome-based sampling:\n\n86 lung cancer patients and 86 controls\ncan’t estimate cancer prevalence\n\n\nA different approach to inference is needed to analyze this data. Next time:\n\ntests of association in two-way tables\ninference for risk and odds ratios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week10-slr.html#todays-agenda",
    "href": "content/week10-slr.html#todays-agenda",
    "title": "Simple linear regression",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] estimation and inference for simple linear regression\ncourse evaluations\nfinal scheduling\n[lab] fitting SLR models in R"
  },
  {
    "objectID": "content/week10-slr.html#prevend-data",
    "href": "content/week10-slr.html#prevend-data",
    "title": "Simple linear regression",
    "section": "PREVEND data",
    "text": "PREVEND data\n\n\nRuff Figural Fluency Test (RFFT) is a cognitive assessment.\n\nmeasures nonverbal capacity for initiation, planning, and divergent reasoning\nscale: 0 (worst) to 175 (best)\n\n\n\n\n\n\n\n\n\n\n\n\ncasenr\nage\nrfft\n\n\n\n\n126\n37\n136\n\n\n33\n36\n80\n\n\n145\n37\n102\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow much does cognitive ability as measured by RFFT decline with age on average?"
  },
  {
    "objectID": "content/week10-slr.html#best-fitting-line",
    "href": "content/week10-slr.html#best-fitting-line",
    "title": "Simple linear regression",
    "section": "Best-fitting line",
    "text": "Best-fitting line\n\n\n\n\n\n\n\n\n\n\n\n\nPreviously you found the best-fitting line:\n\\[\n\\text{RFFT} = 134.098 - 1.191 \\times \\text{age}\n\\]\n\nWith each year of age, RFFT decreases by 1.191 points on average.\n\n\n\n\\[\n\\begin{align}\n\\text{slope}:\n\\quad-1.191 &= \\text{cor}(\\text{age}, \\text{RFFT})\\times\\frac{SD(\\text{RFFT})}{SD(\\text{age})}\n\\\\\n\\text{intercept}:\n\\quad134.098 &= \\text{mean}(\\text{RFFT}) - (-1.191)\\times\\text{mean}(\\text{age})\n\\end{align}\n\\]"
  },
  {
    "objectID": "content/week10-slr.html#bias-and-error",
    "href": "content/week10-slr.html#bias-and-error",
    "title": "Simple linear regression",
    "section": "Bias and error",
    "text": "Bias and error\n\n\nRecall how you found this line:\n\n\n\n\n\n\n\n\n\n\nBias and error are measured via residuals: \\[\n\\textcolor{red}{e_i} = y_i - \\textcolor{blue}{\\hat{y}_i}\n\\]\n\n\\(\\text{bias} = -\\frac{1}{n}\\sum_i \\textcolor{red}{e_i}\\)\n\\(\\text{SSE} = \\sum_i \\textcolor{red}{e_i}^2\\)\n\nWe said that the best-fitting line achieved two conditions:\n\nno bias: underestimates and overestimates equally often\nminimal error: as close as possible to as many data points as possible"
  },
  {
    "objectID": "content/week10-slr.html#the-slr-model",
    "href": "content/week10-slr.html#the-slr-model",
    "title": "Simple linear regression",
    "section": "The SLR model",
    "text": "The SLR model\n\n\nThe simple linear regression model is:\n\\[\nY\n= \\textcolor{blue}{\\underbrace{\\beta_0 + \\beta_1 x}_\\text{mean}} +\n\\textcolor{red}{\\underbrace{\\epsilon}_\\text{error}}\n\\]\n\n\ncontinuous response \\(Y\\)\nexplanatory variable \\(x\\)\nregression coefficients \\(\\beta_0, \\beta_1\\)\nmodel error \\(\\epsilon\\)\n\n\n\nThe values that minimize error subject to the model being unbiased are:\n\\[\\begin{align*}\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} &\\quad(\\text{unbiased}) \\\\\n\\hat{\\beta}_1 &= \\frac{s_y}{s_x}\\times r  &\\quad(\\text{minimizes SSE})\n\\end{align*}\\]\nThese are called the least squares estimates."
  },
  {
    "objectID": "content/week10-slr.html#least-squares-estimates-in-r",
    "href": "content/week10-slr.html#least-squares-estimates-in-r",
    "title": "Simple linear regression",
    "section": "Least squares estimates in R",
    "text": "Least squares estimates in R\nAccording to the model, a one-unit increment in \\(x\\) corresponds to a \\(\\beta_1\\)-unit change in mean \\(Y\\):\n\n\n\n# fit model\nfit &lt;- lm(formula = rfft ~ age, data = prevend)\nfit\n\n\nCall:\nlm(formula = rfft ~ age, data = prevend)\n\nCoefficients:\n(Intercept)          age  \n    134.098       -1.191  \n\n\n\n\nWith each additional year of age, mean RFFT score decreases by an estimated 1.191 points.\n\n\n\n\nformula = &lt;RESPONSE&gt; ~ &lt;EXPLANATORY&gt; specifies the model\ndata = &lt;DATAFRAME&gt; specifies the observations"
  },
  {
    "objectID": "content/week10-slr.html#error-variability-and-model-fit",
    "href": "content/week10-slr.html#error-variability-and-model-fit",
    "title": "Simple linear regression",
    "section": "Error variability and model fit",
    "text": "Error variability and model fit\nThe residual standard deviation provides an estimate of error variability:\n\\[\\textcolor{\\red}{\\hat{\\sigma}} = \\sqrt{\\frac{1}{n - 2} \\sum_i e_i^2} \\qquad\\text{(estimated error variability)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportion of variability explained by the model is: \\[\nR^2 = 1 - \\frac{(n - 2)\\textcolor{red}{\\hat{\\sigma}^2}}{(n - 1)\\textcolor{darkgrey}{s_y^2}}\n\\quad\\left(1 - \\frac{\\text{error variability}}{\\text{total variability}}\\right)\n\\]\n\n1 - (n - 2)*sigma(fit)^2/((n - 1)*var(rfft))\n\n[1] 0.4043103\n\n\n\nAge explains 40.43% of variability in RFFT."
  },
  {
    "objectID": "content/week10-slr.html#standard-errors-for-the-coefficients",
    "href": "content/week10-slr.html#standard-errors-for-the-coefficients",
    "title": "Simple linear regression",
    "section": "Standard errors for the coefficients",
    "text": "Standard errors for the coefficients\nStandard errors for the coefficients are:\n\\[SE\\left(\\hat{\\beta}_0\\right) = \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{(n - 1)s_x^2}} \\qquad\\text{and}\\qquad\nSE\\left(\\hat{\\beta}_1\\right) = \\hat{\\sigma}\\sqrt{\\frac{1}{(n - 1)s_x^2}}\\]\nWhile you won’t need to know these formulae, do notice that:\n\nmore data \\(\\longrightarrow\\) less sampling variability\nmore spread in \\(x\\) \\(\\longrightarrow\\) less sampling variability"
  },
  {
    "objectID": "content/week10-slr.html#inference-for-the-coefficients",
    "href": "content/week10-slr.html#inference-for-the-coefficients",
    "title": "Simple linear regression",
    "section": "Inference for the coefficients",
    "text": "Inference for the coefficients\n\n\nIf the errors are symmetric and unimodal, then the sampling distribution of \\[\nT = \\frac{\\hat{\\beta}_1 - \\beta_1}{SE(\\beta_1)}\n\\] is well-approximated by a \\(t_{n - 2}\\) model.\n\nSignificance test: \\(\\begin{cases} H_0: \\beta_1 = 0 \\\\ H_A: \\beta_1 \\neq 0 \\end{cases}\\)\nConfidence interval: \\(\\hat{\\beta}_1 \\pm c\\times SE\\left(\\hat{\\beta}_1\\right)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P(T &gt; |T_\\text{obs}|) \\approx 0\\): very strong evidence of an association (true slope is not zero)\nconfidence interval using \\(t_{206}\\) critical value: (-1.389, -0.992)"
  },
  {
    "objectID": "content/week10-slr.html#inference-for-the-prevend-study",
    "href": "content/week10-slr.html#inference-for-the-prevend-study",
    "title": "Simple linear regression",
    "section": "Inference for the PREVEND study",
    "text": "Inference for the PREVEND study\n\n\n\nfit &lt;- lm(rfft ~ age, data = prevend)\nsummary(fit)\n\n\nCall:\nlm(formula = rfft ~ age, data = prevend)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.085 -14.690  -2.937  12.744  77.975 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 134.0981     6.0701   22.09   &lt;2e-16 ***\nage          -1.1908     0.1007  -11.82   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.52 on 206 degrees of freedom\nMultiple R-squared:  0.4043,    Adjusted R-squared:  0.4014 \nF-statistic: 139.8 on 1 and 206 DF,  p-value: &lt; 2.2e-16\n\nconfint(fit)\n\n                 2.5 %      97.5 %\n(Intercept) 122.130647 146.0654574\nage          -1.389341  -0.9922471\n\n\n\nFitted model: \\[\n\\text{RFFT} = 134.098 - 1.191 \\times \\text{age}\n\\]\n\nAge explains an estimated 40.43% of variation in RFFT.\nWith each year of age mean RFFT declines by an estimated 1.19 points (SE 0.10).\nThere is a significant association between age and mean RFFT score (T = -11.82 on 206 degrees of freedom, p &lt; 0.0001).\nWith 95% confidence, each additional year of age is associated with an estimated decline in mean RFFT between 0.99 and 1.39 points."
  },
  {
    "objectID": "content/week10-slr.html#kleibers-law",
    "href": "content/week10-slr.html#kleibers-law",
    "title": "Simple linear regression",
    "section": "Kleiber’s law",
    "text": "Kleiber’s law\nKleiber’s law refers to the relationship between metabolic rate and body mass.\n\n\n\n\n\n\n\n\n\n\n\n\nWe can estimate it via the SLR model: \\[\n\\log(\\text{metabolism}) = \\beta_0 + \\beta_1 \\log(\\text{mass}) + \\epsilon\n\\]\nFitted model: \\[\n\\log(\\text{metabolism}) = 5.64 +\n0.74 \\times \\log(\\text{mass})\n\\]\n\nfit &lt;- lm(log.metab ~ log.mass, data = kleiber)"
  },
  {
    "objectID": "content/week10-slr.html#kleibers-law-inference",
    "href": "content/week10-slr.html#kleibers-law-inference",
    "title": "Simple linear regression",
    "section": "Kleiber’s law: inference",
    "text": "Kleiber’s law: inference\n\n\n\nfit &lt;- lm(log.metab ~ log.mass, data = kleiber)\nsummary(fit)\n\n\nCall:\nlm(formula = log.metab ~ log.mass, data = kleiber)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.14216 -0.26466 -0.04889  0.25308  1.37616 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.63833    0.04709  119.73   &lt;2e-16 ***\nlog.mass     0.73874    0.01462   50.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4572 on 93 degrees of freedom\nMultiple R-squared:  0.9649,    Adjusted R-squared:  0.9645 \nF-statistic:  2553 on 1 and 93 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{\\beta}_0 = 5.638\\)\n\\(\\hat{\\beta}_1 = 0.739\\)\n\\(\\hat{\\sigma} = 0.457\\)\n\n\n\n\nThere is a significant association between body mass and metabolism (p &lt; 0.0001): body mass explains 96.49% of variation in metabolism; with 95% confidence, a unit increment in log mass is associated with an estimated increase in mean log metabolism between 0.7097 and 0.7678, with a point estimate of 0.7387."
  },
  {
    "objectID": "content/week10-slr.html#kleibers-law-model-interpretation",
    "href": "content/week10-slr.html#kleibers-law-model-interpretation",
    "title": "Simple linear regression",
    "section": "Kleiber’s law: model interpretation",
    "text": "Kleiber’s law: model interpretation\nExponentiating both sides of the fitted SLR model equation:\n\\[\n\\underbrace{\\text{metabolism}}_{e^{\\log(\\text{metabolism})}} = \\underbrace{280.99}_{e^{5.64}} \\times \\underbrace{\\text{mass}^{0.74}}_{e^{0.74 \\log(\\text{mass})}}\n\\]\nSo we’ve really estimated what’s known as a power law relationship: \\(y = ax^b\\).\n\nmultiplicative, not additive, relationship\ndoubling \\(x\\) corresponds to changing \\(y\\) by a factor of \\(2^b\\)\n\nThe estimate and interval for \\(\\beta_1\\) in the SLR model can be transformed appropriately for a more direct interpretation:\n\nWith 95% confidence, every doubling of body mass is associated with an estimated 63.55-70.26% increase in median metabolism.\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/syllabus.html",
    "href": "content/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Statistics plays a crucial role in the sciences: statistical techniques provide a means of weighing quantitative evidence derived from observation and experimentation while accounting for uncertainty. Statistical thinking and data analysis also facilitate discovery, exploration, and hypothesis generation. This class aims to provide a hands-on introduction to common statistical methods used almost universally across the sciences — descriptive and graphical techniques, inferential methods for comparing population means, analysis of categorical data and contingency tables, and linear regression — while drawing on examples from the life sciences to help illuminate the potential for application in students’ chosen field(s) of study and providing basic training in the use of statistical software.\n\n\nCourse information\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nClass meetings will comprise a mixture of lecture, lab activities, class activities, and discussion.\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment].\nThese times are partitioned into 15 minute intervals that you can schedule via the appointment link above; this system is intended to minimize waiting times and guarantee one-on-one availability. Slots can be scheduled anywhere from 7 calendar days to 10 minutes in advance. While drop-ins are welcome, I can’t guarantee availability outside of scheduled times.\nCatalog Description: Data collection and experimental design, descriptive statistics, confidence intervals, parametric and non parametric one and two-sample hypothesis tests, analysis of variance, correlation, simple linear regression, chi-square tests. Applications of statistics to the life sciences. Substantial use of statistical software. Prerequisite: MATH 96; or MATH 115; or appropriate Math Placement Level. Fulfills GE Area B4 (GE Area B1 for students on the 2019-20 or earlier catalogs); a grade of C- or better is required in one course in this GE area.\n\n\nMaterials\nYou’ll need an internet-connected laptop or tablet (a keyboard is necessary since we will do some web-hosted computation and you will be expected to type assignments). You should expect to bring your laptop or tablet to every class meeting.\nComputing: use of R/RStudio will be hosted online via a posit.cloud workspace [link to join]. To access the workspace, you’ll need to create a posit.cloud account and purchase a $5/month student plan.\nTextbook: Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences, First edition. A PDF and tablet-friendly version are available for free online at the link above. This will be our primary reference and we will cover chapters 1 – 2, 4 – 6, and 8.\nCourse notes: course notes will be posted as slides on the course website.\nOther references:\n\nVan Belle, Fisher, Heagerty, and Lumley (2004). Biostatistics: a methodology for the health sciences. Wiley. A PDF can be obtained through the Kennedy Library via the link above. This text provides a thorough introduction to biostatistics (statistics for life sciences) and is an excellent reference for more depth of coverage. Select readings will be assigned from this book.\nDouglas et al. (2023). An Introduction to R. This online book covers a variety of introductory topics pertaining to R/RStudio: installation, packages, files and directories, objects, functions, data types, data structures, graphics, basic statistics, markdown, and version control. Select readings will be assigned from this book.\n\n\n\nLearning outcomes\nThis course aims to support you in developing the following abilities.\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques\n[L4] construct and interpret confidence intervals for means and differences between means for independent and paired samples\n[L5] conduct parametric and non-parametric two-sample hypothesis tests for means\n[L6] construct and interpret a confidence interval for a single proportion\n[L7] conduct Chi-square goodness-of-fit tests and tests for independence\n[L8] distinguish between case-control and cohort studies and compute relative-risk and odds in the appropriate settings\n[L9] perform analysis of variance tests and post-hoc comparisons for completely randomized designs\n[L10] use simple linear regression to describe relationships between variables\n[L11] apply one or more methods from the course to your major field of study\n\nEmphasis is placed on conceptual fluency, application, and interpretation. In addition, you will learn to perform simple statistical analyses in R and can expect to develop a basic familiarity with the software; however, as this is not a programming class, the R environment will not be discussed in any detail and you will only learn to use a handful of commands.\n\n\nAssessments\nAttainment of learning outcomes will be measured by performance on homework assignments, tests, and a short project with an oral assessment in lieu of a final exam.\n\nHomework assignments will be given at the end of every class meeting and will comprise two practice problems due by the next class meeting. These are your opportunity to practice applying course concepts and methods covered in class and will help you to keep current with the pace and content of the lectures.\nTests will be given every 2-3 weeks and will comprise roughly 10-20 problems each. These are your opportunity to demonstrate that you’ve synthesized course material and achieved learning outcomes, and you will have approximately 48 hours to complete each test. One round of revisions will be allowed for each test in which you can make up full credit for any problems answered incorrectly in your initial attempt.\nA project with an oral assessment will be given in place of a final exam. However, you will need to be available in person during the scheduled final exam time, as this is when the oral assessment will take place.\n\nEvery assessed problem will be matched to one of the learning outcomes L1-L10. All submitted work will be assessed on a question-by-question basis as satisfactory (S) or needing improvement (NI) according to whether responses are fully correct. The percentage of problems matched to a particular learning outcome for which you receive a satisfactory assessment provides a measure of your attainment of that learning outcome. These percentages form a basis for determining your course grade (see below).\nDue to limited resources we will only provide qualitative feedback on a small subset of assessed questions, and only when an assessment of NI is made. As such, it is your responsibility to seek the feedback you need to correct your understanding where needed via class engagement, office hours, peer consultation, further study, and [tutoring resources].\n\n\nLetter grades\nStudents will receive a score for each learning outcome representing the (possibly weighted) proportion of questions matched with that outcome that received a satisfactory assessment across all assignments. The outcome will be assessed as follows:\n\n‘fully met’ if the proportion is at least 0.8\n‘partly met’ if the proportion is between 0.5 and 0.8\n‘unmet’ otherwise\n\nYou will receive periodic email summaries of your progress on each learning outcome. To receive a passing grade in the class, at least six outcomes must be either partly or fully met. Subject to this condition, letter grades are then defined as follows:\n\n\n\nGrade\nNumber of fully met outcomes\n\n\n\n\nA\n10\n\n\nA-\n9\n\n\nB+\n8\n\n\nB\n7\n\n\nB-\n6\n\n\nC+\n5\n\n\nC\n4\n\n\nC-\n3\n\n\nD+\n2\n\n\nD\n1\n\n\nD-\n0\n\n\n\nPlease note that these definitions are tentative and potentially subject to change; however, I will not make the grading requirements more stringent under any circumstances.\nPlease also note that failure to adhere to course policies may result in a lower letter grade than would otherwise be assigned.\n\n\nTentative schedule\nSubject to change.\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings (V&H)\nAssessments\n\n\n\n\n1 (4/1/24)\nIntroduction to statistical thinking and study design\n1.1\n\n\n\n2 (4/8/24)\nData, data types, and data collection\n1.2\n\n\n\n3 (4/15/24)\nDescriptive statistics and graphical summaries\n1.4, 1.5, 1.6\nTest 1 [L1, L2, L3]\n\n\n4 (4/22/24)\nFoundations for inference\n4.1, 4.2\n\n\n\n5 (4/29/24)\nOne-sample inference for numerical data\n4.3, 5.1\n\n\n\n6 (5/6/24)\nTwo-sample inference for numerical data\n5.2, 5.3, 5.4\nTest 2 [L4, L5]\n\n\n7 (5/13/24)\nNonparametric tests; analysis of variance\n5.5\n\n\n\n8 (5/20/24)\nPost-hoc inference in ANOVA; intro to categorical data analysis\n8.1\nTest 3 [L6, L9]\n\n\n9 (5/27/24)\nCategorical data analysis and contingency tables\n8.3, 8.5.1, 8.5.3\n\n\n\n10 (6/3/24)\nSimple linear regression\n6.1, 6.2, 6.4, 6.5\nTest 4 [L7, L8, L10]\n\n\nFinals (6/10/24)\nN/A\nN/A\nOral project assessment [L11]\n\n\n\n\n\nCourse policies\n\nTime commitment\nSTAT218 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including lectures, reading, assignments, and study time. Some variability in workload by week should be expected, and most students will need to budget a few extra hours each week. However, students can expect to be able to meet course expectations with a time commitment of 12-16 hours per week. Considering that class meetings account for four hours per week, students should anticipate devoting 8-12 hours outside of class. If you are spending considerably more time than this on a regular basis, please let me know.\n\n\nAttendance\nRegular attendance is essential for success in the course and required per University policy. Each student may miss two class meetings without notice but additional absences should be excusable and students should notify the instructor. Unexcused absences may negatively impact course grades.\n\n\nDeadlines and extensions\nA one-hour grace period is applied to all deadlines. Work submitted more than one hour after a deadline is considered late. Policies regarding late work are as follows:\n\nYou may turn in as many as four homework assignments up to 48 hours late without penalty at any time during the quarter and without notice. Subsequently, late work may incur a penalty in final grade calculations.\nLate submissions are not allowed for tests. You are expected to plan ahead in order to meet test deadlines; I recommend putting the dates in your calendar at the beginning of the quarter.\nExceptions may be granted for significant and unforeseen challenges (medical absences, family emergencies, and the like).\n\nExtensions may be arranged as needed if warranted by the circumstances and should be requested by email. When requesting an extension, you should explain why it is needed; it is at my discretion to grant the extension or not based on the reason provided. Extensions must be arranged at least 24 hours in advance of the original deadline; requests made after this time will not be considered as a general rule.\nThese policies are intended to provide you with some flexibility to work around unforeseen circumstances while maintaining accountability for completing coursework in a timely manner. That said, if any circumstances arise that the policies do not accommodate well, please let me know and I will do my best to work with you to keep you on track in the course.\n\n\nAcademic integrity\nYou are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR). Particularly important course policies related to academic integrity are discussed below.\nCollaboration. Collaboration among enrolled students is allowed and encouraged on homework assignments subject to the condition that every collaborator must make material contributions. Material contributions might include participation in group discussions, critique or presentation of a proposed solution, comparing numerical answers, and the like. However, group submissions are not allowed and you are expected to write up your own work. Copying the work of another student outright, knowingly allowing another student to copy your work, or submitting a copy of a shared set of answers is not acceptable and amounts to a violation of University policy on academic integrity. The best way to adhere to this policy and ensure your collaborations are productive is to:\n\nattempt problems individually before consulting others\nwrite up your own solutions in private\n\nCollaboration is not permitted on tests and will result in loss of credit.\nUse of AI. Learning to use AI effectively and responsibly for problem-solving in an academic context is a skill unto itself. Submitting problem prompts directly to ChatGPT will, most of the time, return superfluous, tangential, and erroneous answers that do not meet assessment criteria for satisfactory work. Furthermore, even when AI-generated material is technically accurate, outputs rarely conform to the examples set forth in class or the solution strategies that you have been taught.\nSo in the best-case scenario, AI-generated material might be useful but only if you expend additional effort refine the prompts you use and subsequently to parse, understand, and integrate outputs with class content. In the worst-case scenario, AI-generated material will be wrong or irrelevant and simply confuse you. Considering you are learning material that is new to you, you will most likely not be able to distinguish correct from incorrect outputs – if you could, you would have had no need to query in the first place – and it will therefore be difficult if not impossible to use AI effectively. Thus, using AI is more likely to hinder than to help your learning, and for this reason I do not recommend it.\nShould you choose to use AI you must use it as an aid only and not as a substitute for doing your own work. You will be responsible for using it thoughtfully and judiciously. That means critically assessing any outputs and continuing to prepare work to be submitted in your own words and using your own analyses. Submitting AI-generated outputs directly is never acceptable — doing so amounts to falsely representing material that you did not create as your own work and is a violation of University academic integrity policy. I will respond to such violations as follows.\n\nsome AI-generated content detected: loss of credit and warning\nflagrant AI plagiarism, first offense: loss of credit and report to OSRR\nflagrant AI plagiarism, second offense: automatic course failure and report to OSRR\n\nIf you are unsure about where the line is between acceptable and unacceptable use in any particular situation, please discuss the situation with me – I’d much rather help you learn to navigate the issue without the use of penalties wherever possible.\n\n\nAssessments and final grades\nI make every effort to provide consistent, fair, and accurate evaluation of student work. Please notify me of any suspected errors or discrepancies in evaluation promptly on an assignment-by-assignment basis (i.e., not at the end of the quarter) to guarantee consideration. Final (letter) grades will only be changed in the case of clerical errors. Attempting to negotiate scores or final grades is not appropriate.\nPer University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. If you feel your grade is unfairly assigned at the end of the course, you have the right to appeal it according to the procedure outlined here.\n\n\nCommunication and email\nStudents are encouraged to use face-to-face means of communication (class meetings and office hours) when possible. Every effort is made to respond to email within 48 weekday hours; please be aware that a message sent Thursday or Friday may not receive a reply until Monday or Tuesday. Time-sensitive messages should be identified as such in the subject line. For non-time-sensitive messages, please wait one week before sending a reminder.\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nCopyright and distribution of course materials\nStudents are not permitted to share or distribute any course materials without the written consent of the instructor. This includes, in particular, uploading materials or prepared solutions to online services and sharing materials or prepared solutions with students who may take the course in a future term. Transgressions of this policy compromise the effectiveness of instruction and assessment and do a disservice to current and future students."
  },
  {
    "objectID": "content/week5-hypothesis.html#todays-agenda",
    "href": "content/week5-hypothesis.html#todays-agenda",
    "title": "Introduction to hypothesis testing",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nLoose end: working backwards to determine interval coverage\n[lecture] the \\(t\\)-test for a population mean\n[lab] computing test statistics, critical values, and \\(p\\)-values"
  },
  {
    "objectID": "content/week5-hypothesis.html#body-temperatures",
    "href": "content/week5-hypothesis.html#body-temperatures",
    "title": "Introduction to hypothesis testing",
    "section": "Body temperatures",
    "text": "Body temperatures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\nse\n\n\n\n\n98.41\n0.9162\n39\n0.1467\n\n\n\n\n\n\nIs the true mean body temperature actually 98.6°F?\nSeems plausible given our data.\nBut what if the sample mean were instead…\n\n\n\n\\(\\bar{x}\\)\nconsistent with \\(\\mu = 98.6\\)?\n\n\n\n\n98.30\nprobably still yes\n\n\n98.15\nmaybe\n\n\n98.00\nhesitating\n\n\n97.85\nskeptical\n\n\n97.40\nunlikely\n\n\n\n\n\n\nIf the estimation error is “big enough” the hypothesis seems implausible."
  },
  {
    "objectID": "content/week5-hypothesis.html#how-much-error-is-too-much",
    "href": "content/week5-hypothesis.html#how-much-error-is-too-much",
    "title": "Introduction to hypothesis testing",
    "section": "How much error is too much?",
    "text": "How much error is too much?\nConsider how many standard errors away from the hypothesized value we’d be:\n\n\n\n\\(\\bar{x}\\)\nestimation error\nno. SE’s\ninterpretation\n\n\n\n\n98.30\n-0.3\n2\ndouble the average error\n\n\n98.15\n-0.45\n3\ntriple the average error\n\n\n98.00\n-0.6\n4\nquadruple\n\n\n97.85\n-0.75\n5\nquintuple\n\n\n97.40\n-1.2\n8\noctuple!\n\n\n\nWe know from discussing confidence intervals that we’d estimate the mean temperature to be within about 2SE of the sample mean, and from interval coverage that:\n\nan error less than 2SE occurs for about 95% of samples\nan error greater than 2SE occurs for only about 5% of samples\n\nExactly how often would we see the error we did if the population mean is in fact 98.6°F?"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model",
    "href": "content/week5-hypothesis.html#applying-the-t-model",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model."
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-1",
    "href": "content/week5-hypothesis.html#applying-the-t-model-1",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-2",
    "href": "content/week5-hypothesis.html#applying-the-t-model-2",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-3",
    "href": "content/week5-hypothesis.html#applying-the-t-model-3",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate more in 9.6% of samples\noverestimate more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-4",
    "href": "content/week5-hypothesis.html#applying-the-t-model-4",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate more in 9.6% of samples\noverestimate more in 9.6% of samples\n\n\n\n\n\n\n\n\n\n\n\n\\[P(|T| &gt; 1.328) = 0.192\\]\n\n\n\nWe’d see at least as much (absolute) estimation error 19.2% of the time, assuming the hypothesis is true. So this amount of error isn’t surprising."
  },
  {
    "objectID": "content/week5-hypothesis.html#a-more-extreme-scenario",
    "href": "content/week5-hypothesis.html#a-more-extreme-scenario",
    "title": "Introduction to hypothesis testing",
    "section": "A more extreme scenario",
    "text": "A more extreme scenario\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nsuppose instead \\(\\bar{x} = 98.2\\) so \\(T\\) = -2.726\nunderestimate more in 0.48% of samples\noverestimate more in 0.48% of samples\n\n\n\n\n\n\n\n\n\n\n\n\\[P(|T| &gt; 2.726) = 0.0096\\]\n\n\n\nWe’d see at least as much estimation error only 0.96% of the time. So if the hypothesis were true, this sample would be really unusual."
  },
  {
    "objectID": "content/week5-hypothesis.html#evaluating-the-hypothesis",
    "href": "content/week5-hypothesis.html#evaluating-the-hypothesis",
    "title": "Introduction to hypothesis testing",
    "section": "Evaluating the hypothesis",
    "text": "Evaluating the hypothesis\nTo evaluate the hypothesis that \\(\\mu = 98.6\\), we assume it is true and then consider whether the estimation error would be unusually large purely by chance according to the \\(t\\) model:\n\nunusually large error \\(\\longrightarrow\\) hypothesis is implausible \\(\\longrightarrow\\) can be rejected\nnot unusually large error \\(\\longrightarrow\\) hypothesis is plausible \\(\\longrightarrow\\) can’t be rejected\n\nWe just made these assessments:\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\nevaluation\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\nnot unusual\n\n\n98.2\n0.1467\n-2.726\n0.009632\nunusual\n\n\n\n\n\nSeems reasonable, but why exactly isn’t 19.2% of the time ‘unusual’?"
  },
  {
    "objectID": "content/week5-hypothesis.html#decisions-decisions",
    "href": "content/week5-hypothesis.html#decisions-decisions",
    "title": "Introduction to hypothesis testing",
    "section": "Decisions, decisions",
    "text": "Decisions, decisions\n\nWhat would happen if we decided that \\(T = -1.328\\) was unusual?\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\n\n\n98.2\n0.1467\n-2.726\n0.009632\n\n\n\n\n\nSuppose we drew a line at 20%. Then if in fact \\(\\mu = 98.6\\):\n\nerrors in the ‘reject’ regime occur by chance 20% of the time\nso we’ll reach the wrong conclusion for 1 in 5 samples\n\nThis error rate is too high."
  },
  {
    "objectID": "content/week5-hypothesis.html#formalizing-a-test-for-the-mean",
    "href": "content/week5-hypothesis.html#formalizing-a-test-for-the-mean",
    "title": "Introduction to hypothesis testing",
    "section": "Formalizing a test for the mean",
    "text": "Formalizing a test for the mean\nA statistical hypothesis is a statement about a population parameter. For every hypothesis there is an opposing or “alternative” hypothesis.\nA hypothesis test is a procedure for deciding between a hypothesis and its alternative.\n\n\nWe just tested the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = 98.6 \\quad(\\text{\"null\" hypothesis}) \\\\\nH_A: &\\mu \\neq 98.6 \\quad(\\text{\"alternative\" hypothesis})\n\\end{cases}\n\\]\nOur decision was based on the “test statistic”:\n\\[\nT = \\frac{\\bar{x} - 98.6}{SE(\\bar{x})}\n\\]\nIf \\(H_0\\) is true, the sampling distribution of \\(T\\) is well-approximated by a \\(t_{n-1}\\) model.\n\nWe reject \\(H_0\\) if it entails that the estimation error is unusually large relative to the standard error.\n\n‘unusual’ determined by considering error rate\ntwo equivalent approaches:\n\ncritical values\n\\(p\\)-values"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-critical-value-approach",
    "href": "content/week5-hypothesis.html#the-critical-value-approach",
    "title": "Introduction to hypothesis testing",
    "section": "The critical value approach",
    "text": "The critical value approach\n\nReject \\(H_0\\) if \\(|T|\\) exceeds the \\(1 - \\frac{\\alpha}{2}\\) quantile of the \\(t_{n - 1}\\) model\n\n\n\nSteps:\n\nDecide on an error tolerance \\(\\alpha\\).\nFind the \\(1 - \\frac{\\alpha}{2}\\) quantile \\(q\\).\nReject if \\(|T| &gt; q\\).\n\n\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# compute critical value for a 5% error tolerance\ncrit.val &lt;- qt(p = 0.975, df = 38)\ncrit.val\n\n[1] 2.024394\n\n# compare\nabs(tstat) &gt; crit.val \n\n[1] FALSE\n\n\n\n\nRationale: if \\(H_0\\) is true…\n\n\\(|T|\\) will be smaller than the quantile for \\((1 - \\alpha)\\times 100\\)% of samples\nso using this rule you’ll only make a mistake \\(\\alpha\\times 100\\)% of the time"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-p-value-approach",
    "href": "content/week5-hypothesis.html#the-p-value-approach",
    "title": "Introduction to hypothesis testing",
    "section": "The \\(p\\)-value approach",
    "text": "The \\(p\\)-value approach\n\nReject \\(H_0\\) if \\(T\\) exceeds the observed value for less than \\(\\alpha\\times 100\\)% of samples: \\[2\\times P(T &gt; |T_\\text{obs}|) &lt; \\alpha\\]\n\n\n\nSteps:\n\nDecide on an error tolerance \\(\\alpha\\).\nCompute the proportion \\(p\\) of samples for which \\(T\\) exceeds observed value.\nReject if \\(p &lt; \\alpha\\).\n\n\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# proportion of samples where T exceeds observed value\np.val &lt;- 2*pt(abs(tstat), df = 38, lower.tail = F)\np.val\n\n[1] 0.1920133\n\n# decision with error rate controlled at 5%\np.val &lt; 0.05\n\n[1] FALSE\n\n\n\n\nRationale:\n\n\\(p\\)-value conveys exactly how unusual the test statistic is\n\\(p &lt; \\alpha\\) exactly when \\(|T| &gt; q\\), so this rule controls the error rate at \\(\\alpha\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#test-outcomes",
    "href": "content/week5-hypothesis.html#test-outcomes",
    "title": "Introduction to hypothesis testing",
    "section": "Test outcomes",
    "text": "Test outcomes\nThere are two possible findings for a test:\n\n[crosses decision threshold] reject \\(H_0\\) in favor of \\(H_A\\)\n[doesn’t cross decision threshold] fail to reject \\(H_0\\) in favor of \\(H_A\\)\n\nA reject decision is interpreted as:\n\nThe data provide evidence that… [against \\(H_0\\)/favoring \\(H_A\\)]\n\nA fail to reject decision is interpreted as:\n\nThe data do not provide evidence that… [against \\(H_0\\)/favoring \\(H_A\\)]"
  },
  {
    "objectID": "content/week5-hypothesis.html#interpreting-results",
    "href": "content/week5-hypothesis.html#interpreting-results",
    "title": "Introduction to hypothesis testing",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\nCalculations in R:\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# compute critical value for a 5% error tolerance\ncrit.val &lt;- qt(p = 0.975, df = 38)\ncrit.val\n\n[1] 2.024394\n\n# test decision\nabs(tstat) &gt; crit.val\n\n[1] FALSE\n\n# p-value\n2*pt(abs(tstat), df = 38, lower.tail = F)\n\n[1] 0.1920133\n\n\n\nConventional narrative summary style:\n\nThe data do not provide evidence that the mean body temperature differs from 98.6°F (T = -1.328 on 38 degrees of freedom, p = 0.192).\n\nConveys a lot of info succinctly:\n\ntest conclusion\nhypotheses tested\nnumber of standard errors from hypothesized value (\\(T\\))\nsample size (degrees of freedom + 1)\nstrength of evidence (\\(p\\)-value)"
  },
  {
    "objectID": "content/week5-hypothesis.html#components-of-a-test",
    "href": "content/week5-hypothesis.html#components-of-a-test",
    "title": "Introduction to hypothesis testing",
    "section": "Components of a test",
    "text": "Components of a test\n\n\n\n\n\n\n\n\nComponent\nExplanation\nExample\n\n\n\n\nPopulation parameter\nThe quantity of interest\nMean body temp \\(\\mu\\)\n\n\nNull hypothesis\nThe claim to be tested\n\\(\\mu = 98.6\\)\n\n\nAlternative hypothesis\nThe alternative claim\n\\(\\mu \\neq 98.6\\)\n\n\nTest statistic\nA function of the sample data and the hypothetical parameter value\n\\(T = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}} = -1.328\\)\n\n\nModel\nSampling distribution of the test statistic under \\(H_0\\)\n\\(t_{38}\\) model\n\n\n\\(p\\)-value\nProbability under \\(H_0\\) of obtaining a result at least as favorable to \\(H_A\\)\n19.2% of samples produce a test statistic at least as large\n\n\nDecision\nReject or fail to reject \\(H_0\\) in favor of \\(H_A\\)\nFail to reject\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/test1-practice.html",
    "href": "content/test1-practice.html",
    "title": "Extra practice problems",
    "section": "",
    "text": "The test will consist of four questions with multiple parts, and one less structured question in which your task is to explore and summarize a dataset based on descriptive statistics. You’ll have 48 hours to work on the test, and will submit your answers via a fillable form exactly as in the homework assignment. You can use any class resources (notes, textbook, lecture slides, past homeworks) but are required to work alone. You’ll be provided with a Posit cloud project in which to carry out your analyses, and will be expected to upload your script as a supporting document when you fill out the submission form. The practice problems below provide a sample of questions that approximate the test problems (but are slightly shorter)."
  },
  {
    "objectID": "content/test1-practice.html#test-information",
    "href": "content/test1-practice.html#test-information",
    "title": "Extra practice problems",
    "section": "",
    "text": "The test will consist of four questions with multiple parts, and one less structured question in which your task is to explore and summarize a dataset based on descriptive statistics. You’ll have 48 hours to work on the test, and will submit your answers via a fillable form exactly as in the homework assignment. You can use any class resources (notes, textbook, lecture slides, past homeworks) but are required to work alone. You’ll be provided with a Posit cloud project in which to carry out your analyses, and will be expected to upload your script as a supporting document when you fill out the submission form. The practice problems below provide a sample of questions that approximate the test problems (but are slightly shorter)."
  },
  {
    "objectID": "content/test1-practice.html#practice-problems",
    "href": "content/test1-practice.html#practice-problems",
    "title": "Extra practice problems",
    "section": "Practice problems",
    "text": "Practice problems\n\nAnother version of the frog dataset from earlier also includes measurements of egg size and body size. Use this dataset to practice visualizing and describing distributions of numeric variables.\n\nMake histograms of each of the four numeric variables, with appropriate numbers of bins, and describe the shape and number of modes.\nFor each variable, suggest an appropriate measure of center and measure spread and identify any measures that would not be appropriate.\nMake pairwise scatterplots of each of the four numeric variables and describe the association, if any. (Hint: try pairs(frog) for a more efficient way to generate these plots.)\nFor linear associations in (c), compute and interpret the correlation.\n\n\n\n# load and inspect data\nload('data/frog.RData')\nhead(frog)\n\n  site altitude clutch.size clutch.volume egg.size body.size\n1  040    3,462    181.9701      177.8279 1.949845  3.630781\n2  040    3,462    269.1535      257.0396 1.949845  3.630781\n3  040    3,462    158.4893      151.3561 1.949845  3.715352\n4  040    3,462    234.4229      223.8721 1.949845  3.801894\n5  040    3,462    245.4709      234.4229 1.949845  3.890451\n6  040    3,462    301.9952      288.4032 1.949845  3.890451\n\n# part a: histograms of each numeric variable; describe shape and modes\npar(mfrow = c(2, 2), mar = c(4, 4, 4, 1))\nhist(frog$clutch.size)\nhist(frog$clutch.volume)\nhist(frog$egg.size)\nhist(frog$body.size)\n\n\n\n\n\n\n\n# part c: pairwise scatterplots of clutch volume, egg size, body size, clutch size\npairs(frog)\n\n\n\n\n\n\n\n# part d: correlations for linear associations\ncor(frog$clutch.size, frog$clutch.volume)\n\n[1] 0.8077344\n\ncor(frog$clutch.volume, frog$egg.size)\n\n[1] 0.6462605\n\ncor(frog$body.size, frog$clutch.volume, use = 'complete.obs')\n\n[1] 0.6755435\n\ncor(frog$body.size, frog$clutch.size, use = 'complete.obs')\n\n[1] 0.6147564\n\n\n\nThe chick data data come from a study investigating the early growth of chicks on different diets. In the study, 47 chicks were randomly assigned one of four diets at birth and researchers measured body weight in grams daily. The data below show body weights at 18 days since birth for each chick. The question of interest is: which diet is best?\n\nIs this observational or experimental data? Explain your reasoning.\nProduce a visualization that compares body weight distributions by diet. For which diet have chicks grown the most? The least? Explain the statistic(s) or features of the distribution you used to make this determination.\nBased on your plot in (b), suggest a measure of center and measure of spread that would be appropriate for summarizing the data.\nCalculate the measures you suggested in (c) separately for each diet group.\nAssume that in the previous question you found that chicks on diet 3 grew the most, regardless of your actual answer. Can you conclude that diet 3 caused the fastest growth? Explain why or why not.\n\n\n\n# load and inspect data\nload('data/chick.RData')\nhead(chick)\n\n# A tibble: 6 × 3\n  chick.id weight diet  \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n1        1    171 diet 1\n2        2    187 diet 1\n3        3    187 diet 1\n4        4    154 diet 1\n5        5    199 diet 1\n6        6    160 diet 1\n\n# part b: visualize body weights by diet\nboxplot(weight ~ diet, data = chick)\n\n\n\n\n\n\n\n# part c-d: determine and compute appropriate measures of spread and center\nchick |&gt;\n  group_by(diet) |&gt;\n  summarize(avg.weight = mean(weight),\n            sd.weight = sd(weight))\n\n# A tibble: 4 × 3\n  diet   avg.weight sd.weight\n  &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 diet 1       159.      49.2\n2 diet 2       188.      63.3\n3 diet 3       233.      57.6\n4 diet 4       203.      33.6\n\n\n\nThe gss dataset contains observations for 500 respondents in the General Social Survey on a small number of demographic categorical variables. Use this to practice tabular and graphical summaries for categorical variables.\n\nFor each variable, determine whether the variable is nominal or ordinal.\nMake a contingency table of age bracket and whether participants have obtained a college degree.\nVisualize the relationship between age and having obtained a college degree.\nDoes the proportion of respondents with a college degree differ by sex?\nBy political party?\nBy socioeconomic class?\nMake one additional comparison of your choice and interpret the result.\n\n\n\n# load and inspect data\nload('data/gss.RData')\nhead(gss)\n\n# A tibble: 6 × 5\n  age     sex    college.degree political.party class        \n  &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;          &lt;fct&gt;           &lt;fct&gt;        \n1 (29,38] male   degree         ind             middle class \n2 (29,38] female no degree      rep             working class\n3 [18,29] male   degree         ind             working class\n4 (38,50] male   no degree      ind             working class\n5 (29,38] male   degree         rep             middle class \n6 (29,38] female no degree      rep             middle class \n\n# part b: contingency table of age and college degree\ntable(gss$college.degree, gss$age)\n\n           \n            [18,29] (29,38] (38,50] (50,87]\n  degree         36      44      60      34\n  no degree      99      74      64      89\n\n# part c: visualize relationship between age and college degree\ntable(gss$college.degree, gss$age) |&gt; \n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part d: does the proportion of respondents with a degree differ by sex?\ntable(gss$college.degree, gss$sex) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part e: by political party?\ntable(gss$college.degree, gss$political.party) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part f: by class?\ntable(gss$college.degree, gss$class) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part g: one additional comparison of your choosing\n\n\nLong COVID is a multi-systemic and often debilitating condition that develops in at least 10% of patients following a COVID infection. The following is an excerpt of the abstract from a recent study seeking to identify symptoms and risk factors associated with long COVID and published in Nature Medicine1: “We undertook a … study using a UK-based primary care database, Clinical Practice Research Datalink Aurum, to determine symptoms that are associated with confirmed SARS-CoV-2 infection beyond 12 weeks in non-hospitalized adults and the risk factors associated with developing persistent symptoms. We selected 486,149 adults with confirmed SARS-CoV-2 infection … Outcomes included 115 individual symptoms, as well as long COVID, defined as a composite outcome of 33 symptoms by the World Health Organization clinical case definition … Among the patients infected with SARS-CoV-2, risk factors for long COVID included female sex, belonging to an ethnic minority, socioeconomic deprivation, smoking, obesity and a wide range of comorbidities. The risk of developing long COVID was also found to be increased along a gradient of decreasing age.”\n\nIdentify the type of study.\nIdentify the study population.\nDescribe the sample.\nList the study outcomes of interest.\nIdentify any non-outcome variables."
  },
  {
    "objectID": "content/test1-practice.html#footnotes",
    "href": "content/test1-practice.html#footnotes",
    "title": "Extra practice problems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSubramanian et al. (2022). Symptoms and risk factors for long COVID in non-hospitalized adults. Nature medicine, 28(8), 1706-1714.↩︎"
  },
  {
    "objectID": "content/week8-posthoc.html#todays-agenda",
    "href": "content/week8-posthoc.html#todays-agenda",
    "title": "Post-hoc inference in ANOVA",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] inference on group means and contrasts after performing ANOVA\n[lab] estimates, tests, and intervals using emmeans"
  },
  {
    "objectID": "content/week8-posthoc.html#from-before-diet-restriction",
    "href": "content/week8-posthoc.html#from-before-diet-restriction",
    "title": "Post-hoc inference in ANOVA",
    "section": "From before: diet restriction",
    "text": "From before: diet restriction\n\n\nData summaries:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nsd\nn\n\n\n\n\nNP\n27.4\n6.134\n49\n\n\nN/N85\n32.69\n5.125\n57\n\n\nN/R50\n42.3\n7.768\n71\n\n\nN/R40\n45.12\n6.703\n60\n\n\n\n\n\n\nInference using ANOVA:\n\\[\n\\begin{align*}\n&H_0: \\mu_\\text{NP} = \\mu_\\text{N/N85} = \\mu_\\text{N/R50} = \\mu_\\text{N/R40} \\\\\n&H_A: \\text{at least two means differ}\n\\end{align*}\n\\]\n\nfit &lt;- aov(lifetime ~ diet, data = longevity)\nsummary(fit)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndiet          3  11426    3809   87.41 &lt;2e-16 ***\nResiduals   233  10152      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe data provide strong evidence that diet restriction has an effect on mean lifetime among mice (F = 87.41 on 3 and 233 degrees of freedom, p &lt; 0.0001)."
  },
  {
    "objectID": "content/week8-posthoc.html#follow-up-questions",
    "href": "content/week8-posthoc.html#follow-up-questions",
    "title": "Post-hoc inference in ANOVA",
    "section": "Follow-up questions",
    "text": "Follow-up questions\nThe ANOVA tells us there’s evidence of an effect of diet restriction on lifespan.\nSo now we’d want to know:\n\nWhat are the mean lifespans for each level of restriction?\nFor which levels of dietary restriction do mean lifespans differ?\nWhat are the gains in mean lifespan for each level of restriction relative to an unrestricted diet?\nFor which levels of restriction are gains in mean lifespan significant?\n\nAnswers require post-hoc inferences (done after-the-fact) on:\n\ngroup means \\(\\mu_i\\) (question 1)\n“contrasts” \\(\\mu_i - \\mu_j\\) (questions 2-4)"
  },
  {
    "objectID": "content/week8-posthoc.html#estimates-for-group-means",
    "href": "content/week8-posthoc.html#estimates-for-group-means",
    "title": "Post-hoc inference in ANOVA",
    "section": "Estimates for group means",
    "text": "Estimates for group means\n\nInterval estimates for group means in ANOVA use a model-based standard error.\n\n\n\n\nemmeans(object = fit, spec = ~ diet) |&gt; \n  confint(level = 0.95)\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nestimate\nSE\n95% CI\n\n\n\n\nNP\n27.4\n0.943\n(25.54, 29.26)\n\n\nN/N85\n32.69\n0.8743\n(30.97, 34.41)\n\n\nN/R50\n42.3\n0.7834\n(40.75, 43.84)\n\n\nN/R40\n45.12\n0.8522\n(43.44, 46.8)\n\n\n\n\n\n\nInterval estimates use a “pooled” standard deviation:\n\\[SE_i = \\frac{s_\\text{pooled}}{\\sqrt{n_i}} = \\frac{\\sqrt{MSE}}{\\sqrt{n_i}}\\]\nOtherwise identical to \\(t_{n - k}\\) confidence intervals.\n\n\nRationale:\n\nthe ANOVA model assumes equal variability (standard deviations) across groups\nbetter precision (for variability estimates, not means) when assumption holds"
  },
  {
    "objectID": "content/week8-posthoc.html#bonferroni-adjustment",
    "href": "content/week8-posthoc.html#bonferroni-adjustment",
    "title": "Post-hoc inference in ANOVA",
    "section": "Bonferroni adjustment",
    "text": "Bonferroni adjustment\n\nProblem: several 95% intervals don’t have simultaneous 95% coverage.\n\n\nindividual coverage: how often one interval covers the population mean\nsimultaneous coverage: how often all intervals cover population means at the same time\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bonferroni correction for \\(k\\) intervals consists in changing the individual coverage level to \\(\\left(1 - \\frac{\\alpha}{k}\\right)\\%\\).\n\nEffectively a width increase\nGuarantees joint coverage \\((1 - \\alpha)\\%\\)\nTends to be over conservative with many means (large \\(k\\))"
  },
  {
    "objectID": "content/week8-posthoc.html#implementation-in-r",
    "href": "content/week8-posthoc.html#implementation-in-r",
    "title": "Post-hoc inference in ANOVA",
    "section": "Implementation in R",
    "text": "Implementation in R\n\n\n\n# table\nemmeans(object = fit, spec = ~ diet) |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.0     29.8\n N/N85   32.7 0.874 233     30.5     34.9\n N/R50   42.3 0.783 233     40.3     44.3\n N/R40   45.1 0.852 233     43.0     47.3\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \n\n\n\nnote Bonferroni adjustment\nthese are model-based estimates that depend on the fitted ANOVA model\n\n\n\n# visualization\nemmeans(object = fit, spec = ~ diet) |&gt;\n  confint(level = 0.95, adjust = 'bonferroni') |&gt;\n  plot(xlab = 'mean lifespan (months)', \n       ylab = 'diet')\n\n\n\n\n\n\n\n\n\n\n\n\nCaution: these intervals do NOT indicate which means differ significantly."
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons",
    "href": "content/week8-posthoc.html#pairwise-comparisons",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons",
    "text": "Pairwise comparisons\n\nPairwise comparisons are inferences made on “contrasts” between pairs of means.\n\nThe difference \\(\\mu_{NP} - \\mu_{N/N85}\\) is an example of a contrast. It is common to perform inference on all pairwise contrasts to determine which means differ and by how much.\n\n\n\nemmeans(object = fit, specs = ~ diet) |&gt; \n  contrast('pairwise')\n\n\n\n\n\n\n\n\n\n\n\ndifference\nestimate\nSE\n\n\n\n\nNP - (N/N85)\n-5.289\n1.286\n\n\nNP - (N/R50)\n-14.9\n1.226\n\n\nNP - (N/R40)\n-17.71\n1.271\n\n\n(N/N85) - (N/R50)\n-9.606\n1.174\n\n\n(N/N85) - (N/R40)\n-12.43\n1.221\n\n\n(N/R50) - (N/R40)\n-2.819\n1.158\n\n\n\n\n\n\n\nestimates are \\(\\bar{x}_i - \\bar{x}_j\\)\n\\(SE_{ij} = SE(\\bar{x}_i - \\bar{x}_j) = s_\\text{pooled}\\sqrt{\\frac{1}{n_i} + \\frac{1}{n_j}}\\)\ninference based on a \\(t_{n - k}\\) model\n\ndegrees of freedom: \\(n - k\\)\ntests: \\(T_{ij} = \\frac{\\bar{x}_i - \\bar{x}_j}{SE_{ij}}\\)\nintervals: \\(\\bar{x}_i - \\bar{x}_j \\pm c\\times SE_{ij}\\)\n\n\n\n\nMultiplicity corrections must adjust for the number of contrasts (6), not means (4)."
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-tests",
    "href": "content/week8-posthoc.html#pairwise-comparisons-tests",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: tests",
    "text": "Pairwise comparisons: tests\n\nWhich means differ significantly? All except R50 and R40.\n\n\n\nemmeans then contrast then test:\n\nemmeans(object = fit, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  test(adjust = 'bonferroni')\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n\n\n\np-values are adjusted for multiplicity\nreject if adjusted p-value is below the significance threshold\n\n\nHypotheses for pairwise tests:\n\\[\\begin{cases} H_0: \\mu_i - \\mu_j = 0 \\\\ H_A: \\mu_i - \\mu_j \\neq 0 \\end{cases}\\] Test statistic:\n\\[T_{ij} = \\frac{\\bar{x}_i - \\bar{x}_j}{SE_{ij}}\\]\n\\(p\\)-values are obtained from a \\(t_{n - k}\\) model for the sampling distribution of \\(T_{ij}\\)."
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-tests-1",
    "href": "content/week8-posthoc.html#pairwise-comparisons-tests-1",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: tests",
    "text": "Pairwise comparisons: tests\n\nWhich means differ significantly? All except R50 and R40.\n\n\n\nemmeans then contrast then test:\n\nemmeans(object = fit, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  test(adjust = 'bonferroni')\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n\n\n\nInterpretation:\n\nThe data provide evidence at the 5% significance level that mean lifespan differs among all levels of diet restriction except the N/R40 and N/R50 groups (p = 0.0937), for which the evidence is suggestive but inconclusive."
  },
  {
    "objectID": "content/week8-posthoc.html#multiple-testing-correction-matters",
    "href": "content/week8-posthoc.html#multiple-testing-correction-matters",
    "title": "Post-hoc inference in ANOVA",
    "section": "Multiple testing correction matters",
    "text": "Multiple testing correction matters\n\nUsing unadjusted \\(p\\)-values will inflate type I error rates.\n\n\n\nsetting adjust = 'none':\n\nemmeans(object = fit, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  test(adjust = 'none')\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0001\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0156\n\n\n\nFailure to adjust for multiple inferences leads to a different conclusion:\n\nThe data provide evidence at the 5% significance levelthat mean lifespan differs among all levels of diet restriction.\n\n\n\nThis is incorrect, because the joint significance level is not 5%.\nWithout adjustment, type I error could be as high as \\(k\\times\\alpha = 6\\times 0.05 = 0.3\\)!"
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-intervals",
    "href": "content/week8-posthoc.html#pairwise-comparisons-intervals",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: intervals",
    "text": "Pairwise comparisons: intervals\n\nHow much do means differ? Anywhere from 2 - 21 months, depending.\n\n\n\nemmeans then contrast then confint:\n\nemmeans(object = fit, specs = ~ diet) |&gt; \n  contrast('pairwise') |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n\n\n\nlevel specifies joint coverage after adjustment\n\n\nIntervals are for the parameter \\(\\mu_i - \\mu_j\\):\n\\[\\bar{x}_i - \\bar{x}_j \\pm c\\times SE_{ij}\\]\nThe critical value \\(c\\) is obtained from the \\(t_{n - k}\\) model.\nFor a \\((1 - \\alpha)\\times 100\\%\\) confidence interval with Bonferroni correction:\n\\[c = \\left(1 - \\frac{\\alpha}{2k}\\right) \\;\\text{quantile}\\]"
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-intervals-1",
    "href": "content/week8-posthoc.html#pairwise-comparisons-intervals-1",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: intervals",
    "text": "Pairwise comparisons: intervals\n\nHow much do means differ? Anywhere from 2 - 21 months, depending.\n\n\n\nemmeans then contrast then confint:\n\nemmeans(object = fit, specs = ~ diet) |&gt; \n  contrast('pairwise') |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n\n\n\nInterpretations are the same as usual:\n\nWith 95% confidence, mean lifespan on a normal diet is estimated to exceed mean lifespan on an unrestricted diet by between 1.87 and 8.71 months, with a point estimate of 5.29 months difference (SE 1.29)."
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-visualizations",
    "href": "content/week8-posthoc.html#pairwise-comparisons-visualizations",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: visualizations",
    "text": "Pairwise comparisons: visualizations\n\n\nAnother option is to visualize the pairwise comparison inferences by displaying simultaneous 95% intervals.\nEasy to spot significant contrasts:\n\nintervals exclude 0 \\(\\Leftrightarrow\\) tests reject\n\n\nemmeans then contrast then confint then plot:\n\nemmeans(object = fit, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  confint(level = 0.95, adjust = 'bonferroni') |&gt;\n  plot(xlab = 'difference in mean lifetime (months)', \n       ylab = 'contrast')"
  },
  {
    "objectID": "content/week8-posthoc.html#comparisons-with-a-control",
    "href": "content/week8-posthoc.html#comparisons-with-a-control",
    "title": "Post-hoc inference in ANOVA",
    "section": "Comparisons with a control",
    "text": "Comparisons with a control\n\nDoes diet restriction increase mean lifespan, and if so by how much?\n\n\n\nSpecify contrast('trt.vs.ctrl'):\n\nemmeans(object = fit, specs = ~ diet) |&gt; \n  contrast('trt.vs.ctrl') |&gt;\n  confint(level = 0.95, adjust = 'dunnett')\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\n95% CI\n\n\n\n\n(N/N85) - NP\n5.289\n1.286\n(2.23, 8.34)\n\n\n(N/R50) - NP\n14.9\n1.226\n(11.98, 17.81)\n\n\n(N/R40) - NP\n17.71\n1.271\n(14.7, 20.73)\n\n\n\n\n\n\n\nMultiple inference adjustment uses Dunnett’s method\n\nspecialized correction for comparisons with a control\nadjust = 'dunnett'\n\nComparisons will be relative to first group in R\n\n\n\nWith 95% confidence, relative to an unrestricted diet…\n\na 85kcal/day diet increases lifespan by an estimated 2.23 to 8.34 months\na 50kcal/day diet increases lifespan by an estimated 11.98 to 17.81 months\na 40kcal/day diet increases lifespan by an estimated 14.70 to 20.73 months"
  },
  {
    "objectID": "content/week8-posthoc.html#log-contrasts-relative-change",
    "href": "content/week8-posthoc.html#log-contrasts-relative-change",
    "title": "Post-hoc inference in ANOVA",
    "section": "Log contrasts: relative change",
    "text": "Log contrasts: relative change\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%.\n\nExtra credit: work out and interpret the interval estimate for the contrast not shown above."
  },
  {
    "objectID": "content/week8-posthoc.html#power-calculations-for-anova",
    "href": "content/week8-posthoc.html#power-calculations-for-anova",
    "title": "Post-hoc inference in ANOVA",
    "section": "Power calculations for ANOVA",
    "text": "Power calculations for ANOVA\n\nHow much data should we collect to detect a difference in mean lifespan of 1 month?\n\n\n\nTo perform sample size power calculations, one needs:\n\nnumber of groups\ntarget significance level (\\(\\alpha\\))\ntarget power level\nguess or prior estimate of variance ratio \\(\\frac{\\text{group variation}}{\\text{error variation}}\\)\n\nFrom the existing study, \\(\\sqrt{MSE} = 6.6\\)\n\nSo to detect effects on the order of one month at 90% power:\n\npower.anova.test(groups = 4, \n                 sig.level = 0.05, \n                 power = 0.9, \n                 within.var = 6.633, \n                 between.var = 1)\n\n\n     Balanced one-way analysis of variance power calculation \n\n         groups = 4\n              n = 32.32851\n    between.var = 1\n     within.var = 6.633\n      sig.level = 0.05\n          power = 0.9\n\nNOTE: n is number in each group\n\n\n… we need 33 mice per treatment group.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week1-activity-studydesigns.html",
    "href": "content/week1-activity-studydesigns.html",
    "title": "Distinguishing study designs",
    "section": "",
    "text": "Recall that the difference between an observational study and an experiment hinges on whether researchers intentionally intervene on the system of study (experiment) or passively record outcomes (observational study).\nIn this activity you’ll read abstracts from a few published studies and determine what kind of study is described in the abstract. You do not need to consider the examples in order — start with the ones that look most interesting.\nFor each example, identify the following:\n\nstudy type\nstudy population\nsample characteristics\nstudy outcome(s)\n\n\nExample 1: selenium exposure and Mediterranean diet\nThe following is from the abstract of a study investigating dietary mitigation of selenium exposure:\n\nSelenium is a trace element found in many chemical forms. Selenium and its species have nutritional and toxicologic properties, some of which may play a role in the etiology of neurological disease. We hypothesized that adherence to the Mediterranean-Dietary Approach to Stop Hypertension Intervention for Neurodegenerative Delay (MIND) diet could influence intake and endogenous concentrations of selenium and selenium species, thus contributing to the beneficial effects of this dietary pattern. We carried out a cross-sectional study of 137 non-smoking blood donors (75 females and 62 males) from the Reggio Emilia province, Northern Italy. We assessed MIND diet adherence using a semiquantitative food frequency questionnaire. We assessed selenium exposure through dietary intake and measurement of urinary and serum concentrations, including speciation of selenium compound in serum … Adherence to the MIND diet was positively associated with dietary selenium intake and urinary selenium excretion, whereas it was inversely associated with serum concentrations of overall selenium and organic selenium … Our results suggest that greater adherence to the MIND diet is non-linearly associated with lower circulating concentrations of selenium and of 2 potentially neurotoxic species of this element, selenoprotein P and selenate. This may explain why adherence to the MIND dietary pattern may reduce cognitive decline.\n\nUrbano, T., et al. (2023). Adherence to the Mediterranean-DASH Intervention for Neurodegenerative Delay (MIND) diet and exposure to selenium species: A cross-sectional study. Nutrition Research.\n\n\nExample 2: fermented kimchi and glucose metabolism\nThe following is from an abstract of a study investigating possible benefits of kimchi consumption among prediabetic individuals:\n\nWith the increased incidence of diabetes mellitus, the importance of early intervention in prediabetes has been emphasized … We hypothesized that kimchi and its fermented form would have beneficial effects on glucose metabolism in patients with prediabetes. A total of 21 participants with prediabetes were enrolled. During the first 8 weeks, they consumed either fresh (1-day-old) or fermented (10-day-old) kimchi. After a 4-week washout period, they switched to the other type of kimchi for the next 8 weeks. Consumption of both types of kimchi significantly decreased body weight, body mass index, and waist circumference. Fermented kimchi decreased insulin resistance, and increased insulin sensitivity … Systolic and diastolic blood pressure (BP) decreased significantly in the fermented kimchi group. The percentages of participants who showed improved glucose tolerance were 9.5 and 33.3% in the fresh and fermented kimchi groups, respectively.\n\nAn, S. Y., et al. (2013). Beneficial effects of fresh and fermented kimchi in prediabetic individuals. Annals of Nutrition and Metabolism, 63(1-2), 111-119."
  }
]