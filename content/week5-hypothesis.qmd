---
title: "Hypothesis testing"
subtitle: "Directional and two-sided inference for a population mean"
format: 
  revealjs:
    logo: img/poly-logo-2.jpg
    footer: "STAT218"
    smaller: true
    mermaid:
      theme: neutral
execute: 
  echo: false
  warning: false
  message: false
---

## Today's agenda

```{r}
library(tidyverse)
library(oibiostat)
library(pander)
ddt <- MASS::DDT
```

1. Reading quiz \[[2pm section](https://forms.office.com/r/kR8D8tzbzk)\] \[[4pm section](https://forms.office.com/r/CNzztxACjs)\]
2. Hypothesis tests for a population mean
3. Lab: $t$-tests in R
4. (If time) Exploring decision errors

## DDT data

The following are 15 measurements of the pesticide DDT in kale in parts per million (ppm). Each measurement was taken by a different laboratory.

```{r}
pander(ddt)
```

> C. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.

Imagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.

## Hypothesis testing

This is an example of a *hypothesis testing* problem: we want to test the hypothesis that mean DDT in kale is within safe limits. Hypothesis testing is another form of statistical inference.

The general pattern for performing a hypothesis test is:

1.  Formulate the hypothesis to test in terms of the values of a population parameter.
2.  Assess the likelihood of the data under the hypothesis through use of a "test statistic".
3.  Conclude whether the data provide evidence favoring an alternative.

Today we'll cover each step in turn in the context of tests for a population mean.

## 1. Formulating hypotheses

Hypotheses cannot be tested in isolation, but must be considered relative to a specified alternative.

To articulate the hypotheses for a test, we need:

-   population parameter of interest
-   **null hypothesis** $H_0$: possible value(s) under the claim to be tested
-   **alternative hypothesis** $H_A$: possible value(s) if the claim is found to be false

In the context of the DDT example...

$$H_0:\hspace{15cm}$$ $$H_A:\hspace{15cm}$$

## 2. Test statistic

Test statistics are data summaries that:

-   depend on the null value of the population parameter
-   have a known sampling distribution

For a population mean, we use: $$T = \frac{\bar{x} - \mu_0}{s_x/\sqrt{n}}$$

This is well-described by a $t_{n - 1}$ model when $\mu = \mu_0$. It is useful for the test because:

-   large (absolute) values of $T$ are unlikely if $\mu = \mu_0$
-   small (absolute) values of $T$ are expected if $\mu = \mu_0$

## 3. Drawing a conclusion

::: columns
::: {.column width="60%"}
In the DDT example, $T$ = `r t.test(ddt, mu = 3)$stat |> round(3)`. This favors $H_A$, but by how much?

According to the $t$ model, less than 1% of samples would produce a result *more* favorable to $H_A$.

```{r, fig.width = 8, fig.height = 5}
par(mar = c(5, 4, 1, 1),
    cex = 1.5)
x <- seq(-4, 4, length = 10000)
y <- dt(x, df = 14)
tstat <- t.test(ddt, mu = 3)$stat
plot(x, y, type = 'l',
     ylab = 'frequency', 
     xlab = '')
title(xlab = expression(paste("T = ", frac(bar(x) - mu[0], s[x]/sqrt(n)))),
      line = 4)
polygon(c(min(x), x[x<=tstat], tstat), c(y[x<=tstat], 0, 0), col="red")
polygon(c(x[x>=tstat], max(x), tstat), c(y[x>=tstat], 0, 0))
abline(v = tstat)
legend(x = 'topright', 
       legend = c('99.425% of samples', '0.575% of samples'), 
       fill = c('red', 'white'))
```
:::

::: {.column width="40%"}
Point estimate:

```{r}
c(mean = mean(ddt), se = sd(ddt)/sqrt(length(ddt))) %>%
  pander()
```

Test statistic:

$$T = \frac{\bar{x} - \mu_0}{SE(\bar{x})} = $$

> This is strong evidence *against* the claim that the DDT level is 3ppm or less and *in favor of* the claim that the DDT level exceeds 3ppm.
:::
:::

## Recap of DDT example

> Is the mean DDT level in kale 3ppm or less?

::: {.columns}

::: {.column width="55%"}
Data are measurements of DDT levels in ppm from 15 labs. 

Population parameter:

$$H_0:\hspace{15cm}$$ 
$$H_A:\hspace{15cm}$$ 
$$\mu_0 =\hspace{15cm}$$ 
$$T=\hspace{15cm}$$ 
% of samples more favorable to $H_A \approx$ 
:::

::: {.column width="45%"}
Summary statistics:
```{r}
c(mean = mean(ddt),
  sd = sd(ddt),
  se = sd(ddt)/sqrt(length(ddt))) %>%
  pander()
```
$t_{14}$ model:
```{r, fig.width = 4, fig.height = 3}
par(mar = c(5, 4, 0.5, 0.5),
    cex = 1)
curve(dt(x, df = 14), 
      from = -4, to = 4,
      xlab = '',
      ylab = 'frequency')
abline(h = 0)
title(xlab = expression(paste("T = ", frac(bar(x) - mu[0], s[x]/sqrt(n)))),
      line = 3.5)
```

:::

:::


## Another example: sleep

> Does the average U.S. adult sleep at least 7 hours per night?

::: {.columns}

::: {.column width="55%"}
Data are reported average hours of sleep per night from 135 NHANES respondents. 

Population parameter:

$$H_0:\hspace{15cm}$$ 
$$H_A:\hspace{15cm}$$ 
$$\mu_0 =\hspace{15cm}$$ 
$$T=\hspace{15cm}$$ 
% of samples more favorable to $H_A \approx$ 
:::

::: {.column width="45%"}
Summary statistics:
```{r}
data("nhanes.samp.adult")
sleep <- nhanes.samp.adult$SleepHrsNight
c(mean = mean(sleep),
  sd = sd(sleep),
  se = sd(sleep)/sqrt(length(sleep))) %>%
  pander()
```
$t_{134}$ model:
```{r, fig.width = 4, fig.height = 3}
par(mar = c(5, 4, 0.5, 0.5),
    cex = 1)
curve(dt(x, df = 134), 
      from = -4, to = 4,
      xlab = '',
      ylab = 'frequency')
abline(h = 0)
title(xlab = expression(paste("T = ", frac(bar(x) - mu[0], s[x]/sqrt(n)))),
      line = 3.5)
```

:::

:::


## Your turn: body temperatures


> Is mean body temperature actually 98.6 °F, or is it lower?

::: {.columns}

::: {.column width="55%"}
Data are 130 observations of body temperature (°F) [derived from a JAMA study](https://jse.amstat.org/v4n2/datasets.shoemaker.html). 

Population parameter:

$$H_0:\hspace{15cm}$$ 
$$H_A:\hspace{15cm}$$ 
$$\mu_0 =\hspace{15cm}$$ 
$$T=\hspace{15cm}$$ 
% of samples more favorable to $H_A \approx$ 
:::

::: {.column width="45%"}
Summary statistics:
```{r}
data(thermometry)
body.temps <- thermometry$body.temp
c(mean = mean(body.temps),
  sd = sd(body.temps),
  se = sd(body.temps)/sqrt(length(body.temps))) %>%
  pander()
```
$t_{129}$ model:
```{r, fig.width = 4, fig.height = 3}
par(mar = c(5, 4, 0.5, 0.5),
    cex = 1)
curve(dt(x, df = 129), 
      from = -6, to = 4,
      xlab = '',
      ylab = 'frequency')
abline(h = 0)
title(xlab = expression(paste("T = ", frac(bar(x) - mu[0], s[x]/sqrt(n)))),
      line = 3.5)
```

:::

:::


## Strength of evidence

The result that 0.575% of samples would produce a test statistic more strongly favoring the alternative hypothesis is an example of a ***p*****-value**:

> the probability under $H_0$ of obtaining a sample for which the test statistic is at least as favorable to $H_A$ as the value actually observed

In other words, $p$-values assume the null hypothesis is true, and then ask, "what is the chance I'd obtain data at least as suggestive as what I have that the alternative is more likely than the null?"

-   smaller $p$-values: if $H_0$ is true, equally or more favorable results are not expected often by chance
-   larger $p$-values: if $H_0$ is true, equally or more favorable results are expected often by chance


## Evidence thresholds

It remains to define an *evidence threshold* above which we decide to reject $H_0$.

A heuristic is to fix a **significance level** $\alpha$ and reject $H_0$ whenever $p < \alpha$.

-   represents an evidence threshold
-   conventionally, $\alpha = 0.05$
-   controls error rates

Imagine that indeed mean DDT in kale is 3ppm. Then 0.575% of samples produce test statistics at least as favorable to the alternative as what we saw in the study.

-   so if we set the evidence threshold for rejecting $H_0$ exactly here ($\alpha = 0.00575$) we'll be wrong 0.575% of the time
-   if we set the evidence threshold lower (say $\alpha = 0.01$) we'll be wrong more than 0.575% of the time (in fact 1% of the time)

## Interpreting results

Because of the anatomy of hypothesis tests, there are two possible findings:

-   \[above evidence threshold\] reject $H_0$ in favor of $H_A$
-   \[below evidence threshold\] fail to reject $H_0$

> Since the null is assumed to be true to perform the test, the test can only result in (a) evidence against this assumption or (b) no evidence against this assumption. But because it's an assumption, we don't affirm it if the test fails.

In the DDT example: *the data provide sufficiently strong evidence (p = 0.00575) to reject the hypothesis that mean DDT in kale is at most 3ppm in favor of the hypothesis that mean DDT in kale exceeds 3ppm*.

<!-- ## Decision errors -->

<!-- There are two ways to make mistakes, and two ways to get the answer right. -->

<!-- ![](img/testing-errors.png){fig-align="center" width="200"} -->

<!-- You can't optimize both error rates simultaneously. -->

<!-- -   The test heuristics we've outlined control type I error rates at $\alpha$ -->

<!-- -   These tests have the lowest type II error rates *subject to* a limit on type I error -->

## Composite hypotheses

The null hypothesis is a *composite* of values, so why did we choose just one ($\mu_0 = 3$) to perform the test?

Any null value farther from the alternative will produce stronger results.

| Null value     | Test statistic numerator | Proportion of samples more favorable to $H_A$                           |
|------------------------|------------------------|------------------------|
| $\mu_0 = 3$    | `r mean(ddt) - 3`        | `r t.test(ddt, mu = 3, alternative = 'greater')$p.value |> round(5)`    |
| $\mu_0 = 2.99$ | `r mean(ddt) - 2.99`     | `r t.test(ddt, mu = 2.99, alternative = 'greater')$p.value |> round(5)` |
| $\mu_0 = 2.95$ | `r mean(ddt) - 2.95`     | `r t.test(ddt, mu = 2.95, alternative = 'greater')$p.value |> round(5)` |

So by using $\mu_0 = 3$, we are choosing the most conservative null value for the test.

## Components of a test

| Component              | Explanation                                                                  | DDT example                                       |
|------------------------|------------------------|------------------------|
| Population parameter   | The quantity of interest                                                     | Mean DDT $\mu$                                    |
| Null hypothesis        | The claim to be tested                                                       | $\mu \leq 3$                                      |
| Alternative hypothesis | The alternative claim                                                        | $\mu > 3$                                         |
| Test statistic         | A function of the sample data and the null value of the population parameter | $T = \frac{\bar{x} - \mu_0}{s_x/\sqrt{n}} = 2.91$ |
| Model                  | Sampling distribution of the test statistic under $H_0$                      | $t_{df = 14}$ model                               |
| $p$-value              | Probability under $H_0$ of obtaining a result at least as favorable to $H_A$ | 0.575% of samples are more favorable to $H_A$     |
| Decision               | Reject or fail to reject $H_0$                                               | Reject at $\alpha = 0.05$                         |

## Performing tests in R


::: {.columns}

::: {.column}

Inputs:

1. data vector
2. null value of parameter
3. alternative hypothesis

Outputs:

4. test statistic
5. degrees of freedom for $t$ model
6. $p$-value
7. confidence interval
8. point estimate

:::

::: {.column}
`t.test` performs all calculations. Locate each input (1-3) and output (4-7) below:
```{r, echo = T}
t.test(ddt, mu = 3, alternative = 'greater')
```

:::

:::


## Directional hypotheses

Tests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.

| Test type   | Null             | Alternative      | Favors alternative |
|-------------|------------------|------------------|--------------------|
| Upper-sided | $\mu \leq \mu_0$ | $\mu > \mu_0$    | positive $T$       |
| Lower-sided | $\mu \geq \mu_0$ | $\mu < \mu_0$    | negative $T$       |
| Two-sided   | $\mu = \mu_0$    | $\mu \neq \mu_0$ | large $|T|$        |

The direction of the test affects the $p$-value calculation (and thus decision), but *won't* change the test statistic.

```{r, echo = T, eval = F}
# upper-sided
t.test(ddt, mu = mu_0, alternative = 'greater')

# lower-sided
t.test(ddt, mu = mu_0, alternative = 'less')

# two-sided (default)
t.test(ddt, mu = mu_0, alternative = 'two.sided')
```

## Lab: $t$-tests in R

Open up `lab6-hypotesting` in the class workspace. The goals for this lab are:

1. Learn how to implement $t$ tests in R and interpret output
2. Practice formulating and testing hypotheses from simple research questions
3. (If time) Explore decision errors