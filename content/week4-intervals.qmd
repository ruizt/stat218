---
title: "Confidence intervals"
subtitle: "Constructing interval estimates to attain a specified coverage rate"
format: 
  revealjs:
    logo: img/poly-logo-2.jpg
    footer: "STAT218"
    smaller: true
    mermaid:
      theme: neutral
execute: 
  echo: false
  warning: false
  message: false
---

```{r}
library(tidyverse)
library(oibiostat)
library(pander)
library(RColorBrewer)
reds <- brewer.pal(6, 'Reds')
load('data/nhanes.RData')
cholesterol <- nhanes$totchol
red.grad <- colorRampPalette(c('#ff6459', '#4d0500'))
reds <- red.grad(6)

```

## Today's agenda

- [lecture] $t$ confidence intervals for the mean
- [lab] computing and interpreting confidence intervals

## From last time

::: {.columns}

::: {.column}
Under simple random sampling:

- the sample mean $\bar{x}$ provides a good point estimate of the population mean $\mu$
- its estimated sampling variability is given by the standard error $SE(\bar{x}) = \frac{s_x}{\sqrt{n}} = \frac{\text{sample SD}}{\sqrt{\text{sample size}}}$
<!-- - an interval estimate (range of plausible values) is $\bar{x} \pm 2\times SE(\bar{x})$ -->
::: 

::: {.column}
```{r}
c(mean =  mean(cholesterol),
  sd = sd(cholesterol),
  n = length(cholesterol),
  se = sd(cholesterol)/sqrt(length(cholesterol))) |>
  pander()
mle <- MASS::fitdistr(cholesterol, 'gamma')$est |> round(1)

par(mfrow = c(1, 2),
    mar = c(4, 4, 4, 1),
    cex = 2)
curve(dgamma(x, shape = mle[1], rate = mle[2]), 
      from = 2, to = 14,
      xlab = 'total cholesterol',
      ylab = 'density', axes = F,
      ylim = c(0, 0.45),
      main = 'population model',
      col = reds[3])
axis(side = 1, at = seq(2, 14, by = 2))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
abline(v = mle[1]/mle[2], col = reds[3])
text(x = mle[1]/mle[2] + 0.75, y = 0.425, expr(mu), col = reds[3])

hist(cholesterol, 
     breaks = 20, 
     main = 'NHANES sample', 
     xlab = 'total cholesterol',
     ylab = 'frequency',
     ylim = c(0, 700))
abline(v = mean(cholesterol), lty = 4, lwd = 2)
text(x = mean(cholesterol) + 0.75, y = 650, expr(bar(x)))
```
:::

:::

> The mean total HDL cholesterol among the U.S. adult population is estimated to be 5.043 mmol/L (SE 0.0191).

## Interval estimation

A common interval estimate for the population mean is:
$$\bar{x} \pm 2\times SE(\bar{x}) \qquad\text{where}\quad SE(\bar{x}) = \left(\frac{s_x}{\sqrt{n}}\right)$$

> A range of plausible values for the mean total cholesterol among U.S. adults is 5.005 to 5.081 mmol/L.

Two related questions:

1. What do we mean by "plausible"?
2. Where did the number 2 come from?

## The $t$ model

::: {.columns}

::: {.column}
Consider the statistic:

$$
T = \frac{\bar{x} - \mu}{s_x/\sqrt{n}}
\qquad\left(\frac{\text{estimation error}}{\text{standard error}}\right)
$$ 

The sampling distribution of $T$ is well-approximated by a $t_{n - 1}$ model whenever either:

(a) the population model is symmetric and unimodal

OR

(b) the sample size is not too small
    
:::

::: {.column}

```{r, fig.width=6, fig.height=5, fig.align='center'}
n.vec <- seq(2, 12, by = 2)
par(mar = c(5, 3, 1, 1), cex = 1.5)
curve(dt(x, n.vec[6]), from = -4.5, to = 6, col = reds[6], n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = 't model')
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling frequency', line = -1)
axis(side = 1, at = seq(-4, 6, by = 2))
for(i in 5:1){
  curve(dt(x, n.vec[i]), col = reds[i], n = 500,
      xaxt = 'n', yaxt = 'n', xlab = '', ylab = '', add = T)
}
legend('topright', legend = paste('n = ',  n.vec), col = reds, title = 'sample size', lwd = 2)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(0, 19), 3)`% of samples, $T < 0$

```{r, echo = T}
# area less than 0
pt(0, df = 20 - 1) 
```

- written as $P(T < 0) = 0.5$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 0

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')

legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val, df = 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(1, 19), 3)`% of samples, $T < 1$

```{r, echo = T}
# area less than 1
pt(1, df = 20 - 1) 
```

- written as $P(T < 1) = 0.835$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 1

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val, df = 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(2, 19), 3)`% of samples, $T < 2$

```{r, echo = T}
# area less than 2
pt(2, df = 20 - 1) 
```

- written as $P(T < 2) = 0.97$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val, df = 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(2, 19, lower.tail = F), 3)`% of samples, $T > 2$

```{r, echo = T}
# area greater than 2
pt(2, df = 20 - 1, lower.tail = F) 
```

- notice: 
$$
\begin{align*}
P(T > 2) &= 1 - P(T < 2) \\
(0.03) &= 1 - (0.97)
\end{align*}
$$
:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

# polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val] - 0.001, 0, 0), col = reds[1], border = NA)
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val, df = 19, lower.tail = F), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(2, 19) - pt(1, 19), 3)`% of samples, $1 < T < 2$

```{r, echo = T}
# area between 1 and 2
pt(2, df = 20 - 1) - pt(1, df = 20 - 1) 
```

- notice:
$$
\begin{align*}
P(1 < T < 2) &= P(T < 2) - P(T < 1) \\
(0.135) &= (0.97) - (0.835)
\end{align*}
$$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val.lwr <- 1
t.val.upr <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n))), sep = ''), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(t.val.lwr, x[x<=t.val.upr & x>=t.val.lwr], t.val.upr), 
        c(0, y[(x<=t.val.upr) & (x>=t.val.lwr)] - 0.001, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val.upr, df = 19) - pt(t.val.lwr, 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(2, 19) - pt(-2, 19), 3)`% of samples, $-2 < T < 2$

```{r, echo = T}
# area between 1 and 2
pt(2, df = 20 - 1) - pt(-2, df = 20 - 1) 
```

- written $P(-2 < T < 2) = 0.94$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val.lwr <- -2
t.val.upr <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n))), sep = ''), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(t.val.lwr, x[x<=t.val.upr & x>=t.val.lwr], t.val.upr), 
        c(0, y[(x<=t.val.upr) & (x>=t.val.lwr)] - 0.001, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val.upr, df = 19) - pt(t.val.lwr, 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## A closer look at interval construction

So where did that 2 come from in the margin of error for our interval estimate?

$$
\bar{x} \pm \color{blue}{2}\times SE(\bar{x})
$$

Well:

::: {.columns}

::: {.column width="70%"}

$$
\begin{align*}
0.94 &= P(-\color{blue}{2} < T < \color{blue}{2}) \\
&= P\left(-\color{blue}{2} < \frac{\bar{x} - \mu}{s_x/\sqrt{n}} < \color{blue}{2}\right) \\
&= P(\underbrace{\bar{x} - \color{blue}{2}\times SE(\bar{x}) < \mu < \bar{x} + \color{blue}{2}\times SE(\bar{x})}_{\text{interval covers population mean}})
\end{align*}
$$
:::

::: {.column width="30%"}
> For 94% of all random samples, the interval covers the population mean.
:::

:::

So the number 2 determines the proportion of samples for which the interval covers the mean, known as its **coverage**.


## Effect of sample size

> The sample size determines the exact shape of the $t$ model through its 'degrees of freedom' $n - 1$. This changes the areas slightly.

The exact coverage quickly converges to just over 95% as the sample size increases.

::: {.columns}

::: {.column}

```{r}
tibble(n = 2^seq(2, 8, by = 1)) |>
  mutate(coverage = 1 - 2*pt(2, df = n - 1, lower.tail = F)) |>
  pander()
```

:::

::: {.column}
```{r, fig.width = 5, fig.height = 4}
tibble(n = seq(2, 1000, by = 1)) |>
  mutate(coverage = 1 - 2*pt(2, df = n - 1, lower.tail = F)) |>
  ggplot(aes(x = n, y = coverage)) +
  geom_point() +
  geom_path() +
  scale_x_log10(n.breaks = 6) +
  scale_y_continuous(breaks = c(seq(0.75, 0.9, by = 0.05), 0.9545)) +
  theme_minimal(base_size = 18) +
  theme(panel.grid = element_blank(),
        panel.grid.major.x = element_line(linewidth = 0.1, color = 'black'),
        panel.grid.major.y = element_line(linewidth = 0.1, color = 'black')) +
  labs(x = 'sample size (n)')
```
:::

:::

## Changing the coverage

Consider a slightly more general expression for an interval for the mean:

$$
\bar{x} \pm c\times SE(\bar{x})
$$

The number $c$ is called a **critical value**. It determines the coverage.

- larger $c$ $\longrightarrow$ higher coverage
- smaller $c$ $\longrightarrow$ lower coverage

The so-called "empirical rule" is that:

- $c = 1 \longrightarrow$ approximately 68% coverage
- $c = 2 \longrightarrow$ approximately 95% coverage
- $c = 3 \longrightarrow$ approximately 99.7% coverage

## Interpreting critical values

::: {.columns}

::: {.column}
$$
P(\color{#FF6459}{-2 < T < 2}) = 1 - 2\times P(\color{blue}{T > 2})
$$

Look at how the areas add up so that:
$$
P(\color{blue}{T > 2}) = 0.03
$$
Moreover:
$$
P(T < 2) = 1 - 0.03 = 0.97
$$
:::

::: {.column}
```{r, fig.width=7, fig.height=4.5, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val.lwr <- -2
t.val.upr <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n))), sep = ''), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(t.val.lwr, x[x<=t.val.upr & x>=t.val.lwr], t.val.upr), 
        c(0, y[(x<=t.val.upr) & (x>=t.val.lwr)] - 0.001, 0), col=reds[1], border = NA)
polygon(c(x[x>=t.val], max(x), t.val.upr), c(y[x>=t.val.upr] - 0.001, 0, 0), col = 'blue')
legend('topright', fill = c(reds[1], 'blue'), 
       legend = c(paste(round(pt(t.val.upr, df = 19) - pt(t.val.lwr, 19), 3)*100, "% of samples", sep = ''),
                  "3% of samples"),
       cex = 0.7)
```
:::

:::

*So the critical value 2 is actually the 97th percentile of the sampling distribution of $T$.*

- also called the 0.97 "quantile"
- (percentiles expressed in proportions are called quantiles)

## Exact coverage using $t$ quantiles

To engineer an interval with a specific coverage, use the $p$th quantile where:

$$p = \left[1 - \left(\frac{1 - \text{coverage}}{2}\right)\right]$$
In R:
```{r, echo = T}
# coverage 95% using t quantile
coverage <- 0.95
q.val <- 1 - (1 - coverage)/2
crit.val <- qt(q.val, df = 20 - 1)
crit.val
```

The effect of increasing/decreasing coverage on the quantile is:

- increase coverage $\longrightarrow$ larger quantile $\longrightarrow$ wider interval
- decrease coverage $\longrightarrow$ smaller quantile $\longrightarrow$ narrower interval

## Contrasting coverage with precision

> **Precision** refers to how wide or narrow the interval is.

Precision depends on every component of the margin of error:

- critical value used
- sample size
- variability of values

By contrast, coverage depends only on the critical value used.

## Confidence intervals

Interval estimates constructed to achieve a specified coverage are called "confidence intervals"; the coverage is interpreted and reported as a "confidence level".

::: {.columns}

::: {.column width="60%"}
```{r, echo = T}
# ingredients
cholesterol.mean <- mean(cholesterol)
cholesterol.sd <- sd(cholesterol)
cholesterol.n <- length(cholesterol)
cholesterol.se <- cholesterol.sd/sqrt(cholesterol.n)
crit.val <- qt(1 - (1 - 0.95)/2, df = cholesterol.n - 1)

# interval
cholesterol.mean + c(-1, 1)*crit.val*cholesterol.se
```
:::

::: {.column width="40%"}
> With 95% confidence, the mean total cholesterol among U.S. adults is estimated to be between `r round(cholesterol.mean - crit.val*cholesterol.se, 4)` and `r round(cholesterol.mean + crit.val*cholesterol.se, 4)` mmol/L.

:::

:::

The general formula for a confidence interval for the population mean is

$$
\bar{x} \pm c\times SE(\bar{x})
$$

where $c$ is a critical value, obtained as a quantile of the $t_{n - 1}$ model and chosen to ensure a specific coverage.

## Recap

The "common" interval estimate for the mean is actually an approximate 95% confidence interval:

$$
\bar{x} \pm 2 \times SE(\bar{x})
$$

- captures the population mean $\mu$ for roughly 95% of random samples
- replacing 2 with a $t_{n - 1}$ quantile allows the analyst to adjust coverage
- the $t_{n - 1}$ model is an approximation for the sampling distribution of $\frac{\bar{x} - \mu}{SE(\bar{x})}$

    + approximation improves with increasing sample size or symmetry
    + usually good quality except in "extreme" situations

Interval interpretation:

> With [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units].

# Extras

## Simulation of coverage

```{r}
nsim <- 200
n <- 50
crit.val <- qt(0.975, df = n - 1)
set.seed(42324)
intervals <- tibble(sim = 1:nsim, 
                    pop.mean = mean(cholesterol)) %>%
  mutate(data = map(sim, ~sample(cholesterol, size = n)),
         xbar = map(data, mean),
         se = map(data, ~sd(.x)/sqrt(length(.x)))) %>%
  unnest(cols = c(xbar, se)) %>%
  mutate(lwr = xbar - crit.val*se, 
         upr = xbar + crit.val*se,
         coverage = factor((lwr < pop.mean)*(pop.mean < upr),
                           labels = c('misses', 'covers')))
coverage.prop <- intervals |> pull(coverage) |> table() |> proportions()
```

::: {.columns}

::: {.column width="65%"}
Artificially simulating a large number of intervals provides an empirical approximation of coverage. 

- at right, `r nsim` intervals
- `r round(coverage.prop[2], 4)*100`% cover the population mean (vertical dashed line)
- pretty close to nominal coverage level 95%

This is also a handy way to remember the proper interpretation:

> If I made a lot of intervals from independent samples, 95% of them would 'get it right'.

:::

::: {.column width="35%"}
```{r, fig.height = 7, fig.width=4}
intervals %>%
  ggplot(aes(x = xbar, y = sim, color = coverage)) +
  geom_point() +
  geom_linerange(aes(xmin = lwr, 
                     xmax = upr)) +
  geom_vline(xintercept = mean(cholesterol),
             linetype = 'dashed') +
  theme_minimal(base_size = 20) +
  scale_x_continuous(breaks = round(mean(cholesterol) + sd(cholesterol)*c(-3, 0, 3)/sqrt(50), 2)) +
  theme(panel.grid = element_blank(),
        panel.grid.major.x = element_line(color = 'black', linewidth = 0.2),
        axis.text.y = element_blank(),
        legend.position = 'top') +
  labs(y = '', x = '') +
  guides(color = guide_legend(title = NULL))
```
:::

:::

