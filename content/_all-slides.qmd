---
title: "Lecture slides"
subtitle: "Applied Statistics for Life Sciences"
format: docx
execute: 
  echo: false
  warning: false
  message: false
---

# Lecture 1: Welcome to STAT218

```{r}
library(tidyverse)
library(pander)
```

## Today's agenda

1. Course logistics
2. [lecture] Study designs
3. [activity, if time] Distinguishing study types

## Icebreakers

By show of hands...

::: incremental
1.  First statistics class ever?
2.  Last statistics class ever?
3.  Expect to take STAT313?
4.  Expect to use statistics for your degree coursework or senior project?
5.  Considering a statistics or data science minor?
:::

## Class composition

By the numbers...

```{r class info}
theme_set(theme_minimal(base_size = 16))
roster <- read_csv('data/roster-anonymous.csv')

fig1 <- roster %>% 
  mutate(standing = factor(standing, 
                           levels = c('Freshman', 
                                      'Sophomore', 
                                      'Junior', 
                                      'Senior'))) %>%
  ggplot(aes(x = standing)) + 
  geom_bar() +
  labs(x = 'Class standing', y = 'Number of students')

fig2 <- roster %>%
  ggplot(aes(y = fct_rev(fct_infreq(major)))) +
  geom_bar() +
  labs(y = 'Major', x = 'Number of students') +
  scale_x_sqrt(breaks = c(1, 2, 5, 10, 15))

gridExtra::grid.arrange(fig1, fig2, nrow = 1)
```

## Statistics and uncertainty

> Life is full of uncertainty, and this can make a lot of questions hard to answer, because similar situations do not always result in the same outcome.

Statistical thinking: **uncertainty is measurable**.

What statistics can offer:

-   principles for designing studies and collecting data in order to capture outcome variability
-   data analytic tools to distinguish random from systematic variability
-   heuristics to make inferences that account for uncertainty

## Course goal and scope

The overarching goal of 218 is to introduce you to statistics in a hands-on way that is relevant to your major.

So we will focus on:

-   statistical thinking, study design, and data analysis
-   classical methods, mostly developed 1900-1940
-   case studies from life sciences

## Materials

**Computer/tablet.** You'll need a laptop (preferred) or tablet with keyboard (workable).

**Course website**. All materials are hosted/linked on the course website. I *won't* be using Canvas.

**Textbook**. Vu and Harrington (2020). [Introdutory Statistics for the Life and Biomedical Sciences](https://www.openintro.org/book/biostat/). I suggest a $5-15 donation.

**Statistical software**. R/RStudio hosted online via posit.cloud workspace. You will need to create an account and purchase a $5/month student subscription.

## Class meetings

Class meetings will usually consist of a reading quiz, a lecture, a break, and a lab.

**Preparing for class meetings:**

1.   Check the course website for posted reading, materials, and assignments.
2.   Complete readings *in advance* of the class meetings for which they are listed. 
3. Write down one question you have about the reading and bring it to class.
4.   Download and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class.

## Assignments

You will have three categories of assignments:

- **homework** problems: two per class due by next class
- **tests**: every 2-3 weeks, distributed Wednesday, due Friday
- a **project**: find and present a case study

Deadline policies:

- one-hour grace period on all deadlines
- four homework problem sets can be turned in up to 48 hours late without notice
- besides free lates, extensions must be arranged 24 hours in advance of the deadline

## Grades

Every graded question/problem is matched to one or more of the 11 course learning outcomes.

- Questions/problems are evaluated as satisfactory (S), needs improvement (NI), or missing (M).

- For each outcome, the percentage of questions/problems awarded a satisfactory mark is used to determine whether that outcome is fully met, partly met, or not met:

    + fully met: 80% or more of matched questions satisfactory
    + partly met: 50% -- 80% of matched questions satisfactory
    + not met: less than 50% of matched questions satisfactory

Your course grade is based on how many learning outcomes are fully met. To pass, you must partly or fully meet at least 6 outcomes; for a C-, you must fully meet at least 3 outcomes.

## Important policies

- extensions must be confirmed (not simply requested) 24 hours in advance
- collaboration on homework is encouraged, but everyone involved needs to...
  
    + make a contribution
    + write up their own work
    
- submitting AI-generated content in place of your own work is not acceptable

    + responsible use is okay, but not recommended (GPT outputs are misleading)
    + penalties for AI plagiarism depend on precedent and severity

 Minor offense | Major offense | Penalty
---|---|---
First |  | loss of credit and warning
Second | First | loss of credit and OSRR report
Third | Second | course failure and second OSRR report

## What is a study?

A **study** is an effort to collect data in order to answer one or more research questions.

-   studies must be well-matched to research questions to provide good answers

-   how data are obtained is just as important as how the resulting data are analyzed

-   no analysis, no matter how sophisticated will rescue a poorly conceived study

A **study unit** is the smallest object or entity that is measured in a study; also called *experimental unit* or *observational unit.*

## Two types of studies

**Observational studies** collect data from an existing situation without intervention.

-   Aim is to detect associations and patterns

-   Can't be used to establish causal links

**Experiments** collect data from a situation in which one or more interventions have been introduced by the investigator.

-   Aim is to draw conclusions about the causal effect of interventions
-   Stronger form of scientific evidence than an observational study

Often, observational studies are used to explore/generate hypotheses prior to designing an experiment.

## Comparing study types

Either type of study can be used to address a question.

| Question                                          | Observational study                                                      | Experiment                                                                                   |
|---------------------------------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|
| Are diet and mood related?                        | Conduct surveys on diet, lifestyle, and affect                           | Recruit study participants, assign diets, measure affect                                     |
| Is vaping safer than smoking?                     | Follow groups of vapers and smokers over time and record health outcomes | Among a group of smokers, assign some to switch to vaping; compare health outcomes over time |
| Do insecticide applications affect soil microbes? | Analyze soil samples from farms using different insecticides             | \[**Your turn**\]                                                                            |

Can you think of pros and cons for each study type?

## Why does intervention matter?

Control over conditions allows a researcher to study causal effects resulting from interventions. This is not possible in observational studies due to the potential for **confounding**.

::: columns
::: {.column width="60%"}
Confounding: an unobserved condition is associated with both the study condition and the outcome.

- Failure to measure and account for confounders potentially distorts observed associations
- Example: a study finds that dog owners live longer, but doesn't measure exercise; so it might just be the daily walks.

:::

::: {.column width="40%"}
```{mermaid}
flowchart TD
  A(unobserved variable) --- B(study variable) & C(study outcome) 
```
:::
:::

This is very common in observational studies, because you can't measure every study condition.

## Antidote: randomization

The ability to control study conditions allows researchers to randomly allocate interventions among study subjects.

::: columns
**Randomization eliminates confounding** by isolating the condition(s) of interest:

::: {.column width="60%"}
-   interventions are independent of extraneous conditions ⟹ no association possible

-   if outcomes differ systematically according to the intervention, you can be certain that it is not an artifact
:::

::: {.column width="40%"}
```{mermaid}
flowchart TD
  A(unobserved condition) x-.-x B(study condition)
  A --- C(outcome)
```
:::
:::

## Practical consequences

The ability to randomize interventions in experiments means:

-   observed associations are independent of extraneous factors

-   results can support causal inferences

The absence of randomization in observational studies means:

-   confounding is always possible

-   results may be misleading

## Experimental designs

A **treatment** is an experimental intervention; the **design** of an experiment refers to how treatments are allocated to study units.

The most basic design is:

-   \[balanced\] each treatment is replicated an equal number of times

-   \[randomized\] treatments are allocated completely at random to study units

-   \[no crossover\] each study unit receives exactly one treatment

We'll call this a **completely randomized design**. It's the only kind of experimental design we're going to consider in STAT218.

There are many other designs that we won't discuss (but see STAT313); these are all about improving experimental efficiency by controlling extraneous variation.

## Data collection

Study units should be chosen so as to represent a larger collection.

::: columns
::: {.column width="50%"}
![](img/srs.png)
:::

::: {.column width="50%"}
A study **population** is a collection of all study units of interest.

A **sample** is a subcollection from a population:

-   *random* if study units have a known chance of inclusion in the sample
-   *nonrandom* or *convenience* otherwise
:::
:::

The gold standard is the **simple random sample**: each study unit in the population has an equal chance of inclusion in the sample.

## LEAP Study

::: {.columns}

::: {.column width="60%"}
Learning early about peanut allergy (LEAP) study:

-   640 infants in UK with eczema or egg allergy but no peanut allergy enrolled

-   each infant randomly assigned to peanut consumption and peanut avoidance groups

    -   peanut consumption: fed 6g peanut protein daily until 5 years old

    -   peanut avoidance: no peanut consumption until 5 years old

-   at 5 years old, oral food challenge (OFC) allergy test administered

-   13.3% of the avoidance group developed allergies, compared with 1.9% of the consumption group
:::

::: {.column width="40%"}

::: {.callout-note icon=false}
## Study characteristics

**Study type:** experiment

**Study population:** UK infants with eczema or egg allergy but no peanut allergy

**Sample:** 640 infants from population

**Study design:** completely randomized design

**Treatments:** peanut consumption; peanut avoidance

**Study outcome:** development of peanut allergy by 5 years of age
:::

::: {.callout-tip icon=false}
## Study results
Moderated peanut consumption causes a reduction in the likelihood of developing an allergy.
:::

:::

:::


## Checklist for next time

1. Obtain a copy of the textbook.
2. Create a posit.cloud account and purchase a student subscription. Ensure you can access the `stat218-s24` workspace.
3. Complete practice problems and reading before class.
4. Write down one question about the reading.
5. Print a paper or virtual copy of the slides.
    
## Posit cloud account

Go to: course webpage \> syllabus \> materials. Then look for the link to join the class workspace:

![](img/posit-syllabus.png){fig-align="center"}

## Posit cloud account

Follow prompts to create an account. Use your Cal Poly email.

![](img/posit-signup.png){fig-align="center"}

## Posit cloud account

Once your email is verified, return to posit.cloud (or click the link in the syllabus again), and join the class workspace.

![](img/posit-join.png){fig-align="center"}

## Posit cloud account

Upgrade your account to the student plan. Input payment details.

![](img/posit-upgrade.png){fig-align="center"}

## Printing slides

![Open menu from lower left](img/printing-slides-1.png)

## Printing slides

![Navigate to tools](img/printing-slides-2.png)

## Printing slides

![Select PDF export mode](img/printing-slides-3.png)

## Printing slides

![Then print from browser to PDF](img/printing-slides-4.png){width=550 fig-align="center"}

I suggest landscape layout and either 1, 2 or 4 slides per page

# Lecture 2: Data semantics and data types

## Today's agenda

1.  Reading quiz
2.  [lecture] data semantics and data types
3.  [lab] R basics


## Data semantics

-   **Data** are a set of measurements.

-   A **variable** is any measured attribute of study units.

-   An **observation** is a measurement of one or more variables taken on one particular study unit.

It is usually expedient to arrange data values in a table in which each row is an observation and each column is a variable:

![](img/data-matrix.png)

## LEAP example

A table showing the observations and variables for the LEAP study would look like this:

```{r}
library(tidyverse)
library(oibiostat)
library(pander)
data(LEAP)
LEAP %>% 
  select(participant.ID, treatment.group, overall.V60.outcome) %>%
  rename(ofc.test.result = overall.V60.outcome) %>%
  head(4) %>%
  pander()
```

The table you saw in the reading was a **summary** of the data (not the data itself):

```{r}
LEAP %>%
  rename(ofc.test.result = overall.V60.outcome) %>%
  select(treatment.group, ofc.test.result) %>%
  table() %>%
  pander()
```

## Numeric and categorical variables

Variables are classified according to their values. Values can be one of two different types:

-   A variable is **numeric** if its value is a number
-   A variable is **categorical** if its value is a category, usually recorded as a name or label

For example:

-   the value of `sex` can be male or female, so it is categorical
-   whereas `age` (in years) can be any positive integer, so it is numeric

<!-- *Check your understanding: in the LEAP study...* -->

<!-- -   *treatment group is \[numeric/categorical\]* -->
<!-- -   *OFC test result is \[numeric/categorical\]* -->

## Variable subtypes

Further distinctions are made based on the type of number or type of category used to measure an attribute. *Can you match the subtypes to the variables at right?*

::: columns
::: {.column width="0.65"}
![](img/variable-types.png)
:::

::: {.column width="0.35"}
```{r, echo = F}
set.seed(11824)
data(yrbss)
yrbss %>%
  select(age, hispanic, grade, weight) %>%
  sample_n(size = 4) %>%
  pander::pander()
```
:::
:::

-   a numerical variable is **discrete** if there are 'gaps' between its *possible* values
-   a numerical variable is **continuous** if there are no such gaps
-   a categorical variable is **nominal** if its levels are not ordered
-   a categorical variable is **ordinal** if its levels are ordered

## Many ways to measure attributes

Variable type (or subtype) is not an inherent quality --- attributes can often be measured in many different ways.

For instance, `age` might be measured as either a discrete, continuous, or ordinal variable, depending on the situation:

| Age (years) | Age (minutes) | Age (brackets) |
|-------------|---------------|----------------|
| 12          | 6307518.45    | 10-18          |
| 8           | 4209187.18    | 5-10           |
| 21          | 11258103.08   | 18-30          |

> Numeric variables can always be represented as categorical, *but not the other way around*.

## Your turn

Classify each variable as nominal, ordinal, discrete, or continuous:

```{r}
load('data/famuss.RData')
set.seed(11324)
famuss %>% 
  sample_n(size = 4) %>% 
  select(ndrm.ch, genotype, sex, age, race, bmi) |>
  pander()
```

Data are from an observational study investigating demographic, physiological, and genetic characteristics associated with muscle strength.

-   `ndrm.ch` is change in strength in nondominant arm after resistance training
-   `genotype` indicates genotype at a particular location within the ACTN3 gene

## Common summary statistics

> A **statistic** is a data summary: in mathematical terms, a function of several observations

::: {.columns}

::: {.column}
For numeric variables, the most common summary statistic is the **average value**:

$$\text{average} = \frac{\text{sum of values}}{\text{# observations}}$$

For example, the average percent change in nondominant arm strength was `r famuss$ndrm.ch |> mean() |> round(3)`%.
:::

::: {.column}
For categorical variables, the most common summary statistic is a **proportion**:

$$\text{proportion}_i = \frac{\text{# observations in category } i}{\text{# observations}}$$

For example:

```{r}
famuss$genotype |> 
  table() |> 
  proportions() |> 
  pander(caption = 'Genotype proportions')
```


:::

:::

## Descriptive analyses

Sometimes, a few clever summary statistics can be used to answer a research question.

> How much does the average change in arm strength differ by genotype, if at all?

Computing *per-genotype* averages provides an answer:

```{r}
famuss |>
  group_by(genotype) |>
  summarize(avg.change = mean(ndrm.ch),
            n.obs = n()) |>
  ungroup() |>
  mutate(prop.obs = n.obs/sum(n.obs)) |>
  arrange(desc(avg.change)) |>
  pander()
```

Number of observations and proportions are included because they provide information about genotype frequencies in the sample.

- conveys how many individuals were measured
- also provides an estimate of genotype frequencies in the population

## Common mathematical notation

While we won't use mathematical expressions too often in STAT218, it's useful to be aware of some common notations.

Typically, a set of observations is written as:

$$x_1, x_2, \dots, x_n$$

- $x$ represents the variable (*e.g.*, genotype, age, percent change, etc.)
- subscript indexes observations: $x_i$ is the $i$th observation
- $n$ is the total **n**umber of observations

The sum of the observations is written $\sum_i x_i$, where the symbol $\sum$ stands for 'summation'. This is useful for writing the formula for computing an average:

$$\bar{x} = \frac{1}{n}\sum_i x_i$$

## Lab: data basics in R

The variable types we just discussed map pretty neatly (but not perfectly) onto the main "data types" in R:

-   numeric ➜ integer, numeric
-   categorical ➜ character, factor, logical

The primary way data are arranged in R is in a **data frame**. This lab will show you how to load, inspect, and use data frames.

Your objectives in this lab are: 

1. learn to load and inspect datasets
2. learn to recognize data types
3. learn to perform simple calculations (averages, etc.)

## Opening the lab activity

Navigate to posit.cloud. Then:

![](img/posit-landing.png){fig-align="center"}

1.  Make sure the class workspace "stat218-s24" is highlighted at left. If "Your Workspace" is highlighted, you won't see the example assignment.

2.  Click on the `lab1-rbasics`, then wait.

Once everyone is ready, we'll have a look at the example files together.

# Lecture 3: Descriptive statistics

## Today's agenda

1. \[lecture\] frequency distributions; measures of spread and center
2. \[lab\] descriptive statistics and simple graphics in R

## Last time

::: {.columns}

::: {.column width="60%"}

1. Data semantics

- **categorical** data: ordinal (ordered) or nominal (unordered)
- **numeric** data: continuous (no 'gaps') or discrete ('gaps')

2. Data types and data structures in R

- basic types: numeric, character, logical, integer
- a **vector** is a collection of values of one type
- a **data frame** is a type-heterogeneous list of vectors of equal length

:::

::: {.column width="40%"}

Vectors can store observations of one variable:
```{r, echo = T}
# 4 observations of age
ages <- c(18, 22, 18, 12)
ages
```

Data frames can store observations of many variables:
```{r, echo = T}
# 3 observations of 2 variables
data.frame(subject.id = c(11, 2, 31),
           age = c(24, 31, 17),
           sex = c('m', 'm', 'f'))
```

:::

:::

*Techniques for summarizing data depend on the data type*


## What are descriptive statistics?

We learned last time that a statistic is a data summary, *i.e.*, any function of a set of observations.

**Descriptive statistics** refers to analysis of sample characteristics using summary statistics.

- these are data analyses that uses statistics interpreted on face value
- in contrast to **inferential statistics**, which uses statistics interpreted relative to a broader population

Descriptive statistics can be either numerical or graphical; we'll discuss both.

## Dataset: FAMuSS study

Observational study of 595 individuals comparing change in arm strength before and after resistance training between genotypes for a region of interest on the ACTN3 gene.

> Pescatello, L. S., *et al.* (2013). Highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or FAMuSS study. *BioMed research international.*

```{r}
library(tidyverse)
library(oibiostat)
library(pander)
data(famuss)
head(famuss, 4) %>% pander(caption = 'Example data rows')
```

## Categorical frequency distributions

For categorical variables, the frequency distribution is simply an observation count by category. For example:

::: columns
::: {.column width="0.2"}
```{r}
set.seed(11424)
famuss %>% 
  mutate(participant.id = row_number()) %>%
  rename(genotype = actn3.r577x) %>%
  select(participant.id, genotype) %>% 
  sample_n(size = 6) %>% 
  pander(caption = 'Data table')
```
:::

::: {.column width="0.8"}
```{r}
tbl <- famuss %>% 
  mutate(participant = row_number()) %>%
  rename(genotype = actn3.r577x) %>%
  select(genotype) %>%
  table() 

tbl %>%
  pander(caption = 'Frequency distribution')
```

```{r}
par(mar = c(5, 5, 2, 2), cex.lab = 2, cex.axis = 1.5, cex = 1.5)
famuss %>% 
  mutate(participant = row_number()) %>%
  rename(genotype = actn3.r577x) %>%
  pull(genotype) %>%
  plot(xlab = 'genotype', ylab = 'frequency')
```
:::
:::

## Numeric frequency distributions

Frequency distributions of numeric variables are observation counts by *range*; a plot of a numeric frequency distribution is called a **histogram**.

::: columns
::: {.column width="0.3"}
```{r}
famuss %>%
  mutate(participant.id = row_number()) %>%
  select(participant.id, bmi) %>%
  sample_n(size = 6) %>%
  pander(caption = 'Data table')
```
:::

::: {.column width="0.7"}
```{r}
par(mar = c(4, 5, 2, 2), 
    cex = 2.5)
famuss %>%
  transmute(bmi.range = cut(bmi, breaks = seq(10, 50, by = 10))) %>%
  table() %>%
  pander(caption = 'Frequency distribution')
hist(famuss$bmi, 
     breaks = 3, 
     main = 'Histogram', 
     xlab = 'bmi',
     ylab = 'frequency')
```
:::
:::

The operation of dividing a numeric variable into interval ranges is called **binning**.

## Histograms and binning

Binning has a big effect on the visual impression. Which one captures the shape best?

```{r}
layout(matrix(1:4, nrow = 2, byrow = T))
par(mar = c(5, 5, 2, 2), cex.lab = 2, cex.axis = 1.5, cex.main = 2)
hist(famuss$bmi, 
     breaks = 4,
     xlab = 'bmi',
     ylab = 'frequency',
     main = '4 bins',
     xlim = c(10, 50),
     ylim = c(0, 450))
hist(famuss$bmi, 
     breaks = 6,
     ylim = c(0, 450),
     xlim = c(10, 50),
     xlab = 'bmi',
     ylab = 'frequency',
     main = '6 bins')
hist(famuss$bmi, 
     xlim = c(10, 50),
     ylim = c(0, 450),
     breaks = 15,
     xlab = 'bmi',
     ylab = 'frequency',
     main = '15 bins')
hist(famuss$bmi, 
     xlim = c(10, 50),
     ylim = c(0, 450),
     breaks = 58,
     xlab = 'bmi',
     ylab = 'frequency',
     main = '58 bins')
```

## Shapes

For numeric variables, the histogram reveals the **shape** of the distribution:

-   **symmetric** if it shows left-right symmetry about a central value
-   **skewed** if it stretches farther in one direction from a central value

```{r, fig.height=3}
par(mfrow = c(1, 3))
curve(dgamma(abs(10 - x), shape = 3, scale = 1), 
      from = 0, 
      to = 10,
      xlab = '', 
      ylab = '', 
      main = 'left skew', 
      cex.main = 3, 
      xaxt = 'n', 
      yaxt = 'n')
curve(dnorm, 
      from = -3, 
      to = 3,
      xlab = '', 
      ylab = '', 
      main = 'symmetric', 
      cex.main = 3,
      xaxt = 'n', 
      yaxt = 'n')
curve(dgamma(x, shape = 3, scale = 1), 
      from = 0, 
      to = 10,
      xlab = '', 
      ylab = '', 
      main = 'right skew', 
      cex.main = 3, 
      xaxt = 'n', 
      yaxt = 'n')
```

## Modes

Histograms also reveal the number of **modes** or local peaks of frequency distributions.

-   **uniform** if there are zero peaks
-   **unimodal** if there is one peak
-   **bimodal** if there are two peaks
-   **multimodal** if there are two or more peaks

```{r, fig.height=3}
par(mfrow = c(1, 4))

curve(dunif, 
      from = -1, 
      to = 2,
      xlab = '', 
      ylab = '', 
      main = 'uniform', 
      cex.main = 3, 
      xaxt = 'n', 
      yaxt = 'n')
curve(0.4*dnorm(x, mean = 0) + 0.6*dnorm(x, mean = 2), 
      from = -3, 
      to = 6,
      xlab = '', 
      ylab = '', 
      main = 'unimodal', 
      cex.main = 3,
      xaxt = 'n', 
      yaxt = 'n')
curve(0.4*dnorm(x, mean = -2) + 0.6*dnorm(x, mean = 2), 
      from = -5, 
      to = 5,
      xlab = '', 
      ylab = '', 
      main = 'bimodal', 
      cex.main = 3, 
      xaxt = 'n', 
      yaxt = 'n')
curve(0.2*dnorm(x, mean = -2) + 
        0.5*dnorm(x, mean = 2) + 
        0.3*dnorm(x, mean = 6), 
      from = -5, 
      to = 10,
      xlab = '', 
      ylab = '', 
      main = 'multimodal', 
      cex.main = 3, 
      xaxt = 'n', 
      yaxt = 'n')
```

## Your turn: characterizing distributions

Consider four variables from the FAMuSS study. Describe the shape and modality.

```{r}
par(mfrow = c(2, 2),
    mar = c(5, 5, 2, 2), 
    cex.lab = 2, 
    cex.axis = 1.5, 
    cex.main = 2)
hist(famuss$height, 
     main = '', 
     xlab = 'height',
     ylab = 'frequency')
hist(famuss$weight, 
     main = '', 
     xlab = 'weight',
     ylab = 'frequency')
hist(famuss$age, 
     main = '', 
     xlab = 'age',
     ylab = 'frequency')
hist(famuss$bmi, 
     main = '', 
     xlab = 'bmi',
     ylab = 'frequency')
```

## Your turn: characterizing distributions

Here are some made-up data. Describe the shape and modality.

```{r}
par(mfrow = c(2, 2))
par(mar = c(5, 5, 2, 2), cex.lab = 2, cex.axis = 1.5, cex.main = 2)
set.seed(11524)
runif(1000, min = -5, max = 2) %>% 
  hist(main = '', xlab = 'x', ylab = 'frequency')
-rpois(1000, lambda = 2) %>% 
  hist(main = '', xlab = 'x', ylab = 'frequency', breaks = 6)
c(rnorm(400, mean = -4, sd = 2), rnorm(600, mean = 8, sd = 5)) %>%
  hist(main = '', xlab = 'x', ylab = 'frequency', breaks = 20)
c(rexp(500, rate = 1), 10 - rexp(600, rate = 2))  %>%
  hist(main = '', xlab = 'x', ylab = 'frequency', breaks = 20)
```


## Descriptive measures

A **descriptive measure** is a summary statistic that captures a particular feature of the frequency distribution of a numeric variable.

Commonly, measures capture either **location** or **spread**.

::: {.columns}

::: {.column}
Measures of location:

- mean
- median
- mode
- percentiles/quantiles
:::

::: {.column}
Measures of spread:

- range (min and max)
- interquartile range
- average deviation
- variance
- standard deviation
:::

:::

It is common practice to report multiple measures.

## Measures of location

Often location is specified by the "center" of a frequency distribution.

::: columns
::: {.column width="0.5"}
There are three common measures of center, each of which corresponds to a slightly different meaning of "typical":

| Measure | Definition          |
|---------|---------------------|
| Mode    | Most frequent value |
| Mean    | Average value       |
| Median  | Middle value        |
:::

::: {.column width="0.5"}
Suppose your data consisted of the following observations of age in years:

```{r}
c(19, 19, 21, 25, 31) %>% pander()
```

-   the **mode** or most frequent value is 19
-   the **median** or middle value is 21
-   the **mean** or average value is $\frac{19 + 19 + 21 + 25 + 31}{5}$ = 23
:::
:::

## Quick example

Consider the first 8 observations of change in nondominant arm strength from the FAMuSS study data:

```{r}
famuss$ndrm.ch[1:8] %>% pander()
```

Compute the mean, median, and mode.

## Comparing measures of center

Each statistic is a little different, but often they roughly agree; for example, all are between 20 and 25, which seems to capture the typical BMI well enough.

```{r, fig.width = 10, fig.height = 4}
makehist <- function(x, b, t, xlab, leg, nmodes){
h <- hist(x, 
          main = b, 
          breaks = t, 
          xlab = xlab,
          ylab = 'frequency')
abline(v = mean(x), 
       col = 2, 
       lty = 2, 
       lwd = 2)
abline(v = median(x),
       col = 4, 
       lty = 3, 
       lwd = 2)
x_mode <- h$mids[which(h$counts %in% 
                         sort(h$counts, 
                              decreasing = T)[1:nmodes])]
abline(v = x_mode, 
       col = 6, 
       lty = 4, 
       lwd = 2)
if(leg){
legend(quantile(h$mids, 0.7), 
       max(h$counts), 
       legend = c("Mean", "Median", "Mode"), 
       col = c(2, 4, 6), 
       lty = c(2, 3, 4))
}
}
par(mar = c(4, 5, 2, 2), 
    cex.lab = 2, 
    cex.axis = 1.5,
    cex.legend = 1.5,
    cex.main = 2)
makehist(famuss$bmi, '', 15, 'bmi', T, 1)
```

*How do you think the frequency distribution affects which one is "best"?*

## Means, medians, and skewness

The mean and median both get 'pulled' in the direction of skewness, but the mean is more sensitive:

```{r, fig.width = 20, fig.height = 4}
set.seed(11524)
x1 <- rgamma(1000, shape = 3, scale = 2)
x2 <- -rgamma(1000, shape = 3, scale = 2)

par(mfrow = c(1, 2),
    cex = 2,
    mar = c(2, 4, 1, 1))
makehist(x1, '', 20, '', T, 1)
makehist(x2, '', 20, '', F, 1)
```

Comparing means and medians captures information about skewness present since:

- mean $>$ median: right skew
- mean $<$ median: left skew
- mean $\approx$ median: symmetric

## When to use mode(s)

Mode is rarely used unless extreme skewness or multiple modes are present; below are two examples.

```{r, fig.width = 20, fig.height = 6}
set.seed(11524)

x_skew <- c(rgamma(n = 1000, shape = 4, rate = 1/10),

            runif(n = 500, min = 70, max = 300))

x_bi <- c(rnorm(1000, mean = -3),

          rnorm(1000, mean = 3))

par(mfrow = c(1, 2),
    cex = 2)

makehist(x_skew, '', 20, '', T, 1)

makehist(x_bi, '', 20, '', F, 2)
```

## Percentiles

A **percentile** is a value with specified proportions of data lying both above and below that value. 

- measure of location (but not center)
- defined with reference to the percentage of data below

For example, the 20th percentile is the value with 20% of observations below and 80% of observations above. Suppose we have 5 observations:

```{r}
x <- c(19, 20, 21, 25, 31)
rbind(age = x, rank=rank(x)) %>% pander()
```

The 20th percentile is not unique! In fact *any* number between 19 and 20 is a 20th percentile since it would satisfy:

-   20% below (19)
-   80% above (20, 21, 25, 31)


## Cumulative frequency distribution

The *cumulative frequency distribution* is a data summary showing percentiles. Think of it as percentile (y) against value (x).

::: columns
::: {.column width="0.6"}
```{r, fig.height = 5, fig.width = 5, fig.align='center'}
par(mar = c(5, 5, 2, 2), cex.lab = 2, cex.axis = 1.5, cex.main = 2)
famuss$age[famuss$age < 30] %>%
  # sample(size = 50) %>%
  ecdf() %>% 
  plot(main = '', 
       xlab = 'age', 
       ylab = 'cumulative frequency')
```
:::

::: {.column width="0.4"}
Interpretation of some specific values:

-   about 40% of the subjects are 20 or younger
-   about 80% of the subjects are 24 or younger

*Your turn*:

1.  Roughly what percentage of subjects are 22 or younger?
2.  About what age is the 10th percentile?
:::
:::


## Common percentiles

::: columns
::: {.column width="50%"}
The **five-number summary** is a collection of five percentiles that succinctly describe the frequency distribution:

| Statistic name     | Meaning          |
|--------------------|------------------|
| **minimum**        | 0th percentile   |
| **first quartile** | 25th percentile  |
| **median**         | 50th percentile  |
| **third quartile** | 75th percentile  |
| **maximum**        | 100th percentile |
:::

::: {.column width="50%"}
Boxplots provide a graphical display of the five-number summary.

![](img/boxplot-anatomy.png)
:::

:::

## Boxplots vs. histograms

Notice how the two displays align, and also how they differ. The histogram shows shape in greater detail, but the boxplot is much more compact.

::: columns
::: {.column width="50%"}
```{r, fig.width=5, fig.height=5}
par(mar = c(4.5, 5, 2, 2), cex.lab = 2, cex.axis = 1.5, cex.main = 2)
layout(matrix(1:2, nrow = 2), height = c(3, 2))
hist(famuss$ndrm.ch, 
     main = 'nondominant arm', 
     xlab = 'percent change',
     ylab = 'frequency')
boxplot(famuss$ndrm.ch, 
        range = 3, 
        horizontal = T)
```

:::

::: {.column width="50%"}
```{r, fig.width=5, fig.height=5}
par(mar = c(4.5, 5, 2, 2), cex.lab = 2, cex.axis = 1.5, cex.main = 2)
layout(matrix(1:2, nrow = 2), height = c(3, 2))
hist(famuss$drm.ch, 
     main = 'dominant arm', 
     xlab = 'percent change',
     ylab = 'frequency')
boxplot(famuss$drm.ch, 
        range = 3, 
        horizontal = T,
        ylim = c(-40, 100))
```
:::
:::


## Measures of spread

The *spread* of observations refers to how concentrated or diffuse the values are.

```{r, fig.width = 8, fig.height = 2.5, fig.align='center'}
par(mfrow = c(1, 2))
par(mar = c(1, 1, 4, 1),
    cex.main = 2)
curve(dnorm, from = -3, to = 3, 
      main = 'more spread', 
      xaxt = 'n', 
      yaxt = 'n',
      xlab = '',
      ylab = '')
curve(dnorm(x, sd = 0.25), from = -3, to = 3, 
      main = 'less spread', 
      xaxt = 'n', 
      yaxt = 'n',
      xlab = '',
      ylab = '')
```

Two ways to understand and measure spread:

-   *ranges* of values capturing much of the distribution
-   *deviations* of values from a central value

## Range-based measures

A simple way to understand and measure spread is based on ranges. Consider more ages, sorted and ranked:

```{r}
x <- c(16, 18, 19, 20, 21, 22, 25, 26, 28, 29, 30, 34)
rbind(age = x, rank = rank(x)) %>% pander()
```

-   The **range** is the minimum and maximum values: $$\text{range} = (\text{min}, \text{max}) = (16, 34)$$

-   The **interquartile range** (IQR) is the difference \[75th percentile\] - \[25th percentile\] $$\text{IQR} = 29 - 19 = 10$$ *When might you prefer IQR to range? Can you think of an example?*

## Deviation-based measures

Another way is based on *deviations* from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:

```{r}
ages <- c(16, 18, 19, 20, 21, 22, 25, 26, 28, 29, 30, 34)

rbind(age = ages, deviation = ages - mean(ages)) %>% pander()
```

The **average deviation** is defined as the average of the absolute values of the deviations from the mean: $$\frac{8 + 6 + 5 + 4 + 3 + 2 + 1 + 2 + 4 + 5 + 6 + 10}{12}$$

## Deviation-based measures

Another way is based on *deviations* from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:

```{r}
ages <- c(16, 18, 19, 20, 21, 22, 25, 26, 28, 29, 30, 34)

rbind(age = ages, deviation = ages - mean(ages)) %>% pander()
```

The **variance** is the average *squared* deviation from the mean (but divided by one less than the sample size): $$\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}$$

The **standard deviation** is the square root of the variance: $$\sqrt{\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}}$$

## Mathematical notations

Following the convention from before, write a set of $n$ observations as $x_1, x_2, \dots, x_n$.

::: {.columns}

::: {.column}

The **mean** of the observations is written: $$\bar{x} = \frac{1}{n}\sum_i x_i$$

The **average deviation** is: $$\frac{1}{n} \sum_i |x_i - \bar{x}|$$

:::

::: {.column}

The **variance** is: $$s_x^2 = \frac{1}{n - 1}\sum_i (x_i - \bar{x})^2$$

The **standard deviation** is: $$s_x = \sqrt{\frac{1}{n - 1}\sum_i (x_i - \bar{x})^2}$$

:::

:::
## Interpretations

Listed from largest to smallest, here are each of the measures of spread for the 12 ages:

```{r}
c(min = min(ages),
  max = max(ages),
  iqr = IQR(ages),
  variance = var(ages),
  st.dev = sd(ages),
  avg.dev = mean(abs(ages - mean(ages)))) %>%
  pander()
```

The interpretations differ between these statistics:

-   \[range\] all of the data lies on an between 16 and 34 years old on an interval 18 years in width
-   \[IQR\] the middle half of the data lies on an interval 8.5 years in width
-   \[average deviation\] the average distance from the mean is 4.67 years
- \[variance\] the average squared distance from the mean is 30.55 years$^2$
-   \[standard deviation\] the average squared distance from the mean, rescaled to years, is 5.53 years

# Lecture 4: Bivariate summaries


```{r}
library(oibiostat)
library(tidyverse)
library(pander)
data(famuss)
load('data/census.RData')
ages <- c(16, 18, 19, 20, 21, 22, 25, 26, 28, 29, 30, 34)
```

## Today's agenda

1. Reading quiz
2. Loose end: robustness
3. Bivariate numeric and graphical summaries
4. Lab: bivariate graphics in R

## Robustness

> Percentile-based measures of location and spread are less sensitive to outliers

Consider adding an observation of 94 to our 12 ages from last time. This is an outlier.

::: columns
::: {.column width="60%"}

```{r, echo = T}
# append an outlier
ages_add <- c(ages, 94)

# means
c(original = mean(ages), with.outlier = mean(ages_add))

# medians
c(original = median(ages), with.outlier = median(ages_add))

# IQR
c(original = IQR(ages), with.outlier = IQR(ages_add))

# SD
c(original = sd(ages), with.outlier = sd(ages_add))
```
:::

::: {.column width="40%"}
The effect of this outlier on each statistic is:

- mean increases by `r round(100*(mean(ages_add) - mean(ages))/mean(ages), 2)`%
- median increases by `r round(100*(median(ages_add) - median(ages))/median(ages), 2)`%
- IQR increases by `r round(100*(IQR(ages_add) - IQR(ages))/IQR(ages), 2)`%
- SD increases by `r round(100*(sd(ages_add) - sd(ages))/sd(ages), 2)`%

**Robustness** refers to sensitivity to outliers. Mean and SD are less robust than median and IQR.

:::
:::

## Choosing appropriate measures

> When outliers are present, use percentile-based measures; otherwise, use mean and standard deviation or variance

```{r, fig.width=8, fig.height=2.5}
par(mar = c(4, 4, 0.1, 0.1),
    cex = 1.5,
    mfrow = c(1, 2))
hist(census$total_personal_income, 
     main = '', xlab = 'total personal income', ylab = 'frequency')
hist(census$age, 
     main = '', xlab = 'age', ylab = 'frequency')
```

*Check your understanding: which measures are most appropriate for each variable above?*

## Limitations of univariate summaries

> Univariate summaries aim to capture the distribution of values of a single variable.

::: columns
::: {.column width="70%"}
```{r, fig.width=8, fig.height = 4}
library(oibiostat)
data(famuss)
height <- famuss$height
weight <- famuss$weight

par(mfrow = c(1, 2))
par(mar = c(5, 5, 3, 1),
    cex.main = 2, cex.lab = 2, cex.axis = 1.5)
hist(height)
abline(v = mean(height), col = 2, lwd = 2)
hist(weight)
abline(v = mean(weight), col = 2, lwd = 2)
```
:::

::: {.column width="30%"}
-   both unimodal, no obvious outliers
-   heights symmetric
-   weights right-skewed
-   but these observations actually come in *pairs*
:::
:::

Univariate summaries don't reflect how the variables might be related.

## Bivariate summaries

> Bivariate summaries aim to capture a relationship between two variables.

::: columns
::: {.column width="60%"}
A simple example is a scatterplot:

```{r, fig.width=5, fig.height=4}
par(mar = c(5, 5, 1, 1),
    cex.main = 2, cex.lab = 2, cex.axis = 1.5)
plot(height, weight)
points(mean(height), mean(weight), col = 2, pch = 17, cex = 2)
abline(v = mean(height), h = mean(weight), col = 2, lwd = 2, lty = 2)
```

# Lecture 5: Introduction to inference


```{r}
library(tidyverse)
library(oibiostat)
library(pander)
load('data/nhanes.RData')
```

## Today's agenda

1. Test 1 discussion
2. [lecture] Inference vs. description, point estimation, interval estimation
3. [lab] Exploring interval coverage

## Description vs. inference

> **Statistical inferences** are statements about population statistics based on samples. 

To appreciate the meaning of this, consider the contrast with descriptive findings, which are statements about sample statistics:

::: columns
::: {.column width="50%"}
```{r, fig.height = 3, fig.width = 5}
data(famuss)
par(mar = c(5, 5, 3, 1),
    cex = 1.25)
boxplot(ndrm.ch ~ actn3.r577x, 
        data = famuss, 
        horizontal = T,
        range = 2,
        main = 'nondominant arm strength',
        xlab = 'percent change', 
        ylab = 'genotype')
```
:::

::: {.column width="50%"}
Median percent change by genotype:

```{r}
famuss %>% 
  group_by(actn3.r577x) %>%
  summarize(median.change = median(ndrm.ch)) %>%
  pivot_wider(names_from = actn3.r577x, values_from = median.change) %>%
  pander()
```

A descriptive finding is:

*Subjects with genotype TT exhibited the largest median percent change in strength*
:::
:::

The corresponding inference would be:

*The median percent change in strength is highest among adults with genotype TT.*

## Inference or description?

See if you can tell the difference:

1. The proportion of children who developed a peanut allergy was 0.133 in the avoidance group.
2. The proportion of children who develop peanut allergies is estimated to be 0.133 when peanut protein is avoided in infancy.
3. The average lifetime of mice on normal 85kCal diets is estimated to be 32.7 months.
4. The 57 mice on the normal 85kCal diet lived 32.7 months on average.
5. The relative risk of a CHD event in the high trait anger group compared with the low trait anger group was 2.5.
6. The relative risk of a CHD event among adults with high trait anger compared with adults with low trait anger is estimated to be 2.5.

## Random sampling

> Sampling establishes the link (or lack thereof) between a sample and a population.

::: columns
::: {.column width="50%"}
![](img/srs.png)
:::

::: {.column width="50%"}
In a **simple random sample**, units are chosen in such a way that every individual in the population has an equal probability of inclusion. For the SRS:

- sample statistics mirror population statistics (sample is *representative*)
- sampling variability depends only on population variability and sample size

:::
:::


## Population models

> Inference consists in using statistics of a random sample to ascertain population statistics under a population model.

A population model represents the distribution of values you'd see if you measured every individual in the study population. We think of the sample values as a random draw.

::: columns
::: {.column width="50%"}
```{r, fig.width=5, fig.height=3}
totchol <- pull(nhanes, totchol)
mle <- MASS::fitdistr(totchol, 'gamma')$est |> round(1)
names(mle) <- NULL
# c(`Population mean` = mle[1]/mle[2],
#   `Population SD` = mle[1]/(mle[2]^2)) |> 
#   pander()

par(mar = c(4, 4, 1, 1),
    cex = 1.5)
curve(dgamma(x, shape = mle[1], rate = mle[2]), 
      from = 2, to = 14,
      xlab = 'total cholesterol',
      ylab = 'density', axes = F,
      ylim = c(0, 0.4),
      main = 'population model')
axis(side = 1, at = seq(2, 14, by = 2))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
```
:::
::: {.column width="50%"}
```{r, fig.width = 5, fig.height = 3}
# nhanes %>% 
#   summarize(across(totchol, 
#                    .fns = list(mean = mean, SD = sd), 
#                    .names = "Sample {.fn}")) %>%
#   # t() %>%
#   pander()

par(mar = c(4, 4, 1, 1), cex = 1.5)
nhanes %>% pull(totchol) %>% 
  hist(breaks = 30, main = 'sample values', 
       xlab = 'total cholesterol', 
       ylab = 'frequency')
```
:::
:::

(Density is an alternative scale to frequency that is independent of population size.)

## Point estimates

> Sample statistics, viewed as guesses for the values of population statistics, are called 'point estimates'.

We'll focus on inferences involving the following:

| Population statistic     | Parameter | Point estimate |
|--------------------|--------------------|----------------|
| Mean               | $\mu$              | $\bar{x}$      |
| Standard deviation | $\sigma$           | $s_x$          |

## A difficulty

> Different samples yield different estimates.

::: {.columns}

::: {.column width="40%"}
```{r, fig.width=4, fig.height = 5}
set.seed(42024)
samp1 <- sample(totchol, size = 20)
samp2 <- sample(totchol, size = 20)

par(mfrow = c(2, 1), mar = c(4, 4, 3, 1),
    cex = 1.25)

hist(samp1,
     breaks = 3:8,
     xlab = 'total cholesterol',
     main = 'sample 1')
abline(v = mean(samp1), lty = 4, lwd = 2)
hist(samp2,
     breaks = 3:8,
     xlab = 'total cholesterol',
     main = 'sample 2',
     xlim = c(3, 8))
abline(v = mean(samp2), lty = 4, lwd = 2)
```
:::

::: {.column width="60%"}
Sample means:
```{r}
c(sample.1 = mean(samp1),
  sample.2 = mean(samp2)) |> 
  pander()
```

- estimates are close but not identical
- the population mean can't be both `r mean(samp1) |> round(3)` *and* `r mean(samp2) |> round(3)`
- probably neither estimate is exactly correct

*Estimation error and sample-to-sample variability are inherent to point estimation.*
:::

:::


## Simulating sampling variability

```{r, cache = T}
set.seed(12524)
nsamp <- 10000
samps <- tibble(samp.id = 1:nsamp,
                samp = map(samp.id, ~slice_sample(nhanes, n = 20, replace = F)))
```

::: columns
::: {.column width="\"55%"}
```{r, fig.width = 6, fig.height = 7}
samps %>%
  slice_head(n = 20) %>%
  unnest(samp) %>%
  group_by(samp.id) %>%
  mutate(sample = paste('sample', samp.id, sep = ' '),
         mean.totchol = mean(totchol)) |>
  mutate(sample = ordered(sample, levels = paste('sample', 1:20, sep = ' '))) |>
  ggplot(aes(x = totchol, y = ..density..)) +
  facet_wrap(~sample, nrow = 5, ncol = 4) +
  geom_histogram(bins = 10, alpha = 0.5) +
  geom_vline(aes(xintercept = mean.totchol), 
             linetype = 'dotdash') +
  geom_vline(xintercept = mle[1]/mle[2], 
             color = 'red', alpha = 0.5) +
  theme_minimal(base_size = 18) +
  labs(x = 'cholesterol', y = '') +
  geom_function(fun = ~dgamma(.x, shape = mle[1], rate = mle[2]),
                inherit.aes = F,
                color = 'red',
                alpha = 0.5) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank()) +
  scale_x_continuous(breaks = c(2, 4, 6, 8))
```
:::

::: {.column width="45%"}
These are 20 random samples with the sample mean indicated by the dashed line and the population distribution and mean overlaid in red.

-   sample size $n = 20$
-   frequency distributions differ a lot
-   sample means differ some

We can actually measure this variability!

:::
:::

## Simulating sampling variability

If we had means calculated from a much larger number of samples, we could make a frequency distribution for the values of the sample mean.

::: columns
::: {.column width="45%"}
```{r, fig.width=4, fig.height = 3.5}
samp_means <- samps %>%
  mutate(chol.mean = map(samp, ~mean(.x$totchol))) %>%
  unnest(chol.mean) %>%
  pull(chol.mean) 

par(mar = c(4, 4, 3, 1), cex = 1.2)
hist(samp_means,
     breaks = 25, 
     main = 'distribution of 10,000 means', 
     xlab = expr(paste('sample mean ', bar(x))), 
     ylab = 'no. of samples')
# abline(v = mean(totchol),
#        col = 2, lwd = 3, lty = 2)
```

------------ ------- ------- ---------- ---------
 **sample**     1       2     $\cdots$   10,000   

  **mean**    4.957   5.039   $\cdots$   5.24 
------------ ------- ------- ---------- ---------


:::

::: {.column width="55%"}
We could then use the usual measures of center and spread to characterize the distribution of sample means.

-   mean of $\bar{x}$: `r mean(samp_means)`
-   standard deviation of $\bar{x}$: `r sd(samp_means)`

> *Across 10,000 random samples of size 20, the typical sample mean was `r mean(samp_means) |> round(2)` and the root average squared distance of the sample mean from its typical value was `r sd(samp_means) |> round(3)`.*
:::
:::


## Sampling distributions

What we are simulating is known as a **sampling distribution**: the frequency of values of a statistic across all possible random samples.

::: columns
::: {.column width="45%"}
```{r, fig.width=5, fig.height = 3.5}
par(mar = c(4, 4, 1, 1), cex = 1.5)
hist(samp_means,
     freq = T,
     breaks = 25,  
     main = '',
     xlab = expr(paste('sample mean ', bar(x))), 
     ylab = 'no. of samples')
# abline(v = mean(totchol),
#        col = 2, lwd = 3, lty = 2)

curve(0.1*10000*dgamma(x + 0.05, shape = 20*mle[1], rate = 20*mle[2]),
      from = 4, to = 6, add = T, col = 2, lwd = 2)
```

:::
::: {.column width="55%"}



Provided data are from a random sample, the sample mean $\bar{x}$ has a sampling distribution with

- mean $\color{red}{\mu}$ (population mean)
- standard deviation $\color{red}{\frac{\sigma}{\sqrt{n}}}$

regardless of its exact form.

:::

:::

In other words, across all random samples of a fixed size...

1. The average value of the sample mean is the population mean.
2. The average squared error (sample mean - population mean)$^2$ is $\frac{\sigma^2}{n}$ 

## Effect of sample size

The standard deviation of the sampling distribution of $\bar{x}$ is inversely proportional to sample size.

::: {.columns}
::: {.column width="60%"}
```{r, fig.width=8, fig.height = 5}
library(RColorBrewer)
n.vec <- c(5, 10, 20, 40, 80, 160)
reds <- brewer.pal(6, 'Reds')

par(mar = c(4, 4, 1, 1), cex = 1.5)
curve(dgamma(x, shape = n.vec[6]*mle[1], rate = n.vec[6]*mle[2]),
      from = 4, to = 6.5, col = reds[6], n = 500,
      ylab = '', 
      xlab = expr(paste('sample mean ', bar(x))),
      xaxt = 'n',
      yaxt = 'n',
      axes = F)
axis(side = 1, at = seq(4, 6.5, by = 0.5))

for(i in 5:1){
curve(dgamma(x, shape = n.vec[i]*mle[1], rate = n.vec[i]*mle[2]),
      from = 4, to = 6.5, col = reds[i], add = T, n = 500)
}
legend('topright', col = reds, lty = 1, legend = n.vec,
       title = 'sample size')
```
:::

::: {.column width="40%"}

As sample size increases...

- accuracy remains the same
- estimates get more precise
- skewness vanishes

:::
:::

## Measuring sampling variability

In practice $\sigma$ is not known so we use an estimate of sampling variability known as a **standard error**: 
$$
SE(\bar{x}) = \frac{s_x}{\sqrt{n}} 
\qquad \left(\frac{\text{sample SD}}{\sqrt{\text{sample size}}}\right)
$$

For example:

::: columns
::: {.column width="50%"}
```{r, fig.height = 3.5, fig.width = 5}
set.seed(12824)
one_samp <- sample(nhanes$totchol, size = 20)
par(mar = c(5, 5, 3, 1), cex = 1.25)
hist(one_samp,
     breaks = 10, 
     xlab = 'cholesterol',
     ylab = 'frequency', 
     main = 'mean = 5.38, sd = 1.073, n = 20')
```
:::

::: {.column width="50%"}
$$
SE(\bar{x}) = \frac{1.073}{\sqrt{20}} = 0.240
$$

> The root average squared error of the sample mean is estimated to be 0.240 mmol/L.

:::
:::

<!-- ## Interpreting standard errors -->

<!-- The standard error is a point estimate of the (population) standard deviation of sample means across all possible random samples: -->

<!-- $$ -->
<!-- SE(\bar{x}) \text{ estimates } \sqrt{\text{average value of } (\bar{x} - \mu)^2} -->
<!-- $$ -->
<!-- Two phrasings for an interpretation: -->

<!-- 1. Estimated root average squared deviation of the sample mean from the population mean. -->
<!-- 2. Estimated root mean square error. -->

## Reporting point estimates

It is common style to report the value of a point estimate with a standard error given parenthetically.

::: columns
::: {.column width="50%"}
Statistics from full NHANES sample:
```{r, fig.height = 3.5, fig.width = 5}
c(mean = mean(totchol),
  sd = sd(totchol),
  n = length(totchol)) |> 
  pander()
```
:::

::: {.column width="50%"}


> The mean total cholesterol among the population is estimated to be 5.043 mmol/L (SE `r (sd(totchol)/sqrt(length(totchol))) |> round(3)`)

:::
:::

This style of report communicates:

- parameter of interest
- value of point estimate
- error/variability of point estimate

## Interval estimation

> An interval estimate is **a range of plausible values** for a population parameter.

The general form of an interval estimate is: $$\text{point estimate} \pm \text{margin of error}$$

A common interval for the population mean is:
$$\bar{x} \pm 2\times SE(\bar{x}) \qquad\text{where}\quad SE(\bar{x}) = \left(\frac{s_x}{\sqrt{n}}\right)$$

::: {.columns}

::: {.column}
By hand:
$$5.043 \pm 2\times 0.0191 = (5.005, 5.081)$$
:::

::: {.column}
In R:
```{r, echo = T}
avg.totchol <- mean(totchol)
se.totchol <- sd(totchol)/sqrt(length(totchol))
avg.totchol + c(-2, 2)*se.totchol
```
:::

:::

# Lecture 6: Confidence intervals


```{r}
library(tidyverse)
library(oibiostat)
library(pander)
library(RColorBrewer)
reds <- brewer.pal(6, 'Reds')
load('data/nhanes.RData')
cholesterol <- nhanes$totchol
red.grad <- colorRampPalette(c('#ff6459', '#4d0500'))
reds <- red.grad(6)

```

## Today's agenda

- [lecture] $t$ confidence intervals for the mean
- [lab] computing and interpreting confidence intervals

## From last time

::: {.columns}

::: {.column}
Under simple random sampling:

- the sample mean $\bar{x}$ provides a good point estimate of the population mean $\mu$
- its estimated sampling variability is given by the standard error $SE(\bar{x}) = \frac{s_x}{\sqrt{n}} = \frac{\text{sample SD}}{\sqrt{\text{sample size}}}$
<!-- - an interval estimate (range of plausible values) is $\bar{x} \pm 2\times SE(\bar{x})$ -->
::: 

::: {.column}
```{r}
c(mean =  mean(cholesterol),
  sd = sd(cholesterol),
  n = length(cholesterol),
  se = sd(cholesterol)/sqrt(length(cholesterol))) |>
  pander()
mle <- MASS::fitdistr(cholesterol, 'gamma')$est |> round(1)

par(mfrow = c(1, 2),
    mar = c(4, 4, 4, 1),
    cex = 2)
curve(dgamma(x, shape = mle[1], rate = mle[2]), 
      from = 2, to = 14,
      xlab = 'total cholesterol',
      ylab = 'density', axes = F,
      ylim = c(0, 0.45),
      main = 'population model',
      col = reds[3])
axis(side = 1, at = seq(2, 14, by = 2))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
abline(v = mle[1]/mle[2], col = reds[3])
text(x = mle[1]/mle[2] + 0.75, y = 0.425, expr(mu), col = reds[3])

hist(cholesterol, 
     breaks = 20, 
     main = 'NHANES sample', 
     xlab = 'total cholesterol',
     ylab = 'frequency',
     ylim = c(0, 700))
abline(v = mean(cholesterol), lty = 4, lwd = 2)
text(x = mean(cholesterol) + 0.75, y = 650, expr(bar(x)))
```
:::

:::

> The mean total HDL cholesterol among the U.S. adult population is estimated to be 5.043 mmol/L (SE 0.0191).

## Interval estimation

A common interval estimate for the population mean is:
$$\bar{x} \pm 2\times SE(\bar{x}) \qquad\text{where}\quad SE(\bar{x}) = \left(\frac{s_x}{\sqrt{n}}\right)$$

> A range of plausible values for the mean total cholesterol among U.S. adults is 5.005 to 5.081 mmol/L.

Two related questions:

1. What do we mean by "plausible"?
2. Where did the number 2 come from?

## The $t$ model

::: {.columns}

::: {.column}
Consider the statistic:

$$
T = \frac{\bar{x} - \mu}{s_x/\sqrt{n}}
\qquad\left(\frac{\text{estimation error}}{\text{standard error}}\right)
$$ 

The sampling distribution of $T$ is well-approximated by a $t_{n - 1}$ model whenever either:

(a) the population model is symmetric and unimodal

OR

(b) the sample size is not too small
    
:::

::: {.column}

```{r, fig.width=6, fig.height=5, fig.align='center'}
n.vec <- seq(2, 12, by = 2)
par(mar = c(5, 3, 1, 1), cex = 1.5)
curve(dt(x, n.vec[6]), from = -4.5, to = 6, col = reds[6], n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = 't model')
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling frequency', line = -1)
axis(side = 1, at = seq(-4, 6, by = 2))
for(i in 5:1){
  curve(dt(x, n.vec[i]), col = reds[i], n = 500,
      xaxt = 'n', yaxt = 'n', xlab = '', ylab = '', add = T)
}
legend('topright', legend = paste('n = ',  n.vec), col = reds, title = 'sample size', lwd = 2)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(0, 19), 3)`% of samples, $T < 0$

```{r, echo = T}
# area less than 0
pt(0, df = 20 - 1) 
```

- written as $P(T < 0) = 0.5$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 0

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')

legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val, df = 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(1, 19), 3)`% of samples, $T < 1$

```{r, echo = T}
# area less than 1
pt(1, df = 20 - 1) 
```

- written as $P(T < 1) = 0.835$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 1

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val, df = 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(2, 19), 3)`% of samples, $T < 2$

```{r, echo = T}
# area less than 2
pt(2, df = 20 - 1) 
```

- written as $P(T < 2) = 0.97$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val, df = 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(2, 19, lower.tail = F), 3)`% of samples, $T > 2$

```{r, echo = T}
# area greater than 2
pt(2, df = 20 - 1, lower.tail = F) 
```

- notice: 
$$
\begin{align*}
P(T > 2) &= 1 - P(T < 2) \\
(0.03) &= 1 - (0.97)
\end{align*}
$$
:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

# polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val] - 0.001, 0, 0), col = reds[1], border = NA)
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val, df = 19, lower.tail = F), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(2, 19) - pt(1, 19), 3)`% of samples, $1 < T < 2$

```{r, echo = T}
# area between 1 and 2
pt(2, df = 20 - 1) - pt(1, df = 20 - 1) 
```

- notice:
$$
\begin{align*}
P(1 < T < 2) &= P(T < 2) - P(T < 1) \\
(0.135) &= (0.97) - (0.835)
\end{align*}
$$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val.lwr <- 1
t.val.upr <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n))), sep = ''), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(t.val.lwr, x[x<=t.val.upr & x>=t.val.lwr], t.val.upr), 
        c(0, y[(x<=t.val.upr) & (x>=t.val.lwr)] - 0.001, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val.upr, df = 19) - pt(t.val.lwr, 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## $t$ model interpretation

> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.

$$(\text{proportion of area between } a, b) = (\text{proportion of samples where } a < T < b)$$

::: {.columns}

::: {.column}
For example:

- for `r 100*round(pt(2, 19) - pt(-2, 19), 3)`% of samples, $-2 < T < 2$

```{r, echo = T}
# area between 1 and 2
pt(2, df = 20 - 1) - pt(-2, df = 20 - 1) 
```

- written $P(-2 < T < 2) = 0.94$

:::

::: {.column}

```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val.lwr <- -2
t.val.upr <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n))), sep = ''), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(t.val.lwr, x[x<=t.val.upr & x>=t.val.lwr], t.val.upr), 
        c(0, y[(x<=t.val.upr) & (x>=t.val.lwr)] - 0.001, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')
legend('topright', fill = reds[1], 
       legend = paste(round(pt(t.val.upr, df = 19) - pt(t.val.lwr, 19), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::

:::

## A closer look at interval construction

So where did that 2 come from in the margin of error for our interval estimate?

$$
\bar{x} \pm \color{blue}{2}\times SE(\bar{x})
$$

Well:

::: {.columns}

::: {.column width="70%"}

$$
\begin{align*}
0.94 &= P(-\color{blue}{2} < T < \color{blue}{2}) \\
&= P\left(-\color{blue}{2} < \frac{\bar{x} - \mu}{s_x/\sqrt{n}} < \color{blue}{2}\right) \\
&= P(\underbrace{\bar{x} - \color{blue}{2}\times SE(\bar{x}) < \mu < \bar{x} + \color{blue}{2}\times SE(\bar{x})}_{\text{interval covers population mean}})
\end{align*}
$$
:::

::: {.column width="30%"}
> For 94% of all random samples, the interval covers the population mean.
:::

:::

So the number 2 determines the proportion of samples for which the interval covers the mean, known as its **coverage**.


## Effect of sample size

> The sample size determines the exact shape of the $t$ model through its 'degrees of freedom' $n - 1$. This changes the areas slightly.

The exact coverage quickly converges to just over 95% as the sample size increases.

::: {.columns}

::: {.column}

```{r}
tibble(n = 2^seq(2, 8, by = 1)) |>
  mutate(coverage = 1 - 2*pt(2, df = n - 1, lower.tail = F)) |>
  pander()
```

:::

::: {.column}
```{r, fig.width = 5, fig.height = 4}
tibble(n = seq(2, 1000, by = 1)) |>
  mutate(coverage = 1 - 2*pt(2, df = n - 1, lower.tail = F)) |>
  ggplot(aes(x = n, y = coverage)) +
  geom_point() +
  geom_path() +
  scale_x_log10(n.breaks = 6) +
  scale_y_continuous(breaks = c(seq(0.75, 0.9, by = 0.05), 0.9545)) +
  theme_minimal(base_size = 18) +
  theme(panel.grid = element_blank(),
        panel.grid.major.x = element_line(linewidth = 0.1, color = 'black'),
        panel.grid.major.y = element_line(linewidth = 0.1, color = 'black')) +
  labs(x = 'sample size (n)')
```
:::

:::

## Changing the coverage

Consider a slightly more general expression for an interval for the mean:

$$
\bar{x} \pm c\times SE(\bar{x})
$$

The number $c$ is called a **critical value**. It determines the coverage.

- larger $c$ $\longrightarrow$ higher coverage
- smaller $c$ $\longrightarrow$ lower coverage

The so-called "empirical rule" is that:

- $c = 1 \longrightarrow$ approximately 68% coverage
- $c = 2 \longrightarrow$ approximately 95% coverage
- $c = 3 \longrightarrow$ approximately 99.7% coverage

## Interpreting critical values

::: {.columns}

::: {.column}
$$
P(\color{#FF6459}{-2 < T < 2}) = 1 - 2\times P(\color{blue}{T > 2})
$$

Look at how the areas add up so that:
$$
P(\color{blue}{T > 2}) = 0.03
$$
Moreover:
$$
P(T < 2) = 1 - 0.03 = 0.97
$$
:::

::: {.column}
```{r, fig.width=7, fig.height=4.5, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val.lwr <- -2
t.val.upr <- 2

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[20 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - mu, s[x]/sqrt(n))), sep = ''), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

polygon(c(t.val.lwr, x[x<=t.val.upr & x>=t.val.lwr], t.val.upr), 
        c(0, y[(x<=t.val.upr) & (x>=t.val.lwr)] - 0.001, 0), col=reds[1], border = NA)
polygon(c(x[x>=t.val], max(x), t.val.upr), c(y[x>=t.val.upr] - 0.001, 0, 0), col = 'blue')
legend('topright', fill = c(reds[1], 'blue'), 
       legend = c(paste(round(pt(t.val.upr, df = 19) - pt(t.val.lwr, 19), 3)*100, "% of samples", sep = ''),
                  "3% of samples"),
       cex = 0.7)
```
:::

:::

*So the critical value 2 is actually the 97th percentile of the sampling distribution of $T$.*

- also called the 0.97 "quantile"
- (percentiles expressed in proportions are called quantiles)

## Exact coverage using $t$ quantiles

To engineer an interval with a specific coverage, use the $p$th quantile where:

$$p = \left[1 - \left(\frac{1 - \text{coverage}}{2}\right)\right]$$
In R:
```{r, echo = T}
# coverage 95% using t quantile
coverage <- 0.95
q.val <- 1 - (1 - coverage)/2
crit.val <- qt(q.val, df = 20 - 1)
crit.val
```

The effect of increasing/decreasing coverage on the quantile is:

- increase coverage $\longrightarrow$ larger quantile $\longrightarrow$ wider interval
- decrease coverage $\longrightarrow$ smaller quantile $\longrightarrow$ narrower interval

## Contrasting coverage with precision

> **Precision** refers to how wide or narrow the interval is.

Precision depends on every component of the margin of error:

- critical value used
- sample size
- variability of values

By contrast, coverage depends only on the critical value used.

## Confidence intervals

Interval estimates constructed to achieve a specified coverage are called "confidence intervals"; the coverage is interpreted and reported as a "confidence level".

::: {.columns}

::: {.column width="60%"}
```{r, echo = T}
# ingredients
cholesterol.mean <- mean(cholesterol)
cholesterol.sd <- sd(cholesterol)
cholesterol.n <- length(cholesterol)
cholesterol.se <- cholesterol.sd/sqrt(cholesterol.n)
crit.val <- qt(1 - (1 - 0.95)/2, df = cholesterol.n - 1)

# interval
cholesterol.mean + c(-1, 1)*crit.val*cholesterol.se
```
:::

::: {.column width="40%"}
> With 95% confidence, the mean total cholesterol among U.S. adults is estimated to be between `r round(cholesterol.mean - crit.val*cholesterol.se, 4)` and `r round(cholesterol.mean + crit.val*cholesterol.se, 4)` mmol/L.

:::

:::

The general formula for a confidence interval for the population mean is

$$
\bar{x} \pm c\times SE(\bar{x})
$$

where $c$ is a critical value, obtained as a quantile of the $t_{n - 1}$ model and chosen to ensure a specific coverage.

## Recap

The "common" interval estimate for the mean is actually an approximate 95% confidence interval:

$$
\bar{x} \pm 2 \times SE(\bar{x})
$$

- captures the population mean $\mu$ for roughly 95% of random samples
- replacing 2 with a $t_{n - 1}$ quantile allows the analyst to adjust coverage
- the $t_{n - 1}$ model is an approximation for the sampling distribution of $\frac{\bar{x} - \mu}{SE(\bar{x})}$

    + approximation improves with increasing sample size or symmetry
    + usually good quality except in "extreme" situations

Interval interpretation:

> With [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units].

# Extras

## Simulation of coverage

```{r}
nsim <- 200
n <- 50
crit.val <- qt(0.975, df = n - 1)
set.seed(42324)
intervals <- tibble(sim = 1:nsim, 
                    pop.mean = mean(cholesterol)) %>%
  mutate(data = map(sim, ~sample(cholesterol, size = n)),
         xbar = map(data, mean),
         se = map(data, ~sd(.x)/sqrt(length(.x)))) %>%
  unnest(cols = c(xbar, se)) %>%
  mutate(lwr = xbar - crit.val*se, 
         upr = xbar + crit.val*se,
         coverage = factor((lwr < pop.mean)*(pop.mean < upr),
                           labels = c('misses', 'covers')))
coverage.prop <- intervals |> pull(coverage) |> table() |> proportions()
```

::: {.columns}

::: {.column width="65%"}
Artificially simulating a large number of intervals provides an empirical approximation of coverage. 

- at right, `r nsim` intervals
- `r round(coverage.prop[2], 4)*100`% cover the population mean (vertical dashed line)
- pretty close to nominal coverage level 95%

This is also a handy way to remember the proper interpretation:

> If I made a lot of intervals from independent samples, 95% of them would 'get it right'.

:::

::: {.column width="35%"}
```{r, fig.height = 7, fig.width=4}
intervals %>%
  ggplot(aes(x = xbar, y = sim, color = coverage)) +
  geom_point() +
  geom_linerange(aes(xmin = lwr, 
                     xmax = upr)) +
  geom_vline(xintercept = mean(cholesterol),
             linetype = 'dashed') +
  theme_minimal(base_size = 20) +
  scale_x_continuous(breaks = round(mean(cholesterol) + sd(cholesterol)*c(-3, 0, 3)/sqrt(50), 2)) +
  theme(panel.grid = element_blank(),
        panel.grid.major.x = element_line(color = 'black', linewidth = 0.2),
        axis.text.y = element_blank(),
        legend.position = 'top') +
  labs(y = '', x = '') +
  guides(color = guide_legend(title = NULL))
```
:::

:::

# Lecture 7: Hypothesis testing


```{r}
library(tidyverse)
library(pander)
library(RColorBrewer)
load('data/temps.RData')
red.grad <- colorRampPalette(c('#ff6459', '#4d0500'))
reds <- red.grad(6)
```

## Today's agenda

1. Loose end: working backwards to determine interval coverage
2. [lecture] the $t$-test for a population mean
3. [lab] computing test statistics, critical values, and $p$-values

## Body temperatures

::: columns
::: {.column width="45%"}
```{r, fig.width = 4, fig.height = 3.6}
par(mar = c(4, 4, 1, 1), cex = 1.5)
hist(temps$body.temp, breaks = 8, ylim = c(0, 11.1),
     xlab = 'body temperature (°F)', ylab = 'frequency', main = '')
abline(v = mean(temps$body.temp), lty = 4)
text(x = mean(temps$body.temp) + 1.4, y = 11, expr(paste(bar(x), ' = 98.405')))

temps |>
  summarize(mean = mean(body.temp),
            sd = sd(body.temp),
            n = n(),
            se = sd/sqrt(n)) |>
  pander()
```
:::

::: {.column width="55%"}
*Is the true mean body temperature actually 98.6°F?*

Seems plausible given our data.

But what if the sample mean were instead...

| $\bar{x}$ | consistent with $\mu = 98.6$? |
|-----------|-------------------------------|
| 98.30     | probably still yes            |
| 98.15     | maybe                         |
| 98.00     | hesitating                    |
| 97.85     | skeptical                     |
| 97.40     | unlikely                      |
:::
:::

> If the estimation error is "big enough" the hypothesis seems
> implausible.

## How much error is too much?

Consider how many standard errors away from the hypothesized value we'd be:

| $\bar{x}$ | estimation error | no. SE's | interpretation           |
|-----------|------------------|----------|--------------------------|
| 98.30     | -0.3             | 2        | double the average error |
| 98.15     | -0.45            | 3        | triple the average error |
| 98.00     | -0.6             | 4        | quadruple                |
| 97.85     | -0.75            | 5        | quintuple                |
| 97.40     | -1.2             | 8        | octuple!                 |

We know from discussing confidence intervals that we'd estimate the mean
temperature to be within about 2SE of the sample mean, and from interval
coverage that:

-   an error less than 2SE occurs for about 95% of samples
-   an error greater than 2SE occurs for only about 5% of samples

*Exactly how often would we see the error we did if the population mean
is in fact 98.6°F?*

## Applying the $t$ model

::: columns
::: column
***If the population mean is in fact 98.6°F*** then $$
T = \frac{\bar{x} - 98.6}{s_x/\sqrt{n}}
\qquad\left(\frac{\text{estimation error}}{\text{standard error}}\right)
$$ has a sampling distribution that is well-approximated by a
$t_{39 - 1}$ model.
:::

::: column
```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 0

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[39 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - 98.6, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))

# polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')

# legend('topright', fill = reds[1], 
#        legend = paste(round(pt(t.val, df = 19), 3)*100, "% of samples", sep = ''),
#        cex = 0.7)
```
:::
:::

## Applying the $t$ model

```{r}
tstat <- (mean(temps$body.temp) - 98.6)/(sd(temps$body.temp)/sqrt(nrow(temps)))
```

::: columns
::: column
***If the population mean is in fact 98.6°F*** then $$
T = \frac{\bar{x} - 98.6}{s_x/\sqrt{n}}
\qquad\left(\frac{\text{estimation error}}{\text{standard error}}\right)
$$ has a sampling distribution that is well-approximated by a
$t_{39 - 1}$ model.

-   actual summary statistics give $T$ = `r round(tstat, 3)`
:::

::: column
```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 0

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[39 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - 98.6, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
abline(v = tstat, lty = 4, lwd = 2)
text(tstat - 1, 0.3, paste("T = ", round(tstat, 3)))

# polygon(c(min(x), x[x<=t.val], t.val), c(y[x<=t.val] - 0.001, 0, 0), col=reds[1], border = NA)
# polygon(c(x[x>=t.val], max(x), t.val), c(y[x>=t.val], 0, 0), col = 'red')

# legend('topright', fill = reds[1], 
#        legend = paste(round(pt(t.val, df = 19), 3)*100, "% of samples", sep = ''),
#        cex = 0.7)
```
:::
:::

## Applying the $t$ model

```{r}
tstat <- (mean(temps$body.temp) - 98.6)/(sd(temps$body.temp)/sqrt(nrow(temps)))
```

::: columns
::: column
***If the population mean is in fact 98.6°F*** then $$
T = \frac{\bar{x} - 98.6}{s_x/\sqrt{n}}
\qquad\left(\frac{\text{estimation error}}{\text{standard error}}\right)
$$ has a sampling distribution that is well-approximated by a
$t_{39 - 1}$ model.

-   actual summary statistics give $T$ = `r round(tstat, 3)`
-   underestimate more in `r round(100*pt(tstat, nrow(temps) - 1), 2)`%
    of samples
:::

::: column
```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 0

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[39 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - 98.6, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
abline(v = tstat, lty = 4, lwd = 2)
text(tstat - 1, 0.3, paste("T = ", round(tstat, 3)))

polygon(c(min(x), x[x<=tstat], tstat), c(y[x<=tstat] - 0.001, 0, 0), 
        col=reds[1], border = NA)
# polygon(c(x[x>=-tstat], max(x), -tstat), c(y[x>=-tstat], 0, 0), 
#         col = reds[1], border = NA)

legend('topright', fill = reds[1],
       legend = paste(round(pt(tstat, df = 38), 3)*100, "% of samples", sep = ''),
       cex = 0.7)
```
:::
:::

## Applying the $t$ model

```{r}
tstat <- (mean(temps$body.temp) - 98.6)/(sd(temps$body.temp)/sqrt(nrow(temps)))
```

::: columns
::: column
***If the population mean is in fact 98.6°F*** then $$
T = \frac{\bar{x} - 98.6}{s_x/\sqrt{n}}
\qquad\left(\frac{\text{estimation error}}{\text{standard error}}\right)
$$ has a sampling distribution that is well-approximated by a
$t_{39 - 1}$ model.

-   actual summary statistics give $T$ = `r round(tstat, 3)`
-   underestimate more in `r round(100*pt(tstat, nrow(temps) - 1), 2)`%
    of samples
-   overestimate more in `r round(100*pt(tstat, nrow(temps) - 1), 2)`%
    of samples
:::

::: column
```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 0

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[39 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - 98.6, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
abline(v = tstat, lty = 4, lwd = 2)
text(tstat - 1, 0.3, paste("T = ", round(tstat, 3)))

polygon(c(min(x), x[x<=tstat], tstat), c(y[x<=tstat] - 0.001, 0, 0), 
        col=reds[1], border = NA)
polygon(c(x[x>=-tstat], max(x), -tstat), c(y[x>=-tstat], 0, 0),
        col = reds[3], border = NA)

legend('topright', fill = reds[c(1, 3)],
       legend = c(paste(round(pt(tstat, df = 38), 3)*100, "% of samples", sep = ''),
                  paste(round(pt(tstat, df = 38), 3)*100, "% of samples", sep = '')),
       cex = 0.7)
```
:::
:::

## Applying the $t$ model

```{r}
tstat <- (mean(temps$body.temp) - 98.6)/(sd(temps$body.temp)/sqrt(nrow(temps)))
```

::: columns
::: column
***If the population mean is in fact 98.6°F*** then $$
T = \frac{\bar{x} - 98.6}{s_x/\sqrt{n}}
\qquad\left(\frac{\text{estimation error}}{\text{standard error}}\right)
$$ has a sampling distribution that is well-approximated by a
$t_{39 - 1}$ model.

-   actual summary statistics give $T$ = `r round(tstat, 3)`
-   underestimate more in `r round(100*pt(tstat, nrow(temps) - 1), 2)`%
    of samples
-   overestimate more in `r round(100*pt(tstat, nrow(temps) - 1), 2)`%
    of samples
:::

::: column
```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 0

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[39 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - 98.6, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
abline(v = tstat, lty = 4, lwd = 2)
text(tstat - 1, 0.3, paste("T = ", round(tstat, 3)))

polygon(c(min(x), x[x<=tstat], tstat), c(y[x<=tstat] - 0.001, 0, 0), 
        col=reds[1], border = NA)
polygon(c(x[x>=-tstat], max(x), -tstat), c(y[x>=-tstat], 0, 0),
        col = reds[3], border = NA)

legend('topright', fill = reds[c(1, 3)],
       legend = c(paste(round(pt(tstat, df = 38), 3)*100, "% of samples", sep = ''),
                  paste(round(pt(tstat, df = 38), 3)*100, "% of samples", sep = '')),
       cex = 0.7)
```

$$P(|T| > 1.328) = 0.192$$
:::
:::

> We'd see at least as much (absolute) estimation error
> `r round(100*2*pt(tstat, nrow(temps) - 1), 2)`% of the time, assuming
> the hypothesis is true. So this amount of error isn't surprising.

## A more extreme scenario

```{r}
tstat <- (98.2-98.6)/(sd(temps$body.temp)/sqrt(nrow(temps)))
```

::: columns
::: column

***If the population mean is in fact 98.6°F*** then $$
T = \frac{\bar{x} - 98.6}{s_x/\sqrt{n}}
\qquad\left(\frac{\text{estimation error}}{\text{standard error}}\right)
$$ has a sampling distribution that is well-approximated by a
$t_{39 - 1}$ model.

- suppose instead $\bar{x} = 98.2$ so $T$ = `r round(tstat, 3)`
-   underestimate more in `r round(100*pt(tstat, nrow(temps) - 1), 2)`%
    of samples
-   overestimate more in `r round(100*pt(tstat, nrow(temps) - 1), 2)`%
    of samples
:::

::: column
```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 19)
t.val <- 0

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 20), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[39 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - 98.6, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
abline(v = tstat, lty = 4, lwd = 2)
text(tstat + 1, 0.32, paste("T = ", round(tstat, 3)))

polygon(c(min(x), x[x<=tstat], tstat), c(y[x<=tstat] - 0.001, 0, 0), 
        col=reds[1], border = NA)
polygon(c(x[x>=-tstat], max(x), -tstat), c(y[x>=-tstat], 0, 0),
        col = reds[3], border = NA)

legend('topright', fill = reds[c(1, 3)],
       legend = c(paste(round(pt(tstat, df = 38), 4)*100, "% of samples", sep = ''),
                  paste(round(pt(tstat, df = 38), 4)*100, "% of samples", sep = '')),
       cex = 0.7)
```

$$P(|T| > 2.726) = 0.0096$$
:::
:::

> We'd see at least as much estimation error only
> `r round(100*2*pt(tstat, nrow(temps) - 1), 2)`% of the time. So if the
> hypothesis were true, this sample would be really unusual.

## Evaluating the hypothesis

To evaluate the hypothesis that $\mu = 98.6$, we assume it is true and then consider whether the estimation error would be unusually large purely by chance according to the $t$ model:

-   unusually large error $\longrightarrow$ hypothesis is implausible
    $\longrightarrow$ can be rejected
-   not unusually large error $\longrightarrow$ hypothesis is plausible
    $\longrightarrow$ can't be rejected

We just made these assessments:

```{r}
temps |>
  summarize(sample.mean = mean(body.temp),
            se = sd(body.temp)/sqrt(length(body.temp))) |>
  bind_rows(c(sample.mean = 98.2, se = NA)) |>
  fill(se) |>
  mutate(t.stat = (sample.mean - 98.6)/se,
         how.often = 2*pt(t.stat, 38),
         evaluation = cut(how.often, breaks = c(0, 0.05, 1), labels = c('unusual', 'not unusual'))) |>
  pander()
```

*Seems reasonable, but why exactly isn't 19.2% of the time 'unusual'?*

## Decisions, decisions

> What would happen if we decided that $T = -1.328$ was unusual?

```{r}
temps |>
  summarize(sample.mean = mean(body.temp),
            se = sd(body.temp)/sqrt(length(body.temp))) |>
  bind_rows(c(sample.mean = 98.2, se = NA)) |>
  fill(se) |>
  mutate(t.stat = (sample.mean - 98.6)/se,
         how.often = 2*pt(t.stat, 38)) |>
  pander()
```

Suppose we drew a line at 20%. Then if in fact $\mu = 98.6$:

- errors in the 'reject' regime occur by chance 20% of the time
- so we'll reach the wrong conclusion for 1 in 5 samples

This error rate is too high.

## Formalizing a test for the mean

A **statistical hypothesis** is a statement about a population
parameter. For every hypothesis there is an opposing or "alternative"
hypothesis.

A **hypothesis test** is a procedure for deciding between a hypothesis
and its alternative.

::: columns
::: {.column width="60%"}
We just tested the hypotheses: 

$$
\begin{cases}
H_0: &\mu = 98.6 \quad(\text{"null" hypothesis}) \\
H_A: &\mu \neq 98.6 \quad(\text{"alternative" hypothesis})
\end{cases}
$$

Our decision was based on the "test statistic":

$$
T = \frac{\bar{x} - 98.6}{SE(\bar{x})}
$$

If $H_0$ is true, the sampling distribution of $T$ is well-approximated
by a $t_{n-1}$ model.
:::

::: {.column width="40%"}
We reject $H_0$ if it entails that the estimation error is unusually large relative to the standard error.

- 'unusual' determined by considering error rate
- two equivalent approaches: 

    1. critical values 
    2. $p$-values
:::
:::

## The critical value approach

> Reject $H_0$ if $|T|$ exceeds the $1 - \frac{\alpha}{2}$ quantile of the $t_{n - 1}$ model

::: {.columns}
::: {.column width="45%"}
Steps:

1. Decide on an error tolerance $\alpha$.
2. Find the $1 - \frac{\alpha}{2}$ quantile $q$.
3. Reject if $|T| > q$.

:::

::: {.column width="55%"}
```{r}
temp.mean <- mean(temps$body.temp)
temp.mean.se <- sd(temps$body.temp)/sqrt(nrow(temps))
```

```{r, echo = T}
# compute test statistic
tstat <- (temp.mean - 98.6)/temp.mean.se
tstat

# compute critical value for a 5% error tolerance
crit.val <- qt(p = 0.975, df = 38)
crit.val

# compare
abs(tstat) > crit.val 
```

:::

:::

Rationale: if $H_0$ is true...

- $|T|$ will be smaller than the quantile for $(1 - \alpha)\times 100$% of samples
- so using this rule you'll only make a mistake $\alpha\times 100$% of the time


## The $p$-value approach

> Reject $H_0$ if $T$ exceeds the observed value for less than $\alpha\times 100$% of samples: $$2\times P(T > |T_\text{obs}|) < \alpha$$

::: {.columns}

::: {.column width="45%"}
Steps:

1. Decide on an error tolerance $\alpha$.
2. Compute the proportion $p$ of samples for which $T$ exceeds observed value.
3. Reject if $p < \alpha$.
:::

::: {.column width="55%"}
```{r, echo = T}
# compute test statistic
tstat <- (temp.mean - 98.6)/temp.mean.se
tstat

# proportion of samples where T exceeds observed value
p.val <- 2*pt(abs(tstat), df = 38, lower.tail = F)
p.val

# decision with error rate controlled at 5%
p.val < 0.05
```
:::

:::

Rationale: 

- $p$-value conveys exactly how unusual the test statistic is
- $p < \alpha$ exactly when $|T| > q$, so this rule controls the error rate at $\alpha$

## Test outcomes

There are two possible findings for a test:

-   \[crosses decision threshold\] reject $H_0$ in favor of $H_A$
-   \[doesn't cross decision threshold\] fail to reject $H_0$ in favor of $H_A$

A **reject** decision is interpreted as:

> The data provide evidence that... \[against $H_0$/favoring $H_A$\]

A **fail to reject** decision is interpreted as:

> The data *do not* provide evidence that... \[against $H_0$/favoring
> $H_A$\]

## Interpreting results

::: columns
::: {.column width="55%"}
Calculations in R:

```{r, echo = T}
# compute test statistic
tstat <- (temp.mean - 98.6)/temp.mean.se
tstat

# compute critical value for a 5% error tolerance
crit.val <- qt(p = 0.975, df = 38)
crit.val

# test decision
abs(tstat) > crit.val

# p-value
2*pt(abs(tstat), df = 38, lower.tail = F)
```
:::

::: {.column width="45%"}
Conventional narrative summary style:

> The data do not provide evidence that the mean body temperature differs from 98.6°F (*T* = -1.328 on 38 degrees of freedom, *p* = 0.192).

Conveys a lot of info succinctly:

- test conclusion
- hypotheses tested
- number of standard errors from hypothesized value ($T$)
- sample size (degrees of freedom + 1)
- strength of evidence ($p$-value)
:::
:::

<!-- ## Significance conventions -->

<!-- ::: {.columns} -->

<!-- ::: {.column width="43%"} -->

<!-- **Convention 1:** statistical significance -->

<!-- - $p < 0.05$: reject $H_0$ -->

<!-- - $p \geq 0.05$: fail to reject $H_0$ -->

<!-- > "The data provide **significant evidence at level $\alpha$ = 0.05** against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (*T* = -5.4548 on 129 degrees of freedom, *p* = .0000002411)." -->

<!-- ::: -->

<!-- ::: {.column width="57%"} -->

<!-- **Convention 2:** weight of evidence against $H_0$ -->

<!-- - $p < 0.01$: strong evidence -->

<!-- - $0.01 \leq p < 0.05$: moderate evidence -->

<!-- - $0.05 \leq p < 0.1$: weak evidence  -->

<!-- - $0.1 \leq p$: no evidence -->

<!-- > "The data **provide strong evidence** against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (*T* = -5.4548 on 129 degrees of freedom, *p* = .0000002411)." -->

<!-- ::: -->

<!-- ::: -->

<!-- You may use either convention to interpret test results. -->

## Components of a test

| Component              | Explanation                                                                  | Example                                                     |
|----------------|--------------------------------|-------------------------|
| Population parameter   | The quantity of interest                                                     | Mean body temp $\mu$                                        |
| Null hypothesis        | The claim to be tested                                                       | $\mu = 98.6$                                                |
| Alternative hypothesis | The alternative claim                                                        | $\mu \neq 98.6$                                             |
| Test statistic         | A function of the sample data and the hypothetical parameter value | $T = \frac{\bar{x} - 98.6}{s_x/\sqrt{n}} = -1.328$          |
| Model                  | Sampling distribution of the test statistic under $H_0$                      | $t_{38}$ model                                              |
| $p$-value              | Probability under $H_0$ of obtaining a result at least as favorable to $H_A$ | 19.2% of samples produce a test statistic at least as large |
| Decision               | Reject or fail to reject $H_0$ in favor of $H_A$                             | Fail to reject                                              |

# Lecture 8: Directional alternatives


```{r}
library(tidyverse)
library(oibiostat)
library(Sleuth3)
library(pander)
load('data/nhanes.RData')
load('data/temps.RData')
body_temps <- temps$body.temp
ddt <- MASS::DDT
red.grad <- colorRampPalette(c('#ff6459', '#4d0500'))
blue.grad <- colorRampPalette(c('#6880f7', '#0a1a6e'))
reds <- red.grad(6)
blues <- blue.grad(6)
```

## Today's agenda

1. Quick review of decision criteria for the $t$ test
2. [lecture] test-interval relationship; directional $t$ tests
3. [lab] upper-sided, lower-sided, and two-sided tests for the population mean

## Recap: decision criteria

> A hypothesis test boils down to deciding whether your estimate is too far from a hypothetical value for that hypothesis to be plausible.

::: {.columns}

::: {.column width="55%"}
To test the hypotheses:

$$
\begin{cases}
H_0: &\mu = \mu_0 \\
H_A: &\mu \neq \mu_0
\end{cases}
$$

We use the test statistic:

$$
T = \frac{\bar{x} - \mu_0}{SE(\bar{x})}
\quad\left(\frac{\text{estimation error under } H_0}{\text{standard error}}\right)
$$
:::

::: {.column width="40%"}
We say $H_0$ is implausible at level $\alpha$ if either:

- $|T| > q$ for the $\alpha$-critical value $q$

    + $q$ is the $1 - \frac{\alpha}{2}$ quantile of the $t_{n - 1}$ model

- $\underbrace{P(|T| > |T_\text{observed}|)}_\text{p-value} < \alpha$
:::

:::

This procedure controls the error rate $\alpha$: the proportion of samples for which we'd make a false rejection.

## From last time

*Practice problem: test the hypothesis that the average U.S. adult sleeps 8 hours.*

::: {.columns}

::: {.column width="55%"}

```{r}
# nhanes data
load('data/nhanes.RData')
```

```{r, fig.width = 3.5, fig.height = 1.75}
# assess assumptions
sleep <- nhanes$sleephrsnight
par(mar = c(4, 4, 0, 0))
hist(sleep, main = '', breaks = 10)
```

```{r, echo = T}
# calculations
sleep.mean <- mean(sleep) 
sleep.mean.se <- sd(sleep)/sqrt(length(sleep))
tstat <- (sleep.mean - 8)/sleep.mean.se 
crit.val <- qt(0.975, df = 3178) 
p.val <- 2*pt(abs(tstat), df = 3178, lower.tail = F)
ci <- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se
```

```{r}
c(estimate = sleep.mean,
  std.err = sleep.mean.se,
  tstat = tstat, 
  cval = crit.val, 
  pval = p.val) |>
  pander()
```

95% confidence interval: (`r round(ci, 2)`)
:::

::: {.column width="40%"}
A complete narrative summary:

> The data provide evidence that the average U.S. adult does not sleep 8 hours per night (*T* = -42.53 on 3178 degrees of freedom, *p* < 0.0001). With 95% confidence, the mean nightly hours of sleep among U.S. adults is estimated to be between 6.91 and 7.01 hours, with a point estimate of 6.59 hours (SE: 0.0245).
:::

:::

## Tests and intervals

> Tests and intervals are usually reported together

Consider how the test and interval provide complementary information:

- the test tells you U.S. adults don't sleep 8 hours
- the interval tells you how much they *do* sleep

It is reasonable that they should be consistent, and in fact they are.

::: {.columns}

::: {.column width="55%"}
```{r, echo = T}
# calculations
sleep.mean <- mean(sleep) 
sleep.mean.se <- sd(sleep)/sqrt(length(sleep))
tstat <- (sleep.mean - 8)/sleep.mean.se 
crit.val <- qt(0.975, df = 3178) 
p.val <- 2*pt(abs(tstat), df = 3178, lower.tail = F)
ci <- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se
```
:::

::: {.column width="45%"}
> Notice that the critical value in the 5% significance level test is exactly the same as that used in the 95% confidence interval.
:::

:::

## Tests and intervals

The critical value used in a $\alpha$ significance level test is identical to the critical value used in a $(1 - \alpha)$ interval. Consequently:

$$
\underbrace{\bar{x} - c\times SE(\bar{x}) < \mu_0 < \bar{x} + c\times SE(\bar{x})}_\text{hypothesized value is in the interval}
\quad\Longleftrightarrow\quad
\underbrace{-c < \frac{\bar{x} - \mu_{0}}{SE(\bar{x})} < c}_{|T| < c}
$$

Meaning: the interval includes exactly those values that the test fails to reject.

- Sensible considering both use the same information: the distance between the point estimate and population mean, relative to the variability of the estimate

## Tests and intervals

> The level-$\alpha$ test rejects $H_0: \mu = \mu_0$ exactly when $\mu_0$ is outside the $(1 - \alpha)\times 100$% confidence interval for $\mu$.


::: {.columns}
::: {.column}
```{r, fig.width=6, fig.height = 5}
mu_grid <- seq(6.85, 7.05, length = 100)
ci.95 <- t.test(sleep, conf.level = 0.95)$conf[1:2]
ci.99 <- t.test(sleep, conf.level = 0.99)$conf[1:2]

tibble(mu = mu_grid) |>
  mutate(p.value = map(mu, \(x) t.test(sleep, mu = x)$p.value)) |>
  unnest(p.value) |>
  ggplot(aes(x = mu, y = p.value)) +
  geom_path() +
  geom_hline(yintercept = 0.05, linetype = 'dashed', color = reds[1]) +
  geom_ribbon(aes(xmin = ci.95[1], xmax = ci.95[2]),
              fill = reds[4],
              alpha = 0.3) +
  geom_hline(yintercept = 0.01, linetype = 'dashed', color = blues[1]) +
  geom_ribbon(aes(xmin = ci.99[1], xmax = ci.99[2]),
              fill = blues[4],
              alpha = 0.2) +
  theme_minimal(base_size = 20) +
  labs(x = expr(mu[0]), y = 'p value') +
  scale_y_sqrt() +
  annotate('text', x = 7.04, y = 0.07, 
           size = 5, label = expr(paste(alpha, " = 0.05")), color = reds[1]) +
  annotate('text', y = 0.92, x = 6.93, label = '95% CI', color = reds[3], size = 5) +
  annotate('text', x = 7.04, y = 0.02, 
           size = 5, label = expr(paste(alpha, " = 0.01")), color = blues[1]) +
  annotate('text', y = 0.7, x = 6.912, label = '99% CI', color = blues[3], size = 5)
```
:::

::: {.column}
Left, $p$-values for a sequence of tests:

- $p > 0.05$ precisely for $\mu_0$ in 95% CI
- $p > 0.01$ precisely for $\mu_0$ in 99% CI

In other words:

$$\text{level $\alpha$ test rejects} \Longleftrightarrow \text{$1 - \alpha$ CI excludes}$$

::: 

:::


## The `t.test(...)` function

Since tests and intervals go together, there is a single R function that computes both.

::: {.columns}

::: {.column width="50%"}
```{r, echo = T}
t.test(sleep, mu = 8, conf.level = 0.95)
```

::: 

::: {.column width="50%"}
No critical value is reported, so you have to make the decision using the $p$ value:

- $p < \alpha$: reject
- $p > \alpha$: fail to reject
:::

:::

Take a moment to locate each component of the test and estimates from the output.

## Interpreting $p$-values

> $p$-values measure the strength of evidence against $H_0$ and favoring $H_A$: smaller $p$-values indicate stronger evidence; larger $p$-values indicate weaker evidence.

::: {.columns}

::: {.column width="55%"}
The mathematical definition is:
$$p = P(|T| > |T_\text{observed}|)$$ 

- technically, the probability under $H_0$ that $T$ exceeds the observed value in magnitude
- informally, how unusual/rare your data are

:::

::: {.column width="45%"}
As a measure of the strength of evidence favoring the alternative:

value | strength of evidence
---|---
$p < 0.001$ | very strong
$0.001 < p < 0.01$ | strong
$0.01 < p < 0.05$ | moderate
$0.05 < p < 0.1$ | suggestive
$0.1 < p$ | no evidence
:::

:::

## Your turn: interpret these $p$-values

> Don't just match the value to the table; add context, and state the test outcome.

::: {.columns}
::: {.column}
```{r, echo = T}
# example 1
t.test(sleep, mu = 6.8)$p.value

# example 2
t.test(sleep, mu = 6.9)$p.value

# example 3
t.test(sleep, mu = 7)$p.value

# example 4
t.test(sleep, mu = 7.1)$p.value

# example 5
t.test(sleep, mu = 7.2)$p.value

```
:::

::: {.column}
value | strength of evidence
---|---
$p < 0.001$ | very strong
$0.001 < p < 0.01$ | strong
$0.01 < p < 0.05$ | moderate
$0.05 < p < 0.1$ | suggestive
$0.1 < p$ | no evidence
:::

:::

Example: "**the data** [DO/DO NOT] **provide** [STRENGTH] **evidence that** [ALTERNATIVE]"

## A directional test

> Does the average U.S. adult sleep *less than* 7 hours?

::: {.columns}

::: {.column width="55%"}
This example leads to a *directional* test:

$$
\begin{cases}
H_0: &\mu = 7 \\
H_A: &\mu < 7
\end{cases}
$$

The test statistic is the same as before:

$$
T = \frac{\bar{x} - 7}{SE(\bar{x})} = -1.671
$$
:::

::: {.column width="45%"}
The *lower-sided* $p$-value is `r t.test(sleep, mu = 7, alternative = 'less')$p.value |> round(4)`:

```{r, fig.width=6.5, fig.height=4.25, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 3178)
tstat <- t.test(sleep, mu = 7)$stat

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 3178), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[3179 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - 7, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
abline(v = tstat, lty = 4, lwd = 2)
text(tstat - 1, 0.3, paste("T = ", round(tstat, 3)))

polygon(c(min(x), x[x<=tstat], tstat), c(y[x<=tstat] - 0.001, 0, 0), 
        col=reds[1], border = NA)

legend('topright', fill = reds[c(1, 3)],
       legend = c(paste(round(pt(tstat, df = 3178), 3)*100, "% of samples", sep = '')),
       cex = 0.7)
```

:::

:::

For the $p$-value, we look at how often $T$ is larger *in the direction of the alternative*.

- in this case, how often $T$ is smaller (underestimate by more)

## The other direction

> Does the average U.S. adult sleep ***more** than* 7 hours?

::: {.columns}

::: {.column width="55%"}
Now the alternative is the opposite direction:

$$
\begin{cases}
H_0: &\mu = 7 \\
H_A: &\mu > 7
\end{cases}
$$

The test statistic is the same as before:

$$
T = \frac{\bar{x} - 7}{SE(\bar{x})} = -1.671
$$
:::

::: {.column width="45%"}
The ***upper**-sided* $p$-value is `r t.test(sleep, mu = 7, alternative = 'greater')$p.value |> round(4)`:

```{r, fig.width=6.5, fig.height=4.25, fig.align='center'}
x <- seq(-3.5, 3.5, length = 10000)
y <- dt(x, df = 3178)
tstat <- t.test(sleep, mu = 7)$stat

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 3178), from = -3.5, to = 3.5, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[3179 - 1], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - 7, s[x]/sqrt(n)))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-3, 3, by = 1))
axis(side = 2, at = seq(0, 0.4, by = 0.1))
abline(v = tstat, lty = 4, lwd = 2)
text(tstat - 1, 0.3, paste("T = ", round(tstat, 3)))

polygon(c(x[x>=tstat], max(x), tstat), c(y[x>=tstat], 0, 0),
        col = reds[3], border = NA)

legend('topright', fill = reds[c(3)],
       legend = c(paste(round(pt(tstat, df = 3178, lower.tail = F), 3)*100, "% of samples", sep = '')),
       cex = 0.7)
```

:::

:::

For the $p$-value, we look at how often $T$ is larger *in the direction of the alternative*.

- in this case, how often $T$ is **larger** (**over**estimate by more)

## Directional hypotheses

Tests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.

| Test type   | Alternative      | Direction favoring alternative |
|-------------|------------------|--------------------|
| Upper-sided | $\mu > \mu_0$    | larger $T$       |
| Lower-sided | $\mu < \mu_0$    | smaller $T$       |
| Two-sided   | $\mu \neq \mu_0$ | larger $|T|$        |

The direction of the test affects the $p$-value calculation (and thus decision), but *won't* change the test statistic.

Conceptually tricky, but easy in R:

```{r, echo = T, eval = F}
# upper-sided
t.test(ddt, mu = mu_0, alternative = 'greater')

# lower-sided
t.test(ddt, mu = mu_0, alternative = 'less')

# two-sided (default)
t.test(ddt, mu = mu_0, alternative = 'two.sided')
```


## Three $t$-tests

::: {.columns}

::: {.column width="33%"}
Do U.S. adults sleep 7 hours per night?

```{r, echo = T}
# two sided test
t.test(sleep, 
       mu = 7)
```

> Suggestive but insufficient evidence that U.S. adults don't sleep 7 hours

:::

::: {.column width="33%"}
Do U.S. adults sleep less than 7 hours per night?
```{r, echo = T}
# lower-sided test
t.test(sleep, 
       mu = 7, 
       alternative = 'less')
```

> Moderate evidence that U.S. adults sleep less than 7 hours

:::

::: {.column width="34%"}
Do U.S. adults sleep more than 7 hours per night?
```{r, echo = T}
# upper-sided test
t.test(sleep, 
       mu = 7, 
       alternative = 'greater')
```

> No evidence that U.S. adults sleep more than 7 hours
:::

:::


## Another example: DDT data

The following are 15 measurements of the pesticide DDT in kale in parts per million (ppm).

```{r}
pander(ddt)
```

> C. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.

Imagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.

$$
\begin{cases}
H_0: &\mu = 3 \quad(\text{null hypothesis})\\
H_A: &\mu > 3 \quad(\text{alternative hypothesis})
\end{cases}
$$

*We choose this direction because we're concerned with evidence that mean DDT **exceeds** the threshold.* 

## Another example: DDT data

::: columns
::: {.column width="45%"}
**If in fact $\mu = 3$**, then according to the $t$ model 0.58% of samples would produce an error of this magnitude or more in the direction of the alternative:

```{r, fig.width = 6, fig.height = 4.5}
par(mar = c(5, 4, 1, 1),
    cex = 1.5)
x <- seq(-4, 4, length = 10000)
y <- dt(x, df = 14)
tstat <- t.test(ddt, mu = 3)$stat
plot(x, y, type = 'l',
     ylab = 'density', 
     xlab = '',
     main = expr(paste(t[15 - 1], 'model', sep = ' ')),
     xaxt = 'n',
     yaxt = 'n',
     axes = F)
axis(side = 1, at = seq(-4, 4, 2))
title(xlab = expression(paste("T = ", frac(bar(x) - 3, s[x]/sqrt(n)))),
      line = 4)
axis(side = 2, at = seq(0, 0.4, by = 0.1))
# polygon(c(min(x), x[x<=tstat], tstat), c(y[x<=tstat], 0, 0), col=reds[1])
polygon(c(x[x>=tstat], max(x), tstat), c(y[x>=tstat], 0, 0), col = reds[1])
abline(v = tstat, lty = 4, lwd = 2)
legend(x = 'topright', 
       legend = paste(round(1 - pt(tstat, 14), 4)*100, '% of \n samples', sep = ''), 
       fill = reds[1], box.col = NA)
```
:::

::: {.column width="55%"}
```{r, echo = T}
t.test(ddt, mu = 3, alternative = 'greater')
```

> The data provide strong evidence that mean DDT in kale exceeds 3ppm (*T* = 2.9059 on 14 degrees of freedom, *p* = 0.0058). With 95% confidence, the mean DDT is estimated to be at least 3.129, with a point estimate of 3.32 (SE: `r round(sd(ddt)/sqrt(14), 4)`).
:::
:::

*Notice the one-sided interval! (`Inf` = $\infty$.) This is called a "lower confidence bound".*


## Your turn: which alternative?

> Write the hypotheses in notation and identify which test (upper/lower/two sided) should be used.

Using the temperature/heartrate data:

1. Is mean body temperature less than 98.6°F?
2. Is mean heart rate greater than 60 bpm? 
3. Is mean heart rate 65 bpm?

Using the NC births data:

1. Is the mean number of weeks at birth 40?
2. Is the mean birth weight at least 7 lbs?
3. Is the mean birth weight under 8 lbs?

# Lecture 9: Two-sample inference


```{r}
library(tidyverse)
library(oibiostat)
library(pander)
load('data/brfss.RData')
load('data/temps.RData')
data(thermometry)
red.grad <- colorRampPalette(c('#ff6459', '#4d0500'))
blue.grad <- colorRampPalette(c('#6880f7', '#0a1a6e'))
reds <- red.grad(6)
```

## Today's agenda

1. [lecture] two sample inference for means
2. [lab] two-sample $t$ tests in R
3. [test prep] practice problems

## From last time

Practice problem: *test whether actual body weight exceeds desired body weight*.

::: {.columns}

::: {.column width="50%"}
```{r}
brfss |>
  transmute(subject = row_number(),
            actual = weight,
            desired = wtdesire,
            difference = weight - wtdesire) |>
  head(5) |>
  pander()
```
::: 

::: {.column width="50%"}
```{r, echo = T}
weight.diffs <- brfss$weight - brfss$wtdesire
t.test(weight.diffs, 
       mu = 0, 
       alternative = 'greater')
```
:::
:::

> The data provide very strong evidence that the average U.S. adult's actual weight exceeds their desired weight (*T* = 4.2172 on 59 degrees of freedom, *p* < 0.0001).

Inference is on the mean difference: $H_0: \delta = 0$ vs. $H_A: \delta > 0$.

*Can we also do inference on a difference in means?*

## Evolution of Darwin's finches

::: columns
::: {.column width="70%"}
Peter and Rosemary Grant caught and measured birds from more than 20 generations of finches on Daphne Major.

-   severe drought in 1977 limited food to large tough seeds

-   selection pressure favoring larger and stronger beaks

-   hypothesis: beak depth increased in 1978 relative to 1976

:::

::: {.column width="30%"}
```{r}
set.seed(50523)
finch <- Sleuth3::case0201 |> rename_with(tolower) |> sample_n(size = 123)
finch %>%
  group_by(year) %>%
  sample_n(size = 2) %>%
  pander()
```
:::
:::

To answer this, we need to test a hypothesis involving two means:

$$
\begin{cases}
H_0: &\mu_{1976} = \mu_{1978} \\
H_A: &\mu_{1976} < \mu_{1978}
\end{cases}
$$

- can't do inference on a mean difference here (no pairing of observations)
- treat each year as an independent sample

## Two-sample inference

```{r, eval = F}
finch |> count(year)
```

If $x_1, \dots, x_{58}$ are the 1976 observations and $y_1, \dots, y_{65}$ are the 1978 observations:

- $\bar{x}$ is a point estimate for $\mu_{1976}$ with standard error $SE(\bar{x}) = \frac{s_x}{\sqrt{n}}$
- $\bar{y}$ is a point estimate for $\mu_{1978}$ with standard error $SE(\bar{y}) = \frac{s_y}{\sqrt{n}}$

::: {.columns}

::: {.column}

Inference uses a new $T$ statistic:

$$
T = \frac{\bar{x} - \bar{y} - \delta_0}{SE(\bar{x} - \bar{y})}
$$

- $\delta_0$ is the hypothesized difference in means
-   $SE(\bar{x} - \bar{y}) = \sqrt{SE(\bar{x})^2 + SE(\bar{y})^2}$
-   $t_\nu$ model approximates the sampling distribution when each sample meets assumptions for one-sample inference

:::

::: {.column}

```{r, fig.width=6.5, fig.height=5, fig.align='center'}
tt.out <- t.test(depth ~ year, data = finch, alternative = 'less')
df <- tt.out$parameter
x <- seq(-6, 4, length = 10000)
y <- dt(x, df) |> sqrt()
tstat <- tt.out$stat

par(mar = c(5, 4, 2, 1), cex = 1.5)
curve(sqrt(dt(x, df)), from = -6, to = 4, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[111.79], ' model')))
title(xlab = expression(paste("T = ", frac(bar(x) - bar(y), SE(bar(x) - bar(y))))), line = 4)
title(ylab = 'sampling density')
axis(side = 1, at = seq(-6, 4, by = 2))
axis(side = 2, at = sqrt(seq(0, 0.4, by = 0.1)) |> round(2))
abline(v = tstat, lty = 4, lwd = 2)
text(tstat + 1.4, 0.4, paste("T = ", round(tstat, 3)))

polygon(c(min(x), x[x<=tstat], tstat), c(y[x<=tstat] - 0.001, 0, 0), 
        col=reds[1], border = NA)

legend('topleft', fill = reds[c(1, 3)],
       legend = c(paste(format(round(pt(tstat, df = df), 6)*100, scientific = F), "% of samples", sep = '')),
       cex = 0.7)
```

:::

:::

## Checking assumptions

> The two-sample test is appropriate whenever two one-sample tests would be.

::: {.columns}

::: {.column}
In other words, the test assumes that *both* samples are either:

- sufficiently large; or
- have little skew and few outliers

To check, simply inspect each histogram.

- both distributions unimodal
- both a bit left skewed 
- no extreme outliers
- large sample sizes (`r finch |> count(year) |> pull(n)`)
:::

::: {.column}
```{r, fig.height = 5, fig.width = 5}
finch |>
ggplot(aes(x = depth)) +
facet_wrap(~ year, ncol = 1) +
geom_histogram(bins = 10) +
theme_minimal(base_size = 20) +
labs(x = 'beak depth (mm)', y = '')
```
:::

:::


## Checking assumptions (alternative)

> The two-sample test is appropriate whenever two one-sample tests would be.

::: {.columns}

::: {.column}
In other words, the test assumes that *both* samples are either:

- sufficiently large; or
- have little skew and few outliers

Could also check side-by-side boxplots for:

- approximate symmetry of boxes
- outliers far from whiskers

This is also a nice visualization of differences between samples.
:::

::: {.column}
```{r, fig.height = 4, fig.width = 5}
par(mar = c(4, 4, 1, 1), cex = 1.5)
boxplot(depth ~ year, data = finch, horizontal = T,
xlab = 'beak depth (mm)', y = '')
```
:::

:::

## Interpreting outputs and results

::: columns
::: {.column width="45%"}
```{r, echo = T}
t.test(depth ~ year, data = finch,
       mu = 0, alternative = 'less')
```
:::

::: {.column width="55%"}
> The data provide very strong evidence that mean beak depth increased following the drought (*T* = -4.5727 on 111.79 degrees of freedom, *p* < 0.0001). With 95% confidence, the mean increase is estimated to be at least 0.4699 mm, with a point estimate of `r tt.out$estimate |> diff() |> round(4)` (SE `r tt.out$stderr |> round(4)`).
:::
:::

Highly similar, but notice:

- input is a formula `depth ~ year` ("depth depends on year") and data frame `finch`
- `mu` now indicates hypothesized difference in means
- decimal degrees of freedom
- alternative is relative to the order in which groups appear

## Cloud data

> Does seeding clouds with silver iodide increase mean rainfall?

::: columns
::: {.column width="60%"}
Data are rainfall measurements in a target area from 26 days when clouds were seeded and 26 days when clouds were not seeded.

-   `rainfall` gives volume of rainfall in acre-feet
-   `treatment` indicates whether clouds were seeded

Hypotheses to test: 
$$
\begin{cases}
H_0: &\mu_\text{seeded} = \mu_\text{unseeded} \\
H_A: &\mu_\text{seeded} > \mu_\text{unseeded}
\end{cases}
$$
:::

::: {.column width="40%"}
```{r}
cloud <- Sleuth3::case0301 %>% rename_with(tolower) |>
mutate(treatment = tolower(treatment))
cloud %>%
  group_by(treatment) %>%
  sample_n(size = 4) %>%
  pander()
```
:::
:::

## Cloud data: which alternative?

> Does seeding clouds with silver iodide increase mean rainfall?

::: {.columns}

::: {.column}
```{r, echo = T}
t.test(rainfall ~ treatment, data = cloud, 
       mu = 0, alternative = 'less')
```
:::

::: {.column}
```{r, echo = T}
t.test(rainfall ~ treatment, data = cloud, 
       mu = 0, alternative = 'greater')
```
:::

:::

You can tell which group R considers first based on which estimate is printed first.


- `'greater'` is interpreted as [FIRST GROUP] > [SECOND GROUP]
- `'less'` is interpreted as [FIRST GROUP] < [SECOND GROUP]

## Cloud data: interpretation

> Does seeding clouds with silver iodide increase mean rainfall?

::: {.columns}

::: {.column}
```{r, echo = T}
t.test(rainfall ~ treatment, data = cloud, 
       mu = 0, alternative = 'greater')
```
:::

::: {.column}
> The data provide moderate evidence that cloud seeding increases mean rainfall (*T* = 1.9982 on 33.855 degrees of freedom, *p* = 0.02689). With 95% confidence, seeding is estimated to increase mean rainfall by at least 42.63 acre-feet, with a point estimate of `r t.test(rainfall ~ treatment, data = cloud)$estimate |> rev() |> diff() |> round(2)` (SE `r t.test(rainfall~treatment, data = cloud)$stderr |> round(4)`).
:::

:::

## Body temperatures (again)

> Does mean body temperature differ between men and women?

```{r}
tt.out <- t.test(body.temp ~ sex, data = temps, mu = 0, alternative = 'two.sided')
```

::: {.columns}

::: {.column}
```{r, fig.width = 5, fig.height = 3}
par(mar = c(4, 4, 1, 1), cex = 1.25)
boxplot(body.temp ~ sex, data = temps, horizontal = T, 
xlab = 'body temperatue (°F)', ylab = '')
```
:::

::: {.column}
Test $H_0: \mu_F = \mu_M$ against $H_A: \mu_F \neq \mu_M$

```{r, echo = T}
t.test(body.temp ~ sex, data = temps, 
       mu = 0, alternative = 'two.sided')
```
:::

:::

Suggestive but insufficient evidence that mean body temperature differs by sex.

Notice: estimated difference (F - M) is `r tt.out$estimate |> rev() |> diff() |> round(3)` °F (SE `r tt.out$stderr |> round(4)`)

## What if we had more data?

```{r}
temps.aug <- thermometry |>
rename(sex = gender) 
tt.out <- t.test(body.temp ~ sex, data = temps.aug, mu = 0, alternative = 'two.sided')
```

Here are estimates from two larger samples of 65 individuals each (compared with `r count(temps, sex)$n`):
```{r}
temps.aug |>
group_by(sex) |>
summarize(mean.temp = mean(body.temp),
se = sd(body.temp)/sqrt(n()),
n = n()) |>
pander()
```

- estimated difference (F - M) is smaller `r tt.out$estimate |> rev() |> diff() |> round(4)` °F
- but so is the standard error SE `r tt.out$stderr |> round(4)` (recall more data $\longleftrightarrow$ better precision)

::: {.columns}

::: {.column}
```{r, echo = T}
t.test(body.temp ~ sex, data = temps.aug, 
       mu = 0, alternative = 'two.sided')
```
:::

::: {.column}
> The data provide moderate evidence that mean body temperature differs by sex (*T* = 2.29 on 127.51 degrees of freedom, *p* = 0.02394).
:::

:::

## Power calculations

> How much data do you need to collect in order to detect a difference of $\delta$?

::: {.columns}

::: {.column width="55%"}
The statistical **power** of a test captures how often it detects a specified alternative.

- measures how often the test correctly rejects (proportion of samples)
- value depends on...

    a. magnitude of difference between null value and true value of parameter
    b. significance level
    c. sample size

:::

::: {.column width="45%"}
```{r, echo=T}
power.t.test(power = 0.95, 
             delta = 0.5, 
             sig.level = 0.05, 
             type = 'two.sample',
             alternative = 'two.sided')
```
$\Rightarrow$ need 105 observations in each group to detect a difference of 0.5 standard deviations for 95% of samples with a 5% significance level test
:::

:::

## A statistical trap

> If you collect enough data, you can detect an arbitrarily small difference in means almost always.

::: {.columns}

::: {.column width="70%"}
```{r, fig.height=5, fig.width = 8}
expand_grid(delta = exp(seq(from = -10, to = -3, length = 10)),
            n = exp(seq(from = 4, to = 15, length = 100))) |>
mutate(power = map2(n, delta, ~power.t.test(n = .x, delta = .y)$power)) |>
unnest(power) |>
ggplot(aes(x = n, y = power, color = delta, group = delta)) +
geom_path() +
scale_x_log10() +
scale_y_log10() +
scale_color_continuous(trans = 'log10') +
theme_minimal(base_size = 20) +
labs(x = 'sample size (n)', y = 'power (detection rate)', color = 'true \ndifference \nin means')
```
:::

::: {.column width="30%"}
So keep in mind:

- statistical significance $\neq$ practical significance
- always check your point estimates
:::

:::

# Extras

## The equal-variance $t$-test

If it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power by using a different standard error:

$$SE_\text{pooled}(\bar{x} - \bar{y}) = \sqrt{\frac{\color{red}{s_p^2}}{n_x} + \frac{\color{red}{s_p^2}}{n_y}}
\quad\text{where}\quad \color{red}{s_p} = \underbrace{\sqrt{\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\text{weighted average of } s_x^2 \;\&\; s_y^2}$$

Implement by adding `var.equal = T` as an argument to `t.test()`.

- larger df is used, hence more frequent rejections
- avoid unless you have a small sample

# Lecture 10: Power analyses


```{r}
library(tidyverse)
library(oibiostat)
library(Sleuth3)
library(pander)

type2sim <- function(delta, n, sd, alpha, nsim = 1000){
  # simulate nsim tests by...
  sim.pvals <- sapply(1:nsim, function(i){
    # draw sample with true group difference of delta
    samp <- data.frame(variable = c(rnorm(n, mean = 0, sd = sd), rnorm(n, mean = delta, sd = sd)),
                       group = rep(1:2, each = n))
    # perform test and compute p value
    pval <- t.test(variable ~ group, data = samp, mu = 0, alternative = 'two.sided')$p.value
    return(pval)
  })
  # compute proportion of tests that failed to reject (made a type ii error)
  err.rate <- mean(sim.pvals > alpha)
  return(list(rate = err.rate, p = sim.pvals))
}

cloud <- Sleuth3::case0301 |> rename_with(tolower) |> mutate(treatment = tolower(treatment))
cloud.test <- t.test(rainfall ~ treatment, data = cloud,
                     mu = 0, alternative = 'two.sided')
```

## Today's agenda

1. [lecture] Statistical power; post-hoc and sample size power analyses.
2. [review/lab] Test 2 practice problems.

## $p$-values and false rejections

> A $p$-value captures how often you'd make a mistake **if $H_0$ were true**.

::: {.columns}

::: {.column}
```{r, echo = T}
t.test(rainfall ~ treatment, data = cloud, 
       mu = 0, alternative = 'greater')
```

If there is no effect of cloud seeding, then we would see $T > 1.9982$ for 2.689% of samples.

:::

::: {.column}
The test rejects at the 5% significance level ($p < 0.05$), but that doesn't completely rule out $H_0$.

- while unlikely, our sample could have been one of the 26 in 1000 where $T$ exceeds 1.9982 despite no effect
- by rejecting here (when $T = 1.9982$) we are willing to be wrong 2.689% of the time

*By rejecting when $p < \alpha$ we are willing to be wrong $\alpha\times 100$% of the time.*

:::

:::

## A different kind of error?

> But you can also make a mistake when $H_0$ is false!

::: {.columns}

::: {.column}
```{r, echo = T}
t.test(rainfall ~ treatment, data = cloud, 
       mu = 0, alternative = 'two.sided')
```

We'd see $|T| > 1.9982$ for 5.377% of samples if there's no effect. But what if there is an effect?
:::

::: {.column}
The two-sided test fails to reject at the 5% significance level ($p > 0.05$), but that doesn't completely rule out $H_A$.

- the estimated effect -- increase of `r cloud.test$est |> rev() |> diff() |> round(2)` acre-feet -- could be too small relative to the variability in rainfall

- hard to say how often we'd make this kind of mistake without knowing the real difference

*The rate of fail-to-reject errors depends on the (unknown) true parameter value.*
:::

:::

## Decision errors

> There are two ways to make a mistake in a hypothesis test -- two "*error types*".

| | Reject $H_0$ | Fail to reject $H_0$
---|---|---
**True $H_0$** | type I error | [correct decision]{style="color:lightgrey"}
**False $H_0$** | [correct decision]{style='color:lightgrey'} | type II error

::: {.columns}

::: {.column}

Any statistical test will have certain error rates:

- type I error rate is denoted $\alpha$
- type II error rate is denoted $1 - \beta$

:::

::: {.column}

The significance level of a test is its type I error rate.

- reject when $p < \alpha$ $\Longleftrightarrow$ mistakenly reject $\alpha\times 100$% of the time

But we don't know the type II error rate!

- depends on which alternative parameter value is true

:::

:::


## Simulating type II errors

::: columns
::: {.column width="55%"}
Summary stats for cloud data:

```{r}
set.seed(21524)
cloud %>% group_by(treatment) %>% 
  summarize(mean = mean(rainfall), sd = sd(rainfall), n = n()) %>%
  pander()
```

We can approximate the type II error rate by:

1.  simulating datasets with matching statistics
2.  performing two-sided tests of no difference
3.  computing the proportion of fail-to-reject decisions


:::

::: {.column width="45%"}
```{r, echo = T, eval = F}
type2sim(delta = 277, n = 26, sd = 650.8, 
         alpha = 0.05, nsim = 1000)
```
```{r, fig.width=5, fig.height = 4.5}
par(mar = c(4, 4, 2, 1), cex = 1.5)
sim.out <- type2sim(delta = 277, n = 26, sd = 650.8, alpha = 0.05, nsim = 1000)
hist(sim.out$p, breaks = seq(0, 1, by = 0.05),
     main = "1000 simulated tests", xlab = 'p value', ylab = 'frequency')
abline(v = 0.05, lty = 2, lwd = 2, col = 'red')
text(x = 0.2, y = 300, expr(paste(alpha, " = 0.05")), col = 'red')
```
:::
:::

> If in fact the effect size is exactly 277, a level 5% test with similar data will fail to reject ~70% of the time!

## Larger effect size

::: columns
::: {.column width="55%"}
Summary stats for cloud data:

```{r}
set.seed(21524)
cloud %>% group_by(treatment) %>% 
  summarize(mean = mean(rainfall), sd = sd(rainfall), n = n()) %>%
  pander()
```

We can approximate the type II error rate by:

1.  simulating datasets with matching statistics
2.  performing two-sided tests of no difference
3.  computing the proportion of fail-to-reject decisions


:::

::: {.column width="45%"}
```{r, echo = T, eval = F}
type2sim(delta = 350, n = 26, sd = 650.8, 
         alpha = 0.05, nsim = 1000)
```
```{r, fig.width=5, fig.height = 4.5}
par(mar = c(4, 4, 2, 1), cex = 1.5)
sim.out <- type2sim(delta = 400, n = 26, sd = 650.8, alpha = 0.05, nsim = 1000)
hist(sim.out$p, breaks = seq(0, 1, by = 0.05),
     main = "1000 simulated tests", xlab = 'p value', ylab = 'frequency')
abline(v = 0.05, lty = 2, lwd = 2, col = 'red')
text(x = 0.2, y = 500, expr(paste(alpha, " = 0.05")), col = 'red')
```
:::
:::

> If in fact the effect size is exactly 400, a level 5% test with similar data will fail to reject ~40% of the time.

## Smaller effect size

::: columns
::: {.column width="55%"}
Summary stats for cloud data:

```{r}
set.seed(21524)
cloud %>% group_by(treatment) %>% 
  summarize(mean = mean(rainfall), sd = sd(rainfall), n = n()) %>%
  pander()
```

We can approximate the type II error rate by:

1.  simulating datasets with matching statistics
2.  performing two-sided tests of no difference
3.  computing the proportion of fail-to-reject decisions


:::

::: {.column width="45%"}
```{r, echo = T, eval = F}
type2sim(delta = 100, n = 26, sd = 650.8, 
         alpha = 0.05, nsim = 1000)
```
```{r, fig.width=5, fig.height = 4.5}
par(mar = c(4, 4, 2, 1), cex = 1.5)
sim.out <- type2sim(delta = 100, n = 26, sd = 650.8, alpha = 0.05, nsim = 1000)
hist(sim.out$p, breaks = seq(0, 1, by = 0.05),
     main = "1000 simulated tests", xlab = 'p value', ylab = 'frequency')
abline(v = 0.05, lty = 2, lwd = 2, col = 'red')
text(x = 0.2, y = 80, expr(paste(alpha, " = 0.05")), col = 'red')
```
:::
:::

> If in fact the effect size is exactly 100, a level 5% test with similar data will fail to reject ~90% of the time.



## Statistical power

The **power** of a test refers to its **true rejection rate** across alternatives and is defined as: $$\beta = \underbrace{(1 - \text{type II error rate})}_\text{correct decision rate when null is false}$$

Power is often interpreted as a detection rate:

-   high type II error $\longrightarrow$ low power $\longrightarrow$ low detection rate
-   low type II error $\longrightarrow$ high power $\longrightarrow$ high detection rate

> In general tests have low power for alternatives close to the null value (where "close" is relative to sampling variability).

## Power curves

> Power is usually construed as a *curve* depending on the true difference. 

::: {.columns}

::: {.column width="60%"}

Power curve for the test exactly as performed with the cloud seeding data:

```{r, fig.width = 6, fig.height=4}
par(mar = c(4, 4, 1, 1),
    cex = 1.5)
curve(power.t.test(delta = x, n = 26, sd = 650, sig.level = 0.05)$power, 
      from = -1000, to = 1000, 
      xlab = expression(paste('true difference ', delta)),
      ylab = expression(paste('power ', beta)))
```
:::

::: {.column width="40%"}
All other attributes of the test are fixed to approximate the test performed:

- sample size $n = 26$
- significance level $\alpha = 0.05$
- population standard deviation $\sigma = 650$ (larger of two group estimates)

:::

:::


## Two common power analyses

::: columns
::: column
**Post hoc analysis**: how much power does the test I conducted have if the true difference is exactly equal to my estimate?

Helps to interpret negative results:

-   low power $\rightarrow$ failure to reject was likely
-   high power $\rightarrow$ failure to reject was not likely

::: callout-important
## Don't over-interpret post-hoc analyses

Failure to reject using a well-powered test *does not confirm the null hypothesis*.
:::

:::

::: column
**Sample size determination**: how much data do I need to collect to detect a difference of $\delta$ using a particular test?

Helps avoid two potential issues:

-   too little data $\rightarrow$ study not likely to yield significant results
-   too much data $\rightarrow$ study is too likely to yield significant results
:::
:::

## Post-hoc analysis

> Can we estimate the power of a test we already performed?

::: columns
::: {.column width="55%"}
Feasible if we assume (a) a population standard deviation and (b) test conditions are met.

For the cloud seeding test:

```{r, echo=T}
power.t.test(delta = 250, # magnitude of difference
             sd = 650, # largest population SD
             n = 26, # smallest sample size
             sig.level = 0.05, 
             type = 'two.sample', 
             alternative = 'two.sided') 
```
:::

::: {.column width="45%"}
For a conservative estimate, use:

-   *smallest* of the two sample sizes
-   *largest* of the two standard deviations
-   *smaller* difference than observed

> $\Longrightarrow$ our test would only reject in favor of a difference of the observed magnitude about 27% of the time

Failure to reject doesn't strongly rule out the alternative.
:::
:::

## Sample size calculation

> If you were (re)designing the study, how much data should you collect to detect a specified effect size?

::: columns
::: {.column width="55%"}
To detect a difference of 250 or more due to cloud seeding with power 0.9:
```{r, echo=T}
power.t.test(power = 0.9, # target power level
             delta = 250, # smallest difference
             sd = 650, # largest population SD
             sig.level = 0.05, 
             type = 'two.sample', 
             alternative = 'two.sided') 
```
:::

::: {.column width="45%"}
For a conservative estimate, use:

-   *overestimate* of the larger of the two standard deviations
-   *minimum* difference of interest

> $\Longrightarrow$ we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time
:::
:::



## Practical constraints

::: {.columns}

::: {.column width="60%"}

Minimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:

```{r, fig.width = 6, fig.height=4}
library(RColorBrewer)
coul <- brewer.pal(5, "BuPu") 

par(mar = c(4, 4, 1, 1),
    cex = 1.5,
    cex.axis = 0.7)

delta.seq <- sapply(seq(from = 0.5, to = 0.9, by = 0.1), 
                    function(j){
                      sapply(20:300, function(i){
                        del <- power.t.test(n = i, 
                                            power = j, 
                                            sd = 650, 
                                            sig.level = 0.05,
                                            alternative = 'one.sided')$delta
                        return(del)})
})

plot(x = 20:300, y = delta.seq[, 5], type = 'l', 
      xlab = 'sample size',
      ylab = expression(paste('difference ', delta)),
     col = coul[5])

for(j in 4:1){                      
lines(x = 20:300, y = delta.seq[, j], type = 'l', 
     xlab = '',
     ylab = '',
     col = coul[j])
}
abline(h = 300, lty = 2, col = 1)
legend(x = 'topright', legend = paste('power', (5:9)/10), col = coul, fill = coul)

```

Assumes $\sigma = 650$ for a conservative estimate.

:::

::: {.column width="40%"}
It may not be affordable to obtain data for 144 days per treatment group (pilots and planes are expensive). What is achievable within constraints?

- power of 0.8 will require *n* = `r power.t.test(delta = 300, power = 0.8, sd = 650, sig.level = 0.05, alternative = 'one.sided')$n |> ceiling()` per group

    + 138 days total

- decreasing to 0.7 will require *n* = `r power.t.test(delta = 300, power = 0.7, sd = 650, sig.level = 0.05, alternative = 'one.sided')$n |> ceiling()` per group

    + 90 days total

:::

:::

# Lecture 11: Analysis of variance


```{r}
library(tidyverse)
library(oibiostat)
library(Sleuth3)
library(pander)
library(RColorBrewer)
means.plot <- function(groups, means, ses, crit = 2, ang = 90, 
                       xlab = '', ylab = '', main = '', 
                       l = 0.1, ylim = NULL, col = 'black'){
  interval.lwr <- means - crit*ses
  interval.upr <- means + crit*ses
  if(is.null(ylim)){
    yl <- c(min(interval.lwr), max(interval.upr))
  }else{
      yl <- ylim
  }
  plot(x = groups, 
       y = means, 
       ylim = yl,
       xlim = c(0.75, max(groups) + 0.25),
       xlab = xlab, ylab = ylab, main = main,
       xaxt = 'n',
       yaxt = 'n',
       pch = 16)
  # axis(1, at = groups)
  arrows(groups, interval.lwr, groups, interval.upr, length = l, angle = ang, code = 3)
}

red.grad <- colorRampPalette(c('#ff6459', '#4d0500'))
blue.grad <- colorRampPalette(c('#6880f7', '#0a1a6e'))
reds <- red.grad(6)
blues <- blue.grad(6)
```


## Today's agenda

1. \[lecture\] inference comparing several population means
2. \[lab\] fitting ANOVA models in R

## More than two means?

::: columns
::: column
You previously considered this data on chick weights at 20 days of age by diet:

```{r, fig.width=4, fig.height = 3}
chicks <- ChickWeight |>
  filter(Time == 20) |>
  select(weight, Chick, Diet) |>
  rename(chick = Chick, diet = Diet) |>
  mutate(chick = as.numeric(chick))
save(chicks, file = 'data/chicks-20d.RData')

par(mar = c(4, 4, 1, 1),
    cex = 1.5)
boxplot(weight ~ diet, data = chicks, xlab = 'diet', ylab = 'weight (g)')
```

:::

::: column
Here we have *four means* to compare rather than just two. 

```{r}
chicks.summary <- chicks |>
  group_by(diet) |>
  summarize(mean = mean(weight),
            se = sd(weight)/sqrt(n()),
            sd = sd(weight),
            n = n())

chicks.summary |>
  pander()
```

:::
:::

> Does mean weight at 20 days differ by diet? How do you test this?

## Hypotheses for a difference in means

Let $\mu_i = \text{mean weight on diet } i = 1, 2, 3, 4$.

The hypothesis that there are **no differences** in means by diet is:

$$
H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4 \quad (\text{no difference in means})
$$

The alternative, if this is false, is that there is **at least one difference**:

$$
H_A: \mu_i \neq \mu_j \quad (\text{at least one difference})
$$

## How much difference is enough?

Here are two made-up examples of four sample means.

::: columns
::: {.column width="70%"}
```{r, fig.width = 8, fig.height= 3}
set.seed(22124)
par(mfrow = c(1, 2), mar = c(3, 3, 0.1, 0.1))
means.plot(1:4, rnorm(4, mean = 1, sd = 2), 2 + rnorm(4, sd = 0.1), ylim = c(-10, 8), ang = 45)
means.plot(1:4, rnorm(4, mean = 1, sd = 4), 2 + rnorm(4, sd = 0.1), ylim = c(-10, 8), ang = 45)
```
:::

::: {.column width="30%"}

> Why does it look like there's a difference at right but not at left?

:::
:::

Think about the $t$-test: we say there's a difference if $T = \frac{\text{estimate} - \text{hypothesis}}{\text{variability}}$ is large.

Same idea here: we see differences if they are big *relative to the variability in estimates*.

## Partitioning variation

> Partitioning variation into two or more components is called "analysis of variance"

::: columns
::: {.column width="45%"}
```{r, fig.width = 5, fig.height = 4}
par(mar = c(4, 4, 1, 1), cex = 1.5)
fit <- lm(weight ~ diet, data = chicks)
fit.av <- anova(fit)
rmsg <- fit.av$`Mean Sq`[1] |> sqrt()
rmse <- fit.av$`Mean Sq`[2] |> sqrt()
rmst <- fit.av$`Mean Sq` |> sum() |> sqrt()
obs.lwr <- mean(chicks$weight) - rmst
obs.upr <- mean(chicks$weight) + rmst

plot(as.numeric(chicks$diet) + rnorm(46, sd = 0.1), chicks$weight,
     xaxt = 'n', ylab = 'weight(g)', xlab = 'diet',
     col = 'darkgrey', xlim = c(0.25, 4.75), cex = 0.7,
     ylim = c(obs.lwr, obs.upr))
axis(1, at = as.numeric(chicks$diet))
arrows(4.6, obs.lwr, 
       4.6, obs.upr, 
       col = 'darkgrey', length = 0.1, code = 3)
points(chicks.summary$diet, chicks.summary$mean,
       pch = 16, col = blues[3], cex = 0.7)
abline(h = mean(chicks$weight), lty = 2, col = reds[3])

grp.lwr <- chicks.summary$mean - rmse
grp.upr <- chicks.summary$mean + rmse
arrows(1:4, grp.lwr, 1:4, grp.upr, 
       code = 3, length = 0.1, col = blues[3])

btwn.lwr <- mean(chicks$weight) - rmsg*0.7
btwn.upr <- mean(chicks$weight) + rmsg*0.7
arrows(2.5, btwn.lwr, 2.5, btwn.upr,
       code = 3, length = 0.1, col = reds[3])
points(2.5, mean(chicks$weight), col = reds[3], pch = 16)
```
:::

::: {.column width="55%"}
For the chick data, two sources of variability:

-   [group]{style="color:red"} variability between diets

-   [error]{style="color:blue"} variability among chicks

The analysis of variance (ANOVA) model:

$$\color{grey}{\text{total variation}} = \color{red}{\text{group variation}} + \color{blue}{\text{error variation}}$$
:::
:::


We'll base the test on the ratio $F = \frac{\color{red}{\text{group variation}}}{\color{blue}{\text{error variation}}}$.


## The $F$ statistic: a variance ratio

> The $F$ statistic measures variability attributable to group differences relative to variability attributable to individual differences.


::: columns
::: column
Notation:

-   $\bar{x}$: "grand" mean of all observations
-   $\bar{x}_i$: mean of observations in group $i$
-   $s_i$: SD of observations in group $i$
-   $k$ groups
-   $n$ total observations
-   $n_i$ observations per group

:::

::: column
Measures of variability:

$$\color{red}{MSG} = \frac{1}{k - 1}\sum_i n_i(\bar{x}_i - \bar{x})^2 \quad(\color{red}{\text{group}})$$ 
$$\color{blue}{MSE} = \frac{1}{n - k}\sum_i (n_i - 1)s_i^2 \quad(\color{blue}{\text{error}})$$ Ratio:

$$F = \frac{\color{red}{MSG}}{\color{blue}{MSE}} \quad\left(\frac{\color{red}{\text{group variation}}}{\color{blue}{\text{error variation}}}\right)$$
:::
:::

## Sampling distribution for $F$

::: {.columns}

::: {.column width="55%"}
If the data satisfy these conditions:

1. the distribution of values is symmetric and unimodal within each group
2. the variability (standard deviation) is roughly the same across groups

Then the $F$ statistic has a sampling distribution well-approximated by an $F_{k - 1, n - k}$ model.

- numerator degrees of freedom $k - 1$
- denominator degrees of freedom $n - k$
:::

::: {.column width="45%"}
```{r, fig.width=10, fig.height=7}
#| fig-cap: $F$ models for several different numerator degrees of freedom $k - 1$ with fixed $n = 30$.
reds <- red.grad(7)
par(mar = c(4, 4, 2, 1), cex = 2.5)
n <- 30
k <- 10
curve(sqrt(df(x, k - 1, n - k)), from = 0, to = 5,
      xlab = expr(paste("F = ", frac(MSG, MSE))),
      ylab = '',
      main = expr(paste(F[paste(k - 1, ',', n - k)], ' model')),
      xaxt = 'n', yaxt = 'n', axes = F,
      col = reds[7], lwd = 2)
axis(1, at = 0:5)
title(ylab = 'sampling frequency', line = 1)

for(k in 4:9){
curve(sqrt(df(x, k - 1, n - k)), from = 0, to = 5,
      xlab = '', ylab = '', main = '',
      xaxt = 'n', yaxt = 'n',
      col = reds[k/2], add = T, lwd = 2)
}

legend('topright', 
       legend = paste("k = ", 4:10, sep = ''),
       col = reds, lwd = 2, cex = 0.75)
```
:::

:::

## $p$-values for the $F$ test

::: {.columns}

::: {.column}
To test the hypotheses:

$$
\begin{cases}
H_0: &\mu_1 = \mu_2 = \mu_3 = \mu_4 \\
H_0: &\mu_i \neq \mu_j \quad\text{for some}\quad i \neq j
\end{cases}
$$
Calculate the $F$ statistic:

```{r, echo = T}
# ingredients of mean squares
k <- nrow(chicks.summary)
n <- nrow(chicks)
n.i <- chicks.summary$n
xbar.i <- chicks.summary$mean
s.i <- chicks.summary$sd
xbar <- mean(chicks$weight)

# mean squares
msg <- sum(n.i*(xbar.i - xbar)^2)/(k - 1)
mse <- sum((n.i - 1)*s.i^2)/(n - k)

# f statistic
fstat <- msg/mse
fstat
```

And reject $H_0$ when $F$ is large. 
:::

::: {.column}

For a significance level $\alpha$ test, reject $H_0$ when $\underbrace{P(F > F_\text{obs})}_\text{p-value} < \alpha$.

```{r, fig.height = 3, fig.width=5}
par(mar = c(4, 4, 2, 1), cex = 1.5)
n <- nrow(chicks)
k <- nrow(chicks.summary)
curve(sqrt(df(x, k - 1, n - k)), from = 0, to = 6, n = 500,
      xlab = expr(paste("F = ", frac(MSG, MSE))),
      ylab = '',
      main = expr(paste(F[paste(3, ',', 42)], ' model')),
      xaxt = 'n', yaxt = 'n', axes = F,
      col = "black", lwd = 2)
axis(1, at = 0:6)
title(ylab = 'sampling frequency', line = 1)

abline(v = fstat, lty = 4, lwd = 2, col = reds[3])
x <- seq(from = 5, to = 6, length = 100)
y <- df(x, k - 1, n - k) |> sqrt()

polygon(c(x[x>=fstat], max(x), fstat), c(y[x>=fstat], 0, 0),
        col = reds[3], border = NA)

legend('topright', fill = reds[3],
       legend = paste(round(pf(fstat, k - 1, n - k, lower.tail = F), 4)*100, "% of samples", sep = ''),
       cex = 0.7)
```

```{r, echo = T}
pf(fstat, 4 - 1, 46 - 4, lower.tail = F)
```
:::

:::

## Interpreting $F$ statistics and $p$-values

::: {.columns}

::: {.column}
$$
\begin{cases}
H_0: &\mu_1 = \mu_2 = \mu_3 = \mu_4 \\
H_0: &\mu_i \neq \mu_j \quad\text{for some}\quad i \neq j
\end{cases}
$$

$F = \frac{\color{red}{\text{group variation}}}{\color{blue}{\text{error variation}}} = \frac{MSG}{MSE} = `r round(fstat, 4)`$.

```{r, fig.height = 3, fig.width=5}
par(mar = c(4, 4, 2, 1), cex = 1.5)
n <- nrow(chicks)
k <- nrow(chicks.summary)
curve(sqrt(df(x, k - 1, n - k)), from = 0, to = 6, n = 500,
      xlab = expr(paste("F = ", frac(MSG, MSE))),
      ylab = '',
      main = expr(paste(F[paste(3, ',', 42)], ' model')),
      xaxt = 'n', yaxt = 'n', axes = F,
      col = "black", lwd = 2)
axis(1, at = 0:6)
title(ylab = 'sampling frequency', line = 1)

abline(v = fstat, lty = 4, lwd = 2, col = reds[3])
x <- seq(from = 5, to = 6, length = 100)
y <- df(x, k - 1, n - k) |> sqrt()

polygon(c(x[x>=fstat], max(x), fstat), c(y[x>=fstat], 0, 0),
        col = reds[3], border = NA)

legend('topright', fill = reds[3],
       legend = paste(round(pf(fstat, k - 1, n - k, lower.tail = F), 4)*100, "% of samples", sep = ''),
       cex = 0.7)
```

```{r, echo = T}
pf(fstat, 4 - 1, 46 - 4, lower.tail = F)
```
:::

::: {.column}

> *F = 5.4636* means the proportion of variation in weight attributable to diets is 5.46 times greater than the proportion of variation attributable to chicks.

The statistical significance of this result is measured by the $p$-value:

- if there is in fact no difference in means, then only `r round(pf(5.4636, 3, 42, lower.tail = F), 4)*100`% of samples (*i.e.*, 2 in 1000) would produce at least as much diet-to-diet variability as we observed.

- so in this case we reject $H_0$ at the 1% level

:::

:::

## ANOVA in R

The `aov(...)` function fits ANOVA models using a formula/dataframe specification:

```{r, echo = T, results = 'hide'}
# fit anova model
fit <- aov(weight ~ diet, data = chicks)

# generate table
summary(fit)
```

```{r}
summary(fit) |> pander()
```

The typical style for interpretation closely follows that of previous inferences for the mean:

> The data provide strong evidence of an effect of diet on mean weight (*F = 5.464* on *3* and *42* df, *p = 0.0029*).

## Analysis of variance table

The results of an analysis of variance are traditionally displayed in a table.

Source | degrees of freedom | Sum of squares | Mean square | F statistic | *p*-value
---|---|---|---|---|---
Group | $k - 1$ | SSG | $MSG = \frac{SSG}{k - 1}$ | $\frac{MSG}{MSE}$  | $P(F > F_\text{obs})$
Error | $n - k$ | SSE | $MSE = \frac{SSE}{n - k}$ | 

- the sum of square terms are 'raw' measures of variability
- the mean square terms are averages adjusted for the amount of data available to estimate variability due to each source

Formally, the ANOVA model says $(n - 1)s^2 = SSG + SSE$.

## Checking assumptions

::: {.columns}

::: {.column}
The ANOVA test assumes:

1. the distribution of values is symmetric and unimodal within each group
2. the variability (standard deviation) is roughly the same across groups

To check these assumptions:

- compare group standard deviations for similarity
- visually inspect distributions within each group for approximate symmetry

Similar to the $t$ test, greater departures from these assumptions are allowable for larger sample sizes.
:::

::: {.column}

```{r, fig.width=4, fig.height = 3}
chicks <- ChickWeight |>
  filter(Time == 20) |>
  select(weight, Chick, Diet) |>
  rename(chick = Chick, diet = Diet)

par(mar = c(4, 4, 1, 1),
    cex = 1.5)
boxplot(weight ~ diet, data = chicks, xlab = 'diet', ylab = 'weight (g)')

chicks.summary |> pander()
```

:::

:::

## Another example: treating anorexia

::: {.columns}

::: {.column}
Weight change was measured for 72 young female anorexia patients randomly allocated to three treatment groups:

- cognitive behavioral therapy (CBT)
- family treatment (FT)
- a control (Cont)

Grouped summary statistics:

```{r}
anorexia <- MASS::anorexia |>
  rename_with(tolower) |>
  mutate(treat.id = as.numeric(treat),
         change = postwt - prewt)
write_csv(anorexia, 'data/anorexia.csv')

anorexia.summary <- anorexia |>
  group_by(treat) |>
  summarize(`post - pre` = mean(change),
            sd = sd(change),
            n = n())

anorexia.summary |>
  pander()
```
:::

::: {.column}
```{r, fig.width = 6, fig.height=4}
par(mar = c(4, 3, 1, 1), cex = 1.5)
boxplot(change ~ treat, data = anorexia, horizontal = T,
        xlab = 'weight change (lbs)', ylab = '', range = 2)
```

> Were any of the treatments more effective than others?
:::

:::

## Another example: treating anorexia

```{r, echo = T, results = 'hide'}
# fit anova model
fit <- aov(change ~ treat, data = anorexia)

# generate table
summary(fit)
```

```{r}
summary(fit) |> pander()
```

> The data provide strong evidence of an effect of therapeutic treatment on mean weight change among young women with anorexia (*F = 5.422* on *2* and *69* degrees of freedom, *p = 0.0065*).

# Lecture 12: Post-hoc inference


```{r}
library(tidyverse)
library(pander)
library(Sleuth3)
library(emmeans)
```

## Today's agenda

1. [lecture] inference on group means and contrasts after performing ANOVA
2. [lab] estimates, tests, and intervals using `emmeans`

## From before: diet restriction
```{r}
load('data/longevity.RData')
```

::: {.columns}

::: {.column width="40%"}
**Data summaries**:
```{r, fig.width = 5.5, fig.height = 3}
par(mar = c(2, 4, 1, 1),
    cex = 1.5)
boxplot(lifetime ~ diet, data = longevity, 
        xlab = '', ylab = 'lifetime (months)')
longevity |> 
  group_by(diet) |> 
  summarize(mean = mean(lifetime),
            sd = sd(lifetime),
            n = n()) |>
  # arrange(mean) |>
  pander()
```
:::

::: {.column width="60%"}
**Inference using ANOVA**:

$$
\begin{align*}
&H_0: \mu_\text{NP} = \mu_\text{N/N85} = \mu_\text{N/R50} = \mu_\text{N/R40} \\
&H_A: \text{at least two means differ}
\end{align*}
$$


```{r, echo = T}
fit <- aov(lifetime ~ diet, data = longevity)
summary(fit)
```

> The data provide strong evidence that diet restriction has an effect on mean lifetime among mice (*F = 87.41* on *3* and *233* degrees of freedom, *p < 0.0001*).
:::

:::

## Follow-up questions

The ANOVA tells us there's evidence of an effect of diet restriction on lifespan. 

So now we'd want to know:

1. What are the mean lifespans for each level of restriction?
2. For which levels of dietary restriction do mean lifespans differ?
3. What are the gains in mean lifespan for each level of restriction relative to an unrestricted diet?
4. For which levels of restriction are gains in mean lifespan significant?

Answers require *post-hoc* inferences (done after-the-fact) on:

- group means $\mu_i$ (question 1)
- "contrasts" $\mu_i - \mu_j$ (questions 2-4)

## Estimates for group means

> Interval estimates for group means in ANOVA use a model-based standard error.

::: {.columns}

::: {.column width="60%"}
```{r, echo = T, eval = F}
emmeans(object = fit, spec = ~ diet) |> 
  confint(level = 0.95)
```

```{r}
emmeans(fit, ~ diet) |>
  confint(level = 0.95) |> 
  as.tibble() |>
  rename(estimate = emmean) |>
  select(-df) |>
  mutate(across(ends_with('CL'), ~round(.x, 2))) |>
  unite('95% CI', ends_with('CL'), sep = ', ') |>
  mutate(`95% CI` = paste('(', `95% CI`, ')', sep = '')) |>
  pander()
```
:::

::: {.column width="40%"}
Interval estimates use a "pooled" standard deviation:

$$SE_i = \frac{s_\text{pooled}}{\sqrt{n_i}} = \frac{\sqrt{MSE}}{\sqrt{n_i}}$$

Otherwise identical to $t_{n - k}$ confidence intervals.
:::

:::

Rationale: 

- the ANOVA model assumes equal variability (standard deviations) across groups
- better precision (for variability estimates, not means) when assumption holds

## Bonferroni adjustment

> Problem: several 95% intervals don't have simultaneous 95% coverage.

- *individual* coverage: how often *one* interval covers the population mean
- *simultaneous* coverage: how often *all* intervals cover population means *at the same time*

::: {.columns}

::: {.column}
```{r, fig.width = 6, fig.height = 4}
emm.unadjusted <- emmeans(fit, ~ diet) |>
  confint(level = 0.95) |>
  tibble() |>
  mutate(level = 0.95)

emm.adjusted <- emmeans(fit, ~ diet) |>
  confint(level = 1 - 0.05/4) |>
  tibble() |>
  mutate(level = 0.9875)

bind_rows(emm.adjusted, emm.unadjusted) |>
  mutate(coverage = factor(level, labels = c('individual', 'simultaneous'))) |>
  ggplot(aes(y = diet, x = emmean, color = coverage)) +
  geom_errorbar(aes(x = emmean,
                    xmin = lower.CL, 
                    xmax = upper.CL), 
                width = 0.3,
                position = position_dodge(width = 0.2)) +
  geom_point(position = position_dodge(width = 0.2)) +
  theme_minimal(base_size = 20) +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.major.x = element_line(color = 'grey', linewidth = 0.1),
        legend.position = 'top') +
  labs(y = '', x = 'estimated mean lifetime (months)') +
  guides(color = guide_legend(title = ''))
```
:::

::: {.column}
The **Bonferroni correction** for $k$ intervals consists in changing the individual coverage level to $\left(1 - \frac{\alpha}{k}\right)\%$.

- Effectively a width increase
- Guarantees joint coverage $(1 - \alpha)\%$
- Tends to be over conservative with many means (large $k$)
:::

:::

## Implementation in R


::: {.columns}

::: {.column}
```{r, echo = T, fig.width = 5, fig.height = 4}
# table
emmeans(object = fit, spec = ~ diet) |>
  confint(level = 0.95, adjust = 'bonferroni')
```

- note Bonferroni adjustment
- these are model-based estimates that depend on the fitted ANOVA model

:::

::: {.column}
```{r, echo = T, eval = F}
# visualization
emmeans(object = fit, spec = ~ diet) |>
  confint(level = 0.95, adjust = 'bonferroni') |>
  plot(xlab = 'mean lifespan (months)', 
       ylab = 'diet')
```

```{r, echo = F, fig.width = 5, fig.height = 3}
# visualization
emmeans(object = fit, spec = ~ diet) |>
  confint(level = 0.95, adjust = 'bonferroni') |>
  plot(xlab = 'mean lifespan (months)', ylab = 'diet') +
  theme_minimal(base_size = 18)
```

:::

:::

*Caution: these intervals do NOT indicate which means differ significantly.*

## Pairwise comparisons

> Pairwise comparisons are inferences made on "contrasts" between pairs of means.

The difference $\mu_{NP} - \mu_{N/N85}$ is an example of a contrast. It is common to perform inference on all pairwise contrasts to determine which means differ and by how much.

::: {.columns}

::: {.column width="55%"}
```{r, echo = T, eval = F}
emmeans(object = fit, specs = ~ diet) |> 
  contrast('pairwise')
```
```{r}
emmeans(fit, ~ diet) |> 
  pairs(adjust = 'bonferroni') |>
  as_tibble() |> 
  select(contrast, estimate, SE) |>
  rename(difference = contrast) |>
  pander(style = 'simple')
```

```{r, include = F}
sigma(fit)*sqrt(1/49 + 1/57)
```
:::

::: {.column width="45%"}

- estimates are $\bar{x}_i - \bar{x}_j$
- $SE_{ij} = SE(\bar{x}_i - \bar{x}_j) = s_\text{pooled}\sqrt{\frac{1}{n_i} + \frac{1}{n_j}}$
- inference based on a $t_{n - k}$ model
    
    + degrees of freedom: $n - k$
    + tests: $T_{ij} = \frac{\bar{x}_i - \bar{x}_j}{SE_{ij}}$
    + intervals: $\bar{x}_i - \bar{x}_j \pm c\times SE_{ij}$
:::

:::

*Multiplicity corrections must adjust for the number of contrasts (6), not means (4).*


## Pairwise comparisons: tests

> Which means differ significantly? All except R50 and R40.

::: {.columns}

::: {.column}
`emmeans` *then* `contrast` *then* `test`:
```{r, echo = T}
emmeans(object = fit, specs = ~ diet) |>
  contrast('pairwise') |>
  test(adjust = 'bonferroni')
```
- *p*-values are adjusted for multiplicity
- reject if *adjusted* *p*-value is below the significance threshold
:::

::: {.column}
Hypotheses for pairwise tests:

$$\begin{cases} H_0: \mu_i - \mu_j = 0 \\ H_A: \mu_i - \mu_j \neq 0 \end{cases}$$
Test statistic:

$$T_{ij} = \frac{\bar{x}_i - \bar{x}_j}{SE_{ij}}$$

$p$-values are obtained from a $t_{n - k}$ model for the sampling distribution of $T_{ij}$.

:::

:::

## Pairwise comparisons: tests

> Which means differ significantly? All except R50 and R40.

::: {.columns}

::: {.column}
`emmeans` *then* `contrast` *then* `test`:
```{r, echo = T}
emmeans(object = fit, specs = ~ diet) |>
  contrast('pairwise') |>
  test(adjust = 'bonferroni')
```
:::

::: {.column}
Interpretation:

> The data provide evidence at the 5% significance level that mean lifespan differs among all levels of diet restriction except the N/R40 and N/R50 groups (*p* = 0.0937), for which the evidence is suggestive but inconclusive.
:::

:::

## Multiple testing correction matters

> Using unadjusted $p$-values will inflate type I error rates.

::: {.columns}

::: {.column}
setting `adjust = 'none'`:
```{r, echo = T}
emmeans(object = fit, specs = ~ diet) |>
  contrast('pairwise') |>
  test(adjust = 'none')
```
:::

::: {.column}
Failure to adjust for multiple inferences leads to a different conclusion:

> The data provide evidence at the 5% significance levelthat mean lifespan differs among all levels of diet restriction.

:::

:::

This is *incorrect*, because the joint significance level is not 5%.

Without adjustment, type I error could be as high as $k\times\alpha = 6\times 0.05 = 0.3$!

## Pairwise comparisons: intervals

> How much do means differ? Anywhere from 2 - 21 months, depending.

::: {.columns}

::: {.column width="55%"}
`emmeans` *then* `contrast` *then* `confint`:
```{r, echo = T}
emmeans(object = fit, specs = ~ diet) |> 
  contrast('pairwise') |>
  confint(level = 0.95, adjust = 'bonferroni')
```

- `level` specifies *joint* coverage after adjustment
:::

::: {.column width="45%"}
Intervals are for the parameter $\mu_i - \mu_j$:

$$\bar{x}_i - \bar{x}_j \pm c\times SE_{ij}$$

The critical value $c$ is obtained from the $t_{n - k}$ model.

For a $(1 - \alpha)\times 100\%$ confidence interval with Bonferroni correction: 

$$c = \left(1 - \frac{\alpha}{2k}\right) \;\text{quantile}$$

:::

:::

## Pairwise comparisons: intervals

> How much do means differ? Anywhere from 2 - 21 months, depending.

::: {.columns}

::: {.column width="55%"}
`emmeans` *then* `contrast` *then* `confint`:
```{r, echo = T}
emmeans(object = fit, specs = ~ diet) |> 
  contrast('pairwise') |>
  confint(level = 0.95, adjust = 'bonferroni')
```
:::

::: {.column width="45%"}
Interpretations are the same as usual:

> With 95% confidence, mean lifespan on a normal diet is estimated to exceed mean lifespan on an unrestricted diet by between 1.87 and 8.71 months, with a point estimate of 5.29 months difference (SE 1.29).
:::

:::

## Pairwise comparisons: visualizations

::: {.columns}

::: {.column width="45%"}
Another option is to visualize the pairwise comparison inferences by displaying simultaneous 95% intervals.

Easy to spot significant contrasts:

- intervals exclude 0 $\Leftrightarrow$ tests reject
:::

::: {.column width="55%"}
`emmeans` *then* `contrast` *then* `confint` *then* `plot`:
```{r, echo=T, eval = F}
emmeans(object = fit, specs = ~ diet) |>
  contrast('pairwise') |>
  confint(level = 0.95, adjust = 'bonferroni') |>
  plot(xlab = 'difference in mean lifetime (months)', 
       ylab = 'contrast')
```

```{r, fig.width = 6, fig.height = 4}
emmeans(fit, ~ diet) |>
  pairs(level = 0.95, adjust = 'bonferroni') |> 
  plot() + geom_vline(xintercept = 0) +
  theme_minimal(base_size = 18) +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.major.x = element_line(color = 'grey', linewidth = 0.1)) +
  labs(x = 'difference in mean lifetime (months)', y = 'contrast')
```

:::


:::



## Comparisons with a control

> Does diet restriction increase mean lifespan, and if so by how much?

::: {.columns}

::: {.column width="60%"}
Specify `contrast('trt.vs.ctrl')`:
```{r, echo = T, eval = F}
emmeans(object = fit, specs = ~ diet) |> 
  contrast('trt.vs.ctrl') |>
  confint(level = 0.95, adjust = 'dunnett')
```

```{r}
emmeans(fit, trt.vs.ctrl ~ diet)$contrasts |> 
  confint() |>
  as_tibble() |> 
  select(contrast, estimate, SE, lower.CL, upper.CL) |>
  mutate(across(ends_with('CL'), ~round(.x, 2))) |>
  unite('95% CI', ends_with('CL'), sep = ', ') |>
  mutate(`95% CI` = paste('(', `95% CI`, ')', sep = '')) |>
  pander(style = 'simple')
```
:::

::: {.column width="40%"}

- Multiple inference adjustment uses Dunnett's method 
    
    + specialized correction for comparisons with a control
    + `adjust = 'dunnett'`
    
- Comparisons will be relative to first group in R
:::

:::

With 95% confidence, relative to an unrestricted diet...

- a 85kcal/day diet increases lifespan by an estimated 2.23 to 8.34 months
- a 50kcal/day diet increases lifespan by an estimated 11.98 to 17.81 months
- a 40kcal/day diet increases lifespan by an estimated 14.70 to 20.73 months


# Extras

## Log contrasts: relative change

> Can we instead estimate a *percent change* in lifespan relative to the control?

::: {.columns}

::: {.column width="44%"}
The contrast in log-lifetimes would be:

$$
\log(\mu_{N85}) - \log(\mu_{NP}) = \log\left(\frac{\mu_{N85}}{\mu_{NP}}\right)
$$
So to answer the question:

1. refit the ANOVA model with *log* lifetimes
2. compute contrasts with control group using Dunnett's method
3. exponentiate estimates to obtain ratios (and hence percentages)
:::

::: {.column width="56%"}
```{r, echo = T, eval = F}
fit.log <- aov(log(lifetime) ~ diet, data = longevity)
```

```{r}
aov(log(lifetime) ~ diet, data = longevity) |>
  emmeans(specs = ~ diet) |>
  contrast('trt.vs.ctrl') |>
  confint(level = 0.95, adjust = 'dunnett') |>
  as_tibble() |> 
  select(contrast, estimate, lower.CL, upper.CL) |>
  mutate(across(ends_with('CL'), ~round(exp(.x), 2))) |>
  unite('95% CI', ends_with('CL'), sep = ', ') |>
  mutate(`95% CI` = paste('(', `95% CI`, ')', sep = ''),
         contrast = str_replace(contrast, ' - ', '/'),
         estimate = exp(estimate)) |>
  slice(-1) |>
  pander(style = 'simple')
```

Fact: mean log lifetime = log median lifetime.

> With 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%.

*Extra credit: work out and interpret the interval estimate for the contrast not shown above.*
:::

:::


## Power calculations for ANOVA

> How much data should we collect to detect a difference in mean lifespan of 1 month?

::: {.columns}

::: {.column}
To perform sample size power calculations, one needs:

- number of groups
- target significance level ($\alpha$)
- target power level
- guess or prior estimate of variance ratio $\frac{\text{group variation}}{\text{error variation}}$

```{r, eval = F}
1/sigma(fit)
```

From the existing study, $\sqrt{MSE} = `r sigma(fit) |> round(2)`$
:::

::: {.column}
So to detect effects on the order of one month at 90% power:
```{r, echo = T}
power.anova.test(groups = 4, 
                 sig.level = 0.05, 
                 power = 0.9, 
                 within.var = 6.633, 
                 between.var = 1)
```
... we need 33 mice per treatment group.
:::

:::

# Lecture 13: Inference for proportions


```{r}
library(tidyverse)
library(oibiostat)
library(pander)
library(Sleuth3)
library(emmeans)
```

## Today's agenda

1. [lecture] Inference for binomial proportions
2. [lab] Tests for proportions in R

## Binomial proportions

> A binomial variable is a nominal categorical variable with two unique values.

::: columns
::: {.column width="65%"}
Usually, binomial data record the presence/absence of an event, trait, or property of interest.

Inference for binomial data has a different flavor:

-   non-numeric values $\Rightarrow$ can't compute usual statistics (mean, variance, etc.)
-   focus on proportions instead

Example: prevalence of diabetes among US adults?

-   estimate and standard error?
-   confidence interval?
-   hypothesis test?
:::

::: {.column width="35%"}
```{r, fig.width = 4, fig.height = 5}
data(nhanes.samp.adult.500)
nhanes <- nhanes.samp.adult.500 |> 
  rename_with(tolower) |>
  mutate(diabetes = fct_rev(diabetes),
         sleeptrouble = fct_rev(sleeptrouble))
save(nhanes, file = 'data/nhanes500.RData')

par(mar = c(4, 4, 3, 1), cex = 1.5)
table(nhanes$diabetes) |> 
  prop.table() |> 
  barplot(density = T, xlab = 'diabetic', ylab = 'sample proportion', main = 'nhanes data', ylim = c(0, 1))
```
:::
:::

## Estimating proportions

::: columns
::: {.column width="50%"}
```{r}
counts <- nhanes$diabetes |> table()
props <- nhanes$diabetes |> table() |> prop.table()  
diabetes.tbl <- rbind(count = counts, proportion = props)  
cbind(diabetes.tbl, total = rowSums(diabetes.tbl)) |>
  pander(caption = '*Diabetes data summary*')
```

Estimated diabetes prevalence: 11.4%.

-   NHANES data are a random sample of the U.S. adult population
-   sample statistics should approximate population statistics
:::

::: {.column width="50%"}
We'll formalize this as estimating the **population proportion** $$p = \frac{\# \text{ individuals with diabetes}}{\text{total population size } N}$$ Using the **sample proportion** $$\hat{p} = \frac{\# \text{ respondents with diabetes}}{\text{sample size } n}$$
:::
:::

The first step towards inference is a measure of precision for $\hat{p}$. What is $SE(\hat{p})$?

## SE for a sample proportion

> Binomial data are most variable when $p = 0.5$ and least variable when $p \approx 0$ or $1$

::: columns
::: {.column width="45%"}
Measure of spread for binomial data: $$\sqrt{\hat{p}(1 - \hat{p})}$$

-   highest when $\hat{p} \approx 0.5$
-   lowest when $\hat{p} \approx 0 \text{ or } 1$

Analogous to estimating a mean: $$
SE\left(\hat{p}\right) 
= \frac{\text{spread}}{\sqrt{\text{sample size}}} 
= \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}
$$
:::

::: {.column width="55%"}
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
par(mar = c(4, 5, 1, 1), cex = 1.5)
curve(sqrt(x*(1 - x)), from = 0, to = 1, n = 200,
      xlab = expr(hat(p)), 
      ylab = expr(sqrt(hat(p) (1 - hat(p)))))
```
:::
:::

## Sampling distribution of $\hat{p}$

::: columns
::: column
The sample proportion $\hat{p}$ has a sampling distribution that can be approximated by a normal model, provided:

-   $\hat{p}$ isn't too close to 0 or 1
-   $n$ is sufficiently large

A common condition to check:

$$n\hat{p} \geq 10\text{ and }n(1 - \hat{p}) \geq 10$$
:::

::: column
```{r, fig.width=5, fig.height = 4}
par(mar = c(6, 3, 1, 1),
    cex = 1.5)
curve(dnorm, from = -4, to = 4, main = 'Normal model',
      yaxt = 'n', xlab = '', ylab = '', axes = F)
title(xlab = expression(frac(hat(p) - p, SE(hat(p)))), 
      line = 4.5)
title(ylab = 'sampling frequency', line = 1)
axis(1, at = seq(-4, 4, by = 2))
```
:::
:::

This model can be used to construct hypothesis tests and confidence intervals for $p$.

## Confidence interval for $p$

A confidence interval for a binomial proportion $p$ is:

$$\hat{p} \pm c \times SE(\hat{p})$$

The critical value $c$ comes from the normal model.

-   empirical rule:

    -   $c = 1$ gives a 68% interval
    -   $c = 2$ gives a 95% interval
    -   $c = 3$ gives a 99.7% interval

-   for a $(1 - \alpha)\times 100 \%$ confidence interval use the $1 - \frac{\alpha}{2}$ quantile of the normal model

```{r, echo = T, eval = F}
qnorm(1 - 0.1/2) # c for 90% interval
qnorm(1 - 0.05/2) # c for 95% interval
qnorm(1 - 0.01/2) # c for 99% interval
```

## Example: diabetes prevalence

::: columns
::: column
```{r}
nhanes |>
  summarize(p.hat = mean(diabetes == 'Yes'),
            se = sqrt((p.hat*(1 - p.hat))/n()),
            n = n()) |>
  pander(caption = 'Point estimate for diabetes prevalence')
```
:::

::: column
> It is estimated that the proportion of the U.S. adult population with diagnosed diabetes is 11.4% (*SE = 1.42%*).
:::
:::

Check assumptions for the normal model:

$$
500\times 0.114 = `r 500*0.114` \geq 10
\quad\text{and}\quad 500\times 0.886 = `r 500*0.886` \geq 10
$$

::: columns
::: column
95% confidence interval for diabetes prevalence:

$$
0.114 \pm 2\times 0.01421 = (0.0881, 0.1459)
$$
:::

::: column
> With 95% confidence, the proportuion of U.S. adults with diagnosed diabetes is estimated to be between 8.81% and 14.59%.
:::
:::

## Hypothesis tests for $p$

::: columns
::: {.column width="45%"}
To test whether true prevalence is 10%: 
$$
\begin{cases}
H_0: &p = 0.1 \\ 
H_A: &p \neq 0.1
\end{cases}
$$

We can use the test statistic:

$$
Z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0 (1 - p_0)}{n}}}
= \frac{\hat{p} - 0.1}{\sqrt{\frac{0.1 (0.9)}{500}}}
$$
Under $H_0$, the sampling distribution of $Z$ is approximated by a normal model, provided:

- $np_0 \geq 10$
- $n(1 - p_0) \geq 10$
:::

::: {.column width="55%"}

```{r, fig.width = 6.5, fig.height = 4.5}
test.out <- nhanes$diabetes |> 
  table() |> 
  prop.test(p = 0.1, correct = F)
zstat <- test.out$stat |> sqrt()
pval <- test.out$p.value
x <- seq(-4, 3.25, length = 10000)
y <- dnorm(x)

par(mar = c(6, 4, 3, 1),
    cex = 1.5)

curve(dnorm, from = -4, to = 6, main = 'Normal model',
      yaxt = 'n', xlab = '', ylab = '', axes = F, lwd = 2)
axis(1, at = seq(-4, 6, by = 2))
title(xlab = expression(paste("Z = ", 
                              frac(hat(p) - 0.1, 
                                   sqrt(frac(0.1(0.9), 500))))),
      line = 4.5)

abline(v = c(-1, 1)*zstat, lty = 2, lwd = 2)
polygon(c(min(x), x[x<=-zstat], -zstat), c(y[x<=-zstat] - 0.0025, 0, 0), col="red", border = NA)
polygon(c(x[x>=zstat], max(x), zstat), c(y[x>=zstat] - 0.0025, 0, 0), col="red", border = NA)

legend(x = 'topright', 
       legend = paste(round(pval*100, 2), '% of \n samples', sep = ''), 
       fill = 'red', box.lwd = 0)
```

Here $p = P(|Z| > `r round(zstat, 3)`) = `r round(pval, 4)`$, so:

> the data provide no evidence that prevalence differs from 10%.



:::
:::

## Hypothesis tests for $p$

::: columns
::: {.column width="45%"}
To test whether true prevalence is 15%: 
$$
\begin{cases}
H_0: &p = 0.15 \\ 
H_A: &p \neq 0.15
\end{cases}
$$

We can use the test statistic:

$$
Z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0 (1 - p_0)}{n}}}
= \frac{\hat{p} - 0.15}{\sqrt{\frac{0.15 (0.85)}{500}}}
$$
Under $H_0$, the sampling distribution of $Z$ is approximated by a normal model, provided:

- $np_0 \geq 10$
- $n(1 - p_0) \geq 10$
:::

::: {.column width="55%"}

```{r, fig.width = 6.5, fig.height = 4.5}
test.out <- nhanes$diabetes |> 
  table() |> 
  prop.test(p = 0.15, correct = F)
zstat <- test.out$stat |> sqrt()
pval <- test.out$p.value
x <- seq(-4, 3.25, length = 10000)
y <- dnorm(x)

par(mar = c(6, 4, 3, 1),
    cex = 1.5)

curve(dnorm, from = -4, to = 6, main = 'Normal model',
      yaxt = 'n', xlab = '', ylab = '', axes = F, lwd = 2)
axis(1, at = seq(-4, 6, by = 2))
title(xlab = expression(paste("Z = ", 
                              frac(hat(p) - 0.15, 
                                   sqrt(frac(0.15(0.85), 500))))),
      line = 4.5)

abline(v = c(-1, 1)*zstat, lty = 2, lwd = 2)
polygon(c(min(x), x[x<=-zstat], -zstat), c(y[x<=-zstat] - 0.0025, 0, 0), col="red", border = NA)
polygon(c(x[x>=zstat], max(x), zstat), c(y[x>=zstat] - 0.0025, 0, 0), col="red", border = NA)

legend(x = 'topright', 
       legend = paste(round(pval*100, 2), '% of \n samples', sep = ''), 
       fill = 'red', box.lwd = 0)
```

Here $p = P(|Z| > `r round(zstat, 3)`) = `r round(pval, 4)`$, so:

> the data provide moderate evidence that prevalence differs from 15%.

:::
:::

## Hypothesis tests for $p$

::: columns
::: {.column width="45%"}
To test if prevalence is below 14%: 
$$
\begin{cases}
H_0: &p = 0.14 \\ 
H_A: &p < 0.14
\end{cases}
$$

We can use the test statistic:

$$
Z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0 (1 - p_0)}{n}}}
= \frac{\hat{p} - 0.14}{\sqrt{\frac{0.14 (0.86)}{500}}}
$$

Under $H_0$, the sampling distribution of $Z$ is approximated by a normal model, provided:

- $np_0 \geq 10$
- $n(1 - p_0) \geq 10$
:::

::: {.column width="55%"}

```{r, fig.width = 6.5, fig.height = 4.5}
test.out <- nhanes$diabetes |> 
  table() |> 
  prop.test(p = 0.14, correct = F, alternative = 'less')
zstat <- test.out$stat |> sqrt()
pval <- test.out$p.value
x <- seq(-4, 3.25, length = 10000)
y <- dnorm(x)

par(mar = c(6, 4, 3, 1),
    cex = 1.5)

curve(dnorm, from = -4, to = 6, main = 'Normal model',
      yaxt = 'n', xlab = '', ylab = '', axes = F, lwd = 2)
axis(1, at = seq(-4, 6, by = 2))
title(xlab = expression(paste("Z = ", 
                              frac(hat(p) - 0.14, 
                                   sqrt(frac(0.14(0.86), 500))))),
      line = 4.5)

abline(v = -zstat, lty = 2, lwd = 2)
polygon(c(min(x), x[x<=-zstat], -zstat), c(y[x<=-zstat] - 0.0025, 0, 0), col="red", border = NA)

legend(x = 'topright', 
       legend = paste(round(pval*100, 2), '% of \n samples', sep = ''), 
       fill = 'red', box.lwd = 0)
```

Here $p = P(Z < `r round(zstat, 3)`) = `r round(pval, 4)`$, so:

> the data provide moderate evidence that prevalence is less than 14%.



:::
:::

## Inference for a proportion in R

::: columns
::: {.column width="45%"}
Inference using the normal model in R:

1.  Construct a table of the frequency distribution
2.  Pass the table to `prop.test()`

Remarks about output:

-   `X-squared` gives $Z^2$
-   `correct = F` performs the test without continuity correction
:::

::: {.column width="55%"}
```{r, echo = T}
#| code-line-numbers: "5-7"
# variable of interest
dia <- nhanes$diabetes

# pass table to prop.test
table(dia) |> 
  prop.test(p = 0.1, alternative = 'two.sided',
            conf.level = 0.95, correct = F)
```
:::
:::

> The data provide no evidence that diabetes prevalence among U.S. adults differs from 10%. With 95% confidence, prevalence is estimated to be between 8.90% and 14.48%, with a point estimate of 11.4% (SE = 1.42%).

## `correct = F`?

> A "continuity correction" reduces approximation error for the normal model.

::: columns
::: column
```{r, echo = T}
#| code-line-numbers: "5"

table(dia) |> 
  prop.test(p = 0.1, 
            alternative = 'two.sided',
            conf.level = 0.95, 
            correct = F)
```
:::

::: column
```{r, echo = T}
#| code-line-numbers: "5"

table(dia) |> 
  prop.test(p = 0.1, 
            alternative = 'two.sided',
            conf.level = 0.95, 
            correct = T)
```
:::
:::

Omitting the `correct` argument implements the correction by default.

## Exact inference for a proportion

The test can also be performed using the *exact* sampling distribution obtained from a binomial probability model.

```{r, echo = T}
binom.test(x = 57, n = 500, p = 0.1, alternative = 'two.sided')
```

Inputs:

-   `x` gives the number of occurrences of the category of interest
-   `n` gives the sample size



<!-- ## Your turn: sleep trouble -->

<!-- > Use the NHANES data to estimate the proportion of U.S. adults reporting sleep trouble. Test whether at least 20% of U.S. adults report sleep trouble. -->

<!-- ::: columns -->
<!-- ::: column -->
<!-- 1.  Compute point estimate and standard error -->

<!-- 2.  Perform the hypothesis test -->

<!--     -   write the hypotheses -->
<!--     -   calculate the value of the test statistic -->
<!--     -   record the $p$-value -->

<!-- 3.  Construct an interval estimate -->
<!-- ::: -->

<!-- ::: column -->
<!-- ```{r, echo = T} -->
<!-- # to get you started -->
<!-- trouble <- factor(nhanes$SleepTrouble,  -->
<!--                   levels = c('Yes', 'No')) -->
<!-- trouble.tbl <- table(trouble) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- trouble.tbl |>  -->
<!--   pander(caption = 'Having sleep trouble?') -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: -->

## Two-way tables

> Two-way tables or "contingency" tables compare two categorical variables.

::: columns
::: column
```{r}
cold <- case1802 |> column_to_rownames("Treatment") |> as.matrix() |> as.table() 
cold |> cbind(n = rowSums(cold)) |> pander('Vitamin C experiment')
```

:::

::: column

```{r, fig.height = 2.75, fig.width = 6}
par(mar = c(3, 4, 0.1, 6), cex = 1.5)
cold |> 
  t() |> 
  prop.table(margin = 2) |>
  barplot(legend = T, 
          args.legend = list(x = 3.75),
          ylab = 'proportion')
```

:::
:::

-   vitamin C and placebo treatments were randomly allocated to 818 volunteers
-   volunteers took treatments daily for a cold season
-   study recorded how many volunteers came down with a cold

*Is vitamin C effective at preventing common cold?*

## Inference for two proportions

We can first consider inferences on the difference in proportions:

$$\delta = p_\text{placebo} - p_\text{vitC}$$

<!-- $$\begin{cases}H_0: p_\text{placebo} - p_\text{vitamin C} \leq 0 \\ H_A: p_\text{placebo} - p_\text{vitamin C} > 0 \end{cases}$$ -->

Inferences are based on groupwise estimates:

-   point estimate: $\hat{p}_\text{placebo} - \hat{p}_\text{vitC}$

-   standard error: $\sqrt{SE^2(\hat{p}_\text{placebo}) + SE^2(\hat{p}_\text{vitC})}$

When both groups meet the conditions for inference for one proportion, the statistic

$$
Z = \frac{\hat{p}_1 - \hat{p}_2 - \delta}{SE(\hat{p}_1 - \hat{p}_2)}
$$
has a sampling distribution well-approximated by a normal model.

## Confidence interval for the difference


$$
\hat{p}_\text{placebo} - \hat{p}_\text{vitC} \pm c\times SE(\hat{p}_\text{placebo} - \hat{p}_\text{vitC})
$$
For a $(1 - \alpha)\times 100\%$ confidence interval the critical value $c$ is chosen to be the $\left(1 - \frac{\alpha}{2}\right)$ quantile of the normal model.

-   point estimate: $\hat{p}_\text{placebo} - \hat{p}_\text{vitC} = `r prop.table(cold, margin = 1)[,1] |> rev() |> diff() |> round(4)`$

-   standard error: $\sqrt{SE^2(\hat{p}_\text{placebo}) + SE^2(\hat{p}_\text{vitC})} = `r sum(apply(prop.table(cold, margin = 1), 1, prod)/rowSums(cold)) |> sqrt() |> round(4)`$

- critical value for 95% interval: `qnorm(1 - 0.05/2) = `r qnorm(0.975)``

95% confidence interval: (`r round(-diff(prop.table(cold, margin = 1)[,1]) + c(-1, 1)*qnorm(0.975)*sqrt(sum(apply(prop.table(cold, margin = 1), 1, prod)/rowSums(cold))), 4)`)

> With 95% confidence, the prevalence of common cold is estimated to be between 1.64% and 12.98% lower among adults who take daily vitamin C supplements.

## Tests for a difference in proportions

::: columns
::: column
We can also test whether vitamin C prevents common cold:

$$
\begin{cases} 
H_0: &p_\text{placebo} - p_\text{vitC} = 0\\ 
H_A: &p_\text{placebo} - p_\text{vitC} > 0 
\end{cases}
$$

Hypothesis tests use the test statistic:

$$Z = \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\hat{p}(1 - \hat{p})\left(\frac{1}{n_1} + \frac{1}{n_2}\right)}}$$

With a slightly different SE where: 
$$\hat{p} = \frac{n_1\hat{p}_1 + n_2\hat{p}_2}{n_1 + n_2}$$
:::

::: column

```{r, fig.width = 6.5, fig.height = 4}
test.out <- prop.test(cold, correct = F, alternative = 'greater')
zstat <- test.out$stat |> sqrt()
pval <- test.out$p.value

x <- seq(-4, 4, length = 10000)
y <- dnorm(x)

par(mar = c(4, 4, 3, 1),
    cex = 1.75)
curve(dnorm, from = -4, to = 8, n = 200,
     ylab = '', 
     xlab = 'Z',
     main = 'Normal model',
     xaxt = 'n', yaxt = 'n', 
     axes = F, lwd = 2)
axis(1, at = seq(-4, 6, by = 2))
title(ylab = 'sampling frequency', line = 1)

polygon(c(x[x>=zstat], max(x), zstat), c(y[x>=zstat] - 0.001, 0, 0), col="red", border = NA)

abline(v = zstat, lty = 2, lwd = 2)

legend(x = 'topright', 
       legend = paste(100*round(pval, 4), 
                      '% of \nsamples', sep = ''), 
       fill = 'red', box.lwd = 0)
```

Here $p = P(Z > `r round(zstat, 3)`) = `r round(pval, 4)`$, so:

> the data provide strong evidence that vitamin C prevents common cold.

:::
:::


## Inference in R

```{r}
vitamin <- case1802 |>
  rename_with(tolower) |>
  pivot_longer(-treatment, names_to = 'outcome', values_to = 'n') |>
  group_by(treatment, outcome) |>
  sample_n(size = n, replace = T) |>
  mutate(subj.id = row_number()) |>
  select(subj.id, treatment, outcome)
save(vitamin, file = 'data/vitamin.RData')
```

::: columns
::: column
Three steps:

1.  Construct a table of the frequency distribution by group

    -   outcomes should be columns
    -   groups should be rows

2.  Pass to `prop.test()`

The alternative reads the same way as in `t.test`.
:::

::: column
```{r, echo = T}
#| code-line-numbers: "6-8"
# variables of interest
treatment <- vitamin$treatment
outcome <- vitamin$outcome

# pass table to prop.test
table(treatment, outcome) |>
  prop.test(alternative = 'greater', 
            correct = F)
```
:::
:::

> The data provide strong evidence that vitamin C prevents common cold (Z = 2.517, *p* = 0.0059). With 95% confidence, the reduction in probability is estimated to be at least 0.0255, with a point estimate of `r prop.table(cold, margin = 1)[,1] |> rev() |> diff() |> round(4)` (SE = `r sum(apply(prop.table(cold, margin = 1), 1, prod)/rowSums(cold)) |> sqrt() |> round(4)`).


## Sampling and two-way tables

::: {.columns}

::: {.column width="55%"}
Consider this case-control study:

```{r}
smoking <- case1803 |> column_to_rownames('Smoking') |> as.matrix() |> as.table() |> t()
cbind(smoking, n = rowSums(smoking)) |> pander()
```
This is an example of *outcome-based* sampling:

- 86 lung cancer patients and 86 controls 
- can't estimate cancer prevalence

::: 

::: {.column width="45%"}
A different approach to inference is needed to analyze this data. Next time:

- tests of association in two-way tables
- inference for risk and odds ratios

:::

:::


<!-- ## Your turn: two cases -->

<!-- ```{r} -->
<!-- obesity <- case1801 |> column_to_rownames('Obesity') |> as.matrix() |> as.table() -->
<!-- smoking <- case1803 |> column_to_rownames('Smoking') |> as.matrix() |> as.table() |> t() -->

<!-- save(list = c('obesity', 'smoking', 'cold'), file = 'data/prop-cases.RData') -->
<!-- ``` -->

<!-- ::: columns -->
<!-- ::: column -->
<!-- Researchers categorized 3,112 individuals in American Samoa according to whether they were obese and recorded whether subjects had cardiovascular disease (CVD). -->

<!-- ```{r} -->
<!-- obesity |> pander() -->
<!-- ``` -->

<!-- *Test for a difference in disease rates between obese and non-obese populations, and produce an interval estimate for the difference in proportions.* -->
<!-- ::: -->

<!-- ::: column -->
<!-- Researchers identified 86 lung cancer patients and 86 controls (without lung cancer), and categorized them according to whether they were smokers or non-smokers. -->

<!-- ```{r} -->
<!-- smoking |> pander() -->
<!-- ``` -->

<!-- *Test for a difference in the proportion of smokers among cancer patients compared with controls, and produce an interval estimate for the difference.* -->
<!-- ::: -->
<!-- ::: -->

# Lecture 14: Tests of association


```{r setup 1}
library(tidyverse)
library(oibiostat)
library(pander)

red.grad <- colorRampPalette(c('#ff6459', '#4d0500'))
add.totals <- function(tbl){
  rbind(tbl, total = colSums(tbl)) |>
  cbind(total = c(rowSums(tbl), sum(tbl)))
}
asthma <- matrix(data = c(49, 781, 30, 769), 
               nrow = 2, 
               byrow = T, 
               dimnames = list(sex = c('female', 'male'),
                               asthma = c('asthma', 'no asthma'))) |>
as.data.frame() |>
rownames_to_column('sex') |>
pivot_longer(-sex, names_to = 'asthma', values_to = 'n') |>
  group_by(sex, asthma) |>
  sample_n(size = n, replace = T) |>
  mutate(subj.id = row_number()) |>
  select(subj.id, sex, asthma)
```

## Today's agenda

1. [lecture] tests of association in contingency tables
2. [lab] $\chi^2$ tests and residual analysis in R

## Do asthma rates differ by sex?

::: {.columns}

::: {.column width="45%"}
From a subsample of NHANES data:

```{r}
asthma.tbl <- table(asthma$sex, asthma$asthma)
asthma.tbl |> pander()
```

Inference for the difference in proportions:

```{r, echo = T}
table(asthma$sex, asthma$asthma) |>
  prop.test(alternative = 'two.sided', 
            conf.level = 0.95,
            correct = F)
```
:::

::: {.column width="55%"}
Interpretation:

> There is moderate evidence that asthma prevalence differs between men and women (*Z* = 2.108, *p* = 0.0436). With 95% confidence, the difference in prevalence (F - M) is estimated to be between 0.07% and 4.22%, with a point estimate of 2.15%.

This inference relies on a specific measure of association (difference in prevalence) that we can't always estimate.

*Could we test for association between sex and asthma without relying on a specific measure?*
:::

:::

## Association and independence

::: {.columns}

::: {.column width="59%"}
Consider the hypotheses:

$$
\begin{cases}
H_0: &\text{asthma}\perp\text{sex} \; &(\text{independence})\\
H_A: &\neg(\text{asthma}\perp\text{sex}) \; &(\text{association})
\end{cases}
$$

Consider also the proportions:
```{r}
data.frame(row = c(2, 2, 3), col = c(2, 3, 2)) |>
  as.matrix() |>
  emphasize.strong.cells()

asthma.tbl |>
  prop.table() |>
  add.totals() |>
  pander()
```

- $p_{ij}$: proportion of observations in cell $ij$ 
- $p_i$, $p_j$: marginal proportions in row $i$ or column $j$


:::

::: {.column width="41%"}

If sex and asthma are independent:

$$
p_{ij} \approx p_i \times p_j
$$

For example, we'd expect: 

$$
0.4721 \approx 0.4905 \times 0.9515
$$

In other words:

- 49% of respondents are men
- 95% of respondents don't have asthma
- so roughly 49% of 95% would be men without asthma


:::

:::

## Basis for a test: expected counts

Expected proportions translate directly to expected counts:
$$p_{ij} = p_i \times p_j 
\quad\Longleftrightarrow\quad n_{ij} = \frac{n_{i\cdot} \times n_{\cdot j}}{n}$$

::: {.columns}

::: {.column width="40%"}
Actual counts:

&nbsp; | O1 | O2 | total
---+---+---+---
**G1** | $n_{11}$ | $n_{12}$ | $\color{red}{n_{1\cdot}}$
**G2** | $n_{21}$ | $n_{22}$ | $\color{orange}{n_{2\cdot}}$
**total** | $\color{blue}{n_{\cdot 1}}$ | $\color{green}{n_{\cdot 2}}$ | $n$
:::

::: {.column width="60%"}
Expected counts under independence:

&nbsp; | O1 | O2 | total
---+---+---+---
**G1** | $\hat{n}_{11} = \frac{\color{red}{n_{1\cdot}} \color{black}{\times} \color{blue}{n_{\cdot 1}}}{n}$ | $\hat{n}_{12} = \frac{\color{red}{n_{1\cdot}} \color{black}{\times} \color{green}{n_{\cdot 2}}}{n}$ | $\color{red}{n_{1\cdot}}$
**G2** | $\hat{n}_{21} = \frac{\color{orange}{n_{2\cdot}} \color{black}{\times} \color{blue}{n_{\cdot 1}}}{n}$ | $\hat{n}_{22} = \frac{\color{orange}{n_{2\cdot}} \color{black}{\times} \color{green}{n_{\cdot 2}}}{n}$ | $\color{orange}{n_{2\cdot}}$
**total** | $\color{blue}{n_{\cdot 1}}$ | $\color{green}{n_{\cdot 2}}$ | $n$

:::

:::
Idea for a test of independence: 

- reject $H_0$ if actual and expected counts differ enough across the table
- *i.e.*, reject $H_0$ when $n_{ij} - \hat{n}_{ij}$ is large across $i, j$

## Computing expected counts

::: {.columns}

::: {.column width="40%"}
Actual counts:

&nbsp; | O1 | O2 | total
---+---+---+---
**G1** | $n_{11}$ | $n_{12}$ | $\color{red}{n_{1\cdot}}$
**G2** | $n_{21}$ | $n_{22}$ | $\color{orange}{n_{2\cdot}}$
**total** | $\color{blue}{n_{\cdot 1}}$ | $\color{green}{n_{\cdot 2}}$ | $n$
:::

::: {.column width="60%"}
Expected counts under independence:

&nbsp; | O1 | O2 | total
---+---+---+---
**G1** | $\hat{n}_{11} = \frac{\color{red}{n_{1\cdot}} \color{black}{\times} \color{blue}{n_{\cdot 1}}}{n}$ | $\hat{n}_{12} = \frac{\color{red}{n_{1\cdot}} \color{black}{\times} \color{green}{n_{\cdot 2}}}{n}$ | $\color{red}{n_{1\cdot}}$
**G2** | $\hat{n}_{21} = \frac{\color{orange}{n_{2\cdot}} \color{black}{\times} \color{blue}{n_{\cdot 1}}}{n}$ | $\hat{n}_{22} = \frac{\color{orange}{n_{2\cdot}} \color{black}{\times} \color{green}{n_{\cdot 2}}}{n}$ | $\color{orange}{n_{2\cdot}}$
**total** | $\color{blue}{n_{\cdot 1}}$ | $\color{green}{n_{\cdot 2}}$ | $n$

:::

:::

For the asthma example:

::: {.columns}

::: {.column}
```{r}
asthma.tbl |> add.totals() |> pander(caption = 'Actual')
```
:::

::: {.column}
```{r}
rslt <- chisq.test(asthma.tbl, correct = F)
rslt$expected |> 
  add.totals() |>
  pander(caption = 'Expected')
```

:::

:::


## The chi-square ($\chi^2$) statistic

A measure of the amount by which actual counts differ from expected counts under independence is the **chi** (pronounced /ˈkaɪ ) **square statistic**:

$$
\chi^2 = \sum_{ij} \frac{\left(n_{ij} - \hat{n}_{ij}\right)^2}{\hat{n}_{ij}} 
\qquad\left(\sum_\text{all cells} \frac{(\text{observed} - \text{expected})^2}{\text{expected}}\right)
$$

::: {.columns}

::: {.column}
Cell-wise calculation:

------------------------------------------------------------------------------
   &nbsp;     asthma                           no asthma 
------------ -------------------------------- --------------------------------
 **female**   $\frac{(49 - 40.25)^2}{40.25}$  $\frac{(781 - 789.7)^2}{789.7}$  

  **male**    $\frac{(30 - 38.75)^2}{38.75}$  $\frac{(769 - 760.3)^2}{760.3}$   
------------------------------------------------------------------------------
:::

::: {.column}
Result:
```{r}
rslt$residuals^2 |> pander()
```
:::

:::

Chi-square statistic: 
$$
\chi^2 
= `r rslt$residuals^2 |> as.numeric() |> round(4) |> str_flatten(collapse = ' + ')` 
= `r rslt$statistic |> round(4)`
$$

## Sampling distribution for $\chi^2$

::: {.columns}

::: {.column}
Under $H_0$, the $\chi^2$ statistic has a sampling distribution that can be approximated by a $\chi^2_1$ model.

- subscript indicates degrees of freedom parameter

The model assumes no expected counts are too small.

- rule of thumb: at least 10 ($\hat{n}_{ij} \geq 10$)
- consequences: if $\hat{n}_{ij}$ are too small, the statistic is inflated relative to the model, leading to a higher type I error rate
:::

::: {.column}

```{r, fig.width = 10, fig.height = 6.5}
reds <- red.grad(5)
par(mar = c(4, 4, 2, 1), cex = 2.5)
k <- 1
curve(sqrt(dchisq(x, k)), from = 0, to = 15, n = 200,
      xlab = expr(paste(chi^2, ' statistic')),
      ylab = '',
      main = expr(paste(chi[df]^2, ' model')),
      xaxt = 'n', yaxt = 'n', axes = F,
      col = reds[1], lwd = 2)
axis(1, at = seq(0, 15, by = 2))
title(ylab = 'sampling frequency', line = 1)

for(k in 2:5){
curve(sqrt(dchisq(x, k)), from = 0, to = 15, n = 200,
      xlab = '', ylab = '', main = '',
      xaxt = 'n', yaxt = 'n',
      col = reds[k], add = T, lwd = 2)
}

legend('topright', 
       legend = (paste("df = ", 1:5, sep = '')),
       col = reds, lwd = 2, cex = 0.75)
```

:::

:::


## Computing $p$ values


::: {.columns}

::: {.column width="60%"}

$$
\begin{cases}
H_0: &\text{asthma}\perp\text{sex} \; &(\text{independence})\\
H_A: &\neg(\text{asthma}\perp\text{sex}) \; &(\text{association})
\end{cases}
$$

To determine the test outcome, find the $p$-value: 

$$
P(\chi^2_1 > \chi^2_\text{obs}) = P(\chi^2_1 > 4.074) = 0.0435
$$

So if asthma and sex were independent, only 4% of random samples would produce a table that deviates from expected counts by more than what we observed.

:::

::: {.column width="40%"}
```{r, fig.width=6, fig.height=5}
par(mar = c(4, 4, 3, 1),
    cex = 1.75)
x <- seq(0.1, 10, length = 1000)
y <- sqrt(dchisq(x, df = 1))
chistat <- 4.074
curve(sqrt(dchisq(x, df = 1)), from = 0.1, to = 10, n = 500, lwd = 2,
xlab = expression(chi^2), ylab = '', main = expr(paste(chi^2, ' model')),
yaxt = 'n', axes = F)
axis(side = 1, at = (0:10))
title(ylab = 'frequency', line = 0.5)
polygon(c(chistat, x[x>chistat], max(x)), c(0, y[x>chistat] - 0.005, 0), col="red", border = NA)
abline(v = 4.074, lty = 2, col = 'red', lwd = 2)
legend(x = 'topright', 
       legend = c('4.35% of samples', '95.65% of samples'), 
       fill = c('red', 'white'))
```

```{r, echo = T}
pchisq(4.074, df = 1, lower.tail = F)
```

:::

:::

## Implementation in R

The R implementation is `chisq.test(...)`.

- input: contingency table
- no constraints on row/column arrangement

::: {.columns}

::: {.column}
```{r, echo = T}
# construct table and pass to chisq.test
table(asthma$sex, asthma$asthma) |> 
  chisq.test(correct = F)
```
:::

::: {.column}
> The data provide moderate evidence that asthma prevalence is associated with sex ($\chi^2$ = 4.074 on 1 degree of freedom, *p* = 0.0435).
:::

:::

## Residuals in $\chi^2$ tests

::: {.columns}

::: {.column}
The **residual** for each cell is defined as a standardized difference between the observed and expected count:

$$r_{ij} = \frac{n_{ij} - \hat{n}_{ij}}{\sqrt{\hat{n}_{ij}}} $$

Examining residuals can indicate the source(s) of an inferred association. 

- $r_{ij} > 0$: observation exceeds expectation
- $r_{ij} < 0$: observation is under expectation
- large $|r_{ij}|$ explain the association

:::

::: {.column}
```{r, echo = T, eval = F}
# store test result
rslt <- chisq.test(asthma.tbl, correct = F)

# examine residuals
rslt$residuals
```
```{r}
rslt <- chisq.test(asthma.tbl, correct = F)
rslt$residuals |> pander()
```

Look for the largest residuals:

> Asthma prevalence is higher-than-expected among women and lower-than-expected among men.
:::

:::

## Continuity correction

The $\chi^2$ test for independence is typically applied with Yates' continuity correction.

::: {.columns}

::: {.column}
This consists in using a modified version of the test statistic:

$$
\chi^2_\text{Yates} = \sum_{ij} \frac{\left(|n_{ij} - \hat{n}_{ij}| - 0.5\right)^2}{\hat{n}_{ij}}
$$

- every other detail of the test is the same
- doesn't change expected counts
- residuals are still computed as $\frac{n_{ij} - \hat{n}_{ij}}{\sqrt{\hat{n}_{ij}}}$
:::

::: {.column}
Implementation:
```{r, echo = T}
# construct table and pass to chisq.test
table(asthma$sex, asthma$asthma) |> 
  chisq.test(correct = T)
```

Note the larger $p$-value -- this changes the conclusion!
:::

:::

## Spot any similarities?

Compare the $\chi^2$ test with inference on the difference in proportions.

::: {.columns}

::: {.column}
```{r, echo = T}
# chi square test
table(asthma$sex, asthma$asthma) |> 
  chisq.test(correct = T)
```

```{r, echo = T}
# difference in proportions
table(asthma$sex, asthma$asthma) |> 
  prop.test(alternative = 'two.sided', 
            conf.level = 0.95, 
            correct = T)
```
:::

::: {.column}
- the tests are identical!

- the difference in proportions $\hat{p}_F - \hat{p}_M$ is one specific measure of association

- next time we'll learn about other measures, which also have the same inference attached

:::

:::


## Extending to $I \times J$ tables

::: {.columns}

::: {.column}
FAMuSS data:
```{r}
data(famuss)
tbl <- xtabs(~ race + actn3.r577x, data = famuss) 
tbl |> add.totals() |> pander()
```
:::

::: {.column}
Expected counts:
```{r}
chisq.test(tbl)$expected |> round(2) |> add.totals() |> pander()
```
:::

:::

- expected counts and chi-square statistic are calculated exactly the same way
- degrees of freedom are now $(I - 1)\times(J - 1)$
- appropriate provided all $\hat{n}_{ij} > 1$ and most (~80%) $\hat{n}_{ij} \geq 5$

## Extending to $I\times J$ tables

In detail:

---------------------------------------------------------------------------------------------------------------------
     &nbsp;       CC                                CT                                TT  
---------------- --------------------------------  --------------------------------  --------------------------------
 **African Am**   $\frac{(16 - 7.85)^2}{7.85}$     $\frac{(6 - 11.84)^2}{11.84}$     $\frac{(5 - 7.306)^2}{7.306}$  

   **Asian**      $\frac{(21 - 15.99)^2}{15.99}$    $\frac{(18 - 24.13)^2}{24.13}$    $\frac{(16 -14.88)^2}{14.88}$  

 **Caucasian**    $\frac{(125 - 135.8)^2}{135.8}$   $\frac{(216 - 204.9)^2}{204.9}$   $\frac{(126 - 126.4)^2}{126.4}$ 

  **Hispanic**     $\frac{(4 - 6.687)^2}{6.687}$    $\frac{(10 - 10.09)^2}{10.09}$     $\frac{(9 - 6.224)^2}{6.224}$  

   **Other**       $\frac{(7 - 6.687)^2}{6.687}$    $\frac{(11 - 10.09)^2}{10.09}$     $\frac{(5 - 6.224)^2}{6.224}$  
---------------------------------------------------------------------------------------------------------------------

Then:

$$\begin{cases} &\chi^2 = \sum \text{all cells above} = 19.4 \\
&P(\chi^2_{8} > 19.4) = 0.01286 \end{cases}
\quad\Longrightarrow\quad \text{reject hypothesis of no association}$$

## Inference for $I\times J$ tables in R

::: {.columns}

::: {.column}
The implementation is the same as for a $2\times 2$ table:
```{r, echo = T}
# construct table and pass to chisq.test
table(famuss$race, famuss$actn3.r577x) |>
  chisq.test()
```
:::

::: {.column}
> The data provide evidence of an association between race and genotype ($\chi^2$ = 19.4 on 8 degrees of freedom, *p = 0.01286*).

:::

:::

*Which genotype/race combinations are contributing most to this inferred association?*

## Residual analysis

::: {.columns}

::: {.column}
```{r, echo = T, eval = F}
# store result of test; display residuals
rslt <- chisq.test(tbl)
rslt$residuals
```
```{r}
# store result of test; display residuals
rslt <- chisq.test(tbl)
rslt$residuals |> pander()
```

:::

::: {.column}
Again look for the largest absolute residuals to explain inferred association.

> African American and Asian populations have higher CC and lower CT frequencies than would be expected if genotype were independent of race.
:::

:::

# Lecture 15: Measures of association


```{r}
library(tidyverse)
library(pander)
library(Sleuth3)
library(epitools)
load('data/smoking.RData')
load('data/asthma.RData')
```

## Today's agenda

1. [lecture] relative risk and odds ratios
2. [lab] using `epitools` to estimate risk and odds ratios in R
3. [discussion] final project guidelines

## From last time: smoking

::: {.columns}

::: {.column}
Researchers sampled 86 lung cancer patients (cases) and 86 healthy individuals (controls) and recorded smoking status:
```{r}
smoking |> select(-subj.id) |> table() |> pander()
```

At the 5% level, association is significant:

```{r, echo = T}
smoking.tbl <- table(smoking$group, 
                     smoking$smoking) 
smoking.tbl |> chisq.test()
```

:::

::: {.column}
But there is a difficulty for measuring the association:

- data are really *two* independent samples
- so can't estimate cancer prevalence

:::

:::

## Case-control studies

> Outcome-based sampling limits which proportions are estimable

![](img/casecontrol.png)

- cases and controls are two independent samples
- exposure prevalence is estimable (within case and control populations)
- case prevalence is **not** estimable

## Smoking example

::: {.columns}

::: {.column}
So due to study design, the only estimable difference in proportions is:

```{r, echo = T}
smoking.prop.test <- smoking.tbl |> prop.test()
smoking.prop.test$conf
```
:::

::: {.column}

> With 95% confidence, the proportion of smokers among cancer patients is estimated to be betwteen 2.9 and 22.7 percentage points higher than among controls.

:::

:::

This makes for an awkward conclusion:

- we really want to know how smoking affects cancer risk
- ...not how cancer affects smoking risk

So for this kind of study, we need a different measure of association: ***odds ratios***.

## Odds 

> The odds of an outcome measure its relative likelihood

If $p$ is the true cancer prevalence (a population proportion), then the **odds** of cancer are defined as the ratio:
$$
\text{odds} = \frac{p}{1 - p}
$$
The odds represent the factor by which cancer is more likely than not, *e.g.*:

- $\text{odds} = 2$ indicates the outcome (cancer) is twice as likely as not
- $\text{odds} = 1/2$ indicates the outcome (cancer) is half as likely as not

## Odds ratios

> An odds ratio compares the relative likelihood of an outcome between two groups

::: {.columns}

::: {.column width="60%"}
If $a, b, c, d$ are population proportions:

$\;$ | outcome 1 (O1) | outcome 2 (O2)
---|:---:|:---:
group 1 (G1) | [a]{style="color:red"} | [b]{style="color:blue"} 
group 2 (G2) | [c]{style="color:orange"} | [d]{style="color:purple"}


- the odds of outcome 1 in group 1 is $\frac{\textcolor{red}{a}}{\textcolor{blue}{b}}$
- the odds of outcome 1 in group 2 is $\frac{\textcolor{orange}{c}}{\textcolor{purple}{d}}$


:::

::: {.column width="40%"}
The **odds ratio** of outcome 1 comparing group 1 with group 2 is:

$$
\frac{\text{odds}_{G1}(O1)}{\text{odds}_{G2}(O1)}
= \frac{\textcolor{red}{a}/\textcolor{blue}{b}}{\textcolor{orange}{c}/\textcolor{purple}{d}}
= \frac{\textcolor{red}{a}\textcolor{purple}{d}}{\textcolor{blue}{b}\textcolor{orange}{c}}
$$
A surprising algebraic fact is that:

$$
\frac{\text{odds}_{G1}(O1)}{\text{odds}_{G2}(O1)} 
=\frac{\text{odds}_{O1}(G1)}{\text{odds}_{O2}(G1)}
$$

:::

:::

*This means that the ratio of odds of cancer between smokers and nonsmokers can be estimated from the ratio of odds of smoking between cases and controls.*

## Estimating odds ratios

::: {.columns}

::: {.column width="60%"}
For notation let:

- $\hat{p}_\text{case}$ : proportion of smokers among cases
- $\hat{p}_\text{control}$ : proportion of smokers among controls
:::

::: {.column width="40%"}
```{r}
smoking.tbl |> pander()
```
:::

:::

An estimate of ratio of odds of cancer among smokers compared with nonsmokers ($\omega$) is the estimated ratio of odds of smoking among cancer patients compared with controls:
$$
\hat{\omega} 
=\left(\frac{\hat{p}_\text{case}}{1 - \hat{p}_\text{case}}\right)\Bigg/\left(\frac{\hat{p}_\text{control}}{1 - \hat{p}_\text{control}}\right)
= \frac{83/3}{72/14} = 5.38
$$

> It is estimated that the odds of lung cancer are 5.38 times greater for smokers compared with nonsmokers.

## Confidence intervals for odds ratios

The sampling distribution of the log odds ratio can be approximated by a normal model.

$$
\log\left(\hat{\omega}\right) \pm c \times SE\left(\log\left(\hat{\omega}\right)\right) 
\quad\text{where}\quad
SE\left(\log\left(\hat{\omega}\right)\right) = \sqrt{\frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}}
$$


::: {.columns}

::: {.column}
The `oddsratio(...)` function in the `epitools` package will compute and back-transform the interval for you.
```{r, echo = T, eval = F}
oddsratio(smoking.tbl, rev = 'both', 
          method = 'wald', correction = T)
```
```{r}
oddsratio(smoking.tbl, rev = 'both', 
          method = 'wald', correction = T)$measure
```

:::

::: {.column}
- critical value $c$ from the normal model
- exponentiate to obtain an interval for $\omega$

> With 95% confidence, the odds of lung cancer are estimated to be between 1.49 and 19.47 times greater for smokers compared with nonsmokers.
:::

:::

## Implementation details

::: {.columns}

::: {.column width="45%"}
```{r, echo = T, eval = F}
oddsratio(smoking.tbl, rev = 'both',
          method = 'wald', correction = T)
```

```{r}
oddsratio(smoking.tbl, rev = 'both',
          method = 'wald', correction = T)[1:3]
```

:::

::: {.column width="55%"}

`oddsratio` is picky about data inputs:

- outcome of interest should be *second* column
- group of interest should be *second* row
- the `rev` argument will reverse orders
    
    + `rev = neither` keeps original orientation (default)
    + `rev = rows` reverses order of rows
    + `rev = columns` reverses order of columns
    + `rev = both` reverses both

:::

:::

## Interpretation

::: {.columns}

::: {.column width="45%"}
```{r, echo = T, eval = F}
oddsratio(smoking.tbl, rev = 'both',
          method = 'wald', correction = T)
```

```{r}
oddsratio(smoking.tbl, rev = 'both',
          method = 'wald', correction = T)[1:3]
```

:::

::: {.column width="55%"}
First report the test result, then the measure of association:

> The data provide evidence of an association between smoking and lung cancer ($\chi^2$ = 6.53 on 1 degree of freedom, *p* = 0.1062). With 95% confidence, the odds of cancer are estimated to be between 1.49 amd 19.47 times greater among smokers compared with nonsmokers, with a point estimate of 5.38.


:::

:::

Comments:

- the `chi.square` $p$-value is from the test of association/independence
- if assumptions aren't met, can use the `fisher.exact` instead (see V&H 8.3.5)

## Asthma data

::: {.columns}

::: {.column}
Consider estimating the difference in proportions:
```{r}
asthma |> select(-subj.id) |> table() |> pander()
```

:::

::: {.column}
```{r, echo = T}
asthma.tbl <- table(asthma$sex, asthma$asthma)
prop.test(asthma.tbl, conf.level = 0.9)
```
:::

:::

> With 90% confidence, asthma prevalence is estimated to be between 0.28 and 4.01 percentage points higher among women than among men.

Is a difference of up to 4 percentage points practically meaningful? Well, it depends:

- yes if prevalence is very low
- no if prevalence is very high


## Relative risk

If $p_F, p_M$ are the (population) proportions of women and men with asthma, then the **relative risk** of asthma among women compared with men is defined as:

$$
RR = \frac{p_F}{p_M}
\qquad
\left(\frac{\text{risk among women}}{\text{risk among men}}\right)
$$

An estimate of the relative risk is simply the ratio of estimated proportions. For the asthma data, an estimate is:
$$
\widehat{RR} = \frac{\hat{p}_F}{\hat{p}_M} = \frac{0.059}{0.038} = 1.57 
$$

> It is estimated that the risk of asthma among women is 1.57 times greater than among men.

## Confidence intervals for relative risk

A normal model can be used to approximate the sampling distribution of $\log(RR)$ and construct a confidence interval. If $\hat{p}_1$ and $\hat{p}_2$ are the two estimated proportions:

$$\log\left(\widehat{RR}\right) \pm c \times SE\left(\log\left(\widehat{RR}\right)\right)
\quad\text{where}\quad SE\left(\log\left(\widehat{RR}\right)\right) = \sqrt{\frac{1 - p_1}{p_1n_1} + \frac{1 - p_2}{p_2n_2}}$$

::: {.columns}

::: {.column width="50%"}
The `riskratio(...)` function from the `epitools` package will compute and back-transform the interval for you:
```{r, echo = T, eval = F}
riskratio(asthma.tbl, 
          rev = 'both', conf.level = 0.9,
          method = 'wald', correction = T)
```
```{r}
riskratio(asthma.tbl, rev = 'both', conf.level = 0.9,
          method = 'wald', correction = T)$measure
```
:::

::: {.column width="50%"}
- critical value $c$ from normal model
- exponentiate for an interval for $RR$

> With 90% confidence, the risk of asthma is estimated to be betwen 1.08 and 2.28 times greater for women than for men, with a point estimate of 1.57.
:::

:::


## Implementation details

::: {.columns}

::: {.column width="45%"}
```{r, echo = T, eval = F}
riskratio(asthma.tbl, 
          rev = 'both', conf.level = 0.9,
          method = 'wald', correction = T)
```

```{r}
riskratio(asthma.tbl, rev = 'both', conf.level = 0.9,
          method = 'wald', correction = T)[1:3]
```

:::

::: {.column width="55%"}

Implementation is identical to `oddsratio`:

- outcome of interest should be *second* column
- group of interest should be *second* row
- rearrange the input data using `rev`

Also similarly:

- `chi.square` gives the $p$-value for the $\chi^2$ test of association
- `fisher.exact` gives an exact p-value you can use if assumptions aren't met

:::

:::

## Interpretation

::: {.columns}

::: {.column width="45%"}
```{r, echo = T, eval = F}
riskratio(asthma.tbl, 
          rev = 'both', conf.level = 0.9,
          method = 'wald', correction = T)
```

```{r}
riskratio(asthma.tbl, rev = 'both', conf.level = 0.9,
          method = 'wald', correction = T)[1:3]
```

:::

::: {.column width="55%"}
First report the test result, then the measure of association:

> The data provide evidence at the 10% significance level of an association between asthma and sex ($\chi^2$ = 3.62 on 1 degree of freedom, *p* = 0.057). With 90% confidence, the risk of asthma is estimated to be betwen 1.08 and 2.28 times greater for women than for men, with a point estimate of 1.57.


:::

:::

Comments:

- the `chi.square` $p$-value is from the test of association/independence
- if assumptions aren't met, can use the `fisher.exact` instead

## Further examples: cyclosporiasis {.scrollable}

::: {.columns}

::: {.column}
An outbreak of cyclosporiasis was detected among residents of New Jersey. In a case-control study, investigators found that 21 of 30 case-patients and 4 of 60 controls had eaten raspberries.

```{r}
cyclo.tbl <- matrix(data = c(21, 4, 9, 56), 
               nrow = 2, 
               byrow = T, 
               dimnames = list(raspberries = c('raspberries', 'no raspberries'),
                               cyclosporiasis = c('case', 'control'))
                )

cyclo.tbl |> t() |> pander()
```

- outcome-based sampling
- odds ratio should be used

:::

::: {.column}
```{r, echo = T}
# check test assumptions
chisq.test(cyclo.tbl)$expected
```

```{r, echo = T, eval = F}
# compute measure of association and p value
oddsratio(cyclo.tbl, rev = 'both',
          method = 'wald', correction = T)
```

```{r}
oddsratio(cyclo.tbl, rev = 'both',
          method = 'wald', correction = T)[2:3]
```


:::

:::

> The data provide very strong evidence of an association between eating raspberries during the outbreak and case incidence ($\chi^2$ = 36.89 on 1 degree of freedom, *p* < 0.0001). With 95% confidence, the odds of indcidence are estimated to be between 9.08 and 117.5 times higher among NJ residents who ate raspberries during the outbreak.

## Further examples: smoking and CHD

::: {.columns}

::: {.column}
A cohort study of 3,000 smokers and 5,000 nonsmokers investigated the link between smoking and the development of coronary heart disease (CHD) over 1 year.


```{r}
chd.tbl <- matrix(data = c(84, 2916, 87, 4913), 
               nrow = 2, 
               byrow = T, 
               dimnames = list(smoker = c('smoker', 'nonsmoker'),
                               chd = c('CHD', 'no CHD'))
                )

chd.tbl |> pander()
```

- two independent samples, but **not** outcome-based sampling
:::

::: {.column}
```{r, echo = T}
# check test assumptions
chisq.test(chd.tbl)$expected
```
```{r, echo = T, eval = F}
riskratio(chd.tbl, rev = 'both',
          method = 'wald', correction = T)
```
```{r}
riskratio(chd.tbl, rev = 'both',
          method = 'wald', correction = T)[2:3]
```
:::

:::

> The data provide very strong evidence that smoking is associated with coronary heart disease ($\chi^2$ = 9.5711 on 1 degree of freedom, *p* = 0.00198). With 95% confidence, the risk of CHD is estimated to be between 1.196 and 2.164 times greater among smokers compared with nonsmokers, with a point estimate of 1.609.

## Further examples: malaria vaccine {.scrollable}

::: {.columns}

::: {.column}
In a randomized trial for a malaria vaccine, 20 individuals were randomly allocated to receive a dose of the vaccine or a placebo.


```{r}
malaria.tbl <- openintro::malaria |> table()

malaria.tbl |> pander()
```

- small sample size, so expected counts are likely too small to meet $\chi^2$ test assumptions
:::

::: {.column}
```{r, echo = T}
# check test assumptions
chisq.test(malaria.tbl)$expected
```
```{r, echo = T, eval = F}
riskratio(malaria.tbl, rev = 'columns',
          method = 'wald', correction = T)
```
```{r}
riskratio(malaria.tbl, rev = 'columns',
          method = 'wald', correction = T)[2:3]
```
:::

:::

> The data provide moderate evidence that the malaria vaccine is effective (Fisher's exact test, *p* = 0.0141). 

A twist: how to interpret the CI? Answer: $\text{efficacy = 1 - RR}$

> With 95% confidence, the risk of infection is estimated to be between 27.88% and 82.31% lower among vaccinated individuals, with a point estimate of 64.29% efficacy.

# Lecture 16: Simple linear regression


```{r}
library(tidyverse)
library(pander)
load('data/prevend.RData')
load('data/kleiber.RData')
prevend <- prevend
hand.fit <- readRDS('fns/handfit.rds')
red.grad <- colorRampPalette(c('#ff6459', '#4d0500'))
reds <- red.grad(6)
```


## Today's agenda

1. [lecture] estimation and inference for simple linear regression
2. course evaluations
3. final scheduling
4. [lab] fitting SLR models in R

## PREVEND data

::: {.columns}

::: {.column width="70%"}
Ruff Figural Fluency Test (RFFT) is a cognitive assessment.

- measures nonverbal capacity for initiation, planning, and divergent reasoning
- scale: 0 (worst) to 175 (best)
    

:::

::: {.column width="30%"}
```{r}
prevend |>
  head(3) |>
  pander()
```

:::

::: 

::: {.columns}

::: {.column width="50%"}
```{r, fig.width=5, fig.height=3.5, fig.align='center'}
age <- prevend$age
rfft <- prevend$rfft
par(mar = c(4, 4, 1, 1), cex = 1.25)
plot(age, rfft)
```

:::

::: {.column width="50%"}

> How much does cognitive ability as measured by RFFT decline with age on average?

:::

:::

## Best-fitting line

::: {.columns}

::: {.column}
```{r, fig.width = 5, fig.height = 3.5}
par(mar = c(4, 4, 1, 1), cex = 1.25)
b <- cor(age, rfft)*sd(rfft)/sd(age)
a <- mean(rfft) - b*mean(age)
plot(age, rfft)
abline(b = b, a = a, col = 'blue', lwd = 2)
```
:::

::: {.column}
Previously you found the best-fitting line:

$$
\text{RFFT} = `r round(a, 3)` - `r round(abs(b), 3)` \times \text{age}
$$

> With each year of age, RFFT decreases by `r round(abs(b), 3)` points on average.
:::

:::



$$
\begin{align}
\text{slope}: 
\quad`r round(b, 3)` &= \text{cor}(\text{age}, \text{RFFT})\times\frac{SD(\text{RFFT})}{SD(\text{age})} 
\\
\text{intercept}:
\quad`r round(a, 3)` &= \text{mean}(\text{RFFT}) - (`r round(b, 3)`)\times\text{mean}(\text{age})
\end{align}
$$

## Bias and error

::: {.columns}

::: {.column}
Recall how you found this line:

```{r, fig.width = 5, fig.height = 4, results = 'hide'}
par(mar = c(4, 4, 2, 1), cex = 1.25)
hand.fit(age, rfft, b = b, res = T)
```

:::

::: {.column}
Bias and error are measured via residuals:
$$
\textcolor{red}{e_i} = y_i - \textcolor{blue}{\hat{y}_i}
$$

- $\text{bias} = -\frac{1}{n}\sum_i \textcolor{red}{e_i}$
- $\text{SSE} = \sum_i \textcolor{red}{e_i}^2$

We said that the best-fitting line achieved two conditions:

- **no bias**: underestimates and overestimates equally often
- **minimal error**: as close as possible to as many data points as possible

:::

:::





## The SLR model

::: {.columns}

::: {.column}
The simple linear regression model is:

$$
Y 
= \textcolor{blue}{\underbrace{\beta_0 + \beta_1 x}_\text{mean}} + 
\textcolor{red}{\underbrace{\epsilon}_\text{error}}
$$
:::

::: {.column}
- continuous response $Y$
- explanatory variable $x$
- regression coefficients $\beta_0, \beta_1$
- model error $\epsilon$
:::

:::

The values that minimize error subject to the model being unbiased are:

$$\begin{align*}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} &\quad(\text{unbiased}) \\
\hat{\beta}_1 &= \frac{s_y}{s_x}\times r  &\quad(\text{minimizes SSE})
\end{align*}$$

These are called the **least squares estimates**.


## Least squares estimates in R

According to the model, a one-unit increment in $x$ corresponds to a $\beta_1$-unit change in mean $Y$:

::: {.columns}

::: {.column}
```{r, echo = T}
# fit model
fit <- lm(formula = rfft ~ age, data = prevend)
fit
```
:::

::: {.column}
> With each additional year of age, mean RFFT score decreases by an estimated `r -coef(fit)[2] |> round(3)` points.

:::

:::

- `formula = <RESPONSE> ~ <EXPLANATORY>` specifies the model
- `data = <DATAFRAME>` specifies the observations

## Error variability and model fit

The residual standard deviation provides an estimate of error variability:

$$\textcolor{\red}{\hat{\sigma}} = \sqrt{\frac{1}{n - 2} \sum_i e_i^2} \qquad\text{(estimated error variability)}$$

::: {.columns}

::: {.column width="45%"}
```{r fig.width = 5, fig.height = 4, results = 'hide'}
age_seq <- modelr::seq_range(age, 4)[-4]
preds <- predict(fit, newdata = data.frame(age = age_seq), se = T)

par(mar = c(4, 4, 1, 1), cex = 1.2)
plot(age, rfft)
arrows(x0 = age_seq, y0 = preds$fit - 2*sigma(fit), 
       x1 = age_seq, y1 = preds$fit + 2*sigma(fit), 
       lwd = 2, code = 3, length = 0.1, angle = 30, col = 'red')
abline(a = a, b = b, col = 'blue', lwd = 2)
arrows(x0 = 82.5, y0 = min(rfft), x1 = 82.5, y1 = max(rfft), 
       lwd = 2, code = 3, length = 0.1, angle = 30, col = 'darkgrey')

n <- nrow(prevend)
```
:::

::: {.column width="55%"}
The proportion of variability explained by the model is:
$$
R^2 = 1 - \frac{(n - 2)\textcolor{red}{\hat{\sigma}^2}}{(n - 1)\textcolor{darkgrey}{s_y^2}}
\quad\left(1 - \frac{\text{error variability}}{\text{total variability}}\right)
$$

```{r, echo = T}
1 - (n - 2)*sigma(fit)^2/((n - 1)*var(rfft))
```

> Age explains 40.43% of variability in RFFT.
:::

:::

## Standard errors for the coefficients

Standard errors for the coefficients are:

$$SE\left(\hat{\beta}_0\right) = \hat{\sigma}\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{(n - 1)s_x^2}} \qquad\text{and}\qquad
SE\left(\hat{\beta}_1\right) = \hat{\sigma}\sqrt{\frac{1}{(n - 1)s_x^2}}$$

While you won't need to know these formulae, do notice that:

- more data $\longrightarrow$ less sampling variability
- more spread in $x$ $\longrightarrow$ less sampling variability

## Inference for the coefficients 

::: {.columns}

::: {.column}
If the errors are symmetric and unimodal, then the sampling distribution of
$$
T = \frac{\hat{\beta}_1 - \beta_1}{SE(\beta_1)}
$$
is well-approximated by a $t_{n - 2}$ model.

1. Significance test:
$\begin{cases} H_0: \beta_1 = 0 \\ H_A: \beta_1 \neq 0 \end{cases}$

2. Confidence interval: 
$\hat{\beta}_1 \pm c\times SE\left(\hat{\beta}_1\right)$
:::

::: {.column}
```{r, fig.width=6.5, fig.height=4, fig.align='center'}
x <- seq(-14, 14, length = 10000)
y <- dt(x, df = 206)^(1/12)
tstat <- -11.82

par(mar = c(5, 4, 1, 1), cex = 1.5)
curve(dt(x, 206)^(1/12), from = -14, to = 14, n = 500,
      xaxt = 'n', yaxt = 'n', axes = F, xlab = '', ylab = '', main = expr(paste(t[208 - 2], ' model')))
title(xlab = expression(paste("T = ", frac(hat(beta)[1], SE(hat(beta)[1])))), line = 4)
title(ylab = 'sampling density', line = 1)
axis(side = 1, at = seq(-14, 14, by = 4))
abline(v = tstat, lty = 4, lwd = 2)
abline(v = -tstat, lty = 4, lwd = 2)
text(tstat + 4, 0.05^(1/12), paste("T = ", round(tstat, 3)))

polygon(c(min(x), x[x<=tstat], tstat), c(y[x<=tstat] - 0.001, 0, 0), 
        col=reds[1], border = NA)
polygon(c(x[x>=-tstat], max(x), -tstat), c(y[x>=-tstat], 0, 0),
        col = reds[3], border = NA)

legend('topright', fill = reds[c(1, 3)],
       legend = c(paste(round(pt(tstat, df = 206), 3)*100, "% of samples", sep = ''),
                  paste(round(pt(tstat, df = 206), 3)*100, "% of samples", sep = '')),
       cex = 0.7)
```

- $P(T > |T_\text{obs}|) \approx 0$: very strong evidence of an association (true slope is not zero)
- confidence interval using $t_{206}$ critical value: (`r confint(fit)[2, ] |> round(3)`)
:::

:::



## Inference for the PREVEND study

::: {.columns}

::: {.column}
```{r, echo = T}
fit <- lm(rfft ~ age, data = prevend)
summary(fit)
confint(fit)
```
:::

::: {.column}
Fitted model:
$$
\text{RFFT} = `r round(a, 3)` - `r round(abs(b), 3)` \times \text{age}
$$


- Age explains an estimated 40.43% of variation in RFFT.

- With each year of age mean RFFT declines by an estimated 1.19 points (SE 0.10).

- There is a significant association between age and mean RFFT score (*T* = -11.82 on 206 degrees of freedom, *p* < 0.0001). 


- With 95% confidence, each additional year of age is associated with an estimated decline in mean RFFT between 0.99 and 1.39 points.

:::

:::

## Kleiber's law

[Kleiber's law](https://en.wikipedia.org/wiki/Kleiber%27s_law) refers to the relationship between metabolic rate and body mass.

::: columns
::: {.column width="45%"}
```{r, fig.width = 5, fig.height = 4}
fit <- lm(log.metab ~ log.mass, data = kleiber)
par(mar = c(4, 4, 0.1, 0.1), cex = 1.5)
plot(kleiber, xlab = 'log mass (g)', ylab = 'log metabolism (kJ/day)')
abline(reg = fit, lwd = 2, col = 'blue')
```
:::

::: {.column width="55%"}
We can estimate it via the SLR model: 
$$
\log(\text{metabolism}) = \beta_0 + \beta_1 \log(\text{mass}) + \epsilon
$$ 

Fitted model:
$$
\log(\text{metabolism}) = `r coef(fit)[1] |> round(2)` + 
`r coef(fit)[2] |> round(2)` \times \log(\text{mass})
$$

```{r, echo = T}
fit <- lm(log.metab ~ log.mass, data = kleiber)
```

:::
:::

## Kleiber's law: inference

::: {.columns}

::: {.column width="60%"}

```{r, echo = T}
fit <- lm(log.metab ~ log.mass, data = kleiber)
summary(fit)
```
:::

::: {.column width="40%"}
```{r, fig.width = 3, fig.height = 1.8}
par(mar = c(4, 4, 0.1, 0.1), cex = 0.9)
plot(kleiber, xlab = 'log(mass)', ylab = 'log(metabolism)')
abline(reg = fit, lwd = 2, col = 'blue')
```

- $\hat{\beta}_0 = `r coef(fit)[1] |> round(3)`$
- $\hat{\beta}_1 = `r coef(fit)[2] |> round(3)`$
- $\hat{\sigma} = `r sigma(fit) |> round(3)`$

:::

:::

> There is a significant association between body mass and metabolism (*p* < 0.0001): body mass explains 96.49% of variation in metabolism; with 95% confidence, a unit increment in log mass is associated with an estimated increase in mean log metabolism between `r confint(fit)[2, ] |> round(4) |> str_flatten(collapse = ' and ')`, with a point estimate of `r coef(fit)[2] |> round(4)`.

## Kleiber's law: model interpretation

Exponentiating both sides of the fitted SLR model equation:

$$
\underbrace{\text{metabolism}}_{e^{\log(\text{metabolism})}} = \underbrace{`r coef(fit)[1] |> exp() |> round(2)`}_{e^{`r coef(fit)[1] |> round(2)`}} \times \underbrace{\text{mass}^{`r coef(fit)[2] |> round(2)`}}_{e^{`r coef(fit)[2] |> round(2)` \log(\text{mass})}}
$$

So we've really estimated what's known as a *power law* relationship: $y = ax^b$.

- multiplicative, not additive, relationship
- doubling $x$ corresponds to changing $y$ by a factor of $2^b$

The estimate and interval for $\beta_1$ in the SLR model can be transformed appropriately for a more direct interpretation:

> With 95% confidence, every doubling of body mass is associated with an estimated `r round((2^confint(fit)[2, ] - 1)*100, 2) |> str_flatten(collapse = '-')`% increase in median metabolism.

# Activity 1: Nonparametric inference for location

*Self-paced activity in place of one lecture*


```{r setup 2}
library(tidyverse)
load('data/brfss.RData')
load('data/temps.RData')
ddt <- MASS::DDT
```

In this activity you'll learn about nonparametric alternatives to the $t$ test for one and two means. The activity is organized much like a lab, but with extra narrative. You should read through the narrative at your own pace and try the exercises provided as you go. At the end there are two practice problems that you'll be expected to complete before next class.

### Background: parametric and nonparametric inference

Consider the basis for the inferences developed so far: under certain conditions (typically regularity of the underlying population distribution, assessed by checking histograms for unimodality and approximate symmetry) and with sufficient sample sizes, a model is specified for the sampling distribution of a test statistic. 

- inference for one mean: $t_{n - 1}$ model for the sampling distribution of $T = \frac{\bar{x} - \mu}{SE(\bar{x})}$
- inference comparing two means: $t_{\nu}$ model for the sampling distribution of $T = \frac{\bar{x} - \bar{y} - \delta}{SE(\bar{x} - \bar{y})}$
- inference comparing several means: $F_{k - 1, n - k}$ model for the sampling distribution of $F = \frac{MSG}{MSE}$

These are all what are known as *parametric models*, because they are specified through one or two parameters that determine their exact shape. The parameters in this case are the degrees of freedom terms -- $n - 1$ for the one-sample $t$ test, $\nu$ (usually estimated) for the two-sample $t$ test, and $k - 1$ and $n - k$ for the $F$ test.

As such, these procedures are examples of *parametric inference* -- inferences that utilize a parametric model for the data and/or test statistic.

When assumptions for parametric inference aren't tenable, or when a parametric model is not available, there are so-called **nonparametric** methods of inference: **methods that don't depend on a parametric model** such as the $t$ or $F$ models we've learned about in class.

We will consider specifically nonparametric procedures based on ranks, *i.e.*, ordering observations from smallest to largest. 

### Motivating examples

In practice, the situation that most often leads an analyst to consider rank-based nonparametric methods is that the assumptions for the $t$ test either don't hold or are difficult to assess.

Below are two such situations you've already encountered in this class.

#### Small sample sizes

When sample sizes are small, it's awkward to assess assumptions for parametric inference, because with few observations histograms can lack any discernible shape. For example, the most we can say about the following data on heart rates for 19 women and 20 men is that there are no evident outliers.

```{r motivation 1: small sample sizes}
heart.m <- temps |> filter(sex == 'male') |> pull(heart.rate)
heart.f <- temps |> filter(sex == 'female') |> pull(heart.rate)

par(mfrow = c(1, 2))
hist(heart.m)
hist(heart.f)
```

In practical terms, the $t$ test is likely still fine under these circumstances; however, some may wish to consider an inference for comparing heart rates between groups that doesn't depend on distributional assumptions.

#### Assumptions don't hold

On occasion you may go to check assumptions and find that they're clearly violated. For example, the pairwise differences between actual and desired weights from BRFSS respondents showed clear skewness and several large outliers. In that case, the sample size was big enough that we overlooked the issue, but it's not hard to imagine a similar situation cropping up with fewer observations.

Suppose you only had 12 observations that showed the same skew and had one big outlier:

```{r motivation 2: assumptions fail}
set.seed(51424)
weight.diff <- sample(brfss$weight - brfss$wtdesire, 12)
hist(weight.diff, breaks = 10)
```

Here the $t$ test isn't appropriate, and using it anyway would likely result in a true significance level, coverage, and power quite different from the nominal levels specified in the test, so it would be hard to trust the result. This is a great situation to use a rank-based nonparametric alternative.

### Inference on "location" (not mean)

The usual parametric inferences pertain to the population mean; not so with rank-based nonparametrics. Instead, these inferences pertain simply to "location". 

Often "location" is characterized in terms of the "center" of a distribution so that inferences can be interpreted in a manner similar to parametric tests and intervals. 

We will follow this convention and consider the hypotheses to be about the center(s) of the population model(s), denoted $c$. For example, the two-sided test of center would test the hypotheses:

$$
\begin{cases}
H_0: &c = c_0 \\
H_A: &c \neq c_0
\end{cases}
$$

For the two-sided test of difference in centers, we will test the hypotheses:

$$
\begin{cases}
H_0: &c_1 = c_2 \\
H_A: &c_1 \neq c_2
\end{cases}
$$

However, you should keep in mind that "location" is a more general notion that encompasses all measures of location of a distribution.

### One-sample inference: signed rank test

The basic premise of this test is that, if the population model is symmetric, its center should evenly divide the data.

::: callout-tip

## Warm up

Consider the following 15 measurements of DDT in kale in ppm in order from smallest to largest:

```{r signed rank test: warm up}
sort(ddt)
```

Suppose you wish to test whether the center $c$ of the population model is $c_0 = 3$ ppm and assume a symmetric population model.

1.  How many observations would you expect to be smaller than $c_0 = 3$ if 3ppm is in fact the center?
2.  How many observations are actually smaller than 3 ppm?
3.  Based on your answers to 1-2, do you think it is likely that in fact $c = 3$?
:::

::: {.callout-tip collapse="true"}

## Solution

If the population is symmetric about $c_0$, you'd expect roughly half of observations (`r length(ddt)/2`) to be smaller than 3ppm.

The actual number of observations smaller than 3ppm is:

```{r signed rank test: warmup solution}
# number of observations below 3
sum(ddt < 3)
```

This is much less than half, so it seems unlikely that the center is actually 3ppm.
:::

The signed rank test is a nonparametric alternative to the one-sample $t$ test applicable to any symmetric population model. The particular form of symmetry and presence of outliers do not affect the test.

#### Hypotheses

The test can be directional or two-sided, just like the $t$ test. Thus, the possible hypotheses are: 

$$
H_0: c = c_0 \quad\text{vs.}\quad H_A: c \mathrel{\substack{<\\\neq\\ >}} c_0
$$ 

#### Test procedure

While the intuition of the test is that half of observations should be smaller than the true center under population symmetry, the test statistic is not quite as direct as a tally of how many observations are below the hypothetical value. Instead, the procedure is as follows:

1.  \[center\] Calculate deviations $d_i = x_i - c_0$

2.  \[rank\] Sort and rank the absolute deviations $|d_i|$

    -   average ranks in case of ties
    -   drop zeros

3.  \[sum\] Add up the positive 'signed ranks' $\sum_{\text{sign}(d_i) > 0} r_i$

This produces the test statistic:

$$V = \sum_{i = 1}^n \text{sign}(d_i) \times R_i$$

::: callout-tip

## Check your understanding

Try working out the rank sum procedure manually using the DDT data -- it's small enough that you could jot down the steps on some scratch paper. See if you can calculate $V$.

To help, here are the deviations sorted smallest to largest:
```{r signed rank test procedure: check your understanding}
# calculate deviations
di <- ddt - 3
sort(di)
```

1. Start by writing the deviations in order of absolute value in a column. 
2. Then rank them 1-15 in an adjacent column. If there is a tie -- *e.g.*, two absolute deviations of 0.05 -- then assign them both the average of the ranks. For example, if 0.05 occurs twice in positions 2 and 3, then give them both rank 2.5.
3. Write down the sign of the deviation in a new column.
4. Then write down the "signed rank", or product of the sign and the rank, in a fourth column.
5. Add up the positive signed ranks. This is the signed rank statistic.

:::

::: {.callout-tip collapse="true"}
## Solution

This shows the steps in R, but don't worry about the codes; the output is meant to illustrate what you would do on paper to find the rank sum statistic.

```{r signed rank test procedure: check your understanding solution}
# this shows the steps, column by column; focus on output
ddt.srank <- tibble(di = di) |>
  mutate(abs.di = abs(di), 
         rank = rank(abs.di),
         sign = sign(di),
         signed.rank = sign*rank) |>
  arrange(abs.di)
ddt.srank

# signed rank statistic
vstat <- ddt.srank |> filter(sign > 0) |> pull(signed.rank) |> sum()
vstat
```
:::

#### $p$-values for the test

Just like other hypothesis tests, the signed rank test rejects if $V$ is sufficiently large in the direction of the alternative. 

A sampling distribution for $V$ can be found exactly using combinatorics, or approximated using probability theory. In this caseWe won't go into details about either approach, except to indicate that there is a sampling distribution for $V$ that we can use to obtain $p$-values in the same fashion that the $t_{n - 1}$ model was used to obtain $p$-values for the $t$ test.

In this case, there are `r sum(sapply(0:15, function(x){choose(15, x)}))` possible sign combinations; of these, only about 0.375% give a larger value of $V$. That provides a $p$-value for the test, and since $p = 0.0018 < 0.05$ we would reject $H_0$ at the 5% significance level. This result is interpreted as:

> The data provide strong evidence that the typical DDT concentration in kale is not 3ppm (signed rank statistic *V* = 111.5, *p* = 0.00375).

#### Implementation with `wilcox.test(...)`

The implementation in R looks and functions much like `t.test`:

```{r signed rank test: implementation}
# signed rank test at 1% level
wilcox.test(ddt, mu = 3, alternative = 'two.sided', 
            exact = F, conf.int = T, conf.level = 0.99)
```

Some remarks:
- `exact = F` produces approximate $p$-values and confidence intervals; you may see warning messages if this is excluded
-   pseudo-median is a measure of center, but not the same as a median or mean (check!)

::: callout-note
## Your turn 1

Perform the analogous inference using the $t$ test and compare the results. Do the tests agree at the 1% significance level?

```{r, include = F}
# perform t test at 1% level to compare

```
:::

::: {.callout-note collapse="true"}
## Solution
The tests do not agree: the signed rank test rejects at the 1% level, but the $t$ test does not.

```{r}
# perform t test at 1% level to compare
t.test(ddt, mu = 3, alternative = 'two.sided', conf.level = 0.99)
```
:::

::: callout-note
## Your turn 2

Perform the signed rank test to determine whether actual weight exceeds desired weight by more than 10lbs at the 5% significance level. Report the result in the usual narrative format. Compare your result with the inference obtained from a $t$ test.

```{r, include = F}
# small sample of weight differences
set.seed(51424)
weight.diff <- sample(brfss$weight - brfss$wtdesire, 12)

# assumptions don't hold
hist(weight.diff, breaks = 10)

# use signed rank test to determine whether actual exceeds desired by at least 10lbs at 5% significance level

# check t test result to compare

```
:::

::: {.callout-note collapse="true"}
## Solution

```{r}
# use signed rank test to determine whether actual exceeds desired by at least 10lbs
wilcox.test(weight.diff, mu = 10, alternative = 'greater', 
            exact = F, conf.int = T, conf.level = 0.95)

# check t test result to compare
t.test(weight.diff, mu = 10, alternative = 'greater', conf.level = 0.95)
```

Interpretation of the signed rank test test:

> The data provide evidence that actual weight exceeds desired weight by more than 10lbs (signed rank statistic *V* = 61, *p* = 0.0456).

The signed rank test finds evidence that actual weight exceeds desired weight by more than 10lbs, where the $t$ test does not.
:::

### Two-sample inference: rank sum test

The rank-sum test is a nonparametric alternative to the two-sample $t$ test based on ranks. The key idea for the test is that if observations in both groups come from the same population distribution then they should be exchangeable (*i.e.*, groupings don't matter).

Thus, the only assumption for this test is that data are independent. The test can be directional, and thus it is possible to test the following hypotheses comparing centers:

$$
H_0: c_1 = c_2\quad\text{vs}\quad H_A: c_1 \mathrel{\substack{<\\\neq\\>}} c_2
$$

#### The alternative hypothesis

This test is a bit funny in that the null hypothesis is really that *the distributions are exactly the same*. The alternative to this possibility can come about in a number of ways. The alternative is usually interpreted as a *difference in location*, primarily because this is the situation that the test has power to detect. 

So, it is often said that the test also assumes that the samples differ only in location. In other words, the test is most appropriate when the histograms look "shifted", but not fundamentally different, as illustrated below.

```{r, fig.width = 9, fig.height = 2, echo = F}
par(mfrow = c(1, 3),
    mar = c(2, 2, 2, 1),
    cex = 1)
curve(dgamma(x, shape = 2, rate = 1), 
      from = 0, to = 10,
      xaxt = 'n', yaxt = 'n',
      xlab = '', ylab = '')
abline(v = 2, lty = 2)
curve(dgamma(x - 3, shape = 2, rate = 1), 
      from = 3, to = 10,
      xaxt = 'n', yaxt = 'n',
      xlab = '', ylab = '',
      add = T, col = 2)
abline(v = 5, lty = 2, col = 2)
title(ylab = 'frequency', line = 0.5)
title(xlab = 'values', line = 0.5)
title(main = 'assumptions met perfectly')

curve(dgamma(x, shape = 2, rate = 1), 
      from = 0, to = 10,
      xaxt = 'n', yaxt = 'n',
      xlab = '', ylab = '')
abline(v = 2, lty = 2)
curve(dgamma(x - 3, shape = 3, rate = 1), 
      from = 3, to = 10,
      xaxt = 'n', yaxt = 'n',
      xlab = '', ylab = '',
      add = T, col = 2)
abline(v = 6, lty = 2, col = 2)
title(ylab = 'frequency', line = 0.5)
title(xlab = 'values', line = 0.5)
title(main = 'assumptions reasonable')

curve(dgamma(x, shape = 2, rate = 1), 
      from = 0, to = 10,
      xaxt = 'n', yaxt = 'n',
      xlab = '', ylab = '')
abline(v = 2, lty = 2)
curve(dgamma(x, shape = 1, rate = 1/3), 
      from = 0, to = 10,
      xaxt = 'n', yaxt = 'n',
      xlab = '', ylab = '',
      add = T, col = 2)
abline(v = 3, lty = 2, col = 2)
title(ylab = 'frequency', line = 0.5)
title(xlab = 'values', line = 0.5)
title(main = 'assumptions not met')
```

In all of these cases, there is a (true) difference in location, but if shape differs too much, the rank sum test should *not* be used. That said, it can be hard to tell with small samples, so it may not really be practical to to check this assumption.


**Example 1.** Out-of-state tuition costs from 26 public and 26 private universities. These data differ primarily in spread, not location; so the rank-sum test might not work well here. 

```{r, fig.width=4, fig.height=3, echo = F}
Sleuth3::ex0332 %>%
  ggplot(aes(x = OutOfState)) +
  facet_wrap(~Type, nrow = 2, scales = 'free_y') +
  geom_histogram(aes(y = after_stat(density)),
                 alpha = 0.6, binwidth = 5000) +
  stat_density(aes(y = after_stat(density)), 
               bw = 6000, alpha = 0.2) +
  labs(y = '', x = 'tuition') +
  scale_y_continuous(labels = NULL, breaks = NULL) +
  scale_x_continuous(n.breaks = 5, limits = c(0, 50000)) +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank())
```


**Example 2.** Deviations from expected cancer rates in CT in years with high and low sunspot activity. The shape is a bit hard to discern here, but it seems plausible that the distribution for high sunspot years is shifted to the right of that for low sunspot years. So, the rank sum test would be appropriate here.

```{r, fig.width=4, fig.height=3, echo = F}
# load data
cancer <- read_csv('data/cancer.csv')
# cancer <- cancer |> mutate(delta = rate - lag(rate))
cancer %>%
  ggplot(aes(x = delta)) +
  facet_wrap(~sunspot, nrow = 2, scales = 'free_y') +
  geom_histogram(aes(y = after_stat(density)),
                 alpha = 0.6, bins = 8) +
  stat_density(aes(y = after_stat(density)), 
               bw = 0.2, alpha = 0.2) +
  labs(y = '', x = 'change in cancer rate from prior year (per 100K)') +
  scale_y_continuous(labels = NULL, breaks = NULL) +
  scale_x_continuous(n.breaks = 5, limits = c(-0.5, 0.8)) +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank())
```



#### Rank sum test procedure

The rank sum procedure, though a bit opaque, is rather simple:

1.  \[pool\] Combine observations from both groups
2.  \[rank\] Sort and rank pooled observations
3.  \[sum\] Add up ranks in the first group
4.  \[adjust\] Subtract $\frac{n_1(n_1 + 1)}{2}$, where $n_1$ is the sample size of the first group

The test rejects if the sum is larger than expected in the direction of the alternative. As in the signed rank test, combinatorics are used to determine a $p$-value for the test.

The rationale for this procedure is that if the distributions are the same, then the ranks should be evenly distributed among the two groups; this induces a particular sampling distribution on the sum of the ranks in each group. The adjustment facilitates computation of $p$-values.

Let's illustrate using data from an experiment in which participants were randomly assigned to receive a fish oil supplement or a regular oil supplement. For each subject, the reduction in blood pressure was measured after a period of time on the treatments.

```{r fish oil data, fig.width = 4, fig.height = 3}
# load dataset
fish.oil <- Sleuth3::ex0112 |> rename_with(tolower)

# make boxplot
boxplot(bp ~ diet, data = fish.oil, horizontal = T)
```

There's a bit of difference in spread, but enough of a shift in location that the rank sum test is reasonable to apply.

::: callout-tip
## Check your understanding

Carry out the rank sum procedure outlined above and compute the rank sum statistic by summing up the ranks in the fish oil group.

To facilitate calculations, here are the data in order of increasing blood pressure:

```{r rank sum test: check your understanding}
fish.oil |> arrange(bp)
```

Add a column of ranks by hand (or in R if you can figure out how!), averaging ranks for any ties. Then add up the ranks in the `FishOil` group, and subtract $\frac{n_1(n_1 + 1)}{2} = \frac{7\times 8}{2} = 28$ to obtain the rank sum statistic.
:::

::: {.callout-tip collapse="true"}
## Solution

The output below illustrates the rankings and selection of which ones to add up. Don't worry about the codes

```{r rank sum test: check your understanding solution}
# again, ignore the codes; look at output
fish.oil.ranksum <- fish.oil |> 
  mutate(rank = rank(bp),
         ranks.fish = rank*(diet == 'FishOil')) |>
  arrange(bp)
fish.oil.ranksum

# rank sum statistic
n1 <- count(fish.oil, diet)$n[1]
sum(fish.oil.ranksum$ranks.fish) - n1*(n1 + 1)/2
```
:::

#### Implementation using `wilcox.test(...)`

The `wilcox.test` function also implements the rank sum test, using the same syntax as `t.test(...)`. For example, using the fish oil data, we might test at the 1% significance level whether the fish oil supplement caused a greater reduction in blood pressure:


```{r, echo=T}
wilcox.test(bp ~ diet, data = fish.oil, 
            mu = 0, alternative = 'greater', 
            exact = F, conf.int = T, conf.level = 0.99)
```

For this test, the $p$-value gives the percentage of possible rank allocations among the groups for which the rank sum is at least as favorable to $H_A$; again this is computed using combinatorics or approximation methods.

In this instance, the $p$-value indicates that only about 1.96% of all possible rank allocations among the two groups would produce a rank sum statistic at least as large. Thus, testing at the 1% significance level:

> The data provide do not provide sufficient evidence at the 1% significance level that the fish oil supplement caused a greater reduction in blood pressure than the regular oil supplement (rank sum statistic *W* = 41, *p* = 0.01957).

::: callout-note
## Your turn 3

Perform the corresponding $t$ test for comparison at the 1% significance level. Do the tests agree?

```{r, include = F}
# t test for effect of treatment at 1% level

```

:::

::: {.callout-note collapse="true"}
## Solution

The tests do not agree; the $t$ test supports evidence of an effect at the 1% level where the rank sum test does not.

```{r}
# t test for effect of treatment at 1% level
t.test(bp ~ diet, data = fish.oil, mu = 0, 
       alternative = 'greater', conf.level = 0.95)
```
:::

::: callout-note
## Your turn 4

Use the `cancer` dataset (specifically the `delta` variable) to test whether the change in cancer rate is higher in years with high sunspot activity. Carry out the test at the 5% level, and report the result in the usual narrative style.

```{r, include = F}
# test whether change in cancer rate is higher in years with high sunspot activity at the 5% level

```
:::

::: {.callout-note collapse="true"}
## Solution

```{r}
# test whether change in cancer rate is higher in years with high sunspot activity at the 5% level
wilcox.test(delta ~ sunspot, data = cancer, mu = 0,
            exact = F, alternative = 'greater', 
            conf.int = T, conf.level = 0.95)
```

Interpretation:

> The data provide no evidence that the change in cancer rate is higher in years with high sunspot activity (rank sum statistic *W* = 157.5, *p* = 0.3072). 

:::

### Practice problem

1. Is there a difference in cholesterol associated with consuming oat bran compared with consuming corn flakes? Use the `cholesterol` dataset to test for a difference at the 5% level using a nonparametric test.

    a. Construct boxplots to compare the distributions for location shift. Does the nonparametric test seem appropriate?
    b. Carry out the test.
    c. Report the test result in the usual narrative style.

```{r practice problem, include = F}
# read in data and preview
cholesterol <- read_csv('data/cholesterol.csv') |> rename_with(tolower)
head(cholesterol)

# part a: make a boxplot and assess whether the nonparametric test is appropriate
boxplot(cholesterol ~ diet, data = cholesterol, horizontal = T)
cholesterol |> count(diet)

# part b: carry out the test
wilcox.test(cholesterol ~ diet, data = cholesterol, mu = 0,
            exact = F, alternative = 'two.sided', conf.int = T,
            conf.level = 0.95)
```

# Activity 2: Line fitting

*Self-paced activity in place of one lecture*


```{r data prep, echo = F}
library(oibiostat)
library(tidyverse)

# prevend dataset
data(prevend)
set.seed(53124)
prevend <- prevend |>
  rename_with(tolower) |>
  group_by(cut_width(age, 5)) |> 
  sample_n(size = sample(10:30, size = 1)) |>
  ungroup() |>
  select(casenr, age, rfft) 
save(prevend, file = 'data/prevend.RData')

# kleiber's law dataset
kleiber <- Sleuth3::ex0826 |>
  select(Mass, Metab) |>
  mutate(across(everything(), log)) |>
  rename_with(~paste('log', tolower(.x), sep = '.'))
save(kleiber, file = 'data/kleiber.RData')

# hand fit functions
hand.fit <- function(x, y, a = NULL, b = 0, res = F){
  plot(y ~ x, xlab = substitute(x), ylab = substitute(y))
  if(is.null(a)){
    a <- mean(y) - mean(x)*b
    add.ctr <- T
  }else{
    add.ctr <- F
  }
  if(res == T){
    yhat <- a + b*x
    e <- y - yhat
    segments(x0 = x, y0 = y, 
             x1 = x, y1 = yhat,
             col = '#ff7770')
    title(main = paste('sse = ', round(sum(e^2), 2), ', bias = ', -round(mean(e), 4), sep = ''))
  }
  if(add.ctr == T){
    points(mean(x), mean(y), col = 'blue', pch = 17, cex = 2)
    print(c(intercept = a, slope = b))
  }
  abline(a = a, b = b, col = 'blue', lwd = 2)
}
saveRDS(hand.fit, file = 'fns/handfit.rds')


# simulate data by correlation
sim.by.cor <- function(n = 2000, r){
  if(abs(r) > 1){
    stop("correlation must be between -1 and 1")
  }
  sim.data <- mvtnorm::rmvnorm(n, sigma = matrix(c(1, r, r, 1), nrow = 2)) |>
    as_tibble()
  plot(sim.data, 
       xlab = 'x', ylab = 'y', 
       xlim = c(-4, 4), ylim = c(-4, 4), 
       main = paste('r = ', cor(sim.data)[1, 2] |> round(4), sep = ''))
  if(r > 0){
    abline(a = 0, b = 1, col = 'blue', lwd = 2)
  }
  if(r < 0){
    abline(a = 0, b = -1, col = 'blue', lwd = 2)
  }
  if(r == 0){
    abline(a = 0, b = 0, col = 'blue', lwd = 2)
  }
}
saveRDS(sim.by.cor, file = 'fns/simbycor.rds')
```


This activity is a self-paced warmup in preparation for learning about linear regression. Simply put, linear regression consists of fitting a line to data in order to describe the relationship between two numeric variables. 

By 'fitting' we do mean something a bit more specific than drawing any old line through a scatterplot, but it's worth emphasizing that the criteria we'll develop later in class, though formal, do reflect several common intuitions about what a 'well-fitting' line looks like. So, the goal of this activity is to explore some of those intuitions. 

The over-arching question I'd like you to consider as you work through this activity is: ***what makes a line "good" as a representation of the relationship between two variables?***

We'll use two datasets:

- `prevend`, which contains measurements of cognitive assessment score and age for 208 adults
- `mammals`, which contains measurements of brain weight (g) and body weight (kg) for 62 mammal species

```{r setup 3}
library(tidyverse)
load('data/prevend.RData')
load('data/mammals.RData')
hand.fit <- readRDS('fns/handfit.rds')
sim.by.cor <- readRDS('fns/simbycor.rds')
```


### Scatterplots

To start, let's review scatterplots. These are straightforward to construct using `plot(...)`; in addition to what you have seen before, the commands below show you an alternate syntax for making scatterplots using a formula specification of the form `y ~ x`, as well as how to adjust the labels.

```{r constructing scatterplots}
# variables of interest
age <- prevend$age
rfft <- prevend$rfft

# scatterplot of RFFT score against age
plot(age, rfft)

# alternate syntax: formula
plot(rfft ~ age)

# change labels
plot(rfft ~ age, xlab = 'age', ylab = 'RFFT score')
```

> There is a negative linear relationship between RFFT score and age, suggesting cognitive function decreases with age.

Scatterplots allow for visual assessment of trend. A trend is *linear* if it seems to follow a straight line; linear trends are *positive* if they increase from left to right and *negative* if they decrease from left to right. Most of the time, you won't see a perfectly clear straight-line relationship, so what we really mean by this is that (a) there's a trend and (b) a line would describe it about as well as any other path drawn through the scatter.

::: callout-note
## Your turn 1

Using the `mammals` dataset, construct a scatterplot of `log.brain` (y) against `log.body` (x) using the formula syntax. Label the axes "log body weight (kg)" and "log brain weight (g)". Describe the trend you see in the data.

```{r, include = F}
# variables of interest

# scatterplot of log brain weight against log body weight (use formula syntax and adjust labels)

```
:::

::: {.callout-note icon=false collapse=true appearance="minimal"}
## Solution

There is a positive linear relationship between log brain weight and log body weight, suggesting that larger mammals in general have larger brains. 

```{r}
# variables of interest
log.brain <- mammals$log.brain
log.body <- mammals$log.body

# scatterplot of log brain weight against log body weight 
plot(log.brain ~ log.body, 
     xlab = 'log body weight (kg)', 
     ylab = 'log brain weight (g)')
```
:::

In most contexts one variable is of direct interest, and the other is considered as potentially explaining the variable of interest. For example:

- in the `prevend` data, age might explain cognitive functioning
- in the `mammals` data, body size might explain brain size

Plots are typically oriented so that the variable of interest, or *response*, is on the vertical (y) axis and the *explanatory variable* is on the horizontal (x) axis. Throughout the rest of this activity, you'll see these terms and the notation $x, y$ used correspondingly.

### Correlation

Both of the above examples show linear trends of differing strength. The `prevend` data are not as close to falling on a straight line as the `mammals` data, so there is a stronger linear relationship between log brain and log body weights than there is between RFFT score and age. 

Further, the two examples show trends of differing direction: the trend in the `prevend` data is negative (RFFT declines with age) whereas the trend in the `mammals` data is positive (brain size increases with body size).

The direction and strength of a linear relationship can be measured directly by the correlation coefficient, defined as:

$$
r = \frac{\sum_{i = 1}^n(x_i - \bar{x})(y_i - \bar{y})}{s_x s_y}
$$
The correlation coefficient is always between -1 and 1:

- $r \rightarrow 0$ indicates no relationship
- $r \rightarrow 1$ indicates a perfect linear relationship
- $r > 0$ indicates a positive linear relationship
- $r < 0$ indicates a negative linear relationship

Correlations are simple to compute in R:

```{r computing a correlation}
# correlation between age and rfft
cor(age, rfft)
```

Interpretations should note the direction and strength. Strength is a little subjective, but as a rule of thumb:

- $|r| < 0.3$: no relationship
- $0.3 \leq |r| < 0.6$: weak to moderate relationship
- $0.6 \leq |r| < 1$: moderate to strong relationship

In this case:

> There is a moderate negative linear relationship between age and RFFT.

::: callout-note
## Your turn 2

Compute the correlation between log brain weight and log body weight; interpret the correlation in context.

```{r your turn 2}
# correlation between log brain and log body

```
:::

::: {.callout-note icon=false collapse=true appearance="minimal"}
## Solution

There is a strong positive linear relationship between log body weight and log brain weight.

```{r}
# correlation between log brain and log body
cor(log.brain, log.body)
```
:::

Generally, the stronger the correlation, the easier it will be to visualize a line passing through the data.

::: callout-tip
## Exploration: visualizing correlations

The `sim.by.cor` function simulates observations of two numeric variables that have a specified correlation `r`. It returns a plot of the simulated data showing the scatter, the $y = x$ line, and the sample correlation. The sample correlation will differ a little bit from the true value, so don't be surprised if the sample correlation doesn't exactly match the value of `r` that you input.

Use this to explore the correspondence between the sample correlation and the visual appearance of scatterplots:

- try several negative correlations
- try several positive correlations
- try both large (near $\pm$ 1) and small (near 0) values

For each value of `r` you try, run the command a few times to see several simulated datasets.

```{r visualize data with different correlations}
# adjust r to see what different correlations look like in simulated data
sim.by.cor(r = 0, n = 500)
```
:::


### Hand-fitting lines

Now we'll turn to actually drawing lines through data scatter to approximate linear relationships. We'll do this by specifying a slope and (sometimes) intercept, so a quick reminder of the slope-intercept form for the equation of a line may be handy:

$$
y = a + bx
$$


- the intercept $a$ is the value at which the line passes through the vertical axis (the value of $y$ when $x = 0$)
- the slope $b$ is the amount by which $y$ increases for each increment in $x$ (sometimes called 'rise over run')

I've written a function called `hand.fit` that allows you to construct a scatterplot and easily add a line by specifying the slope and intercept. 

First we'll consider "eye-fitting" a line by trying out different slopes and intercepts until a combination is found that looks good. For example, for the `prevend` dataset a slope of -1.3 and an intercept of 145 look reasonable:

```{r hand fitting a line to the prevend data}
# i adjusted a and b until the line reflected the trend well
hand.fit(x = age, y = rfft, a = 145, b = -1.3)
```

Consider what this line says about the relationship between RFFT and age:

> With each year of age, RFFT score decreases by 1.3 points.

The intercept doesn't have a clear interpretation, because $x = 0$ is not meaningful for this dataset: technically, it says that RFFT for a 0-year-old would be 145, which of course is nonsensical.

::: callout-note
## Your turn 3

For the log brain weight and log body weight variables in the `mammals` dataset, experiment with the intercept `a` and the slope `b` until you find a pair of values that seem to reflect the trend well. Interpret the relationship described by the line.

I suggest starting with `a = 0` and `b = 0`. Note that if the values are too extreme, the line may not appear at all on the plot.

```{r, include = F}
# adjust a and b until the line reflects the trend between log brain and log body weights well

```
:::

::: {.callout-note icon=false collapse=true appearance="minimal"}
## Solution

There are many possible lines that visually fit the data well. One possible solution:

```{r}
# adjust a and b until the line reflects the trend well
hand.fit(x = log.body, y = log.brain, a = 1.9, b = 0.85)
```

This line says that each 1-unit increment in the log of a mammal's body weight is associated with a 0.85-unit increase in log brain weight. (There is a better interpretation, but it's a bit more mathematically complicated.)
:::

Now we'll develop some ideas that will help to assess how good these lines are as representations of the $x-y$ relationship.

### Residuals

A residual is something left over; in this context, the difference between the line and a data point. There is one residual for every data point. If we denote the value of the response variable on the line as:

$$
\hat{y}_i = a + bx
$$

Then the residual for the $i$th observation is:

$$
e_i = y_i - \hat{y}_i
$$

Residuals help to capture the fit of the model, because taken together, they convey how close the line is to each data point.

#### Visualizing residuals

Adding the argument `res = T` to the `hand.fit(...)` function will show the residuals visually as vertical red line segments. 

```{r visualize residuals}
# add residuals to the plot
hand.fit(x = age, y = rfft, a = 145, b = -1.3, res = T)
```

Data points below the line have negative residuals, and data points above the line have positive residuals. 

::: callout-note 
## Your turn 4

Plot the residuals for the line you identified in the last exercise.

```{r, include = F}
# add residuals to your line from before

```

:::

::: {.callout-note icon=false collapse=true appearance="minimal"}
## Solution

For the values I identified, the residuals look like this:

```{r}
# add residuals to your line from before
hand.fit(x = log.body, y = log.brain, a = 1.9, b = 0.85, res = T)
```
::: 

Take a moment to consider what the residuals in your plot and the example above convey about line fit:

- What would the residuals look like if fit is good? 
- What would they look like if fit is good?

#### Bias

In this context, bias refers to how often the line overestimates and underestimates data values:

- if the line tends to overestimate and underestimate equally often, it is unbiased
- if the line tends to overestimate more often or underestimate more often, then it is biased

The average residual captures whether the line is biased:

- positive average residual $\longrightarrow$ underestimates more often $\longrightarrow$ negative bias
- negative average residual $\longrightarrow$ overestimates more often $\longrightarrow$ positive bias
- average residual near zero $\longrightarrow$ unbiased

So, define as a measure of bias the negative average residual:
$$
\text{bias} = -\frac{1}{n}\sum_{i = 1}^n e_i
$$

In the example above, the bias of 4.5 means that on average, the line overestimates RFFT score by 4.5 points. If we increase the intercept, we will overestimate even more often, and thus increase the bias:

```{r more positive bias}
# obvious positive bias
hand.fit(x = age, y = rfft, a = 165, b = -1.3, res = T)
```

::: callout-note
## Your turn 5

Using the log brain and log body weights in the `mammals` dataset, see if you can fit lines with positive, negative, and no bias.

1. Find a line that has obvious positive bias.
2. Find a line that has obvious negative bias.
3. Find a line that has low bias.

For simplicity, keep your slope the same, and just change the intercept.

```{r, include = F}
# positive bias

# negative bias

# low bias

```
:::

::: {.callout-note icon=false collapse=true appearance="minimal"}
## Solution

Individual results will vary a lot. Here are a few scenarios:

```{r}
# positive bias
hand.fit(x = log.body, y = log.brain, a = 3, b = 0.85, res = T)

# negative bias
hand.fit(x = log.body, y = log.brain, a = 1, b = 0.85, res = T)

# low bias 
hand.fit(x = log.body, y = log.brain, a = 2, b = 0.85, res = T)
```


:::

Clearly, low bias is a desirable property of a fitted line. However, it is not the only desirable property.

#### Error

The total magnitude of the residuals conveys how close the line is to the data scatter in general and thus, error. One way to measure this is by the sum of squared residuals:

$$
\text{SSE} = \sum_{i = 1}^n e_i^2
$$

This quantity will be smallest whenever the line is as close as it can be to as many data points as possible at once. As a result, a good fit will generally have low error.

While high error can arise from bias, it is also possible for a line with no bias to have high error. For example, a horizontal line through the mean RFFT will have zero bias, but still be a bad estimate:

```{r no bias but high error}
# no bias but high error
hand.fit(x = age, y = rfft, a = mean(rfft), b = 0, res = T)
```

The reason the bias is zero is that the residuals balance each other out in the average. It is a mathematical fact that whenever the line passes exactly through the point defined by the sample mean of each variable -- *i.e.*, the point $(\bar{x}, \bar{y})$ -- the bias is exactly zero. 

::: callout-note
## Your turn 6

Mimic the example above using the log brain and log body weights in the `mammals` dataset: find a line that has low bias but high error.


```{r, include = F}
# low bias but high error

```
:::

::: {.callout-note icon=false collapse=true appearance="minimal"}
## Solution

Individual results will vary. A horizontal line through the mean log brain weight will work. Here's a more extreme example:

```{r}
# low bias but high error
hand.fit(x = log.body, y = log.brain, 
         a = mean(log.brain) + mean(log.body), 
         b = -1, res = T)
```
:::



#### Best unbiased line

Considering that there is an easy way to ensure the bias of the line is zero -- constrain it to pass through the center $(\bar{x}, \bar{y})$ -- we should concern ourselves with finding the unbiased line with lowest error. Finding the best unbiased line reduces, essentially, to finding a slope.

Omitting the intercept altogether in the `hand.fit(...)` function will constrain it to pass through the center, add a point to visualize the center of the data, and print the intercept resulting from the constraint:

```{r constrain the line to pass through center}
# omit intercept to force the line through the center point
hand.fit(x = age, y = rfft, b = -1.2, res = T)
```

Once we have a slope in hand, the intercept for the fitted line can be found by direct arithmetic:

```{r see how the intercept is calculated}
# calculate intercept by direct arithmetic
mean(rfft) - (-1.2)*mean(age)
```

So, finding the best unbiased line amounts to fine-tuning the slope until SSE reaches a minimum. For the `prevend` data, the best unbiased line is about this:

```{r approximate best unbiased line}
# fine tune slope to minimize sse by hand
hand.fit(x = age, y = rfft, b = -1.19, res = T)
```

So the best unbiased line results around $a = 134.05$ and $b = -1.19$.

There is an analytic solution for the slope of the best unbiased line:

$$
a = r\times \frac{s_y}{s_x}
$$

```{r exact best unbiased line}
# analytic solution 
hand.fit(age, rfft, res = T,
         b = cor(rfft, age)*sd(rfft)/sd(age))
```

The exact best unbiased line is therefore:
$$
y = `r  round(mean(rfft) - mean(age)*cor(age, rfft)*sd(rfft)/sd(age), 4)`  + `r round(cor(age, rfft)*sd(rfft)/sd(age), 4)`x
$$

::: callout-note
## Your turn 7

Find the best unbiased line for the `mammals` data.

```{r, include = F}
# find exact best unbiased line for mammals data

```
:::

::: {.callout-note icon=false collapse=true appearance="minimal"}
## Solution

```{r}
# find exact best unbiased line for mammals data
hand.fit(x = log.body, y = log.brain, res = T,
         b = cor(log.body, log.brain)*sd(log.brain)/sd(log.body))
```
:::

### Practice problem

1. [L10] The `kleiber` dataset contains observations of log-transformed average mass (kg) and log-transformed metabolic rate (kJ/day). Kleiber's law refers to the relationship by which metabolism depends on body mass.

    a. Construct a scatterplot of the data, and be sure to orient the response variable and explanatory variable properly on the plot. Is there a trend, and if so, is it linear?
    b. Compute the correlation and comment on the strength and direction of linear relationship between log-mass and log-metabolism.
    c. Find the equation of the best unbiased line in slope-intercept form.
    d. [extra credit] Find an expression of the form $y = a x^b$ for the relationship in (c) on the original (*i.e.*, not log-transformed) scale. What is the exponent $b$? 

```{r practice problem 1, include = F}
# load and inspect data
load('data/kleiber.RData')
head(kleiber)

# part a: scatterplot
plot(kleiber)

# part b: compute and interpret correlation
cor(kleiber)

# part c: find the best unbiased line
fit <- lm(log.metab ~ log.mass, data = kleiber) 
fit

# part d: back-transform coefficients
coef(fit)[2]
```





