---
title: "Sampling variability"
subtitle: "Frequency distributions of summary statistics under random sampling"
format: 
  revealjs:
    logo: img/poly-logo-2.jpg
    footer: "STAT218"
    smaller: true
    mermaid:
      theme: neutral
execute: 
  echo: false
  warning: false
  message: false
---

## Today's agenda

```{r}
library(tidyverse)
library(oibiostat)
library(pander)
library(NHANES)
nhanes <- NHANES %>% 
  filter(SurveyYr == "2009_10") %>%
  mutate(subj.id = row_number()) %>%
  select(subj.id, Gender, Age, Poverty, Pulse, BPSys1, BPDia1, TotChol, SleepHrsNight) %>%
  drop_na() 
```

Today we'll focus on understanding sampling variability. This is a foundation for the development of inferential statistics.

1.  Reading quiz \[[2pm section](https://forms.office.com/r/gs613rUX1z)\] \[[4pm section](https://forms.office.com/r/f9asSVpc8y)\]
2.  \[lecture/lab\] Effect of sampling variability on summary statistics
3.  \[lecture/lab\] Effect of sample size on sampling variability of summary statistics
4.  The normal model

## Inferential statistics

Consider this descriptive finding from the FAMuSS study:

::: columns
::: {.column width="50%"}
```{r, fig.height = 3, fig.width = 5}
data(famuss)
par(mar = c(5, 5, 3, 1),
    cex = 1)
boxplot(ndrm.ch ~ actn3.r577x, 
        data = famuss, 
        horizontal = T,
        range = 2,
        main = 'nondominant arm strength',
        xlab = 'percent change', 
        ylab = 'genotype')
```
:::

::: {.column width="50%"}
Median percent change by genotype:

```{r}
famuss %>% 
  group_by(actn3.r577x) %>%
  summarize(median.change = median(ndrm.ch)) %>%
  pivot_wider(names_from = actn3.r577x, values_from = median.change) %>%
  pander()
```

*Subjects with genotype TT exhibited the largest median percent change in strength*
:::
:::

**Inferential statistics** will allow us to address questions like:

> Should we expect these differences to persist among the general population?

To do so we will need to articulate how samples relate to populations.

## Sampling variability

Different samples inevitably produce different outcomes. This is **sampling variation**:

> If we re-run the study with new participants we'll get different results

The basis for statistical inference is distinguishing sampling variation from systematic variation.

What we really want to know in the FAMuSS study:

> Is the CC/TT/CT difference systematic in the population or an artefact of the sample?

In other words...

-   was the result due to chance?
-   or was it genuine?

<!-- How often we'd observe a difference depends on a combination of factors: -->

<!-- |             |                    |            | -->

<!-- |-------------|--------------------|------------| -->

<!-- |             | Sampling variation |            | -->

<!-- | Effect size | **Low**            | **High**   | -->

<!-- | **Low**     | Less often         | More often | -->

<!-- | **High**    | More often         | Less often | -->

## Random sampling

If we assume study units are sampled at random from a broader population, we can quantify how much summary statistics are expected to change from sample to sample.

::: columns
::: {.column width="50%"}
![](img/srs.png)
:::

::: {.column width="50%"}
A study **population** is a collection of all study units of interest.

A **sample** is a subcollection from a population:

-   *random* if study units have a known chance of inclusion in the sample
-   *nonrandom* or *convenience* otherwise
:::
:::

In STAT218 we'll limit attention to **simple random samples**: each study unit in the population has an equal chance of inclusion in the sample.

## A pretend population: NHANES data

The National Health and Nutrition Esamination Survey (NHANES) is an annual CDC program to collect health and nutrition data on the non-institutionalized civilian resident population of the United States. Here are a few variables:

```{r}
nhanes %>% head(4) %>% pander()
```

I've selected 3,179 responses from the 2009-2010 survey; let's pretend the corresponding individuals form a population of interest.

## Population distribution of a variable

Consider the `TotChol` variable: total HDL cholesterol in mmol/L. It has a certain frequency distribution among the population that we'll call its *population distribution*.

::: columns
::: {.column width="40%"}
```{r, fig.width = 5, fig.height = 4}
nhanes %>% 
  summarize(across(TotChol, 
                   .fns = list(mean = mean, SD = sd), 
                   .names = "Population {.fn}")) %>%
  # t() %>%
  pander()

par(mar = c(5, 5, 1, 1), cex = 1.5)
nhanes %>% pull(TotChol) %>% hist(breaks = 30, main = '', xlab = 'cholesterol', ylab = 'frequency')
```
:::

::: {.column width="60%"}
If we draw a random sample of 50 individuals...

-   how closely will the sample align with the population distribution?
-   how much will alignment change if we select a new sample?
:::
:::

## Simulating sampling variability

Open the class activity `lab4-sampling`. The first part of this lab will load the NHANES data and provide you with a command for extracting a sample.

Your task:

1.  Have each person in your group extract a sample.
2.  Calculate the mean and standard deviation.
3.  Make a histogram.
4.  Compare your results to the population.
5.  Compare your results to each other.

After you've had a chance to try in groups, we'll compare across the class.

## Simulating sampling variability

```{r, cache = T}
set.seed(12524)
nsamp <- 10000
samps <- tibble(samp.id = 1:nsamp,
                samp = map(samp.id, ~slice_sample(nhanes, n = 50, replace = F)))
```

::: columns
::: {.column width="\"55%"}
```{r, fig.width = 8, fig.height = 8}
samps %>%
  slice_head(n = 16) %>%
  unnest(samp) %>%
  group_by(samp.id) %>%
  mutate(sample = paste('sample', samp.id, sep = ' '),
         mean.totchol = mean(TotChol)) %>%
  ggplot(aes(x = TotChol)) +
  facet_wrap(~sample, nrow = 4) +
  geom_histogram(bins = 15) +
  geom_vline(aes(xintercept = mean.totchol), 
             color = 'blue', 
             linetype = 'dotdash') +
  geom_vline(xintercept = mean(nhanes$TotChol), 
             color = 'red', alpha = 0.6) +
  theme_bw(base_size = 18) +
  labs(x = 'cholesterol', y = 'frequency')
```
:::

::: {.column width="45%"}
You should have observed something a bit like this.

These are 16 random samples with the sample mean indicated by the blue dashed line and the population mean indicated by the red solid line.

-   frequency distributions differ a lot
-   sample means vary a little
-   most means are close to 5
-   most standard deviations are near 1
:::
:::

## Point estimation

It should seem plausible that the sample mean and standard deviation provide good estimates of the corresponding population quantities.

We call them point estimates of population parameters.

| Parameter name     | Parameter notation | Point estimate |
|--------------------|--------------------|----------------|
| Mean               | $\mu$              | $\bar{x}$      |
| Standard deviation | $\sigma$           | $s_x$          |

Now we can more formally describe statistical inference:

-   a population parameter is any numeric characteristic of a population distribution
-   an inference is a conclusion about the value of a population parameter based on point estimates *and their sampling variability*

We will focus initially on inferences about the mean $\mu$ based on the point estimate $\bar{x}$.

## Measuring sampling variability

If we had means calculated from a large number of samples, we could make a frequency distribution for the values of the sample mean. This is called a **sampling distribution**.

::: columns
::: {.column width="40%"}
```{r, fig.width=4, fig.height = 4}
samps %>%
  mutate(chol.mean = map(samp, ~mean(.x$TotChol))) %>%
  unnest(chol.mean) %>%
  pull(chol.mean) %>%
  hist(breaks = 30, 
       main = 'sampling distribution (10K means)', 
       xlab = 'sample mean', 
       ylab = 'sampling frequency')
abline(v = mean(nhanes$TotChol),
       col = 2, lwd = 3, lty = 2)
```
:::

::: {.column width="60%"}
```{r}
samp_means <- samps %>%
  mutate(chol.mean = map(samp, ~mean(.x$TotChol))) %>%
  unnest(chol.mean) %>%
  pull(chol.mean) 

rbind(sample = 1:5, mean = samp_means[1:5]) %>%
  pander()
```

Could measure the sampling variability using any measure of spread.

-   standard deviation: `r sd(samp_means)`

*On average, the sample mean varies about the population mean by 0.15 mmol/L across simple random samples.*
:::
:::

## Measuring sampling variability

Theory indicates the standard deviation of the sample mean under random sampling is: $$
SD(\bar{x}) = \frac{\sigma}{\sqrt{n}} 
\qquad \left(\frac{\text{population SD}}{\sqrt{\text{sample size}}}\right)
$$

For `TotChol`, the theoretical standard deviation is $SD(\bar{x}) = \frac{1.0747}{\sqrt{50}} =$ `r sd(nhanes$TotChol)/sqrt(50)`.

We can estimate this quantity by replacing $\sigma$ with the point estimate $s_x$, resulting in a **standard error** (estimated standard deviation): $$SE(\bar{x}) = \frac{s_x}{\sqrt{n}} 
\qquad \left(\frac{\text{sample SD}}{\sqrt{\text{sample size}}}\right)$$

<!-- ## Summing up -->

<!-- So far we have established two key points: -->

<!-- -   the sample mean and standard deviation provide good point estimates of the corresponding population parameters under random sampling -->

<!-- -   we can estimate the sampling variability of the mean under random sampling using its standard error $\frac{s_x}{\sqrt{n}}$ -->

## Example with one sample

The simulations we've done so far have been a means of understanding just what a standard error is meant to capture; these are *not* a practicable method for measuring sampling variation.

In practice we'd simply compute a point estimate and standard error.

::: columns
::: {.column width="50%"}
```{r, fig.height = 6, fig.width = 8}
set.seed(12824)
one_samp <- sample(nhanes$TotChol, size = 50)
par(mar = c(5, 5, 3, 1), cex = 2)
hist(one_samp,
     breaks = 10, 
     xlab = 'cholesterol',
     ylab = 'frequency', 
     main = 'hypothetical sample, n = 50')
```
:::

::: {.column width="50%"}
```{r}
c(mean = mean(one_samp),
  se = sd(one_samp)/sqrt(50)) %>%
  pander()
```

-   *The estimated mean total HDL cholesterol among the population is 5.031 mmol/L.*

-   *The point estimate is expected to deviate by 0.1396 mmol/L on average from the population mean.*
:::
:::

## Effect of sample size

The formula for the theoretical standard deviation of $\bar{x}$ suggests that sampling variability diminishes with sample size. For example:

```{r}
tibble(n = c(10, 50, 100, 1000, 10000)) %>%
  mutate(SD = sd(nhanes$TotChol)/sqrt(n)) %>%
  t() %>%
  pander()
```

The second section of `lab4-sampling` explores this. With your group:

1.  Start with a sample size of 10.
2.  Have each person draw a sample and compute the mean (or draw a series of samples and compute the mean each time).
3.  Compare and observe how big the differences between your means are.
4.  Repeat with a sample size of 1000.

You should see much less sampling variability after increasing $n$.

## Visualizing effect of sample size

Generating a large number (10K) of samples allows us to approximate the sampling distribution for a variety of sample sizes.

```{r, cache = T}
set.seed(12524)
chol <- nhanes$TotChol
nsamp <- 10000
samp_dists <- tibble(samp.id = 1:nsamp,
                `sample size 10` = map(samp.id, 
                           ~mean(sample(chol, size = 10, replace = F))),
                `sample size 100` = map(samp.id, 
                           ~mean(sample(chol, size = 50, replace = F))),
                `sample size 1000` = map(samp.id, 
                           ~mean(sample(chol, size = 100, replace = F)))
                )
```

```{r, fig.height = 2, fig.width = 8}
unnest(samp_dists, cols = starts_with('sample')) %>%
  select(-samp.id) %>%
  gather() %>%
  ggplot(aes(x = value)) + 
  facet_wrap(~key, scales = 'free_y') +
  geom_histogram(bins = 40) + 
  theme_bw() +
  labs(x = 'sample mean', 
       y = 'frequency')
```

-   spread diminishes with sample size
-   less variability among estimates from larger samples

So estimates are more accurate with more data, *assuming data are from a random sample.*

## Normal model

Notice that each simulated sampling distribution has produced a unimodal, symmetric, bell-shaped histogram.

::: columns
::: {.column width="60%"}
The normal model is a theoretical frequency distribution characterized by two parameters:

-   a mean (center)
-   a standard deviation (spread)

Theory dictates that the sampling distribution of the sample mean is well-approximated by a normal model under simple random sampling.
:::

::: {.column width="40%"}
```{r, fig.height = 5, fig.width = 8}
par(mar = c(1, 1, 3, 1),
    cex = 2)
curve(dnorm, from = -4, to = 4,
      lwd = 2,
      xlab = '', ylab = '',
      main = 'normal model',
      xaxt = 'n', yaxt = 'n')
```
:::
:::

*Based on discussion thus far, what do you think the model parameters might be?*
