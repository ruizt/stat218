---
title: "Introduction to categorical data analysis"
subtitle: "One- and two-sample inference for proportions"
format: 
  revealjs:
    logo: img/poly-logo-2.jpg
    footer: "STAT218"
    smaller: true
    mermaid:
      theme: neutral
execute: 
  echo: false
  warning: false
  message: false
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

```{r}
library(tidyverse)
library(oibiostat)
library(pander)
library(Sleuth3)
library(emmeans)
```

## Today's agenda

1. Loose ends from last time
2. [lecture/discussion] Intro to CDA: inference for one and two proportions
3. [lab] 3 case studies

## Loose end 1: practice problem


```{r}
mussels <- read_csv('data/mussels.csv') |>
  gather(location, aam.length) |>
  drop_na()
write_csv(mussels, file = 'labs/data/mussels.csv')
```

::: {.columns}

::: {.column}

```{r, fig.width=5, fig.height = 5}
par(mar = c(3, 6, 1, 1))
boxplot(aam.length ~ location, data = mussels, 
        horizontal = T, ylab = '', las = 1)
```
:::

::: {.column}
Data are standardized lengths of the anterior adductor muscle (AAM) of *Mytilus trossulus* mussels from five populations.

> Estimate mean AAM lengths for each population and test for differences between populations. If differences are determined to be significant, determine which populations differ significantly and provide interval estimates for the differences.

:::

:::

## Loose end 1: practice problem

::: {.columns}

::: {.column}
**Summaries**

```{r, eval = F, echo = T}
boxplot(aam.length ~ location, data = mussels)
```

```{r, fig.width=4, fig.height = 2}
par(mar = c(3, 6, 1, 1))
boxplot(aam.length ~ location, data = mussels, 
        horizontal = T, ylab = '', las = 1)
```

```{r, eval = T, echo=T}
mussels |> group_by(location) |> 
  summarize(mean = mean(aam.length), 
            sd = sd(aam.length), 
            n = n())
```


:::

::: {.column}

**ANOVA model**

```{r, echo=T, eval=T}
fit <- aov(aam.length ~ location, data = mussels)
summary(fit)
```

```{r, echo = T}
emmeans(fit, ~ location, adjust = 'bonferroni')
```

:::

:::

## Loose end 1: practice problem

::: {.columns}

::: {.column}
```{r, eval = F, echo = T}
emmeans(fit, ~ location) |>
  confint(adjust = 'bonferroni') |>
  plot()
```

```{r}
emmeans(fit, ~ location, adjust = 'bonferroni') |>
  plot() +
  theme_bw(base_size = 24) +
  labs(x = 'estimated mean AAM', y = '')
```

:::

::: {.column}

```{r, eval = F, echo = T}
emmeans(fit, ~ location) |>
  pairs(adjust = 'bonferroni') |>
  plot()
```

```{r}
emmeans(fit, ~ location) |>
  pairs(adjust = 'bonferroni') |>
  plot() +
  theme_bw(base_size = 24) +
  labs(x = 'estimated mean AAM difference', y = '') +
  geom_vline(xintercept = 0, color = 'black')
```

:::

:::

::: {.columns}

::: {.column width="65%"}
```{r}
emmeans(fit, ~ location) |>
  pairs(adjust = 'bonferroni')
```
:::

::: {.column width="35%"}

:::

:::

## Loose end 2: power calculations

> How much data should I collect to detect differing means with a target power level?

::: {.columns}

::: {.column}
To perform sample size power calculations, one needs:

- number of groups
- target significance level ($\alpha$)
- target power level
- guess or prior estimate of variance ratio $\frac{\text{group variation}}{\text{error variation}}$
:::

::: {.column}
To detect means that differ among 5 groups with half of the error variability with 90% power:
```{r, echo = T}
power.anova.test(groups = 5, 
                 sig.level = 0.05, 
                 power = 0.9, 
                 within.var = 1, 
                 between.var = 0.5)
```
:::

:::

## Review: categorical data

> A variable is categorical if its values are one of several categories.

::: {.columns}

::: {.column width="65%"}
What we have seen so far:

- **frequency distributions** for one categorical variable
- **contingency tables** for two categorical variables
- use of categorical variables for grouping in inference for two or more means

Inference for categorical data has a different flavor:

- non-numeric values $\Rightarrow$ can't compute usual statistics
- focus on *proportions* instead
:::

::: {.column width="35%"}
```{r, fig.width = 6, fig.height=4}
data(famuss) 
famuss$actn3.r577x |> table() |> pander()
par(mar = c(3, 3, 1, 1),
    cex = 2)
famuss$actn3.r577x |> plot()

famuss$actn3.r577x |> table() |> prop.table() |> pander()
```
:::

:::

## Two settings for CDA

> Inferential techniques for categorical data analysis depend on how many variables and how many categories per variable.

::: {.columns}

::: {.column width="60%"}

We will distinguish two broad settings:

1. [binomial data] Two categories
2. [multinomial data] More than two categories

These get their names -- binomial and multinomial -- from the probability models that describe the frequency distributions.

*We will focus on inferences for population proportions from binomial data.*

:::

::: {.column width="40%"}
```{r, fig.height = 8, fig.width = 7}
data("nhanes.samp.adult.500")
nhanes <- nhanes.samp.adult.500
par(mfrow = c(2, 1),
    mar = c(4, 2, 3, 1),
    cex = 1.5)
nhanes$Diabetes |> plot(main = 'binomial data', xlab = 'diabetic')
nhanes$HealthGen |> plot(main = 'multinomial data', xlab = 'general health')
```

:::

:::

## On binomial proportions

> Inference for categorical data focuses on population proportions

::: {.columns}

::: {.column width="70%"}

Consider the binomial data setting:

- a single categorical variable with two categories
- one category represents an outcome, trait, or property of interest

The **population proportion** is the frequency of occurrence of the category of interest.

$$p = \frac{\# \text{ total occurrences}}{\text{population size } N}$$

If an individual is selected at random, $p$ also gives the probability of an occurrence.

:::

::: {.column width="30%"}
Example binomial data:

```{r}
nhanes |>
  select(ID, Diabetes) |>
  group_by(Diabetes) |>
  sample_n(size = 2) |> 
  pander()
```
:::

:::


## Estimating proportions


::: {.columns}

::: {.column width="45%"}
Consider using the diabetes data to estimate prevalence.

- data are a random sample
- so sample statistics should approximate population statistics
- sample proportions provide estimates of population proportions

```{r}
data("nhanes.samp.adult.500")
nhanes <- nhanes.samp.adult.500
counts <- nhanes$Diabetes |> table()
props <- nhanes$Diabetes |> table() |> prop.table()  
diabetes.tbl <- rbind(count = counts, proportion = props)  
cbind(diabetes.tbl, row.sum = rowSums(diabetes.tbl)) |>
  pander(caption = '*Diabetes data summary*')
```
:::

::: {.column width="55%"}
We'll formalize this as estimating the **population proportion**
$$p = \frac{\# \text{ individuals with diabetes}}{\text{total population size } N}$$
Using the **sample proportion**
$$\hat{p} = \frac{\# \text{ respondents with diabetes}}{\text{sample size } n}$$
So $\hat{p} = 0.114$ estimates the prevalence of diabetes (population proportion $p$).
:::

:::

## Two interpretations

The point estimate

$$\hat{p} = \frac{\# \text{ respondents with diabetes}}{\text{sample size } n} = \frac{57}{500} = 0.114$$

has two (equivalent) interpretations:

1. the prevalence of diagnosed diabetes among U.S. adults is estimated to be 11.4% 
2. the probability that a randomly chosen U.S. adult has diagnosed diabetes is 0.114

## Variability of binomial data

> Binomial data are most variable when $p = 0.5$ and least variable when $p \approx 0$ or $1$

::: {.columns}

::: {.column width="65%"}
If $p$ represents the proportion of the population in a category of interest (*e.g.*, has diabetes):

- $p$ near 1 or 0: minimum "spread"
    
    + most individuals are in one category
    
- $p$ near 0.5: maximum "spread"

A measure of spread is therefore:

$$p(1 - p)$$
:::

::: {.column width="35%"}

```{r, fig.width = 5, fig.height = 4}
par(mar = c(4, 4, 1, 1),
    cex = 1.75, cex.axis = 0.7)
curve(x*(1 - x), from = 0, to = 1, xlab = 'p', ylab = 'p (1 - p)')
```

:::

:::

## Sampling distribution of $\hat{p}$

::: {.columns}

::: {.column}
```{r, fig.width=5, fig.height = 4}
par(mar = c(6, 3, 1, 1),
    cex = 1.5, cex.axis = 0.6)
curve(dnorm, from = -4, to = 4, yaxt = 'n', xlab = '', ylab = '', axes = F)
title(xlab = expression(frac(hat(p) - p, sqrt(frac(p(1 - p), n)))), line = 4.5)
title(ylab = 'sampling frequency', line = 1)
axis(1, at = -4:4)
```
:::

::: {.column}
The sample proportion $\hat{p}$ has a sampling distribution that can be approximated by a normal model.

- centered at true proportion $p$
- standard deviation is $$SD(\hat{p}) = \sqrt{\frac{p(1 - p)}{n}} \quad\left(\frac{\text{variability}}{\text{sample size}}\right)$$
:::

:::

## Estimation uncertainty

> Recall: a standard error estimates the variability of a sample statistic across samples

The standard error for the sample proportion is:
$$SE\left(\hat{p}\right) = \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}$$

For the diabetes data this estimate is $SE(\hat{p})$ = `r sqrt((nhanes$Diabetes |> table() |> prop.table() |> prod())/500)`.

- on average, the sample proportion varies by 0.014 from sample to sample
- using a normal model:

    + 68% of samples produce a $\hat{p}$ within 0.014 of $p$
    + 95% of samples produce a $\hat{p}$ within 0.028 of $p$

## Inference for a proportion

::: {.columns}

::: {.column width="45%"}

```{r}
nhanes |>
  summarize(p.hat = mean(Diabetes == 'Yes'),
            p.hat.se = sqrt((p.hat*(1 - p.hat))/n()),
            n = n()) |>
  pander(caption = 'Point estimate for diabetes prevalence')
```

:::

::: {.column width="55%"}
> *Interpretation:* It is estimated that the proportion of the U.S. adult population with diagnosed diabetes is 11.4% (*SE = 1.42%*, *n = 500*).
:::

:::

Given a point estimate and standard error, we can carry out inference using the normal model.

- intervals: $\hat{p} \pm c \times SE(\hat{p})$
- tests: $Z = \frac{\hat{p} - p_0}{SD_0(\hat{p})}$
- critical value $c$ uses normal model

## Confidence intervals

::: {.columns}

::: {.column}
To calculate a 95% confidence interval "by hand" for the diabetes prevalence:

$$
0.114 \pm 2\times 0.01421 = (0.0881, 0.1459)
$$
:::

::: {.column}
> With 95% confidence, it is estimated that between 8.81% and 14.59% of the U.S. adult population has diagosed diabetes.

:::

:::

The critical value $c$ comes from the normal model.

- exact value depends on desired coverage
- empirical rule:

    + $c = 1$ gives a 68% interval
    + $c = 2$ gives a 95% interval
    + $c = 3$ gives a 99.7% interval
- calculation in R: `qnorm((1 - coverage)/2, lower.tail = F)`

## Hypothesis tests

::: {.columns}

::: {.column width="50%"}
Consider testing:
$$\begin{cases}H_0: p = p_0 \\ H_A: p \mathrel{\substack{< \\\neq \\ >}} p_0\end{cases}$$

Hypothesis tests use the test statistic:

$$Z = \frac{\hat{p} - p_0}{SD_0(\hat{p})} = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0 (1 - p_0)}{n}}}$$

And reject when $Z$ is large in the direction of the alternative.

Under $H_0$, the sampling distribution of $Z$ is approximated by a normal model.
:::

::: {.column width="50%"}
For instance, to test whether diabetes prevalence exceeds 10%:
$$Z = \frac{0.114 - 0.1}{\sqrt{\frac{0.1\times 0.9}{500}}} = 1.043$$

```{r, fig.width = 8, fig.height = 5}
par(mar = c(6, 4, 3, 1),
    cex = 1.5)
x <- seq(-4, 4, length = 10000)
y <- dnorm(x)
zstat = 1.043
plot(x, y, type = 'l',
     ylab = 'frequency', 
     xlab = '',
     main = 'z model')
title(xlab = expression(paste("Z = ", 
                              frac(hat(p) - p[0], 
                                   sqrt(frac(p[0](1 - p[0]), n))))),
      line = 4.5)
polygon(c(min(x), x[x<=zstat], zstat), c(y[x<=zstat], 0, 0), col="white")
polygon(c(x[x>=zstat], max(x), zstat), c(y[x>=zstat], 0, 0), col="red")
abline(v = zstat)
legend(x = 'topright', 
       legend = c('14.85% of samples', '85.15% of samples'), 
       fill = c('red', 'white'))
```
:::

:::

## Inference in R

::: {.columns}

::: {.column width="45%"}
There are two steps to performing the appropriate inference in R:

1. Format the variable of interest as a factor with the category of interest as the first level
2. Construct a table of the frequency distribution
3. Pass the table to `prop.test()`

Remarks about output:

- `X-squared` gives $Z^2$
- `correct = F` performs the test without continuity correction
:::

::: {.column width="55%"}
The test from the previous slide:
```{r, echo = T}
# 1. construct factor
diabetes <- factor(nhanes$Diabetes, 
                   levels = c('Yes', 'No'))

# 2. store table
diabetes.tbl <- table(diabetes)

# 3. pass to prop.test
prop.test(diabetes.tbl, p = 0.1, 
          alternative = 'greater', correct = F)
```

:::

:::

## Exact inference

> The normal model is an approximation.

The test can also be performed using the *exact* sampling distribution obtained from a binomial probability model.

```{r, echo = T}
binom.test(x = 57, n = 500, p = 0.1, alternative = 'greater')
```
Inputs:

- `x` gives the number of occurrences of the category of interest
- `n` gives the sample size

## Continuity correction

> The approximation error for the normal model may be adjusted using a "continuity correction"

Idea: adjust the test statistic slightly for a more conservative test.

::: {.columns}

::: {.column}
```{r, echo = T}
prop.test(diabetes.tbl, p = 0.1, 
          alternative = 'greater', correct = F)
```
:::

::: {.column}
```{r, echo = T}
prop.test(diabetes.tbl, p = 0.1, 
          alternative = 'greater', correct = T)
```
:::

:::

Notice that the correction (right) results in a test that much more closely matches the exact test.

## Your turn: sleep trouble

> Use the NHANES data to estimate the proportion of U.S. adults reporting sleep trouble. Test whether at least 20% of U.S. adults report sleep trouble. 

::: {.columns}

::: {.column}
1. Compute point estimate and standard error
2. Perform the hypothesis test

    + write the hypotheses
    + calculate the value of the test statistic
    + record the $p$-value
    
3. Construct an interval estimate
:::

::: {.column}
```{r, echo = T}
# to get you started
trouble <- factor(nhanes$SleepTrouble, 
                  levels = c('Yes', 'No'))
trouble.tbl <- table(trouble)
```

```{r}
trouble.tbl |> 
  pander(caption = 'Having sleep trouble?')
```
:::

:::

## Comparing two proportions

> Inference on the *difference between proportions* uses substantially similar methods

::: {.columns}

::: {.column}

```{r}
cold <- case1802 |> column_to_rownames("Treatment") |> as.matrix() |> as.table() 
cold |> cbind(n = rowSums(cold)) |> pander('Vitamin C experiment')
```

- vitamin C and placebo treatments were randomly allocated to 818 volunteers
- volunteers took treatments daily for a cold season
- study recorded how many volunteers came down with a cold

:::

::: {.column}
Consider inference on the difference

$$p_\text{placebo} - p_\text{vitamin C}$$

<!-- $$\begin{cases}H_0: p_\text{placebo} - p_\text{vitamin C} \leq 0 \\ H_A: p_\text{placebo} - p_\text{vitamin C} > 0 \end{cases}$$ -->

Inferences based on groupwise estimates:

- point estimate: $\hat{p}_\text{placebo} - \hat{p}_\text{vitamin C}$
- standard error: $\sqrt{SE^2(\hat{p}_\text{placebo}) + SE^2(\hat{p}_\text{vitamin C})}$

- interval: $\hat{p}_\text{placebo} - \hat{p}_\text{vitamin C} \pm c\times SE$

:::

:::

## Tests for a difference in proportions

::: {.columns}

::: {.column}
Consider testing:

$$\begin{cases} H_0: p_1 = p_2 \\ H_A: p_1 \mathrel{\substack{< \\ \neq \\ >}} p_2 \end{cases}$$

Hypothesis tests use the test statistic:

$$Z = \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\hat{p}(1 - \hat{p})\left(\frac{1}{n_1} + \frac{1}{n_2}\right)}}$$

Where:
$$\hat{p} = \frac{n_1\hat{p}_1 + n_2\hat{p}_2}{n_1 + n_2}$$
:::

::: {.column}
Under $H_0$, the sampling distribution of $Z$ is approximated by a normal model.

- reject when $Z$ is sufficiently large in the direction of the alternative

Upper-sided test for vitamin C data:

```{r, fig.width = 8, fig.height = 5}
par(mar = c(6, 4, 3, 1),
    cex = 1.5)
x <- seq(-4, 4, length = 10000)
y <- dnorm(x)
zstat = prop.test(cold, correct = F, alternative = 'greater')$stat |> sqrt()
plot(x, y, type = 'l',
     ylab = 'frequency', 
     xlab = 'Z(placebo - vitamin C)',
     main = 'z model')
polygon(c(min(x), x[x<=zstat], zstat), c(y[x<=zstat], 0, 0), col="white")
polygon(c(x[x>=zstat], max(x), zstat), c(y[x>=zstat], 0, 0), col="red")
abline(v = zstat)
legend(x = 'topright', 
       legend = c('0.59% of samples', '99.41% of samples'), 
       fill = c('red', 'white'))
```


:::

:::

## Vitamin C example in detail

::: {.columns}

::: {.column}
Sample proportions:
```{r}
cold |> prop.table(margin = 1) |> pander()
```
Hypotheses
$$\begin{cases} H_0: p_\text{placebo} \leq p_\text{vitaminC} \\ H_A: p_\text{placebo} > p_\text{vitaminC} \end{cases}$$
This corresponds to testing whether vitamin C reduces the probability of getting a cold.

:::

::: {.column}
$$\begin{align*} &\hat{p}_\text{placebo} - \hat{p}_\text{vitaminC} = \hspace{15cm} \\\\
&\hat{p} = \hspace{15cm} \\\\
&Z = 
\end{align*}$$

$p$ value from normal model:

```{r, fig.width = 8, fig.height = 5}
par(mar = c(6, 4, 3, 1),
    cex = 1.5)
x <- seq(-4, 4, length = 10000)
y <- dnorm(x)
zstat = prop.test(cold, correct = F, alternative = 'greater')$stat |> sqrt()
plot(x, y, type = 'l',
     ylab = 'frequency', 
     xlab = 'Z(placebo - vitamin C)',
     main = 'z model')
polygon(c(min(x), x[x<=zstat], zstat), c(y[x<=zstat], 0, 0), col="white")
polygon(c(x[x>=zstat], max(x), zstat), c(y[x>=zstat], 0, 0), col="red")
abline(v = zstat)
legend(x = 'topright', 
       legend = c('0.59% of samples', '99.41% of samples'), 
       fill = c('red', 'white'))
```


:::

:::

## An interval for the difference

To compute a confidence interval for the difference in proportions:

$$\hat{p}_\text{placebo} - \hat{p}_\text{vitaminC} \pm 2\times \sqrt{SE^2(\hat{p}_\text{placebo}) + SE^2(\hat{p}_\text{vitaminC})}$$

Carrying out the calculation by hand:

$$\begin{align*}
SE^2(\hat{p}_\text{placebo}) &= \hspace{8cm}\\\\
SE^2(\hat{p}_\text{vitaminC}) &= \hspace{8cm}\\\\
\hspace{3cm} \pm 2\times \hspace{5cm} &= \Big( \hspace{2cm}, \hspace{2cm}\Big)
\end{align*}$$

## Inference in R

::: {.columns}

::: {.column}
Three steps:

1. Construct a table of the frequency distribution by group

    + outcome/trait of interest should be first column
    + groups should be rows
    
2. Pass to `prop.test()`

Identify at left:

- point estimates
- test statistic
- $p$-value
- confidence interval
:::

::: {.column}
```{r, echo = T}
# 1. construct table
cold

# 2. pass to prop.test
prop.test(cold, alternative = 'two.sided', 
          correct = F)
```
:::

:::

## Your turn: two cases

```{r}
obesity <- case1801 |> column_to_rownames('Obesity') |> as.matrix() |> as.table()
smoking <- case1803 |> column_to_rownames('Smoking') |> as.matrix() |> as.table() |> t()

save(list = c('obesity', 'smoking', 'cold'), file = 'labs/data/prop-cases.RData')
```

::: {.columns}

::: {.column}
Researchers categorized 3,112 individuals in American Samoa according to whether they were obese and recorded whether subjects had cardiovascular disease (CVD).
```{r}
obesity |> pander()
```

*Test for a difference in disease rates between obese and non-obese populations, and produce an interval estimate for the difference in proportions.*
:::

::: {.column}
Researchers identified 86 lung cancer patients and 86 controls (without lung cancer), and categorized them according to whether they were smokers or non-smokers. 
```{r}
smoking |> pander()
```

*Test for a difference in the proportion of smokers among cancer patients compared with controls, and produce an interval estimate for the difference.*
:::

:::
