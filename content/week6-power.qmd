---
title: "Power analyses"
subtitle: "Post hoc power analyses and determination of sample sizes for study design"
format: 
  revealjs:
    logo: img/poly-logo-2.jpg
    footer: "STAT218"
    smaller: true
    mermaid:
      theme: neutral
execute: 
  echo: false
  warning: false
  message: false
---

## Today's agenda

```{r}
library(tidyverse)
library(oibiostat)
library(Sleuth3)
library(pander)
cloud <- case0301
cloud.test <- t.test(Rainfall ~ Treatment, data = cloud,
                     mu = 0, alternative = 'two.sided')
load('labs/type2sim.RData')
```

1.  Reading quiz \[[2pm section](https://forms.office.com/r/URwE52tFbU)\] \[[4pm section](https://forms.office.com/r/V1U6QVAQDP)\]

2.  Type II errors: review and further exploration

3.  \[lecture/lab\] Power analysis

    a.  Sample size calculations
    b.  Post-hoc power analyses

## A thought experiment

::: columns
::: {.column width="45%"}
Suppose that for the cloud data you'd performed a two-sided test: $$H_0: \mu_\text{seeded} = \mu_\text{unseeded}$$ $$H_A: \mu_\text{seeded} \neq \mu_\text{unseeded}$$

```{r}
t.test(Rainfall ~ Treatment, data = cloud,
       mu = 0, alternative = 'two.sided')
```

*Almost* below the significance threshold but not quite.
:::

::: {.column width="55%"} 

> The data **do not provide sufficient evidence to reject** the null hypothesis that seeding has no effect relative to the alternative of an increase or decrease in mean rainfall due to seeding (*T* = 1.998 on 33.86 degrees of freedom, *p* = 0.05377).

The point estimate for the difference is `r round(diff(rev(cloud.test$estimate)), 2)` acre-feet. 

- The test says this observed difference could plausibly be due to sampling variation
- But is it also plausible that our test result is wrong if the difference is real?

<!-- - So how should we interpret the finding of a positive but not statistically significant difference? Could the difference be real? -->
<!-- - What is the likelihood that the difference is real, despite our test result? -->

<!-- > Did we make a type II error? -->
:::
:::

## Type II error rates

> Recall: a type II error is failing to reject a false null hypothesis.

In the context of two-sample inference a type II error occurs when:

-   the true difference is $\delta \neq 0$
-   we test and fail to reject $H_0: \delta \neq 0$

The type II error rate depends on both known and unknown factors:

-   \[unknown\] magnitude of $\delta$
-   \[unknown\] population variability $\sigma$
-   \[known\] significance level
-   \[known\] sample sizes

*What was the type II error rate for the cloud seeding test?*

## Simulating type II errors

::: columns
::: {.column width="65%"}
Summary stats for cloud data:

```{r}
set.seed(21524)
cloud %>% group_by(Treatment) %>% 
  summarize(mean = mean(Rainfall), sd = sd(Rainfall), n = n()) %>%
  pander()
```

We can approximate the type II error rate by:

1.  simulating datasets with matching summary statistics
2.  performing two-sided tests of no difference
3.  computing the proportion of fail-to-reject decisions

```{r, echo = T}
type2sim(delta = 277, n = 26, sd = 650, alpha = 0.05)
```

$\Rightarrow$ if the true difference were exactly as estimated, our test result would be incorrect nearly 70% of the time!
:::

::: {.column width="35%"}
What would happen to the error rate if...

a.  the true difference `delta` were bigger?
b.  the significance level `alpha` were smaller?
c.  the sample size `n` was larger?
d.  the variability of rainfall `sd` were less?
:::
:::

## Simulating type II errors

::: columns
::: {.column width="30%"}
Open the lab and use the simulation function `type2sim` to fill in the table by changing arguments accordingly.

-   try a few magnitudes of difference for each scenario
-   repeat runs for each setting once or twice to confirm effect
:::

::: {.column width="70%"}
| Factor                   | Change  | Effect on error rate |
|--------------------------|---------|----------------------|
| true difference in means | larger  |                      |
| true difference in means | smaller |                      |
| population variability   | larger  |                      |
| population variability   | smaller |                      |
| sample size              | larger  |                      |
| sample size              | smaller |                      |
| significance level       | larger  |                      |
| significance level       | smaller |                      |
:::
:::

> Based on your explorations, do you think our original test decision was erroneous?

## Statistical power

The **power** of a test refers to its true rejection rates across alternatives and is defined as: $$\beta(\delta) = \underbrace{(1 - \text{type II error rate}_\delta )}_\text{correct decision rate when null is false}$$

Power is often interpreted as a detection rate for a specified alternative $\delta$:

-   high type II error $\longrightarrow$ low power $\longrightarrow$ low detection rate
-   low type II error $\longrightarrow$ high power $\longrightarrow$ high detection rate

> In general tests have low power for alternatives close to the null value (where "close" is relative to sampling variability).

Theory allows a direct calculation of power, given sample size, significance level, population standard deviation, and population difference in means.

## Power curves

> Power is usually construed as a *curve* depending on the true difference. 

::: {.columns}

::: {.column width="60%"}

Power curve for the test exactly as performed with the cloud seeding data:

```{r, fig.width = 6, fig.height=4}
par(mar = c(4, 4, 1, 1),
    cex = 1.5)
curve(power.t.test(delta = x, n = 26, sd = 650, sig.level = 0.05)$power, 
      from = -1000, to = 1000, 
      xlab = expression(paste('true difference ', delta)),
      ylab = expression(paste('power function ', beta(delta))))
```
:::

::: {.column width="40%"}
All other attributes of the test are fixed to approximate the test performed:

- sample size $n = 26$
- significance level $\alpha = 0.05$
- population standard deviation $\sigma = 650$ (larger of two group estimates)

:::

:::

## Factors affecting power

> Power depends on all the same factors as type II error rates

| Factor                   | Change  | Effect on error rate | Effect on power |
|--------------------------|---------|----------------------|-----------------|
| true difference in means | larger  |                      |                 |
| true difference in means | smaller |                      |                 |
| population variability   | larger  |                      |                 |
| population variability   | smaller |                      |                 |
| sample size              | larger  |                      |                 |
| sample size              | smaller |                      |                 |
| significance level       | larger  |                      |                 |
| significance level       | smaller |                      |                 |

## Two common power analyses

::: columns
::: column
**Post hoc analysis**: how much power does the test I conducted have if the true difference is exactly equal to my estimate?

Helps to interpret negative results:

-   low power $\rightarrow$ failure to reject was likely
-   high power $\rightarrow$ failure to reject was not likely

::: callout-important
## Don't over-interpret post-hoc analyses

Failure to reject using a well-powered test *does not confirm the null hypothesis*.
:::

:::

::: column
**Sample size determination**: how much data do I need to collect to detect a difference of $\delta$ using a particular test?

Helps avoid two potential issues:

-   too little data $\rightarrow$ study not likely to yield significant results
-   too much data $\rightarrow$ study is too likely to yield significant results
:::
:::

## Post-hoc analysis

> Can we estimate the power of a test we already performed?

::: columns
::: {.column width="55%"}
Feasible if we assume (a) a population standard deviation and (b) test conditions are met.

For the cloud seeding test:

```{r, echo=T}
power.t.test(delta = 250, # magnitude of difference
             sd = 650, # largest population SD
             n = 26, # smallest sample size
             sig.level = 0.05, 
             type = 'two.sample', 
             alternative = 'two.sided') 
```
:::

::: {.column width="45%"}
For a conservative estimate, use:

-   *smallest* of the two sample sizes
-   *largest* of the two standard deviations
-   *smaller* difference than observed

> $\Longrightarrow$ our test would only reject in favor of a difference of the observed magnitude about 27% of the time

Failure to reject doesn't strongly rule out the alternative.
:::
:::

## Your turn: post-hoc analysis

::: columns
::: {.column width="55%"}
Consider testing whether body temperature differs by sex.

Summary stats and test result:

```{r}
data(thermometry)
set.seed(21424)
temps <- sample_n(thermometry, size = 39) %>% rename(sex = gender)
# count(temps, gender)
save(temps, file = 'labs/data/temps.RData')
temps %>% group_by(sex) %>% summarize(mean = mean(body.temp), sd = sd(body.temp), n = n()) %>% pander()
```

```{r, echo = T}
t.test(body.temp ~ sex, data = temps)
```
:::

::: {.column width="45%"}
Assume the true difference is actually 0.5 °F. Determine the power of the test above when:

1. Population SD is the smaller of the two groups
2. Population SD is the larger of the two groups
3. A one-sided test is used instead

> Based on your answers, do you think the negative test result rules out the alternative?


```{r, eval = F}
power.t.test(delta = 0.5, sd = 0.99, n = 19, sig.level = 0.05, alternative = 'two.sided')
t.test(body.temp ~ sex, data = temps)
```
:::
:::

## Power curve for body temps

::: {.columns}

::: {.column width="55%"}

Assuming we underestimated the population standard deviation a bit, the power curve for a one-sided test would look like this:

```{r, fig.width = 6, fig.height=4}
par(mar = c(4, 4, 1, 1),
    cex = 1.5,
    cex.axis = 0.7)
curve(power.t.test(delta = x, n = 19, sd = 1.2, sig.level = 0.05, alternative = 'one.sided')$power, 
      from = 0, to = 2, 
      xlab = expression(paste('true difference ', delta, ' = ', mu[F] - mu[M])),
      ylab = expression(paste('power function ', beta(delta))))
abline(v = 0.5, lty = 2)
```

:::

::: {.column width="45%"}
Assumptions:

- *n* = 19 per group
- $\sigma = 1.2$ per group
- significance level $\alpha = 0.05$
- one-sided test

> Fairly low power for alternatives near the estimated difference (dashed line), so failure to reject doesn't strongly rule out the alternative.
:::

:::

## The equal-variance $t$-test

If it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power by using a different standard error:

$$SE_\text{pooled}(\bar{x} - \bar{y}) = \sqrt{\frac{\color{red}{s_p^2}}{n_x} + \frac{\color{red}{s_p^2}}{n_y}}
\quad\text{where}\quad \color{red}{s_p} = \underbrace{\sqrt{\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\text{weighted average of } s_x^2 \;\&\; s_y^2}$$

In the case of the body temperature data, $s_p$ = `r sqrt((t.test(body.temp ~ sex, data = temps, var.equal = T)$stderr^2)/(1/19 + 1/20)) |> round(4)`. Check:

-   How much power do we gain if we assume a common SD of 0.89?
-   Does it change the outcome of the test (add `var.equal = T`)?

> Produces minimal gains and inflates type I error if not warranted, so better avoided unless you have a small sample size

## Sample size calculation

> If you were (re)designing the study, how much data should you collect to detect a specified effect size?

::: columns
::: {.column width="55%"}
To detect a difference of 250 or more due to cloud seeding with power 0.9:
```{r, echo=T}
power.t.test(power = 0.9, # target power level
             delta = 250, # smallest difference
             sd = 650, # largest population SD
             sig.level = 0.05, 
             type = 'two.sample', 
             alternative = 'two.sided') 
```
:::

::: {.column width="45%"}
For a conservative estimate, use:

-   *overestimate* of the larger of the two standard deviations
-   *minimum* difference of interest

> $\Longrightarrow$ we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time
:::
:::



## Your turn: sample size calculation

Suppose you are designing a follow-up study and wish to detect a difference of 0.4 °F at least 70% of the time. You know women have slightly higher body temperatures than men on average.

| Known direction? | Population SD                                    | Minimum $n$ |
|------------------|--------------------------------------------------|-------------|
| No               | larger of prior estimates                        |             |
| No               | 1.2 times larger than larger of prior estimates |             |
| Yes              | larger of prior estimates                        |             |
| Yes              | 1.2 times larger than larger of prior estimates |             |

> If it costs \$10 per participant to run the study, what's the best power achievable within a $2K budget for the target detection magnitude?

```{r eval = F}
power.t.test(delta = 0.5, sd = 1.2, power = 0.7, sig.level = 0.05, alternative = 'one.sided')
```

## Power vs. sample size curves

::: {.columns}

::: {.column width="60%"}

Minimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:

```{r, fig.width = 6, fig.height=4}
library(RColorBrewer)
coul <- brewer.pal(5, "BuPu") 

par(mar = c(4, 4, 1, 1),
    cex = 1.5,
    cex.axis = 0.7)

delta.seq <- sapply(seq(from = 0.5, to = 0.9, by = 0.1), 
                    function(j){
                      sapply(20:300, function(i){
                        del <- power.t.test(n = i, 
                                            power = j, 
                                            sd = 1.2, 
                                            sig.level = 0.05,
                                            alternative = 'one.sided')$delta
                        return(del)})
})

plot(x = 20:300, y = delta.seq[, 5], type = 'l', 
      xlab = 'sample size',
      ylab = expression(paste('minimum difference ', delta)),
     col = coul[5])

for(j in 4:1){                      
lines(x = 20:300, y = delta.seq[, j], type = 'l', 
     xlab = '',
     ylab = '',
     col = coul[j])
}
abline(h = 0.4, lty = 2, col = 1)
legend(x = 'topright', legend = paste('power', (5:9)/10), col = coul, fill = coul)

```

Assumes $\sigma = 1.2$ for a conservative estimate.

:::

::: {.column width="40%"}
The best power achievable within budget for the target detection range is `r power.t.test(delta = 0.4, n = 100, sd = 1.2, sig.level = 0.05, alternative = 'one.sided')$power`.

- increasing power to 0.8 will require *n* = `r power.t.test(delta = 0.4, power = 0.8, sd = 1.2, sig.level = 0.05, alternative = 'one.sided')$n |> ceiling()` per group

    + \$240 over budget

- increasing power to 0.9 will require *n* = `r power.t.test(delta = 0.4, power = 0.9, sd = 1.2, sig.level = 0.05, alternative = 'one.sided')$n |> ceiling()` per group

    + \$1050 over budget

:::

:::