---
title: "Power analyses"
subtitle: "Post hoc power analyses and determination of sample sizes for study design"
format: 
  revealjs:
    logo: img/poly-logo-2.jpg
    footer: "STAT218"
    smaller: true
    mermaid:
      theme: neutral
execute: 
  echo: false
  warning: false
  message: false
---

```{r}
library(tidyverse)
library(oibiostat)
library(Sleuth3)
library(pander)

type2sim <- function(delta, n, sd, alpha, nsim = 1000){
  # simulate nsim tests by...
  sim.pvals <- sapply(1:nsim, function(i){
    # draw sample with true group difference of delta
    samp <- data.frame(variable = c(rnorm(n, mean = 0, sd = sd), rnorm(n, mean = delta, sd = sd)),
                       group = rep(1:2, each = n))
    # perform test and compute p value
    pval <- t.test(variable ~ group, data = samp, mu = 0, alternative = 'two.sided')$p.value
    return(pval)
  })
  # compute proportion of tests that failed to reject (made a type ii error)
  err.rate <- mean(sim.pvals > alpha)
  return(list(rate = err.rate, p = sim.pvals))
}

cloud <- Sleuth3::case0301 |> rename_with(tolower) |> mutate(treatment = tolower(treatment))
cloud.test <- t.test(rainfall ~ treatment, data = cloud,
                     mu = 0, alternative = 'two.sided')
```

## Today's agenda

1. [lecture] Statistical power; post-hoc and sample size power analyses.
2. [review/lab] Test 2 practice problems.

## $p$-values and false rejections

> A $p$-value captures how often you'd make a mistake **if $H_0$ were true**.

::: {.columns}

::: {.column}
```{r, echo = T}
t.test(rainfall ~ treatment, data = cloud, 
       mu = 0, alternative = 'greater')
```

If there is no effect of cloud seeding, then we would see $T > 1.9982$ for 2.689% of samples.

:::

::: {.column}
The test rejects at the 5% significance level ($p < 0.05$), but that doesn't completely rule out $H_0$.

- while unlikely, our sample could have been one of the 26 in 1000 where $T$ exceeds 1.9982 despite no effect
- by rejecting here (when $T = 1.9982$) we are willing to be wrong 2.689% of the time

*By rejecting when $p < \alpha$ we are willing to be wrong $\alpha\times 100$% of the time.*

:::

:::

## A different kind of error?

> But you can also make a mistake when $H_0$ is false!

::: {.columns}

::: {.column}
```{r, echo = T}
t.test(rainfall ~ treatment, data = cloud, 
       mu = 0, alternative = 'two.sided')
```

We'd see $|T| > 1.9982$ for 5.377% of samples if there's no effect. But what if there is an effect?
:::

::: {.column}
The two-sided test fails to reject at the 5% significance level ($p > 0.05$), but that doesn't completely rule out $H_A$.

- the estimated effect -- increase of `r cloud.test$est |> rev() |> diff() |> round(2)` acre-feet -- could be too small relative to the variability in rainfall

- hard to say how often we'd make this kind of mistake without knowing the real difference

*The rate of fail-to-reject errors depends on the (unknown) true parameter value.*
:::

:::

## Decision errors

> There are two ways to make a mistake in a hypothesis test -- two "*error types*".

| | Reject $H_0$ | Fail to reject $H_0$
---|---|---
**True $H_0$** | type I error | [correct decision]{style="color:lightgrey"}
**False $H_0$** | [correct decision]{style='color:lightgrey'} | type II error

::: {.columns}

::: {.column}

Any statistical test will have certain error rates:

- type I error rate is denoted $\alpha$
- type II error rate is denoted $1 - \beta$

:::

::: {.column}

The significance level of a test is its type I error rate.

- reject when $p < \alpha$ $\Longleftrightarrow$ mistakenly reject $\alpha\times 100$% of the time

But we don't know the type II error rate!

- depends on which alternative parameter value is true

:::

:::


## Simulating type II errors

::: columns
::: {.column width="55%"}
Summary stats for cloud data:

```{r}
set.seed(21524)
cloud %>% group_by(treatment) %>% 
  summarize(mean = mean(rainfall), sd = sd(rainfall), n = n()) %>%
  pander()
```

We can approximate the type II error rate by:

1.  simulating datasets with matching statistics
2.  performing two-sided tests of no difference
3.  computing the proportion of fail-to-reject decisions


:::

::: {.column width="45%"}
```{r, echo = T, eval = F}
type2sim(delta = 277, n = 26, sd = 650.8, 
         alpha = 0.05, nsim = 1000)
```
```{r, fig.width=5, fig.height = 4.5}
par(mar = c(4, 4, 2, 1), cex = 1.5)
sim.out <- type2sim(delta = 277, n = 26, sd = 650.8, alpha = 0.05, nsim = 1000)
hist(sim.out$p, breaks = seq(0, 1, by = 0.05),
     main = "1000 simulated tests", xlab = 'p value', ylab = 'frequency')
abline(v = 0.05, lty = 2, lwd = 2, col = 'red')
text(x = 0.2, y = 300, expr(paste(alpha, " = 0.05")), col = 'red')
```
:::
:::

> If in fact the effect size is exactly 277, a level 5% test with similar data will fail to reject ~70% of the time!

## Larger effect size

::: columns
::: {.column width="55%"}
Summary stats for cloud data:

```{r}
set.seed(21524)
cloud %>% group_by(treatment) %>% 
  summarize(mean = mean(rainfall), sd = sd(rainfall), n = n()) %>%
  pander()
```

We can approximate the type II error rate by:

1.  simulating datasets with matching statistics
2.  performing two-sided tests of no difference
3.  computing the proportion of fail-to-reject decisions


:::

::: {.column width="45%"}
```{r, echo = T, eval = F}
type2sim(delta = 350, n = 26, sd = 650.8, 
         alpha = 0.05, nsim = 1000)
```
```{r, fig.width=5, fig.height = 4.5}
par(mar = c(4, 4, 2, 1), cex = 1.5)
sim.out <- type2sim(delta = 400, n = 26, sd = 650.8, alpha = 0.05, nsim = 1000)
hist(sim.out$p, breaks = seq(0, 1, by = 0.05),
     main = "1000 simulated tests", xlab = 'p value', ylab = 'frequency')
abline(v = 0.05, lty = 2, lwd = 2, col = 'red')
text(x = 0.2, y = 500, expr(paste(alpha, " = 0.05")), col = 'red')
```
:::
:::

> If in fact the effect size is exactly 400, a level 5% test with similar data will fail to reject ~40% of the time.

## Smaller effect size

::: columns
::: {.column width="55%"}
Summary stats for cloud data:

```{r}
set.seed(21524)
cloud %>% group_by(treatment) %>% 
  summarize(mean = mean(rainfall), sd = sd(rainfall), n = n()) %>%
  pander()
```

We can approximate the type II error rate by:

1.  simulating datasets with matching statistics
2.  performing two-sided tests of no difference
3.  computing the proportion of fail-to-reject decisions


:::

::: {.column width="45%"}
```{r, echo = T, eval = F}
type2sim(delta = 100, n = 26, sd = 650.8, 
         alpha = 0.05, nsim = 1000)
```
```{r, fig.width=5, fig.height = 4.5}
par(mar = c(4, 4, 2, 1), cex = 1.5)
sim.out <- type2sim(delta = 100, n = 26, sd = 650.8, alpha = 0.05, nsim = 1000)
hist(sim.out$p, breaks = seq(0, 1, by = 0.05),
     main = "1000 simulated tests", xlab = 'p value', ylab = 'frequency')
abline(v = 0.05, lty = 2, lwd = 2, col = 'red')
text(x = 0.2, y = 80, expr(paste(alpha, " = 0.05")), col = 'red')
```
:::
:::

> If in fact the effect size is exactly 100, a level 5% test with similar data will fail to reject ~90% of the time.



## Statistical power

The **power** of a test refers to its **true rejection rate** across alternatives and is defined as: $$\beta = \underbrace{(1 - \text{type II error rate})}_\text{correct decision rate when null is false}$$

Power is often interpreted as a detection rate:

-   high type II error $\longrightarrow$ low power $\longrightarrow$ low detection rate
-   low type II error $\longrightarrow$ high power $\longrightarrow$ high detection rate

> In general tests have low power for alternatives close to the null value (where "close" is relative to sampling variability).

## Power curves

> Power is usually construed as a *curve* depending on the true difference. 

::: {.columns}

::: {.column width="60%"}

Power curve for the test exactly as performed with the cloud seeding data:

```{r, fig.width = 6, fig.height=4}
par(mar = c(4, 4, 1, 1),
    cex = 1.5)
curve(power.t.test(delta = x, n = 26, sd = 650, sig.level = 0.05)$power, 
      from = -1000, to = 1000, 
      xlab = expression(paste('true difference ', delta)),
      ylab = expression(paste('power ', beta)))
```
:::

::: {.column width="40%"}
All other attributes of the test are fixed to approximate the test performed:

- sample size $n = 26$
- significance level $\alpha = 0.05$
- population standard deviation $\sigma = 650$ (larger of two group estimates)

:::

:::


## Two common power analyses

::: columns
::: column
**Post hoc analysis**: how much power does the test I conducted have if the true difference is exactly equal to my estimate?

Helps to interpret negative results:

-   low power $\rightarrow$ failure to reject was likely
-   high power $\rightarrow$ failure to reject was not likely

::: callout-important
## Don't over-interpret post-hoc analyses

Failure to reject using a well-powered test *does not confirm the null hypothesis*.
:::

:::

::: column
**Sample size determination**: how much data do I need to collect to detect a difference of $\delta$ using a particular test?

Helps avoid two potential issues:

-   too little data $\rightarrow$ study not likely to yield significant results
-   too much data $\rightarrow$ study is too likely to yield significant results
:::
:::

## Post-hoc analysis

> Can we estimate the power of a test we already performed?

::: columns
::: {.column width="55%"}
Feasible if we assume (a) a population standard deviation and (b) test conditions are met.

For the cloud seeding test:

```{r, echo=T}
power.t.test(delta = 250, # magnitude of difference
             sd = 650, # largest population SD
             n = 26, # smallest sample size
             sig.level = 0.05, 
             type = 'two.sample', 
             alternative = 'two.sided') 
```
:::

::: {.column width="45%"}
For a conservative estimate, use:

-   *smallest* of the two sample sizes
-   *largest* of the two standard deviations
-   *smaller* difference than observed

> $\Longrightarrow$ our test would only reject in favor of a difference of the observed magnitude about 27% of the time

Failure to reject doesn't strongly rule out the alternative.
:::
:::

## Sample size calculation

> If you were (re)designing the study, how much data should you collect to detect a specified effect size?

::: columns
::: {.column width="55%"}
To detect a difference of 250 or more due to cloud seeding with power 0.9:
```{r, echo=T}
power.t.test(power = 0.9, # target power level
             delta = 250, # smallest difference
             sd = 650, # largest population SD
             sig.level = 0.05, 
             type = 'two.sample', 
             alternative = 'two.sided') 
```
:::

::: {.column width="45%"}
For a conservative estimate, use:

-   *overestimate* of the larger of the two standard deviations
-   *minimum* difference of interest

> $\Longrightarrow$ we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time
:::
:::



## Practical constraints

::: {.columns}

::: {.column width="60%"}

Minimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:

```{r, fig.width = 6, fig.height=4}
library(RColorBrewer)
coul <- brewer.pal(5, "BuPu") 

par(mar = c(4, 4, 1, 1),
    cex = 1.5,
    cex.axis = 0.7)

delta.seq <- sapply(seq(from = 0.5, to = 0.9, by = 0.1), 
                    function(j){
                      sapply(20:300, function(i){
                        del <- power.t.test(n = i, 
                                            power = j, 
                                            sd = 650, 
                                            sig.level = 0.05,
                                            alternative = 'one.sided')$delta
                        return(del)})
})

plot(x = 20:300, y = delta.seq[, 5], type = 'l', 
      xlab = 'sample size',
      ylab = expression(paste('difference ', delta)),
     col = coul[5])

for(j in 4:1){                      
lines(x = 20:300, y = delta.seq[, j], type = 'l', 
     xlab = '',
     ylab = '',
     col = coul[j])
}
abline(h = 300, lty = 2, col = 1)
legend(x = 'topright', legend = paste('power', (5:9)/10), col = coul, fill = coul)

```

Assumes $\sigma = 650$ for a conservative estimate.

:::

::: {.column width="40%"}
It may not be affordable to obtain data for 144 days per treatment group (pilots and planes are expensive). What is achievable within constraints?

- power of 0.8 will require *n* = `r power.t.test(delta = 300, power = 0.8, sd = 650, sig.level = 0.05, alternative = 'one.sided')$n |> ceiling()` per group

    + 138 days total

- decreasing to 0.7 will require *n* = `r power.t.test(delta = 300, power = 0.7, sd = 650, sig.level = 0.05, alternative = 'one.sided')$n |> ceiling()` per group

    + 90 days total

:::

:::