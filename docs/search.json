[
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "Refer to this page for a detailed schedule of coursework and links to all materials. This information is organized by week, and you should check this page on a weekly basis for updates as the quarter progresses.\nI will aim to post/update outlines for class meetings at the beginning of each week (Monday), and update with links to relevant materials as the week progresses. So, you should look at the beginning of the week to see what’s ahead, and check back for newly posted materials before and after class meetings.\n\nPreparing for class meetings\n\nComplete readings in advance of the class meetings for which they are listed. Take notes or make annotations, and bring your notes to class.\nDownload and/or print copies of posted slides and handouts and bring these to class.\nIf a lab is indicated, it is recommended that you bring your laptop/tablet. While these are group activities and it is an option to share, you will not have direct access to your groupmates’ work after the class meeting.\n\n\n\nWeek 1 (1/8/24)\nIntroduction to statistical thinking and study designs\nTuesday class meeting [slides]\n\n[lecture/discussion] course introduction and syllabus review\n[activity] working groups and icebreakers\n[activity] orientation to posit.cloud\n\nThursday class meeting: [slides] [handout]\n\n[reading] van Belle et al. 2.1 - 2.5; Vu and Harrington 1.1\n[lecture/discussion] study designs\n[activity] distinguishing types of studies\n[case study] preventing peanut allergies\n\nReading quiz responses: [2pm section] [4pm section]\nAssignment: [HW1] due Thursday 1/18 11:59pm PST [L1, L2]\n\n\nWeek 2 (1/15/24)\nAcademic holiday 1/15/24 and Monday class schedule on 1/16/24\nIntroduction to data and data types\nThursday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 1.2; Douglas et al. 3.1 - 3.2\n[lecture/discussion] data types and data structures\n[activity] distinguishing data types in practice\n[lab] R basics and data in R\n\nReading quiz responses: [2pm section] [4pm section]\nNo assignment this week\n\n\nWeek 3 (1/22/24)\nDescriptive statistics and graphical summaries\nTuesday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 1.4 - 1.5; van Belle et al. 3.4\n[lecture/discussion] descriptive statistics for numerical and categorical data\n[lab] descriptive statistics in R; robustness\n[case study] functional SNPs associated with muscle strength and size (FAMuSS)\n\nThursday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 1.6\n[lecture/discussion] descriptive statistics for relationships between two variables\n[lab] bivariate summaries in R\n[case study] FAMUSS\n\nReading quiz responses: [2pm section] [4pm section]\nAssignment: extended to next week\n\n\nWeek 4 (1/29/24)\nFoundations for inference\nTuesday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 4.1\n[lecture/discussion] sampling and sampling distributions\n[lab] exploring sampling variability\n[case study] National health and nutrition examination survey (NHANES)\n\nReading quiz responses: [2pm section] [4pm section]\nThursday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 3.3.1, 3.3.2, and 3.3.3; and 4.2\n[lecture/discussion] interval estimation for a population mean\n[lab] computing confidence intervals in R\n[case study] NHANES\n\nAssignment: [HW2] due Thursday 2/8 11:59pm PST [L3, L4]\n\n\nWeek 5 (2/5/24)\nOne-sample inference for numerical data\nTuesday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 4.3\n[lecture/discussion] hypothesis tests for a population mean\n[lab] one-sample \\(t\\) tests \n[case study] DDT in kale \n\nReading quiz responses: [2pm section] [4pm section]\nThursday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 5.1\n[lecture/discussion] interpreting hypothesis tests and understanding decision errors\n[lab] exploring decision errors\n[case study] is true body temperature 98.6F?\n\nReading quiz responses: [2pm section] [4pm section]\nAssignment: [Test 1] due Saturday 2/10 11:59pm PST [L1, L2, L3]\n\n\nWeek 6 (2/12/24)\nTwo-sample inference for numerical data\nTuesday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 5.2 and 5.3\n[lecture/discussion] two-sample t tests and intervals for paired and independent data\n[lab] two-sample t tests \n[case study] evolution of Darwin’s finches \n\nReading quiz responses: [2pm section] [4pm section]\nThursday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 5.4; van Belle et al. 8.4 and 8.5 up to 8.5.4\n[lecture/discussion] statistical power calculations for two-sample inference\n[lab] post hoc and study design power analyses in R\n\nReading quiz responses: [2pm section] [4pm section]\nAssignment: [HW3] due Thursday 2/22 11:59pm PST [L4, L5]\n\n\nWeek 7 (2/19/24)\nNonparametric tests; analysis of variance\nAcademic holiday 2/19/24\nTuesday class meeting [slides] [lab]\n\n[reading] van Belle et al. 8.4 and 8.5 up to 8.5.4\n[lecture/discussion] nonparametric inferences for one- and two-sample problems\n[lab] three case studies\nPlease complete feedback survey by Thursday 2/22 [2pm section] [4pm section]\n\nThursday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 5.5\n[lecture/discussion] analysis of variance\n[lab] grouped summaries; ANOVA in R\n\nAssignment: HW4 due Thursday 2/29 11:59pm PST [L9]\n\n\nWeek 8 (2/26/24)\nInference for categorical data\nAssignment: Test 2 due Friday 3/1 11:59pm PST [L4, L5, L9]\n\n\n\nWeek 9 (3/4/24)\nSimple linear regression\n\nAssignment: HW5 due Thursday 3/14 11:59pm PST [L6, L7, L8]\n\n\nWeek 10 (3/11/24)\nSimple linear regression\n\nAssignment: Test 3 due Friday 3/15 11:59pm PST [L6, L7, L8, L10]\n\n\nFinals week (3/18/24)\nOral exams to be held during scheduled exam time"
  },
  {
    "objectID": "content/week5-moretests.html#todays-agenda",
    "href": "content/week5-moretests.html#todays-agenda",
    "title": "More one-sample tests",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz [2pm section] [4pm section]\nMore one-sample inference: reporting test results; decision errors\nComparing two population means via paired differences\nLab: more \\(t\\)-tests in R"
  },
  {
    "objectID": "content/week5-moretests.html#from-last-time",
    "href": "content/week5-moretests.html#from-last-time",
    "title": "More one-sample tests",
    "section": "From last time",
    "text": "From last time\n\n\n\nWhat hypotheses were tested? \\[  \\]\nWhat was the test statistic and p-value? \\[  \\]\nWhat was the sample size? \\[  \\]\nWhat is the conclusion of the test? \\[  \\]\nInterpret the confidence interval.\n\n\nInference for the body temperature data.\n\ndata(thermometry)\nbody_temps &lt;- thermometry$body.temp\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923"
  },
  {
    "objectID": "content/week5-moretests.html#significance-conventions",
    "href": "content/week5-moretests.html#significance-conventions",
    "title": "More one-sample tests",
    "section": "Significance conventions",
    "text": "Significance conventions\n\n\nConvention 1: statistical significance\n\n\\(p &lt; 0.05\\): reject \\(H_0\\)\n\\(p \\geq 0.05\\): fail to reject \\(H_0\\)\n\n\n“The data provide significant evidence at level \\(\\alpha\\) = 0.05 against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (T = -5.4548 on 129 degrees of freedom, p = .0000002411).”\n\n\nConvention 2: weight of evidence against \\(H_0\\)\n\n\\(p &lt; 0.01\\): strong evidence\n\\(0.01 \\leq p &lt; 0.05\\): moderate evidence\n\\(0.05 \\leq p &lt; 0.1\\): weak evidence\n\\(0.1 \\leq p\\): no evidence\n\n\n“The data provide strong evidence against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (T = -5.4548 on 129 degrees of freedom, p = .0000002411).”\n\n\n\nYou may use either convention to interpret test results."
  },
  {
    "objectID": "content/week5-moretests.html#choosing-alternatives",
    "href": "content/week5-moretests.html#choosing-alternatives",
    "title": "More one-sample tests",
    "section": "Choosing alternatives",
    "text": "Choosing alternatives\n\n\n\nIs the mean body temp less than 98.6?\n\nWhich test should you use? Consider the interpretations:\n\n[lower] evidence favoring lower temp\n[upper] no evidence against lower temp\n\nThe conclusions are consistent but not equivalent – (a) is a better answer.\nYour alternative should be the claim you hope to support with evidence, and your null the claim you hope to refute.\n\n\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -5.4548, df = 129, p-value = 1.205e-07\nalternative hypothesis: true mean is less than 98.6\n95 percent confidence interval:\n     -Inf 98.35577\nsample estimates:\nmean of x \n 98.24923 \n\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -5.4548, df = 129, p-value = 1\nalternative hypothesis: true mean is greater than 98.6\n95 percent confidence interval:\n 98.14269      Inf\nsample estimates:\nmean of x \n 98.24923"
  },
  {
    "objectID": "content/week5-moretests.html#reporting-test-results",
    "href": "content/week5-moretests.html#reporting-test-results",
    "title": "More one-sample tests",
    "section": "Reporting test results",
    "text": "Reporting test results\n\n\nTo report the result of a hypothesis test, you should:\n\nLead with your conclusion\nState the hypotheses tested\nInterpret the conclusion of the test in context\nProvide the \\(T\\) statistic, degrees of freedom, and \\(p\\)-value\nState and interpret the point estimate and interval for the mean\n\n\n\nOur results suggest mean body temperature is less than 98.6 °F. We tested the null hypothesis that mean body temperature is 98.6 °F or greater against the alternative that mean body temperature is less than 98.6 °F. The data provide sufficiently strong evidence against the hypothesis that mean body temperature is 98.6 °F or greater in favor of the alternative that mean body temperature is less than 98.6 °F (T = -5.4548 on 129 degrees of freedom, p-value = .0000001205). With 95% confidence, the mean nightly hours of sleep is estimated to be at most 98.36 °F, with a point estimate of 98.23 (SE = 0.0643)."
  },
  {
    "objectID": "content/week5-moretests.html#your-turn-sleep-data",
    "href": "content/week5-moretests.html#your-turn-sleep-data",
    "title": "More one-sample tests",
    "section": "Your turn: sleep data",
    "text": "Your turn: sleep data\nOpen lab7-moretests in the class workspace.\n\n\nWork with your group on just one of the questions below.\n\nDo US adults sleep 7.5 hours per night on average?\nDo US adults sleep less than 7.5 hours per night on average?\nDo US adults sleep more than 7.5 hours per night on average?\nDo US adults sleep more than 6.5 hours per night on average?\n\n\nYour task is to determine and carry out an appropriate test, and then write a complete report of the test outcome:\n\nanswer the question\nhypotheses tested\ntest conclusion, interpreted in context\ntest statistic, degrees of freedom, \\(p\\)-value\nconfidence interval and point estimate, interpreted in context\n\nThese elements should be summarized together in complete sentences."
  },
  {
    "objectID": "content/week5-moretests.html#tests-and-intervals",
    "href": "content/week5-moretests.html#tests-and-intervals",
    "title": "More one-sample tests",
    "section": "Tests and intervals",
    "text": "Tests and intervals\nNotice that t.test produces a confidence interval.\n\nfor the two-sided test, the interval is exactly the one we calculated before\nfor the one-sided tests, the interval is a lower/upper confidence bound: \\[\\begin{align*}\n\\bar{x} - c \\times SE(\\bar{x}) &\\qquad \\text{lower confidence bound} \\quad\\Longleftrightarrow\\quad \\text{upper-sided test}\\\\\n\\bar{x} + c \\times SE(\\bar{x}) &\\qquad \\text{upper confidence bound} \\quad\\Longleftrightarrow\\quad \\text{lower-sided test}\n\\end{align*}\\] where \\(c\\) is chosen to ensure a specified coverage level\n\nThe tests and intervals correspond in the following sense:\n\nThe level \\(\\alpha\\) test rejects just in case the \\((1 - \\alpha)\\) confidence interval excludes \\(\\mu_0\\)\n\n\n# changing confidence level for interval\nt.test(body_temps, mu = 98.6, alternative = 'less', conf.level = 0.99)"
  },
  {
    "objectID": "content/week5-moretests.html#decision-errors",
    "href": "content/week5-moretests.html#decision-errors",
    "title": "More one-sample tests",
    "section": "Decision errors",
    "text": "Decision errors\n\n\nThere are two ways to make a mistake in a hypothesis test:\n\nreject a true \\(H_0\\)\nfail to reject a false \\(H_0\\)\n\nThese are known as type I and type II errors.\nBecause rejecting \\(H_0\\) is a stronger conclusion, type I errors are considered more severe.\n\n\n\nThe significance level of a test is a cap on the type I error rate."
  },
  {
    "objectID": "content/week5-moretests.html#exploring-type-i-error-rates",
    "href": "content/week5-moretests.html#exploring-type-i-error-rates",
    "title": "More one-sample tests",
    "section": "Exploring type I error rates",
    "text": "Exploring type I error rates\nIn lab7-moretests, there are codes to draw a sample from a mock population and test a true null hypothesis. Run these and take note of your \\(p\\)-value.\n\n\nIn this situation, a type I error is rejecting \\(H_0\\).\n\nrule: reject if \\(p &lt; \\alpha\\).\nwe will tally rejections for various \\(\\alpha\\) values\n\n\n\n\n\nSignificance level\nError frequency\n\n\n\n\n\\(\\alpha = 0.2\\)\n\n\n\n\\(\\alpha = 0.1\\)\n\n\n\n\\(\\alpha = 0.05\\)\n\n\n\n\\(\\alpha = 0.02\\)\n\n\n\n\n\n\n\nTakeaway: using larger significance thresholds leads to [more/less] type 1 errors"
  },
  {
    "objectID": "content/week5-moretests.html#exploring-type-ii-errors",
    "href": "content/week5-moretests.html#exploring-type-ii-errors",
    "title": "More one-sample tests",
    "section": "Exploring type II errors",
    "text": "Exploring type II errors\nNow let’s test a false null hypothesis.\n\n\nA type II error is failing to reject \\(H_0\\).\n\nrule: \\(p &lt; 0.05\\)\nuse example commands to test each hypothesis at right\nuse a two-sided test\n\n\n\n\n\nNull value \\(\\mu_0\\)\nError frequency\n\n\n\n\n4.2\n\n\n\n4.6\n\n\n\n4.9\n\n\n\n5.1\n\n\n\n5.4\n\n\n\n5.7\n\n\n\n\n\n\n\nNotice that the type II error is quite high for null values near the true mean; this indicates the test has little power to detect such alternatives."
  },
  {
    "objectID": "content/week5-moretests.html#swimsuit-data",
    "href": "content/week5-moretests.html#swimsuit-data",
    "title": "More one-sample tests",
    "section": "Swimsuit data",
    "text": "Swimsuit data\nLet’s consider our first two-sample problem.\n\nAre swimmers faster in bodysuits than in regular swimsuits?\n\nBelow are the first few observations of the average velocity of competitive swimmers in a 1500m; one measurement was taken in a swimsuit, the other in a bodysuit.\n\n\n\n\n\n\n\n\n\n\nswimmer.number\nwet.suit.velocity\nswim.suit.velocity\n\n\n\n\n1\n1.57\n1.49\n\n\n2\n1.47\n1.37\n\n\n3\n1.42\n1.35\n\n\n\n\n\nCan you formulate a pair of hypotheses to test to answer this question using methods from last time?"
  },
  {
    "objectID": "content/week5-moretests.html#comparing-two-means",
    "href": "content/week5-moretests.html#comparing-two-means",
    "title": "More one-sample tests",
    "section": "Comparing two means",
    "text": "Comparing two means\nWe can formulate the question as a comparison of two means:\n\\[H_0: \\mu_\\text{bodysuit} \\leq \\mu_\\text{swimsuit}\\] \\[H_A: \\mu_\\text{bodysuit} &gt; \\mu_\\text{swimsuit}\\] It is common to express hypotheses of this form in terms of a difference in means: \\[H_0: \\underbrace{\\mu_\\text{bodysuit} - \\mu_\\text{swimsuit}}_\\delta \\leq 0\\] \\[H_A: \\underbrace{\\mu_\\text{bodysuit} - \\mu_\\text{swimsuit}}_\\delta &gt; 0\\]"
  },
  {
    "objectID": "content/week5-moretests.html#pairing",
    "href": "content/week5-moretests.html#pairing",
    "title": "More one-sample tests",
    "section": "Pairing",
    "text": "Pairing\nThe velocities in the swimsuit dataset are paired because every swimmer is measured in both suits.\n\nData are paired just in case the measurements in each group are taken on exactly the same study units or the study units can be placed in one-to-one correspondence.\n\nThis is the easiest situation to handle, because it reduces to a one-sample problem with the observed differences."
  },
  {
    "objectID": "content/week5-moretests.html#inference-for-paired-data",
    "href": "content/week5-moretests.html#inference-for-paired-data",
    "title": "More one-sample tests",
    "section": "Inference for paired data",
    "text": "Inference for paired data\n\nCalculate paired differences.\n\n\n\n\n\n\n\n\n\n\n\n\nswimmer.number\nwet.suit.velocity\nswim.suit.velocity\nvelocity.diff\n\n\n\n\n1\n1.57\n1.49\n0.08\n\n\n2\n1.47\n1.37\n0.1\n\n\n3\n1.42\n1.35\n0.07\n\n\n\n\n\n\n\n\nPerform test as before.\n\n\n# test for paired difference\nt.test(diffs, mu = 0, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  diffs\nt = 12.318, df = 11, p-value = 4.443e-08\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 0.06620114        Inf\nsample estimates:\nmean of x \n   0.0775 \n\n\n\n\nReport the result of the test. You try.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-hypothesis.html#todays-agenda",
    "href": "content/week5-hypothesis.html#todays-agenda",
    "title": "Hypothesis testing",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz [2pm section] [4pm section]\nHypothesis tests for a population mean\nLab: \\(t\\)-tests in R\n(If time) Exploring decision errors"
  },
  {
    "objectID": "content/week5-hypothesis.html#ddt-data",
    "href": "content/week5-hypothesis.html#ddt-data",
    "title": "Hypothesis testing",
    "section": "DDT data",
    "text": "DDT data\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm). Each measurement was taken by a different laboratory.\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits."
  },
  {
    "objectID": "content/week5-hypothesis.html#hypothesis-testing",
    "href": "content/week5-hypothesis.html#hypothesis-testing",
    "title": "Hypothesis testing",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nThis is an example of a hypothesis testing problem: we want to test the hypothesis that mean DDT in kale is within safe limits. Hypothesis testing is another form of statistical inference.\nThe general pattern for performing a hypothesis test is:\n\nFormulate the hypothesis to test in terms of the values of a population parameter.\nAssess the likelihood of the data under the hypothesis through use of a “test statistic”.\nConclude whether the data provide evidence favoring an alternative.\n\nToday we’ll cover each step in turn in the context of tests for a population mean."
  },
  {
    "objectID": "content/week5-hypothesis.html#formulating-hypotheses",
    "href": "content/week5-hypothesis.html#formulating-hypotheses",
    "title": "Hypothesis testing",
    "section": "1. Formulating hypotheses",
    "text": "1. Formulating hypotheses\nHypotheses cannot be tested in isolation, but must be considered relative to a specified alternative.\nTo articulate the hypotheses for a test, we need:\n\npopulation parameter of interest\nnull hypothesis \\(H_0\\): possible value(s) under the claim to be tested\nalternative hypothesis \\(H_A\\): possible value(s) if the claim is found to be false\n\nIn the context of the DDT example…\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\]"
  },
  {
    "objectID": "content/week5-hypothesis.html#test-statistic",
    "href": "content/week5-hypothesis.html#test-statistic",
    "title": "Hypothesis testing",
    "section": "2. Test statistic",
    "text": "2. Test statistic\nTest statistics are data summaries that:\n\ndepend on the null value of the population parameter\nhave a known sampling distribution\n\nFor a population mean, we use: \\[T = \\frac{\\bar{x} - \\mu_0}{s_x/\\sqrt{n}}\\]\nThis is well-described by a \\(t_{n - 1}\\) model when \\(\\mu = \\mu_0\\). It is useful for the test because:\n\nlarge (absolute) values of \\(T\\) are unlikely if \\(\\mu = \\mu_0\\)\nsmall (absolute) values of \\(T\\) are expected if \\(\\mu = \\mu_0\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#drawing-a-conclusion",
    "href": "content/week5-hypothesis.html#drawing-a-conclusion",
    "title": "Hypothesis testing",
    "section": "3. Drawing a conclusion",
    "text": "3. Drawing a conclusion\n\n\nIn the DDT example, \\(T\\) = 2.906. This favors \\(H_A\\), but by how much?\nAccording to the \\(t\\) model, less than 1% of samples would produce a result more favorable to \\(H_A\\).\n\n\n\n\n\n\nPoint estimate:\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n3.328\n0.1129\n\n\n\n\n\nTest statistic:\n\\[T = \\frac{\\bar{x} - \\mu_0}{SE(\\bar{x})} = \\]\n\nThis is strong evidence against the claim that the DDT level is 3ppm or less and in favor of the claim that the DDT level exceeds 3ppm."
  },
  {
    "objectID": "content/week5-hypothesis.html#recap-of-ddt-example",
    "href": "content/week5-hypothesis.html#recap-of-ddt-example",
    "title": "Hypothesis testing",
    "section": "Recap of DDT example",
    "text": "Recap of DDT example\n\nIs the mean DDT level in kale 3ppm or less?\n\n\n\nData are measurements of DDT levels in ppm from 15 labs.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n3.328\n0.4372\n0.1129\n\n\n\n\n\n\\(t_{14}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#another-example-sleep",
    "href": "content/week5-hypothesis.html#another-example-sleep",
    "title": "Hypothesis testing",
    "section": "Another example: sleep",
    "text": "Another example: sleep\n\nDoes the average U.S. adult sleep at least 7 hours per night?\n\n\n\nData are reported average hours of sleep per night from 135 NHANES respondents.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n6.896\n1.394\n0.12\n\n\n\n\n\n\\(t_{134}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#your-turn-body-temperatures",
    "href": "content/week5-hypothesis.html#your-turn-body-temperatures",
    "title": "Hypothesis testing",
    "section": "Your turn: body temperatures",
    "text": "Your turn: body temperatures\n\nIs mean body temperature actually 98.6 °F, or is it lower?\n\n\n\nData are 130 observations of body temperature (°F) derived from a JAMA study.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n98.25\n0.7332\n0.0643\n\n\n\n\n\n\\(t_{129}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#strength-of-evidence",
    "href": "content/week5-hypothesis.html#strength-of-evidence",
    "title": "Hypothesis testing",
    "section": "Strength of evidence",
    "text": "Strength of evidence\nThe result that 0.575% of samples would produce a test statistic more strongly favoring the alternative hypothesis is an example of a p-value:\n\nthe probability under \\(H_0\\) of obtaining a sample for which the test statistic is at least as favorable to \\(H_A\\) as the value actually observed\n\nIn other words, \\(p\\)-values assume the null hypothesis is true, and then ask, “what is the chance I’d obtain data at least as suggestive as what I have that the alternative is more likely than the null?”\n\nsmaller \\(p\\)-values: if \\(H_0\\) is true, equally or more favorable results are not expected often by chance\nlarger \\(p\\)-values: if \\(H_0\\) is true, equally or more favorable results are expected often by chance"
  },
  {
    "objectID": "content/week5-hypothesis.html#evidence-thresholds",
    "href": "content/week5-hypothesis.html#evidence-thresholds",
    "title": "Hypothesis testing",
    "section": "Evidence thresholds",
    "text": "Evidence thresholds\nIt remains to define an evidence threshold above which we decide to reject \\(H_0\\).\nA heuristic is to fix a significance level \\(\\alpha\\) and reject \\(H_0\\) whenever \\(p &lt; \\alpha\\).\n\nrepresents an evidence threshold\nconventionally, \\(\\alpha = 0.05\\)\ncontrols error rates\n\nImagine that indeed mean DDT in kale is 3ppm. Then 0.575% of samples produce test statistics at least as favorable to the alternative as what we saw in the study.\n\nso if we set the evidence threshold for rejecting \\(H_0\\) exactly here (\\(\\alpha = 0.00575\\)) we’ll be wrong 0.575% of the time\nif we set the evidence threshold lower (say \\(\\alpha = 0.01\\)) we’ll be wrong more than 0.575% of the time (in fact 1% of the time)"
  },
  {
    "objectID": "content/week5-hypothesis.html#interpreting-results",
    "href": "content/week5-hypothesis.html#interpreting-results",
    "title": "Hypothesis testing",
    "section": "Interpreting results",
    "text": "Interpreting results\nBecause of the anatomy of hypothesis tests, there are two possible findings:\n\n[above evidence threshold] reject \\(H_0\\) in favor of \\(H_A\\)\n[below evidence threshold] fail to reject \\(H_0\\)\n\n\nSince the null is assumed to be true to perform the test, the test can only result in (a) evidence against this assumption or (b) no evidence against this assumption. But because it’s an assumption, we don’t affirm it if the test fails.\n\nIn the DDT example: the data provide sufficiently strong evidence (p = 0.00575) to reject the hypothesis that mean DDT in kale is at most 3ppm in favor of the hypothesis that mean DDT in kale exceeds 3ppm."
  },
  {
    "objectID": "content/week5-hypothesis.html#composite-hypotheses",
    "href": "content/week5-hypothesis.html#composite-hypotheses",
    "title": "Hypothesis testing",
    "section": "Composite hypotheses",
    "text": "Composite hypotheses\nThe null hypothesis is a composite of values, so why did we choose just one (\\(\\mu_0 = 3\\)) to perform the test?\nAny null value farther from the alternative will produce stronger results.\n\n\n\n\n\n\n\n\nNull value\nTest statistic numerator\nProportion of samples more favorable to \\(H_A\\)\n\n\n\n\n\\(\\mu_0 = 3\\)\n0.328\n0.00575\n\n\n\\(\\mu_0 = 2.99\\)\n0.338\n0.00483\n\n\n\\(\\mu_0 = 2.95\\)\n0.378\n0.00239\n\n\n\nSo by using \\(\\mu_0 = 3\\), we are choosing the most conservative null value for the test."
  },
  {
    "objectID": "content/week5-hypothesis.html#components-of-a-test",
    "href": "content/week5-hypothesis.html#components-of-a-test",
    "title": "Hypothesis testing",
    "section": "Components of a test",
    "text": "Components of a test\n\n\n\n\n\n\n\n\nComponent\nExplanation\nDDT example\n\n\n\n\nPopulation parameter\nThe quantity of interest\nMean DDT \\(\\mu\\)\n\n\nNull hypothesis\nThe claim to be tested\n\\(\\mu \\leq 3\\)\n\n\nAlternative hypothesis\nThe alternative claim\n\\(\\mu &gt; 3\\)\n\n\nTest statistic\nA function of the sample data and the null value of the population parameter\n\\(T = \\frac{\\bar{x} - \\mu_0}{s_x/\\sqrt{n}} = 2.91\\)\n\n\nModel\nSampling distribution of the test statistic under \\(H_0\\)\n\\(t_{df = 14}\\) model\n\n\n\\(p\\)-value\nProbability under \\(H_0\\) of obtaining a result at least as favorable to \\(H_A\\)\n0.575% of samples are more favorable to \\(H_A\\)\n\n\nDecision\nReject or fail to reject \\(H_0\\)\nReject at \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#performing-tests-in-r",
    "href": "content/week5-hypothesis.html#performing-tests-in-r",
    "title": "Hypothesis testing",
    "section": "Performing tests in R",
    "text": "Performing tests in R\n\n\nInputs:\n\ndata vector\nnull value of parameter\nalternative hypothesis\n\nOutputs:\n\ntest statistic\ndegrees of freedom for \\(t\\) model\n\\(p\\)-value\nconfidence interval\npoint estimate\n\n\nt.test performs all calculations. Locate each input (1-3) and output (4-7) below:\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#directional-hypotheses",
    "href": "content/week5-hypothesis.html#directional-hypotheses",
    "title": "Hypothesis testing",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nTests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.\n\n\n\n\n\n\n\n\n\nTest type\nNull\nAlternative\nFavors alternative\n\n\n\n\nUpper-sided\n\\(\\mu \\leq \\mu_0\\)\n\\(\\mu &gt; \\mu_0\\)\npositive \\(T\\)\n\n\nLower-sided\n\\(\\mu \\geq \\mu_0\\)\n\\(\\mu &lt; \\mu_0\\)\nnegative \\(T\\)\n\n\nTwo-sided\n\\(\\mu = \\mu_0\\)\n\\(\\mu \\neq \\mu_0\\)\nlarge \\(|T|\\)\n\n\n\nThe direction of the test affects the \\(p\\)-value calculation (and thus decision), but won’t change the test statistic.\n\n# upper-sided\nt.test(ddt, mu = mu_0, alternative = 'greater')\n\n# lower-sided\nt.test(ddt, mu = mu_0, alternative = 'less')\n\n# two-sided (default)\nt.test(ddt, mu = mu_0, alternative = 'two.sided')"
  },
  {
    "objectID": "content/week5-hypothesis.html#lab-t-tests-in-r",
    "href": "content/week5-hypothesis.html#lab-t-tests-in-r",
    "title": "Hypothesis testing",
    "section": "Lab: \\(t\\)-tests in R",
    "text": "Lab: \\(t\\)-tests in R\nOpen up lab6-hypotesting in the class workspace. The goals for this lab are:\n\nLearn how to implement \\(t\\) tests in R and interpret output\nPractice formulating and testing hypotheses from simple research questions\n(If time) Explore decision errors\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/syllabus.html",
    "href": "content/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Statistics plays a crucial role in the sciences: statistical techniques provide a means of weighing quantitative evidence derived from observation and experimentation in the face of uncertainty. Statistical thinking and data analysis also facilitate discovery, exploration, and hypothesis generation. Likewise, the sciences play a crucial role in statistics: technological and knowledge innovations in methods of scientific investigation motivate the development of new statistical methods for data analysis.\nThis class aims to provide a hands-on introduction to common statistical methods used almost universally across the sciences — descriptive and graphical techniques, inferential methods for comparing population means, analysis of categorical data and contingency tables, and linear regression — while drawing on examples from the life sciences to help illuminate the potential for application in students’ chosen field(s) of study and providing basic training in the use of statistical software. The class also creates a unique opportunity for students to interact broadly and make connections across majors and class standing.\n\nCourse information\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05/8273] 2:10pm — 4:00pm TR Construction Innovations Center Room C202\n[Section 06/8274] 4:10pm — 6:00pm TR Business Room 113\n\nClass meetings will comprise a mixture of lecture, lab activities, class activities, and discussion.\nOffice hours: 12:10pm — 2:00pm MW 25-236 or Zoom [by appointment].\nThese times are partitioned into 10 minute intervals that you can schedule via the appointment link above; this system is intended to minimize waiting times and guarantee one-on-one availability. Slots can be scheduled anywhere from 7 calendar days to 10 minutes in advance. While drop-ins are welcome, I can’t guarantee availability outside of scheduled times.\n\n\nCatalog Description\nData collection and experimental design, descriptive statistics, confidence intervals, parametric and non parametric one and two-sample hypothesis tests, analysis of variance, correlation, simple linear regression, chi-square tests. Applications of statistics to the life sciences. Substantial use of statistical software.\nPrerequisite: MATH 96; or MATH 115; or appropriate Math Placement Level.\nFulfills GE Area B4 (GE Area B1 for students on the 2019-20 or earlier catalogs); a grade of C- or better is required in one course in this GE area.\n\n\nMaterials\nTo access course materials, engage in class, and complete assignments, you’ll need an internet-connected (a) laptop or (b) tablet with a keyboard — the keyboard is necessary since we will do some web-hosted computation and you will be expected to type assignments. I’ll let you know in advance when you need to bring a laptop/tablet to class. Besides your personal computer, all materials are free.\nTextbooks:\n\nVu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences, First edition. A PDF and tablet-friendly version are available for free online at the link above. This will be our primary reference and we will cover chapters 1 – 2, 4 – 6, and 8.\nVan Belle, Fisher, Heagerty, and Lumley (2004). Biostatistics: a methodology for the health sciences. Wiley. A PDF can be obtained through the Kennedy Library via the link above. This text provides a thorough introduction to biostatistics (statistics for life sciences) and is an excellent reference for more depth of coverage. Select readings will be assigned from this book.\nDouglas et al. (2023). An Introduction to R. This online book covers a variety of introductory topics pertaining to R/RStudio: installation, packages, files and directories, objects, functions, data types, data structures, graphics, basic statistics, markdown, and version control. Select readings will be assigned from this book.\n\nComputing: computing will be hosted online via a [posit.cloud workspace]. To access the workspace, you’ll need to create a (free) posit.cloud account — open the invitation by clicking the link above and follow prompts. Activities and assignments will be distributed via this workspace. Please be aware that any files you create on the workspace will be visible to admins.\nCourse notes: notes to supplement readings will be posted as needed on the course website.\n\n\nLearning outcomes\nThe course aims to enable you to demonstrate the following abilities.\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques\n[L4] construct and interpret confidence intervals for means and differences between means for independent and paired samples\n[L5] conduct parametric and non-parametric two-sample hypothesis tests for means\n[L6] construct and interpret a confidence interval for a single proportion\n[L7] conduct Chi-square goodness-of-fit tests and tests for independence\n[L8] distinguish between case-control and cohort studies and compute relative-risk and odds in the appropriate settings\n[L9] perform analysis of variance tests and post-hoc comparisons for completely randomized designs\n[L10] use simple linear regression to describe relationships between variables\n[L11] apply one or more methods from the course to your major field of study\n\nIn addition, you will learn to perform simple analyses and computations in R and can expect to attain a basic level of proficiency with the software; however, as this is not a programming class, emphasis will be placed on obtaining and interpreting relevant outputs in the context of the analyses indicated above.\n\n\nAssessments\nAttainment of learning outcomes will be measured by performance on homework assignments, tests, and a final oral exam.\nHomeworks are your opportunity to practice using course concepts and methods covered in class and comprise both graded and ungraded questions, marked as such. Graded questions will be largely data analytic and will correspond to specific marked learning outcomes; responses will be assessed as satisfactory (S) or needing improvement (NI) according to whether they are fully correct, and qualitative feedback will be offered if an assessment of NI is given. Ungraded questions are entirely for your benefit as extra practice. You can submit revisions to any graded responses needing improvement for reassessment.\nTests are your opportunity to demonstrate that you’ve synthesized course material and achieved learning outcomes. Tests will comprise sets of questions, some conceptual and some data analytic, corresponding to specific marked learning outcomes. Responses will be assessed as satisfactory (S) or needing improvement (NI) according to whether they are fully correct, and qualitative feedback will be offered if an assessment of NI is given. You will be given a 24 hour window to complete each test, and you can submit revisions to responses needing improvement.\nThe final oral exam will require you to find an application of course material in the field of your major and present it as a case study in 5 minutes. You will be expected to describe the research question, study design, analysis, and conclusion, and answer 1-2 questions. These will be carried out in private and scheduled during the final exam time. This assignment applies to learning outcome L11 only, and you will receive an assessment of ‘fully met’, ‘partly met’, or ‘unmet’ for that outcome depending on your presentation and ability to answer questions. A rubric will be provided in advance to clearly define the expectations associated with each possible assessment.\n\n\nLetter grades\nStudents will receive a score for each learning outcome representing the weighted proportion of questions matched with that outcome that received a satisfactory assessment across all assignments, with relatively more weight given to questions from tests. The outcome will be determined as ‘fully met’ if the weighted proportion is at least 0.8, ‘partly met’ if the weighted proportion is between 0.5 and 0.8, and ‘unmet’ otherwise. To receive a passing grade in the class, at least six outcomes must be partly met. Letter grades are then defined as follows:\n\n\n\n\n\n\n\n\nGrade\nMinimum number of partly + fully met outcomes\nMinimum number of fully met outcomes\n\n\n\n\nA\n10\n9\n\n\nA-\n10\n8\n\n\nB+\n8\n7\n\n\nB\n8\n6\n\n\nB-\n8\n5\n\n\nC+\n6\n5\n\n\nC\n6\n4\n\n\nC-\n6\n3\n\n\nD+\n6\n2\n\n\nD\n6\n1\n\n\nD-\n6\n0\n\n\n\nPlease note that these definitions are tentative and potentially subject to change. Please also note that failure to adhere to course policies — particularly collaboration, academic integrity, and attendance policies — may result in a lower letter grade than would otherwise be assigned.\n\n\nTentative schedule\nSubject to change.\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings (V&H)\nAssignments\n\n\n\n\n1 (1/8/24)\nIntroduction to statistical thinking and study design\n1.1\nHW1\n\n\n2 (1/15/24)\nData, data types, and data collection\n1.2\nNone\n\n\n3 (1/22/24)\nDescriptive statistics and graphical summaries\n1.4 – 1.6\nHW2\n\n\n4 (1/29/24)\nFoundations for inference\n4.1 – 4.2\nHW2\n\n\n5 (2/5/24)\nOne- and two-sample inference for numerical data\n4.3; 5.1 — 5.3\nTest 1\n\n\n6 (2/12/24)\nNonparametric inference\nTBD\nHW3\n\n\n7 (2/19/24)\nComparing many means with analysis of variance\n5.5\nHW4\n\n\n8 (2/26/24)\nInference for categorical data\n8.1 — 8.4\nTest 2\n\n\n9 (3/4/24)\nSimple linear regression: model framework and estimation\n6.1 — 6.3\nHW5\n\n\n10 (3/11/24)\nSimple linear regression: inference\n6.4 — 6.5\nTest 3\n\n\nFinals (3/18/24)\nN/A\nN/A\nOral exam\n\n\n\n\n\nCourse policies\n\nCollaboration\nCollaboration within the class and across class sections is allowed and encouraged on homework assignments, subject to certain conditions outlined in the paragraph below. Collaborations should not include individuals outside of the class. Students collaborating with a group are expected to prepare their own assignment submissions, in their own words and writing, and should to indicate their collaborators in writing on their submission. This is limited to peers that were consulted closely in completing the assignment; brief or passing interactions are not considered collaborations.\nA collaboration is a shared effort. Students that choose to work together on homework assignments are expected to make material contributions towards producing one or more shared answers or solutions. Material contributions might include participation in discussions, critique of a proposed solution, or presentation of a problem approach. In the absence of such contributions, submitting solutions prepared in a group is not appropriate. The best way to adhere to this policy is to attempt problems individually before consulting others and exchanging work.\n\n\nAttendance\nRegular attendance is essential for success in the course and required per University policy. Each student may miss two class meetings without notice but additional absences should be excusable and students should notify the instructor. Unexcused absences may negatively impact course grades.\n\n\nCommunication and email\nStudents are encouraged to use face-to-face means of communication (office hours, class meetings, appointments) when possible. Email may be used on a secondary basis or when a written record of communication is needed. Every effort is made to respond to email within 48 weekday hours — so a message sent Thursday or Friday may not receive a reply until Monday or Tuesday. Time-sensitive messages should be identified as such in the subject line. For non-time-sensitive messages, please wait one week before sending a reminder.\n\n\nTime commitment\nSTAT218 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including lectures, reading, assignments, and study time. Some variability in workload by week should be expected, and most students will need to budget a few extra hours each week. However, students can expect to be able to meet course expectations with a time commitment of 12-16 hours per week. Considering that lecture accounts for four hours per week, students should anticipate devoting 8-12 hours outside of class.\n\n\nAssignment scores and final grades\nEvery effort will be made to provide consistent, fair, and accurate evaluation of student work. Students should notify the instructor of any suspected errors or discrepancies in evaluation promptly on an assignment-by-assignment basis (i.e., not at the end of the quarter) to guarantee consideration. Final (letter) grades will only be changed in the case of clerical errors. Attempting to negotiate scores or final grades is not appropriate.\nPer University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. If any student feels their grade is unfairly assigned at the end of the course, they have the right to appeal it according to the procedure outlined here.\n\n\nDeadlines and late work\nEach student may turn in two homework assignments up to one week late without penalty at any time during the quarter and without notice. Subsequently, late work will incur a penalty in final grade calculations unless an extension is granted in advance. As a general policy, late work will be not accepted beyond one week after the original due date. Deadlines for tests are strict.\nThese policies are intended to provide you with some flexibility to work around unforeseen circumstances while maintaining accountability for completing coursework in a timely manner. That said, if any circumstances arise that the policies do not accommodate well, please let me know and I will do my best to work with you to keep you on track in the course. Exceptions may be granted for significant and unforeseen challenges (medical absences, family emergencies, and the like).\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nConduct and Academic Integrity\nStudents are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR).\nInstances of academic dishonesty will be reported to OSRR and will result in penalty ranging from credit reduction to grade reduction to course failure, depending on the severity of the situation.\n\n\nCopyright and distribution of course materials\nAll course materials, including handouts, homework assignments, lab assignments, study guides, course notes, exams, and solutions are subject to copyright; students are not permitted to share or distribute any course materials without the written consent of the instructor. This includes, in particular, uploading materials or prepared solutions to online services and sharing materials or prepared solutions with students who may take the course in a future term. Transgressions of this policy compromise the effectiveness of instruction and assessment and do a disservice to current and future students."
  },
  {
    "objectID": "content/week3-multivariate.html#todays-agenda",
    "href": "content/week3-multivariate.html#todays-agenda",
    "title": "Bivariate summaries",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nHW discussion\nReading quiz [2pm section] [4pm section]\n(Univariate) Measures of spread\nGraphical summmaries for two variables\n\nnumeric/numeric\nnumeric/categorical\ncategorial/categorical\n\nLab: bivariate graphics in R"
  },
  {
    "objectID": "content/week3-multivariate.html#measures-of-spread",
    "href": "content/week3-multivariate.html#measures-of-spread",
    "title": "Bivariate summaries",
    "section": "Measures of spread",
    "text": "Measures of spread\nThe spread of observations refers to how concentrated or diffuse the values are.\n\nTwo ways to understand and measure spread:\n\nranges of values capturing much of the distribution\ndeviations of values from a central value"
  },
  {
    "objectID": "content/week3-multivariate.html#range-based-measures",
    "href": "content/week3-multivariate.html#range-based-measures",
    "title": "Bivariate summaries",
    "section": "Range-based measures",
    "text": "Range-based measures\nA simple way to understand and measure spread is based on ranges. Consider more ages, sorted and ranked:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\nrank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\nThe range is the difference [maximum value] - [minimum value] \\[\\text{range} = 34 - 16 = 18\\]\nThe interquartile range (IQR) is the difference [75th percentile] - [25th percentile] \\[\\text{IQR} = 29 - 19 = 10\\] When might you prefer IQR to range? Can you think of an example?"
  },
  {
    "objectID": "content/week3-multivariate.html#deviation-based-measures",
    "href": "content/week3-multivariate.html#deviation-based-measures",
    "title": "Bivariate summaries",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe average deviation is defined as the average of the absolute values of the deviations from the mean: \\[\\frac{8 + 6 + 5 + 4 + 3 + 2 + 1 + 2 + 4 + 5 + 6}{12}\\]\nThe standard deviation is defined in terms of the squared deviations from the mean: \\[\\sqrt{\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2}{12 - 1}}\\]"
  },
  {
    "objectID": "content/week3-multivariate.html#mathematical-notations",
    "href": "content/week3-multivariate.html#mathematical-notations",
    "title": "Bivariate summaries",
    "section": "Mathematical notations",
    "text": "Mathematical notations\n\n\nDenote \\(n\\) observations of a variable \\(x\\) by \\(x_1, \\dots, x_n\\), so that \\(x_i\\) indicates the value of the \\(i\\)th observation. Our ages:\n\n\n16, 18, 19, 20, 21, 22, 25, 26, 28, 29, 30 and 34\n\n\nApplying the notation at right:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(x_i\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_i - \\bar{x}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\((x_i - \\bar{x})^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean of the observations is written: \\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]\nThe standard deviation is: \\[s_x = \\sqrt{\\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2}\\]\n\n\nHow would you write the formula for calculating average deviation using this notation?"
  },
  {
    "objectID": "content/week3-multivariate.html#interpretations",
    "href": "content/week3-multivariate.html#interpretations",
    "title": "Bivariate summaries",
    "section": "Interpretations",
    "text": "Interpretations\nListed from largest to smallest, here are each of the measures of spread for the 12 ages:\n\n\n\n\n\n\n\n\n\n\n\nrange\niqr\nst.dev\navg.dev\n\n\n\n\n18\n8.5\n5.527\n4.667\n\n\n\n\n\nThe interpretations differ between these statistics:\n\n[range] all of the data lies on an interval of 18 years\n[IQR] the middle half of the data lies on an interval of 8.5 years\n[average deviation] the average distance from the mean is 4.67 years\n[standard deviation] the average squared distance from the mean, rescaled to years, is 5.53 years"
  },
  {
    "objectID": "content/week3-multivariate.html#robustness",
    "href": "content/week3-multivariate.html#robustness",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\nThe IQR is more robust than any of the other measures, because outliers only affect extreme percentiles.\n\n\nConsider adding an observation of 94 to our 12 ages:\n\n# initial range\nrange(ages)\n\n[1] 16 34\n\n# append an outlier\nages_add &lt;- c(ages, 94)\n\n# relative change in IQR\n(IQR(ages_add) - IQR(ages))/IQR(ages)\n\n[1] 0.05882353\n\n# relative change in SD\n(sd(ages_add) - sd(ages))/sd(ages)\n\n[1] 2.640935\n\n\n\nThe effect of the outlier on each measure is captured by the ratio \\(\\frac{\\text{measure with outlier}}{\\text{measure without outlier}}\\), which shows:\n\nthe IQR increases by 5.88%\nthe standard deviation increases by 264%"
  },
  {
    "objectID": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "href": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "title": "Bivariate summaries",
    "section": "Limitations of univariate summaries",
    "text": "Limitations of univariate summaries\nSo far we have discussed univariate descriptive techniques — those that pertain to one variable at a time. Consider, for example, the height and weight of participants in the FAMuSS study:\n\n\n\n\n\n\n\n\n\nboth unimodal, no obvious outliers\nheights symmetric\nweights right-skewed\nbut these observations actually come in pairs\n\n\n\nThe summaries we know how to make don’t reflect how the variables might be related."
  },
  {
    "objectID": "content/week3-multivariate.html#bivariate-summaries",
    "href": "content/week3-multivariate.html#bivariate-summaries",
    "title": "Bivariate summaries",
    "section": "Bivariate summaries",
    "text": "Bivariate summaries\nBivariate (and by extension multivariate) summaries are graphical or numerical descriptions that represent two (or more) variables jointly.\n\n\nA simple example is a scatterplot:\n\n\n\n\n\n\nEach point represents a pair of values \\((h, w)\\) for one study participant.\n\nReveals a relationship: taller participants tend to be heavier\nBut no longer shows individual distributions clearly\n\nNotice, though, that the marginal means (dashed red lines) still capture the center well."
  },
  {
    "objectID": "content/week3-multivariate.html#summary-types",
    "href": "content/week3-multivariate.html#summary-types",
    "title": "Bivariate summaries",
    "section": "Summary types",
    "text": "Summary types\nBivariate summary techniques differ depending on the data types of the variables being compared. Some examples the context of the FAMuSS study:\n\n\n\n\n\n\n\nQuestion\nComparison\n\n\n\n\nDid genotype frequencies differ by race or sex among study participants?\ncategorical/categorical\n\n\nWere differential changes in arm strength observed according to genotype?\nnumeric/categorical\n\n\nDid change in arm strength appear related in any way to body size among study participants?\nnumeric/numeric\n\n\nDid study participants experience similar or different changes in arm strength depending on arm dominance?\n??"
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical",
    "href": "content/week3-multivariate.html#categoricalcategorical",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\n\nA contingency table is a bivariate tabular summary of two categorical variables; it shows the frequency of each pair of values. Usually the marginal totals are also shown.\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n106\n149\n98\n353\n\n\nMale\n67\n112\n63\n242\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\nThere are multiple ways to convert to proportions by using different denominators:\n\ngrand total\nrow total\ncolumn total\n\nEach has a different interpretation and should be chosen according to the question of interest."
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-1",
    "href": "content/week3-multivariate.html#categoricalcategorical-1",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid genotype frequencies differ by sex among study participants?\nFor this question, the row totals should be used to see the genotype composition of each sex.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.3003\n0.4221\n0.2776\n\n\nMale\n0.2769\n0.4628\n0.2603\n\n\n\n\n\n\nAs a graphic:\n\n\n\n\n\n\n\nThe proportions are quite close, suggesting minimal sex differences."
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-2",
    "href": "content/week3-multivariate.html#categoricalcategorical-2",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid sex frequencies differ by genotype among study participants?\nFor this question, the column totals should be used to see the sex composition of each genotype.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.6127\n0.5709\n0.6087\n\n\nMale\n0.3873\n0.4291\n0.3913\n\n\n\n\n\n\nAs a graphic:\n\n\n\n\n\n\n\nThe proportions are close, suggesting minimal genotype differences."
  },
  {
    "objectID": "content/week3-multivariate.html#numericnumeric",
    "href": "content/week3-multivariate.html#numericnumeric",
    "title": "Bivariate summaries",
    "section": "Numeric/numeric",
    "text": "Numeric/numeric\nDid change in arm strength appear related in any way to body size among study participants?\nComparing numeric variables is easiest accomplished by scatterplots."
  },
  {
    "objectID": "content/week3-multivariate.html#correlation",
    "href": "content/week3-multivariate.html#correlation",
    "title": "Bivariate summaries",
    "section": "Correlation",
    "text": "Correlation\nIn addition to graphical techniques, for numeric/numeric comparisons, there are also quantiative measures of relationship.\n\n\n\n\nCorrelation measures the strength of linear relationship, and is defined as: \\[r_{xy} = \\frac{1}{n - 1}\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\\]\n\n\\(r \\rightarrow 1\\): positive relationship\n\\(r \\rightarrow -1\\): negative relationship\n\\(r \\rightarrow 0\\): no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "href": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "title": "Bivariate summaries",
    "section": "Uncorrelated \\(\\neq\\) no relationship",
    "text": "Uncorrelated \\(\\neq\\) no relationship\nCorrelation only captures linear relationships. Always do a graphical check.\n\nCommon misconceptions:\n\nstronger correlation \\(\\longleftrightarrow\\) greater slope\nweaker correlation \\(\\longleftrightarrow\\) no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-correlations",
    "href": "content/week3-multivariate.html#interpreting-correlations",
    "title": "Bivariate summaries",
    "section": "Interpreting correlations",
    "text": "Interpreting correlations\nDid change in arm strength appear related in any way to body size among study participants?\nHere are the correlations corresponding to the plots we checked earlier.\n\n\n\n\n\n\n\n\n\n\n\n \nheight\nweight\nbmi\n\n\n\n\ndrm.ch\n-0.1104\n-0.1159\n-0.07267\n\n\nndrm.ch\n-0.265\n-0.2529\n-0.1436\n\n\n\n\n\nSo there aren’t any linear relationships here. A rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\\(|r| = 1\\): either a mistake or not real data"
  },
  {
    "objectID": "content/week3-multivariate.html#numericcategorical",
    "href": "content/week3-multivariate.html#numericcategorical",
    "title": "Bivariate summaries",
    "section": "Numeric/categorical",
    "text": "Numeric/categorical\nSide-by-side boxplots are usually a good option. Avoid stacked histograms.\nWere differential changes in arm strength observed according to genotype?\n\n\n\n\n\n\n\n\nLook for differences:\n\nlocation shift\nspread\ncenter\n\nWhat do you think? Any notable relationships?\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/tests/test1.html",
    "href": "content/tests/test1.html",
    "title": "Test 1",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers. Please use the word document provided (download from the class website) and write in your answers below each prompt. You should submit your work via Gradescope; please skip the page matching step and do not match pages to parts of the outline.\nThe test comprises two parts: concepts, containing short multiple-choice questions; and applications, which requires some data analysis. Revisions will be allowed for the applications part, but not for the concepts part."
  },
  {
    "objectID": "content/tests/test1.html#part-i-concepts",
    "href": "content/tests/test1.html#part-i-concepts",
    "title": "Test 1",
    "section": "Part I: concepts",
    "text": "Part I: concepts\nThis part comprises multiple-choice questions about key concepts discussed in class. You can only answer these once; revisions will not be allowed. To indicate your selection in response to each question, please type your answer explicitly, as shown below:\n\nExample question prompt.\n\n\n\ncorrect option\nincorrect option\ncorrect option\n\n\n\nAnswer: (a) and (c).\n\nIn each question you should select every option that applies. Some may have multiple correct options.\n\n[L2] Which of the following is an example of an experimental intervention?\n\nContacting prospective study participants during recruitment\nContacting study participants for follow-up surveys\nOffering study participants an incentive, such as a gift card\nAllocating different stimuli to study participants\n\nAnswer:\n[L1] Suppose you wish to conduct a study involving a survey of California residents. Which of the following sampling schemes would identify a random sample from the target population?\n\nPhysically visit CVS stores around the state of California and survey shoppers at random\nObtain a list of all residential addresses, use a computer to draw 5,000 at random, and mail your survey to each selected address\nObtain a list of all residential addresses, use a computer to draw 5,000 at random, and recruit volunteers to physically visit each selected address\nGenerate a large number of random phone numbers with California area codes and send text messages to every generated number.\n\nAnswer:\n[L1] Suppose you read a study wherein 300 people with a self-reported history of substance abuse and 200 people without any such history are surveyed to determine how many experienced adverse events in childhood, and the study finds that a much higher proportion of those with substance abuse histories report adverse childhood experiences than those without substance abuse histories. Identify the study type.\n\nRetrospective\nProspective\nExperiment\nCohort study\n\n\nThe next two questions are based on the figure below, which shows deaths among residents in Virginia in 1940 by age bracket and demographic.\n\n\n\n\n\n\n[L3] Which age group has the highest death rate?\n\n50-54\n55-59\n60-64\n65-69\n70-74\ncan’t tell\n\nAnswer:\n[L3] For which age group do urban women account for the highest proportion of deaths?\n\n50-54\n55-59\n60-64\n65-69\n70-74\ncan’t tell\n\nAnswer:\n[L4] A 95% confidence interval for the diameter in inches of black cherry trees estimated from 31 observations made on felled trees is (12.097, 14.399). Select the correct interpretation of this estimate:\n\nWith 95% probability, the diameter of black cherry trees is between 12.097 and 14.399.\nWith 95% confidence, the diameter of black cherry trees is between 12.097 and 14.399 inches.\nWith 95% confidence, the mean diameter of black cherry trees is between 12.097 and 14.399.\nWith 95% confidence, the mean diameter of black cherry trees is between 12.097 and 14.399 inches.\n\nAnswer:\n[L4] From the interval in the previous question, determine the point estimate for the population mean.\n\n2.3\n13.2\n14.5\n12.1\ncan’t tell\n\nAnswer:\n\nThe next three questions relate to the following data on in-state tuition in dollars from a sample of 50 public and private colleges. Note that 1e+04 indicates ten thousand dollars, 2e+04 indicates twenty thousand, and so on.\n\n\n\n\n\n\n[L3] Describe the shape of the distribution.\n\nLeft-skewed\nRight-skewed\nSymmetric\nNone of the above\n\nAnswer:\n[L3] The sample mean is 17781 dollars. Identify below the median.\n\n13852\n17501\n19307\n8000\nnone of the above\n\nAnswer:\n[L3] Which measure of spread should be used to describe this data?\n\nStandard deviation\nInterquartile range\nRange\nNone of the above\n\nAnswer:"
  },
  {
    "objectID": "content/tests/test1.html#part-ii-applications",
    "href": "content/tests/test1.html#part-ii-applications",
    "title": "Test 1",
    "section": "Part II: applications",
    "text": "Part II: applications\nIn this part you are given data and asked to answer questions pertaining to that data. The datasets, and commands to import them, are available to you in a project called test1 in the class posit workspace. You will need to perform calculations to answer the questions, but do not need to provide codes used to perform the calculations with your answers. When in doubt, look to the lab activities for examples.\n\nDiets and chick weights\nThe following data come from a study investigating the early growth of chicks on different diets. In the study, 47 chicks were randomly assigned one of four diets at birth and researchers measured body weight in grams daily. The data below show body weights at 18 days since birth for each chick. The question of interest is: which diet is best?\n\n# read in data\nchick &lt;- read.csv('data/chick.csv')\n\n# preview\nhead(chick)\n\n\n[L2] Is this observational or experimental data? Explain your reasoning.\n[L3] Produce a visualization that compares body weight distributions by diet. For which diet have chicks grown the most? The least? Explain the statistic(s) or features of the distribution you used to make this determination.\n[L3] Calculate point estimates and standard errors for the mean body weight at 18 days after birth on each diet.\n[L2] Assume that in the previous question you found that chicks on diet 3 grew the most, regardless of your actual answer. Can you conclude that diet 3 caused the fastest growth? Explain why or why not.\n[L4] Calculate a 95% confidence interval for the mean body weight of chicks on diet 3 at 18 days after birth. Interpret the interval in context.\n\n\n\nGSS data\nThe General Social Survey (GSS) is an effort to measure behaviors, opinions, and demographics of Americans and has been conducted annually since 1972. The data below are observations of a small collection of variables for 500 respondents from across several survey years.\n\n# import GSS data\ngss &lt;- read.csv('data/gss.csv')\n\n# preview\nhead(gss)\n\n\n[L3] Make a histogram of the weekly hours worked and describe the shape and modality. Choose an appropriate number of breaks.\n[L3] What is the 20th percentile of weekly hours worked among respondents? What is the 80th percentile?\n[L3] Compute a point estimate of the mean hours worked. Report the estimate and its standard error.\n[L4] Produce and interpret an 85% confidence interval for the mean weekly hours worked.\n[L3] Use an appropriate graphical summary to assess whether mean hours worked seems to differ by class. Explain the plot you produce and interpret any patterns observed.\n[L3] Make any additional bivariate comparison addressing a question of your choice. State the question in non-technical terms, produce a visualization that conveys the comparison, and interpret any patterns observed."
  },
  {
    "objectID": "content/week6-twosample.html#todays-agenda",
    "href": "content/week6-twosample.html#todays-agenda",
    "title": "Two sample inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz [2pm section] [4pm section]\n[lecture/lab] Two-sample inference for population means\n\nPaired data\nIndependent data\n\n[if time] Introduction to power analysis"
  },
  {
    "objectID": "content/week6-twosample.html#from-last-time",
    "href": "content/week6-twosample.html#from-last-time",
    "title": "Two sample inference",
    "section": "From last time",
    "text": "From last time\n\nAre swimmers faster in bodysuits than in regular swimsuits?\n\nBelow are the first few observations of the average velocity of competitive swimmers in a 1500m; one measurement was taken in a swimsuit, the other in a bodysuit.\n\n\n\n\n\n\n\n\n\n\nswimmer\nbody.suit.velocity\nswim.suit.velocity\n\n\n\n\n1\n1.57\n1.49\n\n\n2\n1.47\n1.37\n\n\n3\n1.42\n1.35\n\n\n\n\n\n\ntwo-sample problem because there are two sets of observations\nobservations are paired by swimmer\n\nThis is the easiest kind of two-sample problem because we can reduce it to a one-sample problem by performing inference on paired differences."
  },
  {
    "objectID": "content/week6-twosample.html#inference-for-paired-data",
    "href": "content/week6-twosample.html#inference-for-paired-data",
    "title": "Two sample inference",
    "section": "Inference for paired data",
    "text": "Inference for paired data\n\nCalculate paired differences.\n\n\n\n\n\n\n\n\n\n\n\n\nswimmer\nbody.suit.velocity\nswim.suit.velocity\nvelocity.diff\n\n\n\n\n1\n1.57\n1.49\n0.08\n\n\n2\n1.47\n1.37\n0.1\n\n\n3\n1.42\n1.35\n0.07\n\n\n\n\n\n\n\n\nPerform test as before.\n\n\n# test for paired difference\nt.test(diffs, mu = 0, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  diffs\nt = 12.318, df = 11, p-value = 4.443e-08\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 0.06620114        Inf\nsample estimates:\nmean of x \n   0.0775 \n\n\n\n\nReport the result of the test. You try."
  },
  {
    "objectID": "content/week6-twosample.html#formulating-a-two-sample-problem",
    "href": "content/week6-twosample.html#formulating-a-two-sample-problem",
    "title": "Two sample inference",
    "section": "Formulating a two-sample problem",
    "text": "Formulating a two-sample problem\n\n\nTwo-sample problems are characterized by:\n\none variable of interest\ntwo groups of observations\nobjective to compare group means\n\nInference concerns the difference in means\n\\[\\delta = \\mu_1 - \\mu_2\\]\nWe just tested:\n\\[H_0: \\mu_\\text{body} \\leq \\mu_\\text{swim}\\] \\[H_A: \\mu_\\text{body} &gt; \\mu_\\text{swim}\\]\n\nRearranging the data to emphasize two-sample problem structure:\n\n\n\n\n\n\n\n\n\n\nswimmer\nsuit\nvelocity\n\n\n\n\n1\nbody\n1.57\n\n\n1\nswim\n1.49\n\n\n2\nbody\n1.47\n\n\n2\nswim\n1.37\n\n\n3\nbody\n1.42\n\n\n\n\n\n\nvariable of interest: velocity\ngrouping: suit\npairing: swimmer"
  },
  {
    "objectID": "content/week6-twosample.html#hypotheses-for-two-sample-tests",
    "href": "content/week6-twosample.html#hypotheses-for-two-sample-tests",
    "title": "Two sample inference",
    "section": "Hypotheses for two-sample tests",
    "text": "Hypotheses for two-sample tests\nWe can articulate two-sided and directional tests for the difference in means \\(\\delta = \\mu_1 - \\mu_2\\) and the corresponding interpretation in terms of the group means.\n\n\nDifference in means\n\\[\\text{two-sided}\\begin{cases} H_0: \\delta \\qquad 0 \\\\ H_A: \\delta \\qquad 0 \\end{cases}\\]\n\\[\\text{lower-sided}\\begin{cases} H_0: \\delta \\qquad 0 \\\\ H_A: \\delta \\qquad 0 \\end{cases}\\]\n\\[\\text{upper-sided}\\begin{cases} H_0: \\delta \\qquad 0 \\\\ H_A: \\delta \\qquad 0 \\end{cases}\\]\n\nGroup interpretation\n\\[\\begin{cases} H_0: \\mu_1 \\qquad \\mu_2 \\\\ H_A: \\mu_1 \\qquad \\mu_2 \\end{cases}\\]\n\\[\\begin{cases} H_0: \\mu_1 \\qquad \\mu_2 \\\\ H_A: \\mu_1 \\qquad \\mu_2 \\end{cases}\\]\n\\[\\begin{cases} H_0: \\mu_1 \\qquad \\mu_2 \\\\ H_A: \\mu_1 \\qquad \\mu_2 \\end{cases}\\]"
  },
  {
    "objectID": "content/week6-twosample.html#your-turn-famuss",
    "href": "content/week6-twosample.html#your-turn-famuss",
    "title": "Two sample inference",
    "section": "Your turn: FAMuSS",
    "text": "Your turn: FAMuSS\n\nDoes resistance training lead to greater strength gains on the nondominant arm?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n\n\n\nArticulate and test an appropriate hypothesis for \\(\\delta = \\mu_\\text{ndrm} - \\mu_\\text{drm}\\)\n\n\n\nHypotheses: \\[H_0: \\hspace{15cm}\\] \\[H_A: \\hspace{15cm}\\]\n\n\n\nResult:"
  },
  {
    "objectID": "content/week6-twosample.html#evolution-of-darwins-finches",
    "href": "content/week6-twosample.html#evolution-of-darwins-finches",
    "title": "Two sample inference",
    "section": "Evolution of Darwin’s finches",
    "text": "Evolution of Darwin’s finches\n\n\n\nGrant, P. (1986). Ecology and Evolution of Darwin’s Finches, Princeton University Press, Princeton, N.J.\n\nPeter and Rosemary Grant caught and measured all the birds from more than 20 generations of finches on the Galapagos island of Daphne Major.\n\nsevere drought in 1977 limited food to large tough seeds\nselection pressure favoring larger and stronger beaks\nhypothesis: beak depth increased in 1978 relative to 1976\n\nHow do we test for a difference in the absence of pairing?\n\nFinch beak data:\n\n\n\n\n\n\n\n\n\nYear\nDepth\n\n\n\n\n1976\n8.9\n\n\n1976\n8.2\n\n\n1976\n6.2\n\n\n1978\n9.6\n\n\n1978\n11.1\n\n\n1978\n10.4\n\n\n\n\n\n\nVariable: Depth\nGrouping: Year\nNo pairing!"
  },
  {
    "objectID": "content/week6-twosample.html#inference-for-independent-data",
    "href": "content/week6-twosample.html#inference-for-independent-data",
    "title": "Two sample inference",
    "section": "Inference for independent data",
    "text": "Inference for independent data\n\nBeak depths exemplify independent data: the groups of observations are unrelated.\n\n\n\nInference is based on the difference in group means:\n\\[\nT = \\frac{\\bar{x} - \\bar{y}}{SE(\\bar{x} - \\bar{y})}\n\\]\n\n\\(\\bar{x}, \\bar{y}\\) are groupwise sample means\n\\(SE(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{s_x^2}{n_x} + \\frac{s_y^2}{n_y}}\\)\ndegrees of freedom for \\(t\\) model are approximated\n\n\n\\[H_0: \\mu_{1976} \\geq \\mu_{1978}\\] \\[H_A: \\mu_{1976} &lt; \\mu_{1978}\\]\n\nt.test(Depth ~ Year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  Depth by Year\nt = -4.5833, df = 172.98, p-value = 4.37e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n      -Inf -0.427321\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.469663          10.138202"
  },
  {
    "objectID": "content/week6-twosample.html#two-input-formats",
    "href": "content/week6-twosample.html#two-input-formats",
    "title": "Two sample inference",
    "section": "Two input formats",
    "text": "Two input formats\n\n\nThe formula format takes inputs:\n\nan R formula\na data frame\n\n\n# two-sample test (formula inputs)\nt.test(Depth ~ Year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  Depth by Year\nt = -4.5833, df = 172.98, p-value = 4.37e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n      -Inf -0.427321\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.469663          10.138202 \n\n\nDepth ~ Year: “depth depends on year”\n\nThe vector format takes inputs:\n\nvector of observations for one group\nvector of observations for the other group\n\n\n# two-sample test (vector inputs)\nt.test(depth76, depth78,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth76 and depth78\nt = -4.5833, df = 172.98, p-value = 4.37e-06\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf -0.427321\nsample estimates:\nmean of x mean of y \n 9.469663 10.138202"
  },
  {
    "objectID": "content/week6-twosample.html#interpreting-results",
    "href": "content/week6-twosample.html#interpreting-results",
    "title": "Two sample inference",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  Depth by Year\nt = -4.5833, df = 172.98, p-value = 4.37e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n      -Inf -0.427321\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.469663          10.138202 \n\n\nTo report the results:\n\nState conclusion\nInterpret test result in context\nReport statistics (T, df, p-value)\nProvide point estimates\n\nCareful about signs and directions!\n\n\nOur findings suggest finch beak depth on Daphne Major increased as a result of natural selection following drought. The data provide strong evidence against the null hypothesis that mean beak depth remained comparable or diminished in the generation following the drought in favor of the alternative that mean beak depth increased (T = -4.58 on 172.98 degrees of freedom, p = 0.00000437). With 95% confidence, the increase in beak depth is estimated to be at least 0.427 mm, with a point estimate of 0.669 mm (SE = 0.1459)."
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-paired-or-independent",
    "href": "content/week6-twosample.html#cloud-data-paired-or-independent",
    "title": "Two sample inference",
    "section": "Cloud data: paired or independent?",
    "text": "Cloud data: paired or independent?\n\nDoes dropping silver iodide onto clouds increase rainfall?\n\n\n\nData are rainfall measurements in a target area from 26 days when clouds were seeded and 26 days when clouds were not seeded.\n\nrainfall gives volume of rainfall in acre-feet\ntreatment indicates whether clouds were seeded\n\nHypotheses to test: \\[H_0: \\mu_\\text{seeded} \\quad \\mu_\\text{unseeded}\\] \\[H_A: \\mu_\\text{seeded} \\quad \\mu_\\text{unseeded}\\]\n\n\n\n\n\n\n\n\n\n\nrainfall\ntreatment\n\n\n\n\n274.7\nSeeded\n\n\n115.3\nSeeded\n\n\n978\nSeeded\n\n\n7.7\nSeeded\n\n\n1\nUnseeded\n\n\n830.1\nUnseeded\n\n\n244.3\nUnseeded\n\n\n95\nUnseeded"
  },
  {
    "objectID": "content/week6-twosample.html#sleep-drugs-paired-or-independent",
    "href": "content/week6-twosample.html#sleep-drugs-paired-or-independent",
    "title": "Two sample inference",
    "section": "Sleep drugs: paired or independent?",
    "text": "Sleep drugs: paired or independent?\n\nWhich (if either) of two soporific drugs is more effective?\n\n\n\nData are extra hours of sleep for 10 study participants when taking each of two drugs.\n\nextra.sleep gives hours of additional sleep relative to control\ndrug indicates which sleep drug was taken\nsubject indicates study participant id\n\nHypotheses to test: \\[H_0: \\mu_1 \\quad \\mu_2\\] \\[H_A: \\mu_1 \\quad \\mu_2\\]\n\n\n\n\n\n\n\n\n\n\n\nextra.sleep\ndrug\nsubject\n\n\n\n\n0.7\n1\n1\n\n\n1.9\n2\n1\n\n\n-1.6\n1\n2\n\n\n0.8\n2\n2\n\n\n-0.2\n1\n3\n\n\n1.1\n2\n3"
  },
  {
    "objectID": "content/week6-twosample.html#test-assumptions",
    "href": "content/week6-twosample.html#test-assumptions",
    "title": "Two sample inference",
    "section": "Test assumptions",
    "text": "Test assumptions\n\n\nInference relies on a \\(t\\) model providing a good approximation to the sampling distribution. This requires three assumptions:\n\nvariable of interest is numeric and not too discrete\nobservations are independent (besides pairing)\neither:\n\nsample sizes are not too small\nor distribution(s) are symmetric and unimodal\n\n\n\nCommon issues:\n\n\n\n\n\n\n\nIssue\nConsequence\n\n\n\n\nHighly discrete data\n\\(t\\) model not appropriate\n\n\nDependent observations\n\\(SE\\) is a biased estimate: nominal error rates and coverage are inaccurate\n\n\nSmall samples with heavy skew or extreme outliers\n\\(SE\\) too small: inflated type I error and under-coverage\n\n\n\n\nIn each of these scenarios, different inference procedures should be used."
  },
  {
    "objectID": "content/week6-twosample.html#power-calculations",
    "href": "content/week6-twosample.html#power-calculations",
    "title": "Two sample inference",
    "section": "Power calculations",
    "text": "Power calculations\n\nHow much data do you need to collect in order to detect a difference of \\(\\delta\\)?\n\n\n\nThe statistical power of a test captures how often it detects a specified alternative.\n\ndefined as \\(\\beta = (1 - \\text{type II error rate})\\)\nmeasures how often the test correctly rejects\nvalue depends on…\n\nmagnitude of difference between null value and true value of parameter\nsignificance level\nsample size\n\n\n\n\npower.t.test(power = 0.95, \n             delta = 0.5, \n             sig.level = 0.05, \n             type = 'two.sample',\n             alternative = 'two.sided')\n\n\n     Two-sample t test power calculation \n\n              n = 104.928\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\\(\\Rightarrow\\) need 105 observations in each group to detect a difference of 0.5 standard deviations at level 0.05 with type II error rate 5% or less"
  },
  {
    "objectID": "content/week6-twosample.html#the-equal-variance-t-test",
    "href": "content/week6-twosample.html#the-equal-variance-t-test",
    "title": "Two sample inference",
    "section": "The equal-variance \\(t\\)-test",
    "text": "The equal-variance \\(t\\)-test\nIf it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power (lower type II error rate) by using a different standard error:\n\\[SE_\\text{pooled}(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{\\color{red}{s_p^2}}{n_x} + \\frac{\\color{red}{s_p^2}}{n_y}}\n\\quad\\text{where}\\quad \\color{red}{s_p} = \\underbrace{\\sqrt{\\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\\text{weighted average of } s_x^2 \\;\\&\\; s_y^2}\\]\nImplement by adding var.equal = T as an argument to t.test().\n\nlarger df is used, hence more frequent rejections\navoid unless you have a small sample\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/hw/hw2.html",
    "href": "content/hw/hw2.html",
    "title": "Homework 2: Descriptive statistics",
    "section": "",
    "text": "Instructions: type up your answers and submit your work electronically via Gradescope. Questions with a learning outcome indicated in brackets will be evaluated for credit; other questions are provided for additional practice. You are expected to answer all questions. Note that an R project with datasets and prompts is provided on the class posit.cloud workspace. Please do not submit R codes; show only output or graphics relevant to answering the question.\n\n[L3] The oibiostat::frog dataset contains measurements on samples of frog egg clutches collected at various study sites in early 2013 to investigate the effect of altitude on relative investment in egg size versus clutch size (number of eggs). Visualize the frequency distributions of clutch volume, egg size, and clutch size. For each variable, describe the shape and modality of the distribution and calculate appropriate measures of spread and center.\n\n\nlibrary(oibiostat)\ndata(frog)\n?oibiostat::frog\n\n\nVu and Harrington exercise 1.8. Below are some observations from a study collecting data to analyze smoking habits of UK residents.\n\nWhat does each row of this table represent?\nHow many participants were included in the study?\nFor each variable, indicate whether it is numerical or categorical. If numerical, indicate whether it is continuous or discrete; if categorical, indicate if it is nominal or ordinal.\n\n\n\n\n[L2] Vu and Harrington exercise 1.17. The following scatterplot shows life expectancies and percentages of internet users for 208 countries.\n\nDescribe the relationship between life expectancy and percentage of internet users. Specifically: is there an apparent association, and if so, is it positive or negative and linear or nonlinear?\nState a possible confounding variable that might explain this relationship and describe how the confounder might relate to both the percentage of internet users and the life expectancy of a country.\nAre these data experimental or observational? And if you had to guess, were they obtained by a random or nonrandom sample, and why?\n\n\n\n\nVu and Harrington exercise 1.28. For each of the distributions (a), (b), and (c), describe the shape and modality and identify the matching boxplot from (1), (2), and (3).\n\n\n\nVu and Harrington exercise 1.36.\n\n\n\n[L3] The oibiostat::yrbss dataset contains measurements on a small collection of variables from 13,583 survey responses collected as part of the CDC’s Youth Risk Behavior Surveillance System (YRBSS) from 1991-2013. The objective of the survey program is to track behaviors with potential negative physical and mental health impacts among adolescents.\n\nSummarize the racial composition of survey respondents by academic grade. Produce both a contingency table and a proportional bar plot. Make sure to choose the correct (row or column) normalization for your bar plot so that it shows the racial composition by grade (not the grade composition by race). Are there apparent differences in racial composition across grades?\nMake a bar plot showing the frequency distribution of hours of sleep on school nights. Based on the summary, what is the typical amount of sleep respondents get on school nights?\nProduce a tabular or graphical summary that addresses the question: do older students sleep more on school nights than younger students?\nVisualize the frequency distribution of the number of days per week that survey participants are physically active. Describe the distribution and indicate whether the variable is discrete or continuous. Is the mean an appropriate measure of center for this data? Why or why not?\nProduce side-by-side boxplots visualizing the number of days per week that survey participants are physically active by grade. Based on the graphical summary, do there appear to be differences in physical activity by grade? Explain.\n\n\n\nlibrary(oibiostat)\ndata(yrbss)\n?oibiostat::yrbss\n\n\n[L3] Vu and Harrington exercise 1.39. Trait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. People with high trait anger have rage and fury more often, more intensely, and with long-laster episodes than people with low trait anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease; 12,986 participants were recruited for a study examining this hypothesis. Participants were followed for five years. The following table shows data for the participants identified as having normal blood pressure (normotensives).\n\nWhat percentage of participants have moderate anger scores?\nWhat percentage of participants who experienced a CHD event have moderate anger scores?\nWhat percentage of participants with high trait anger scores experienced a CHD event?\nWhat percentage of participants with low trait anger scores experienced a CHD event?\nWhat is the ratio of the percentages in (c) and (d)? (This is called a “relative risk” of CHD events.) Based on this, does it appear that the risk of a CHD event is higher in the high trait anger group?\nProduce a proportional bar plot that substantiates your answer in (e)."
  },
  {
    "objectID": "content/week3-descriptive.html#last-time",
    "href": "content/week3-descriptive.html#last-time",
    "title": "Descriptive statistics",
    "section": "Last time",
    "text": "Last time\n\n\n\nData semantics\n\n\ncategorical data: ordinal (ordered) or nominal (unordered)\nnumeric data: continuous (no ‘gaps’) or discrete (‘gaps’)\n\n\nData types and data structures in R\n\n\nbasic types: numeric, character, logical, integer\na vector is a collection of values of one type\na data frame is a type-heterogeneous list of vectors of equal length\n\n\nVectors can store observations of one variable:\n\n# 4 observations of age\nages &lt;- c(18, 22, 18, 12)\nages\n\n[1] 18 22 18 12\n\n\nData frames can store observations of many variables:\n\n# 3 observations of 2 variables\ndata.frame(subject.id = c(11, 2, 31),\n           age = c(24, 31, 17),\n           sex = c('m', 'm', 'f'))\n\n  subject.id age sex\n1         11  24   m\n2          2  31   m\n3         31  17   f\n\n\n\n\nTechniques for summarizing data depend on the data type"
  },
  {
    "objectID": "content/week3-descriptive.html#todays-agenda",
    "href": "content/week3-descriptive.html#todays-agenda",
    "title": "Descriptive statistics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nGraphical and tabular summaries for numeric and categorical data\n\nfrequency distributions\nbarplots\nhistograms\n\nQuantitative summaries for numeric data\n\npercentiles\nmeasures of center\nmeasures of spread\n\nLab exploring descriptive statistics and robustness"
  },
  {
    "objectID": "content/week3-descriptive.html#what-are-descriptive-statistics",
    "href": "content/week3-descriptive.html#what-are-descriptive-statistics",
    "title": "Descriptive statistics",
    "section": "What are descriptive statistics?",
    "text": "What are descriptive statistics?\nDescriptive statistics are quantitative summaries of the observations of one or more variables. They usually serve one of three aims:\n\nIdentify typical or “central” values\nCharacterize the variability or “spread” of values\nCharacterize relationships between variables\n\nDescriptive statistics are often accompanied by graphical summaries that aid in visualizing these same characteristics."
  },
  {
    "objectID": "content/week3-descriptive.html#todays-example-data-famuss",
    "href": "content/week3-descriptive.html#todays-example-data-famuss",
    "title": "Descriptive statistics",
    "section": "Today’s example data: FAMuSS",
    "text": "Today’s example data: FAMuSS\nObservational study of 595 individuals comparing change in arm strength before and after resistance training between genotypes for a region of interest on the ACTN3 gene.\n\nPescatello, L. S., et al. (2013). Highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or FAMuSS study. BioMed research international, 2013.\n\n\n\n\nExample data rows\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n125\n0\nFemale\n40\nCaucasian\n68\n171\nCT\n26"
  },
  {
    "objectID": "content/week3-descriptive.html#categorical-frequency-distributions",
    "href": "content/week3-descriptive.html#categorical-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Categorical frequency distributions",
    "text": "Categorical frequency distributions\nFor categorical variables, the frequency distribution is simply an observation count by category. For example:\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\ngenotype\n\n\n\n\n494\nTT\n\n\n510\nTT\n\n\n216\nCT\n\n\n19\nTT\n\n\n278\nCT\n\n\n86\nTT\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n173\n261\n161"
  },
  {
    "objectID": "content/week3-descriptive.html#numeric-frequency-distributions",
    "href": "content/week3-descriptive.html#numeric-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Numeric frequency distributions",
    "text": "Numeric frequency distributions\nFrequency distributions of numeric variables are observation counts by range.\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\nbmi\n\n\n\n\n194\n22.3\n\n\n141\n20.76\n\n\n313\n23.48\n\n\n522\n29.29\n\n\n504\n42.28\n\n\n273\n20.34\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\n\n(10,20]\n(20,30]\n(30,40]\n(40,50]\n\n\n\n\n69\n461\n58\n7\n\n\n\n\n\n\n\n\n\n\nThe operation of dividing a numeric variable into interval ranges is called binning."
  },
  {
    "objectID": "content/week3-descriptive.html#histograms",
    "href": "content/week3-descriptive.html#histograms",
    "title": "Descriptive statistics",
    "section": "Histograms",
    "text": "Histograms\nThe graphical display of a frequency distribution for a numeric variable is called a histogram. Binning has a big effect on the visual impression."
  },
  {
    "objectID": "content/week3-descriptive.html#shapes",
    "href": "content/week3-descriptive.html#shapes",
    "title": "Descriptive statistics",
    "section": "Shapes",
    "text": "Shapes\nFor numeric variables, the histogram reveals the shape of the distribution:\n\nsymmetric if it shows left-right symmetry about a central value\nskewed if it stretches farther in one direction from a central value"
  },
  {
    "objectID": "content/week3-descriptive.html#modes",
    "href": "content/week3-descriptive.html#modes",
    "title": "Descriptive statistics",
    "section": "Modes",
    "text": "Modes\nHistograms also reveal the number of modes or local peaks of frequency distributions.\n\nuniform if there are zero peaks\nunimodal if there is one peak\nbimodal if there are two peaks\nmultimodal if there are two or more peaks"
  },
  {
    "objectID": "content/week3-descriptive.html#your-turn-characterizing-distributions",
    "href": "content/week3-descriptive.html#your-turn-characterizing-distributions",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nConsider four variables from the FAMuSS study. Describe the shape and modality."
  },
  {
    "objectID": "content/week3-descriptive.html#your-turn-characterizing-distributions-1",
    "href": "content/week3-descriptive.html#your-turn-characterizing-distributions-1",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nHere are some made-up data. Describe the shape and modality."
  },
  {
    "objectID": "content/week3-descriptive.html#descriptive-measures",
    "href": "content/week3-descriptive.html#descriptive-measures",
    "title": "Descriptive statistics",
    "section": "Descriptive measures",
    "text": "Descriptive measures\nFrequency distributions are great for many purposes but they have limitations:\n\nminimal “data reduction”, especially for many bins/categories\nsensitive to choice of binning\nperception of pattern is subjective\n\nDescriptive measures, by contrast, reduce all observations of a variable down to just one number. There are two common types of measures:\n\nmeasures of center: mean, median, mode\nmeasures of spread: absolute deviation, standard deviation, interquartile range, range"
  },
  {
    "objectID": "content/week3-descriptive.html#measures-of-center",
    "href": "content/week3-descriptive.html#measures-of-center",
    "title": "Descriptive statistics",
    "section": "Measures of center",
    "text": "Measures of center\nA measure of center is a statistic that reflects the typical value of one or more variables.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23\n\n\n\nThese measures are only used with numeric variables."
  },
  {
    "objectID": "content/week3-descriptive.html#your-turn",
    "href": "content/week3-descriptive.html#your-turn",
    "title": "Descriptive statistics",
    "section": "Your turn",
    "text": "Your turn\nConsider the first 8 observations of change in nondominant arm strength from the FAMuSS study data:\n\n\n40, 25, 40, 125, 40, 75, 100 and 57.1\n\n\nCompute the mean, median, and mode."
  },
  {
    "objectID": "content/week3-descriptive.html#comparing-measures-of-center",
    "href": "content/week3-descriptive.html#comparing-measures-of-center",
    "title": "Descriptive statistics",
    "section": "Comparing measures of center",
    "text": "Comparing measures of center\nEach statistic is a little different, but often they roughly agree; for example, all are between 20 and 25, which seems to capture the typical age well enough.\n\nHow do you think the frequency distribution affects which one is “best”?"
  },
  {
    "objectID": "content/week3-descriptive.html#percentiles",
    "href": "content/week3-descriptive.html#percentiles",
    "title": "Descriptive statistics",
    "section": "Percentiles",
    "text": "Percentiles\nThe median is an example of a percentile: a value with specified proportions of data lying both above and below. For example, the 20th percentile is the value with 20% of observations below and 80% of observations above.\nRanking observations helps to find this number. Suppose we have 5 observations:\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n19\n20\n21\n25\n31\n\n\nrank\n1\n2\n3\n4\n5\n\n\n\n\n\nThe 20th percentile is 20 since it is ranked second when observations are listed in order:\n\n20% below (19, 20)\n80% above (20, 21, 25, 31)\n\n\nSoftware implementations have a variety of ways for calculating percentiles when an exact solution isn’t available due to ties (repeated values) or sample size."
  },
  {
    "objectID": "content/week3-descriptive.html#cumulative-frequency-distribution",
    "href": "content/week3-descriptive.html#cumulative-frequency-distribution",
    "title": "Descriptive statistics",
    "section": "Cumulative frequency distribution",
    "text": "Cumulative frequency distribution\nThe cumulative frequency distribution is a function showing all percentiles with an exact solution. Think of it as percentile (y) against value (x).\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of some specific values:\n\nabout 40% of the subjects are 20 or younger\nabout 80% of the subjects are 24 or younger\n\nYour turn:\n\nRoughly what percentage of subjects are 22 or younger?\nAbout what age is the 10th percentile?"
  },
  {
    "objectID": "content/week3-descriptive.html#common-percentiles",
    "href": "content/week3-descriptive.html#common-percentiles",
    "title": "Descriptive statistics",
    "section": "Common percentiles",
    "text": "Common percentiles\n\n\nThe five-number summary is a collection of five percentiles that succinctly describe the frequency distribution:\n\n\n\nStatistic name\nMeaning\n\n\n\n\nminimum\n0th percentile\n\n\nfirst quartile\n25th percentile\n\n\nmedian\n50th percentile\n\n\nthird quartile\n75th percentile\n\n\nmaximum\n100th percentile\n\n\n\n\nBoxplots provide a graphical display of the five-number summary."
  },
  {
    "objectID": "content/week3-descriptive.html#boxplots-vs.-histograms",
    "href": "content/week3-descriptive.html#boxplots-vs.-histograms",
    "title": "Descriptive statistics",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\nNotice how the two displays align, and also how they differ. The histogram shows shape in greater detail, but the boxplot is much more compact."
  },
  {
    "objectID": "content/week3-descriptive.html#boxplots-vs.-histograms-1",
    "href": "content/week3-descriptive.html#boxplots-vs.-histograms-1",
    "title": "Descriptive statistics",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\nSuppose we wanted to compare the change in dominant arm strength with the change in nondominant arm strength in the FAMuSS (sports gene) study.\nThe boxplot is a cleaner display due to its compactness."
  },
  {
    "objectID": "content/week3-descriptive.html#lab-robustness",
    "href": "content/week3-descriptive.html#lab-robustness",
    "title": "Descriptive statistics",
    "section": "Lab: robustness",
    "text": "Lab: robustness\nFor this lab we’ll continue to work with the FAMuSS data as we have throughout lecture.\nThis lab has two objectives:\n\nTeach you to compute descriptive statistics and prepare graphical summaries for a single variable in R\nLearn when and why to use certain descriptive statistics in place of others"
  },
  {
    "objectID": "content/week3-descriptive.html#up-next-multivariate-summaries",
    "href": "content/week3-descriptive.html#up-next-multivariate-summaries",
    "title": "Descriptive statistics",
    "section": "Up next: multivariate summaries",
    "text": "Up next: multivariate summaries\nToday we discussed numeric and graphical summaries for a single variable. These are univariate techniques.\n\nWill reveal basic statistical properties (shape, skew, outliers)\nWon’t reveal relationships\n\nWhat if you wish to understand relationships? The fact is, most data are multivariate because several variables are measured together.\nNext time we’ll discuss\n\nMeasures of spread/variability\nBivariate descriptive and graphical techniques\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/labs/lab3-multivariate.html",
    "href": "content/labs/lab3-multivariate.html",
    "title": "Lab 3: More descriptive statistics",
    "section": "",
    "text": "This lab covers two separate topics: measures of spread and bivariate graphical summaries. There are two goals for the activity:\n\nlearn to calculate measures of spread (IQR and standard deviation) and explore their robustness to outliers\nlearn to produce joint summaries of two variables for identifying relationships\n\ncontingency tables\nproportional barplots\nscatterplots\nside-by-side boxplots\n\n\nWe will use the FAMuSS dataset again.\n\n# openintro biostat package\nlibrary(oibiostat)\n\n# famuss data\ndata(famuss)\n\n\nRobustness and measures of spread\nLet’s explore how some of the other descriptive statistics we’ve discussed behave in response to outliers. Specifically, measures of spread: standard deviation and IQR.\n\n# extract dominant arm percent change in strength\ndrm &lt;- famuss$drm.ch\n\n# calculate standard deviation\nsd(drm)\n\n# interquartile range\nIQR(drm)\n\n# average deviation\nmean(abs(drm - mean(drm)))\n\n# range\nmax(drm) - min(drm)\n\n# range endpoints\nrange(drm)\n\nThe variable you just looked at — dominant arm percent change in strength — has a group of observations over 60%.\n\n# boxplot of percent change in dominant arm strength\nboxplot(drm, horizontal = T, range = 2)\n\nIf these are removed, the standard deviation increases by 24%, but the IQR only increases by 18%.\n\n# drop the observations over 60%\ndrm.drop &lt;- drm[drm &lt; 60]\n\n# compute the numeric summary with and without outliers\nsummary(drm)\nsummary(drm.drop)\n\n# compare standard deviations\nsd(drm)/sd(drm.drop)\n\n# compare interquartile ranges\nIQR(drm)/IQR(drm.drop)\n\nThis may not seem very notable, so let’s make up an example that’s a bit more extreme: let’s add a very large positive observation, say, 1000. Then, the IQR does not change at all, but the standard deviation more than doubles!\n\n# add a large observation\ndrm.add &lt;- c(drm, 1000)\n\n# compare IQR with and without\nIQR(drm.add)/IQR(drm)\n\n# compare SD with and without\nsd(drm.add)/sd(drm)\n\nThe differences in robustness between IQR and standard deviation, and between mean and median, are largely why both the five-number summary and the mean and standard deviation are reported. When these statistics differ dramatically, it is most likely due to the presence of outliers!\n\n\n\n\n\n\nYour turn\n\n\n\nCompute the numeric summary for a variable from a different dataset and based on this alone attempt a guess at whether there are outliers. If so, are they more likely outliers to the left or right?\n\n# load a new dataset (census)\ndata(census.2010)\n\n# number of doctors per state (thousands)\ndoctors &lt;- census.2010$doctors\n\n# compute numeric summary -- guess whether there are outliers?\n\n# make a histogram or boxplot to confirm your guess\n\n# bonus: can you figure out which state??\n\n\n\n\n\nBivariate graphics\nThis part of the lab is organized according to which types of variables are being compared as potentially related. In each sub-part, you’ll see a series of examples that illustrate how to produce a given graphic or other summary, and then you’ll have an opportunity to try it with a different pair of variables from the FAMuSS study data.\n\nCategorical/categorical\nConsider: is there differential expression of the ACTN gene region of interest between sexes?\nThis can be answered by comparing the proportions of study participants of each genotype by sex. The steps are:\n\nStart by making a contingency table\nConvert to proportions using the appropriate row/column sums\nVisualize and compare genotype composition by group\n\nThe examples below illustrate how to perform these steps. As you’re walking through them, consider which summary answers the question — do you want to compute proportions using the genotype totals or the sex totals?\n\n# retrieve the genotype and sex columns\ngenotype &lt;- famuss$actn3.r577x\nsex &lt;- famuss$sex\n\n# construct a contingency table\ntable(genotype, sex)\n\n# stacked bar plots -- automatically groups by column\ntbl &lt;- table(famuss$actn3.r577x, famuss$sex)\nbarplot(tbl, legend = T)\n\n# turn the table on its side with t() to group by row\nbarplot(t(tbl), legend = T)\n\n# row and column margins\nrowSums(tbl)\ncolSums(tbl)\n\n# proportions, grouping by row\ntbl_row &lt;- tbl/rowSums(tbl)\ntbl_row\n\n# proportional stacked bar plot, grouped by row\nbarplot(t(tbl_row), legend = T)\n\n# proportions, grouping by column (a little trickier)\ntbl_col &lt;- t(t(tbl)/colSums(tbl))\ntbl_col\n\n# proportional stacked bar plot, groupde by column\nbarplot(tbl_col, legend = T, horiz = T)\n\n\n\n\n\n\n\nYour turn\n\n\n\nIs there differential expression of the ACTN gene region by racial group?\nFollow the examples above to make a contingency table and bar plot of genotype composition for each racial group. Do you see differences?\n\n# retrieve the genotype and race columns\n\n# make a contingency table of genotype and race\n\n# are there apparent genotype differences by race? make an appropriate bar plot\n\n\n\n\n\nNumeric/numeric\nTaller people tend to be heavier. We should expect a relationship between weight and height. The example below shows a scatterplot of the two variables, and computes the correlation (measure of linear relationship).\n\n# retrieve height and weight columns\nheight &lt;- famuss$height\nweight &lt;- famuss$weight\n\n# basic scatterplot\nplot(height, weight)\n\n# correlation\ncor(weight, height)\n\nA rough rule of thumb for interpreting correlations is as follows:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case, the correlation of 0.53 indicates a moderate positive relationship.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there a relationship between nondominant and dominant percent change in arm strength? Make a scatterplot and compute the correlation.\n\n# retrieve the percent change variables\n\n# construct a scatterplot\n\n# compute the correlation\n\n\n\n\n\nCategorical/numeric\nConsider one of the main questions for the study:\n\nWere differences on the ACTN gene region associated with differential change in arm strength after resistance training?\n\nThis is a comparison between a categorical variable (genotype) and numeric variable (percent change in arm strength). Of course, we have measurements for both dominant and non-dominant arms; while there are other ways of handling this, we’ll just make comparisons separately for each arm.\nThe examples below produce boxplots for a quick comparison of the summary statistics of percent change in arm strength between genotypes. Recall that the summary statistics are summarizing the frequency distribution.\n\n# side-by-side boxplots for non-dominant arm\nboxplot(ndrm.ch ~ actn3.r577x, data = famuss)\n\n# change the orientation \nboxplot(ndrm.ch ~ actn3.r577x, data = famuss, horizontal = T)\n\n# change the whisker length (range = multiples of IQR)\nboxplot(ndrm.ch ~ actn3.r577x, data = famuss, horizontal = T, range = 2)\n\n# side-by-side boxplots for dominant arm\nboxplot(drm.ch ~ actn3.r577x, data = famuss, horizontal = T)\n\nThere are some slight observed differences for the non-dominant arm, but it’s unclear whether they are meaningful. We’ll return to that later, but for now, try the graphical technique with a different set of variables.\n\n\n\n\n\n\nYour turn\n\n\n\nInvestigate whether BMI seems to differ by racial group among the FAMuSS study participants.\n\n# make side-by-side boxplots of BMI by race"
  },
  {
    "objectID": "content/labs/lab6-hypotesting.html",
    "href": "content/labs/lab6-hypotesting.html",
    "title": "Lab 6: Hypothesis testing",
    "section": "",
    "text": "The objective of this lab is to learn how to perform \\(t\\)-tests for a population mean in R. We will cover:"
  },
  {
    "objectID": "content/labs/lab6-hypotesting.html#datasets",
    "href": "content/labs/lab6-hypotesting.html#datasets",
    "title": "Lab 6: Hypothesis testing",
    "section": "Datasets",
    "text": "Datasets\nIn this lab we’ll use two datasets:\n\nddt are the 15 measurements from lecture of DDT level (ppm) in kale\nbody.temps are 130 observations of body temperature from a sample of US adults\n\n\nlibrary(oibiostat)\n\nddt &lt;- MASS::DDT\nstr(ddt)\n\n num [1:15] 2.79 2.93 3.22 3.78 3.22 3.38 3.18 3.33 3.34 3.06 ...\n\ndata(thermometry)\nbody.temps &lt;- thermometry$body.temp\nstr(body.temps)\n\n num [1:130] 96.3 96.7 96.9 97 97.1 97.1 97.1 97.2 97.3 97.4 ..."
  },
  {
    "objectID": "content/labs/lab6-hypotesting.html#performing-a-t-test-in-r",
    "href": "content/labs/lab6-hypotesting.html#performing-a-t-test-in-r",
    "title": "Lab 6: Hypothesis testing",
    "section": "Performing a \\(t\\) test in R",
    "text": "Performing a \\(t\\) test in R\nWe’ll illustrate the use of t.test with the ddt data. The parameter of interest here is:\n\\[\n\\mu = \\text{mean DDT in kale}\n\\]\n\nUpper-sided test\nConsider first testing the hypotheses:\n\\[\n\\begin{align*}\nH_0: \\mu \\leq 3 \\qquad(\\text{mean DDT in kale} \\leq 3) \\\\\nH_A: \\mu &gt; 3 \\qquad(\\text{mean DDT in kale} &gt; 3)\n\\end{align*}\n\\]\nThe null parameter value is the cutoff point \\(\\mu_0 = 3\\). This is an upper-sided test because the alternative specifies that the parameter exceeds the null value. We implement this test by providing as arguments to t.test:\n\ndata vector ddt\nnull parameter value mu = 3\nupper sided alternative alternative = 'greater'\n\n\n# upper sided test H0: mean ddt &lt;= 3 vs HA: mean ddt &gt; 3\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328 \n\n\nTake a moment to locate each of the following outputs:\n\ntest statistic value\ndegrees of freedom for the \\(t\\) model\n\\(p\\)-value\nconfidence interval for the mean\npoint estiamte for the mean\n\nSince \\(p &lt; 0.05\\), we reject \\(H_0\\) at significance level \\(\\alpha = 0.05\\). Interpret the result:\n\nThe data provide sufficiently strong evidence to reject the null hypothesis that mean DDT in kale is at most 3 ppm in favor of the alternative hypothesis that mean DDT in kale exceeds 3ppm (p = 0.00575).\n\n\n\nLower-sided test\nNow suppose you wish to test whether the mean DDT in kale is at least 3.5ppm.\n\\[\n\\begin{align*}\nH_0: \\mu \\geq 3.5 \\qquad(\\text{mean DDT in kale} \\geq 3.5) \\\\\nH_A: \\mu &lt; 3.5 \\qquad(\\text{mean DDT in kale} &lt; 3.5)\n\\end{align*}\n\\]\nThe null parameter value is the cutoff point \\(\\mu_0 = 3.5\\). This is a lower-sided test because the alternative specifies that the parameter is smaller than the null value. We implement this test by providing as arguments to t.test:\n\nnull parameter value mu = 3.5\nupper sided alternative alternative = 'less'\n\n\n# lower-sided test, H0: mean ddt &gt;= 3.5 vs HA: mean ddt &lt; 3.5\nt.test(ddt, mu = 3.5, alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = -1.5238, df = 14, p-value = 0.07491\nalternative hypothesis: true mean is less than 3.5\n95 percent confidence interval:\n     -Inf 3.526803\nsample estimates:\nmean of x \n    3.328 \n\n\nSince \\(p &gt; 0.05\\), we fail to reject \\(H_0\\) at significance level \\(\\alpha = 0.05\\). Interpret the result:\n\nThe data do not provide sufficiently strong evidence to reject the null hypothesis that mean DDT in kale is at least 3.5 ppm in favor of the alternative hypothesis that mean DDT in kale exceeds 3.5ppm (p = 0.075).\n\n\n\nTwo-sided test\nLastly, suppose we wish to test whether mean DDT is 3 or not.\n\\[\n\\begin{align*}\nH_0: \\mu = 3 \\qquad(\\text{mean DDT in kale} = 3) \\\\\nH_A: \\mu \\neq 3 \\qquad(\\text{mean DDT in kale} \\neq 3)\n\\end{align*}\n\\]\nThe null parameter value is the point \\(\\mu_0 = 3\\). This is a two-sided test because the alternative specifies that the parameter is either greater or smaller than the null value. We implement this test by providing as arguments to t.test:\n\nnull parameter value mu = 3\nupper sided alternative alternative = 'two.sided'\n\n\n# two-sided test, H0: mean ddt == 3 vs HA: mean ddt =!= 3\nt.test(ddt, mu = 3, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.01151\nalternative hypothesis: true mean is not equal to 3\n95 percent confidence interval:\n 3.085913 3.570087\nsample estimates:\nmean of x \n    3.328 \n\n\nSince \\(p &lt; 0.05\\), we reject \\(H_0\\) at significance level \\(\\alpha = 0.05\\). Interpret the result:\n\nThe data provide sufficiently strong evidence to reject the null hypothesis that mean DDT in kale is 3 ppm in favor of the alternative hypothesis that mean DDT in kale is not 3ppm (p = 0.012).\n\nNotice that, the evidence against the null is slightly weaker with respect to the two-sided alternative than with respect to the upper-sided alternative. This makes sense, because the alternative comprises a larger range of values, some of which are not very consistent with the data."
  },
  {
    "objectID": "content/labs/lab6-hypotesting.html#answering-questions-with-t-tests",
    "href": "content/labs/lab6-hypotesting.html#answering-questions-with-t-tests",
    "title": "Lab 6: Hypothesis testing",
    "section": "Answering questions with \\(t\\) tests",
    "text": "Answering questions with \\(t\\) tests\n\nBody temperatures\nAnswer the following questions with hypothesis tests using the body.temps data. Be sure to consider how to frame the hypotheses appropriately to answer the question.\n\nIs the mean body temperature different from 98.6 °F?\nIs the mean body temperature higher than 98 °F?\nIs the mean body temperature higher than 98.2 °F?\nIs the mean body temperature lower than 98.2 °F?\nIs the mean body temperature actually 98.2 °F?\nIs the mean body temperature actually 98.3 °F?\n\n\n\n\n\n\n\nYour turn\n\n\n\nWrite commands to perform tests to answer questions 1-6 above.\n\n# 1. Is the mean body temperature different from 98.6 °F?\n\n# 2. Is the mean body temperature higher than 98 °F?\n\n# 3. Is the mean body temperature higher than 98.2 °F?\n\n# 4. Is the mean body temperature lower than 98.2 °F?\n\n# 5. Is the mean body temperature actually 98.2 °F?\n\n# 6. Is the mean body temperature actually 98.3 °F?\n\nInterpret the result of each test in context. Use significance level \\(\\alpha = 0.05\\) to make a decision.\n\n\n\n\n\n\n\n\nRemark\n\n\n\nThe hypothesis tests suggest that in fact, mean body temperature is lower than commonly thought. However, a visual inspection of the data reveals an interesting twist: the body temperature data are actually bimodal!\n\n\n\n\n\nThis doesn’t invalidate any of our inferences, but suggests that we might be asking the wrong question by focusing on the population mean if, in fact, there is no one mean body temperature. We should perhaps ask instead, what feature of the population explains the bimodal distribution?"
  },
  {
    "objectID": "content/labs/lab8-twosample.html",
    "href": "content/labs/lab8-twosample.html",
    "title": "Lab 8: Two-sample inference",
    "section": "",
    "text": "This lab focuses on two-sample inference for differences in population means using paired and independent data. The main objectives are:\nOur goal is to learn implementations, so less focus will be placed on interpretation in these instructions; however, we will discuss interpretations as we go."
  },
  {
    "objectID": "content/labs/lab8-twosample.html#datasets",
    "href": "content/labs/lab8-twosample.html#datasets",
    "title": "Lab 8: Two-sample inference",
    "section": "Datasets",
    "text": "Datasets\nThe lab uses several datasets for which we will consider two-sample comparisons:\n\nswim: compare swimming velocities in a bodysuit and in a swimsuit\nfamuss: compare changes in dominant and nondominant arm strength after resistance training\nfinch: compare finch beak depths the generation before a drought and the generation after\ndiets: compare blood pressure on a fish oil diet and a regular oil diet\nsleep: compare extra sleep gained on two different soporific drugs\n\nYou should run the lines below before beginning the lab to load all five datasets into your environment.\n\nlibrary(oibiostat)\nlibrary(Sleuth3)\ndata(swim)\ndata(famuss)\ndata(sleep)\nfinch &lt;- case0201\ndiets &lt;- ex0112\ncloud &lt;- case0301"
  },
  {
    "objectID": "content/labs/lab8-twosample.html#performing-two-sample-tests",
    "href": "content/labs/lab8-twosample.html#performing-two-sample-tests",
    "title": "Lab 8: Two-sample inference",
    "section": "Performing two sample tests",
    "text": "Performing two sample tests\nFirst we’ll cover examples of tests for paired and independent tests, and you’ll practice implementing each on a new dataset.\nThe goal of two-sample inference is to make a comparison of means between two groups based on two sets of observations (one per group). This is usually structured as follows:\n\none variable of interest\na grouping of observations into two groups\npossibly, a pairing of observations between groups\n\n\nPaired data\nRecall that pairing refers to matching the observations between the two groups of interest. This is most often done when the study units are the same in each group. In the swim data, for example, each swimmer’s velocity is measured once in a bodysuit and once in a swimsuit. If \\(\\mu\\) denotes mean velocity, we want to test:\n\\[H_0: \\mu_\\text{bodysuit} \\leq \\mu_\\text{swimsuit} \\] \\[H_A: \\mu_\\text{bodysuit} &gt; \\mu_\\text{swimsuit} \\]\nBecause of the pairing, we can compute the difference in velocity for each swimmer and use that to test the hypotheses. There are two steps in this case:\n\nCalculate pairwise differences\nPerform a one-sample test with the appropriate direction\n\nThis is shown below:\n\n# step 1: compute difference wetsuit - swimsuit\nvelocity.diff &lt;- swim$wet.suit.velocity - swim$swim.suit.velocity\n\n# step 2: perform one-sample t test\nt.test(velocity.diff, mu = 0, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  velocity.diff\nt = 12.318, df = 11, p-value = 4.443e-08\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 0.06620114        Inf\nsample estimates:\nmean of x \n   0.0775 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nIn the FAMuSS data, changes in arm strength by dominant and nondominant arms are paired by participant. Use this pairing to test the hypotheses\n\\[H_0: \\mu_\\text{drm} \\geq \\mu_\\text{ndrm}\\] \\[H_A: \\mu_\\text{drm} &lt; \\mu_\\text{ndrm}\\] where \\(\\mu\\) denotes mean change in arm strength. Be careful about the direction of the test; make sure this matches the sign of the pairwise differences appropriately.\n\n# inspect data\nhead(famuss)\n\n# compute difference ndrm.ch - drm.ch\nstrength.diff &lt;- ...\n\n# perform t test\nt.test(...)\n\n\n\n\n\nIndependent data\nFor independent data, the problem is the same but without the pairing. If the sample sizes differ between groups, that’s a dead giveaway that data are very likely not paired (unless observations are missing). Otherwise, you’ll need to reflect on whether it is possible to match the observations in each group.\nIn the finch data, the observations in each group come from different individual finches of generations two years apart, so there is no sensible way to pair the data. The goal is to determine whether beak depth increased as a result of selection pressure following a drought in 1977. So, with \\(\\mu\\) denoting mean beak depth, let’s test the hypotheses:\n\\[H_0: \\mu_\\text{1978} \\leq \\mu_\\text{1976}\\] \\[H_A: \\mu_\\text{1978} &gt; \\mu_\\text{1976}\\] The implementation is shown below. Notice that unlike in the one-sample case, we provide the data as a data frame with a grouping column and a column for the variable of interest, and specify the grouping using the formula variable ~ group. Otherwise the input format is analogous to the one-sample case.\n\n# examine data; note that grouping is shown as a variable\nhead(finch)\n\n  Year Depth\n1 1976   6.2\n2 1976   6.8\n3 1976   7.1\n4 1976   7.1\n5 1976   7.4\n6 1976   7.8\n\n# perform t test\nt.test(Depth ~ Year, data = finch, mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  Depth by Year\nt = -4.5833, df = 172.98, p-value = 4.37e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n      -Inf -0.427321\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.469663          10.138202 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry implementing the two-sample test for independent data using a new dataset. The diets data show reductions in diastolic blood pressures for 14 participants who were randomly assigned one of two diets: fish oil and regular oil. Because the participants in each group are unrelated, there is no pairing of observations.\nUse these data to test whether fish oil leads to a greater reduction in blood pressure:\n\\[H_0: \\mu_\\text{fish} \\leq \\mu_\\text{regular}\\] \\[H_A: \\mu_\\text{fish} &gt; \\mu_\\text{regular}\\] Again, careful about the direction of the alternative. You might need to try once and inspect output to make sure you performed the test indicated above.\n\n# inspect data\nhead(diets)\n\n# perform test\nt.test(..., data = ..., mu = ..., alternative = ...)"
  },
  {
    "objectID": "content/labs/lab8-twosample.html#distinguishing-paired-from-independent-data",
    "href": "content/labs/lab8-twosample.html#distinguishing-paired-from-independent-data",
    "title": "Lab 8: Two-sample inference",
    "section": "Distinguishing paired from independent data",
    "text": "Distinguishing paired from independent data\nHere you’ll practice distinguishing the paired and independent settings, as well as get further practice implementing the tests above. You’re given two datasets, and must:\n\nDetermine whether pairing is present\nConduct the appropriate test\n\nYou should discuss the pairing question with your group after inspecting the datasets but before carrying out any inference.\n\nCloud seeding data\nThe cloud seeding data are shown below, and comprise observations of rainfall in a target area on 26 days when clouds were seeded with silver iodide and on 26 different days when clouds were not seeded.\n\n# inspect data\nhead(cloud)\n\n  Rainfall Treatment\n1   1202.6  Unseeded\n2    830.1  Unseeded\n3    372.4  Unseeded\n4    345.5  Unseeded\n5    321.2  Unseeded\n6    244.3  Unseeded\n\n\nWith \\(\\mu\\) denoting mean rainfall (in acre-feet), the goal is to test:\n\\[H_0: \\mu_\\text{seeded} \\leq \\mu_\\text{unseeded}\\] \\[H_A: \\mu_\\text{seeded} &gt; \\mu_\\text{unseeded}\\]\n\n\nSleep data\nThe sleep data are shown below and comprise observations of extra hours of sleep of 10 study participants on one of two drugs. The variable extra records extra hours of sleep, group records which drug was taken, and ID records the study participant.\n\n# inspect data after ordering by participant ID\ndplyr::arrange(sleep, ID) |&gt; head()\n\n  extra group ID\n1   0.7     1  1\n2   1.9     2  1\n3  -1.6     1  2\n4   0.8     2  2\n5  -0.2     1  3\n6   1.1     2  3\n\n\nWith \\(\\mu\\) denoting mean extra hours of sleep, the goal is to test with this data whether one drug is better than the other:\n\\[H_0: \\mu_1 = \\mu_2\\] \\[H_A: \\mu_1 \\neq \\mu_2\\] The hypotheses are non-directional because there is not a prior hypothesis about which drug might be better.\n\n\n\n\n\n\nYour turn\n\n\n\n\n# test whether cloud seeding increases rainfall\n\n# test whether sleep drugs produce different effects"
  },
  {
    "objectID": "content/labs/lab5-intervals.html",
    "href": "content/labs/lab5-intervals.html",
    "title": "Lab 5: Confidence intervals",
    "section": "",
    "text": "The focus of this lab is on confidence intervals, with a particular emphasis on understanding the meaning of coverage (i.e., confidence level) and exploring this concept through simulation. There are three main learning objectives:\nWe will use the same total cholesterol variable from the NHANES data and continue to pretend that these 3,179 observations form a population for which we have complete data. You’ll need to load the dataset and extract the cholesterol variable.\n# load dataset\nload('data/nhanes.RData')\n\n# extract cholesterol variable\ncholesterol &lt;- nhanes$TotChol\nRemember, we’re pretending that cholesterol contains all population values."
  },
  {
    "objectID": "content/labs/lab5-intervals.html#exploring-interval-coverage",
    "href": "content/labs/lab5-intervals.html#exploring-interval-coverage",
    "title": "Lab 5: Confidence intervals",
    "section": "Exploring interval coverage",
    "text": "Exploring interval coverage\nFirst you’ll draw a single sample and calculate an interval estimate for the mean. The commands below will do this for you with no changes. Your goal will be to obtain an interval using these commands and compare your result with those of your groupmates. Since each of you is drawing a sample at random, your results will differ slightly.\n\n# fix sample size\nn &lt;- 35\n\n# draw a sample of size 35\nmy_samp &lt;- sample(cholesterol, size = n)\n\n# calculate mean and standard error\nxbar &lt;- mean(my_samp)\nxbar.se &lt;- sd(my_samp)/sqrt(n)\n\n# calculate the interval\nxbar.interval &lt;- xbar + c(-1, 1)*2*xbar.se\n\n# display\nxbar.interval\n\n[1] 4.667852 5.295576\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nDetermine whether your interval captured the parameter of interest. Compute the population mean and compare with the interval estimate.\n\n# does the interval contain the population mean?\n\n\n\nNow imagine your group was really big, say, 10,000 students, and you all generated one interval each. The proportion of you who captured the population mean measures the coverage of the interval. We will simulate this in class to determine the coverage of the interval you just calculated."
  },
  {
    "objectID": "content/labs/lab5-intervals.html#simulating-deviations",
    "href": "content/labs/lab5-intervals.html#simulating-deviations",
    "title": "Lab 5: Confidence intervals",
    "section": "Simulating deviations",
    "text": "Simulating deviations\nThe multiplier \\(2\\times SE\\) comes from the normal model for the sampling distribution of the point estimate \\(\\bar{x}\\). This particular multiplier is intended to achieve 95% coverage — so in theory, 95 out of every 100 intervals will capture the population mean. The normal model can be expressed as a model for the scaled deviations \\(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\\), where the scaling factor uses the population standard deviation \\(\\sigma/\\sqrt{n}\\). A nominal 95% coverage arises from the assumption that 95% of these deviations are between -2 and 2. But when we calculate an interval, we’re actually applying the normal model to \\(\\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\\), where the deviations have been scaled by the standard error \\(s_x/\\sqrt{n}\\). Are 95% of these deviations between -2 and 2?\nThe commands below allow you to simulate scaled deviations of sample means from a large number of samples, using both (a) population standard deviation and (b) standard error as scaling factors. Don’t worry about understanding the codes too much — as long as you understand the output, that is sufficient. Your task is to run these commands and compare the coverage of the normal model when SE is used in place of SD.\n\n# fix sample size \nn &lt;- 5\n\n# function to simulate scaled deviation of one sample mean\nsim_dev &lt;- function(n, data){\n  samp &lt;- sample(data, size = n)\n  xbar &lt;- mean(samp)\n  xbar.se &lt;- sd(samp)/sqrt(n)\n  xbar.sd &lt;- sd(data)/sqrt(n)\n  mu &lt;- mean(data)\n  dev &lt;- c(se = (xbar - mu)/xbar.se, \n           sd = (xbar - mu)/xbar.sd)\n  return(dev)\n}\n\n# repeat many simulations\nnsim &lt;- 1000\nsim.devs &lt;- sapply(1:nsim, function(i){sim_dev(n, data = cholesterol)}) |&gt; \n  t() |&gt; \n  as.data.frame()\n\n# how many are between -2 and 2 using SD?\ncoverage.sd &lt;- sum(abs(sim.devs$sd) &lt; 2)/nsim\ncoverage.sd\n\n[1] 0.956\n\n# using SE?\ncoverage.se &lt;- sum(abs(sim.devs$se) &lt; 2)/nsim\ncoverage.se\n\n[1] 0.862\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nRun the codes above and discuss with your group:\n\nWhat did these commands do? Affirm your understanding of this simulation by listing the steps performed.\nDo either, none, or both scaled deviations achieve the nominal coverage of 95%?\nIf you change the sample size, do you get a different answer?"
  },
  {
    "objectID": "content/labs/lab5-intervals.html#calculating-an-interval",
    "href": "content/labs/lab5-intervals.html#calculating-an-interval",
    "title": "Lab 5: Confidence intervals",
    "section": "Calculating an interval",
    "text": "Calculating an interval\nIn light of the under-coverage resulting from the normal model for small sample sizes, the \\(t\\) model provides a better interval in practice.\n\\[\\bar{x} \\pm c \\times SE(\\bar{x})\\]\nIn the expression above, \\(c\\) is a “critical value” obtained from the \\(t\\) model with \\(n - 1\\) degrees of freedom; its exact value depends on the desired coverage. The commands below show you how to calculate each piece and form an interval.\n\n# fix sample size and desired coverage\nn &lt;- 10\ncoverage &lt;- 0.95\n\n# draw one sample\nsamp &lt;- sample(cholesterol, size = n)\n\n# interval ingredients\nxbar &lt;- mean(samp)\nxbar.se &lt;- sd(samp)/sqrt(n)\nc.val &lt;- qt((1 - coverage)/2, df = n - 1, lower.tail = F)\n\n# calculate interval\nxbar + c(-1, 1)*c.val*xbar.se\n\n[1] 4.453504 5.938496\n\n\nYour answer will differ slightly from this result, since you will draw a different sample when you run these commands.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the same sample above, compute a 90% confidence interval for mean total HDL cholesterol. Discuss with your group and interpret the interval in context.\n\n# adjust coverage and calculate a new critical value\n\n# calculate the interval"
  },
  {
    "objectID": "content/labs/lab1-rbasics.html",
    "href": "content/labs/lab1-rbasics.html",
    "title": "Lab 1: R Basics",
    "section": "",
    "text": "This lab is intended to introduce you to the nuts and bolts of R: arithmetic operations, data types, vectors, and other data structures. While you won’t be expected to do much data manipulation or manual calculation for this class, it will be helpful for you to have some understanding of this material going forward, as most R objects are arranged as data structures comprising one or more data types.\nThis lab covers a lot of ground; consider it a reference you can return to later as needed.\n\nArithmetic operations\nWhile R does a lot more than function as a calculator, it can be used to perform basic arithmetic. All of the usual operations — addition, subtraction, multiplication, division, exponentiation — can be performed as shown below.\n\n# addition\n2 + 1 \n\n# subtraction\n2 - 1\n\n# multiplication\n2*3\n\n# division\n2/3\n\n# exponentiation\n3^2\n\n# parentheses for order of operations: compare\n(2 + 1)/4\n2 + 1/4\n\nBasic functions can also be evaluated. To name a few:\n\n# square root\nsqrt(9)\n\n# exponential function e^x\nexp(1)\n\n# logarithm: compare, default base is e (i.e., natural log)\nlog(2, base = exp(1))\nlog(2)\n\nThe result of each of these calculations is a value. Values are the most elementary objects in the R environment. Values can be assigned names using the assignment operator &lt;-:\n\n# store a value\nx &lt;- log(2)\n\n# print value in console\nx\n\nNames can consist of any combination of letters, numbers, and the delimiters _ and .; names cannot start with numbers (try it and see what happens).\nIf a name is used multiple times, the most recently assigned value will be stored. However, ‘most recent’ in this context means ‘most recently executed’: R will not pay attention to the order of lines in your script, but the order in which you run them. To see this, try redefining x to be a new value. if you then go back and run the previous line that displays the value, you’ll notice you get the new value, not the old one, even though the line appears before the new assignment.\n\n# assign a new value the name 'x': this overwrites the first value\nx &lt;- 4\n\n\n\n\n\n\n\nYour turn\n\n\n\nWrite one line of code each to perform the following calculations:\n\nCalculate the sum of the ages of each member of your group, in years.\nHow about in months? Multiply by 12.\nCalculate the average number of siblings among the members of your group.\n\n\n# sum of ages of group members\n\n# convert to months: multiply by 12\n\n# average number of siblings among group members\n\n\n\n\n\nData types\nValues may be of different types. The function class(...) will return the “class” of object it is given; for values, the object class is simply the data type.\nThere are four main data types in R.\n\n# numeric\nclass(12)\n\n# character\nclass('text')\n\n# logical\nclass(TRUE)\n\n# integer\nclass(12L)\n\nArithmetic only works with logical, integer, and numeric values:\n\n# valid operations\n12*12\n12L*12\n12L*12L\n12*TRUE\n12*FALSE\n12L*TRUE\n12L*FALSE\n\nIt will not work with character values. Notice the text of the error message:\n\n# invalid operation\n'12'*'12'\n\nError in \"12\" * \"12\": non-numeric argument to binary operator\n\n\nThis is a very common error message — when you see it, you’ll know that most likely, somewhere R attempted to perform arithmetic with character values, so it’s probably a data type issue.\n\n\n\n\n\n\nYour turn\n\n\n\n\nCalculate the product of TRUE and FALSE. Which data type results from this operation?\nWhat do you think will happen if you add a logical and numeric value? Discuss first, then verify.\nWhat kind of value does each of the above operations return? Discuss first, then verify.\n\n\n# product of true and false\n\n# add a logical and numeric value\n\n# check data type of the above\n\n\n\n\n\nVectors\nIn practice, data usually consists of multiple values, not just one; in R, the most basic type of collection of values is called a vector. Technically, a vector is a “concatenation” of values of the same data type. Vectors can be formed using the concatenation function c(...):\n\n# concatenate values with c(...) to form a vector\nc(1, 2, 3)\n\n# store it\nx &lt;- c(1, 2, 3)\n\n# data type\nclass(x)\n\nIf values of different types are concatenated, they are coerced to the same data type; vectors cannot contain values of mixed type. The lines below demonstrate this behavior:\n\n# integer and logical -&gt; integer\nclass(c(2L, TRUE))\n\n# integer and numeric -&gt; numeric\nclass(c(2L, 12))\n\n# numeric and logical -&gt; numeric\nclass(c(12, TRUE))\n\n# character and anything -&gt; character\nclass(c('text', TRUE))\nclass(c('text', 2L))\nclass(c('text', 12))\n\n\nVectorization\nArithmetic between vectors is carried out elementwise; arithmetic operations between vectors and values are ‘vectorized’, meaning operations are applied over each element. This makes certain calculations quite efficient. The following examples illustrate vectorized arithmetic operations in two verbose ways: first as an elementwise operation between two vectors; then as a concatenation of the results of each elementwise operation.\n\n# equivalent\n2*c(1, 2, 3) # vectorized\nc(2, 2, 2)*c(1, 2, 3) # verbose\nc(2*1, 2*2, 2*3) # verbose\n\n# equivalent\nc(1, 2, 3) + 1 # vectorized\nc(1, 2, 3) + c(1, 1, 1) # verbose\nc(1 + 1, 2 + 1, 3 + 1) # verbose\n\n# equivalent\nc(1, 2, 3)/3 # vectorized\nc(1, 2, 3)/c(3, 3, 3) # verbose\nc(1/3, 2/3, 3/3) # verbose\n\n# equivalent\nc(1, 2, 3)^2 # vectorized\nc(1, 2, 3)^c(2, 2, 2) # verbose\nc(1^2, 2^2, 3^2) # verbose\n\nIf an operation is carried out between vectors of different lengths, a warning is printed indicating as much.\n\n# can you figure out what calculation was performed?\nc(1, 2, 3)*c(4, 5)\n\nWarning in c(1, 2, 3) * c(4, 5): longer object length is not a multiple of\nshorter object length\n\n\n[1]  4 10 12\n\n\nMany functions, including those mentioned at the outset, are also vectorized (calculations are performed elementwise).\n\nsqrt(c(1, 4, 9))\nlog(c(1, 10, 100), base = 10)\nexp(c(0, 1, 2))\n\n\n\nIndexing\nElements of vectors are indexed by consecutive integers; elements can be retrieved by specifying the indices in square brackets after the object name.\n\n# define a vector\nx &lt;- c(10, 20, 30, 40)\n\n# second element\nx[2]\n\n# first and third elements\nx[c(1, 3)]\n\n# second through fourth elements\nx[2:4]\n\n\n\nMissing values\nMany datasets have missing values that occur for various reasons: equipment failure, participant dropout, survey nonresponse, and so on. These can be represented in R as well. This is useful as a means of retaining the information that there is an observation that was somehow lost. Missing values are displayed as the special character NA in R.\n\n# vector with a missing third element\nc(1, 2, NA, 4)\n\n# note: still numeric\nclass(c(1, 2, NA, 4))\n\n\n\n\n\n\n\nYour turn\n\n\n\n\nCreate a vector with the ages of each person in your group (in years). Call it ages.\nConvert to months using vectorized arithmetic.\nNow pretend one of you is absent; repeat (1) but replace their age by a missing value. call the vector ages_incomplete.\nConvert to months again using the same operation as in (2); how is the missing value handled?\n\n\n# vector of ages in years\n\n# convert to months with vectorized arithmetic\n\n# pretend one person is absent: input a missing value\n\n# repeat conversion to months: how is the NA handled?\n\n\n\n\n\nMatrices, arrays, lists, and data frames\nMatrices can be constructed from vectors by specifying row and column dimensions, and how to arrange the values – by row or by column.\n\nx &lt;- c(1, 2, 3, 4)\nmatrix(data = x, nrow = 2, ncol = 2, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nWe won’t really see arrays much, but since they are mentioned in the reading, here’s an example of how to construct one, and how it is displayed.\n\nx &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)\narray(data = x, dim = c(2, 2, 2))\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nA list is a highly general data structure; it’s just an indexed amalgamation of objects of any type.\n\n# make a list\nlist('anchor', c(22, 5), log)\n\n[[1]]\n[1] \"anchor\"\n\n[[2]]\n[1] 22  5\n\n[[3]]\nfunction (x, base = exp(1))  .Primitive(\"log\")\n\n# the list elements can be named, which makes for easy retrieval\nmy_list &lt;- list(words = 'anchor', numbers = c(22, 5), functions = log)\nmy_list$words\n\n[1] \"anchor\"\n\n# even so, indexing can still be used\nmy_list[[1]]\n\n[1] \"anchor\"\n\n\nData frames are like lists, except each element is a vector of the same length; they appear much like matrices, but behave differently in important ways.\n\n# make a data frame\ndata.frame(col1 = c(1, 2, 3, 4), \n           col2 = c(T, F, T, T), \n           col3 = c('red', 'blue', 'green', 'yellow'))\n\n  col1  col2   col3\n1    1  TRUE    red\n2    2 FALSE   blue\n3    3  TRUE  green\n4    4  TRUE yellow\n\n# unlike matrices, however, columns can be retrieved by name\nmy_df &lt;- data.frame(number = c(1, 2, 3, 4), \n                    truth = c(T, F, T, T), \n                    color = c('red', 'blue', 'green', 'yellow'))\n\n# the result is a vector containing the values of that column\nmy_df$color\n\n[1] \"red\"    \"blue\"   \"green\"  \"yellow\"\n\n\nData frames are the fundamental data structure on which a large portion of R software is built. When data files are read into R, as you saw very briefly earlier, they are read in by default as data frames. Many R functions used in performing data analyses require data to be supplied as a data frame.\n\n\n\n\n\n\nYour turn\n\n\n\nCreate a data frame in which each row corresponds to one member of your group and columns are the variables age, number of siblings, and favorite singer.\n\n# data frame with columns age, no. of siblings, and favorite singer; each row corresponds to one group member"
  },
  {
    "objectID": "content/labs/lab7-moretests.html",
    "href": "content/labs/lab7-moretests.html",
    "title": "Lab 7: More hypothesis testing",
    "section": "",
    "text": "The objective of this lab is to learn how to perform \\(t\\)-tests for a population mean in R. We will cover:"
  },
  {
    "objectID": "content/labs/lab7-moretests.html#datasets",
    "href": "content/labs/lab7-moretests.html#datasets",
    "title": "Lab 7: More hypothesis testing",
    "section": "Datasets",
    "text": "Datasets\nThroughout this lab we’ll use the datasets:\n\nsleep are 135 observations of average hours of sleep per night from NHANES respondents\n\nYou’ll need to run the commands below in order to load these datasets before proceeding with the activity.\n\nlibrary(oibiostat)\n\ndata(nhanes.samp.adult)\nsleep &lt;- nhanes.samp.adult$SleepHrsNight\nstr(sleep)\n\n int [1:135] 9 3 6 6 6 6 6 4 7 4 ...\n\n\nBy way of review, you’ll recall from last time that hypothesis tests are performed in R using t.test().\n\n# do US adults sleep either more or less than 8 hours a night? [result: yes]\nt.test(sleep, mu = 8, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -9.1966, df = 134, p-value = 6.328e-16\nalternative hypothesis: true mean is not equal to 8\n95 percent confidence interval:\n 6.658933 7.133659\nsample estimates:\nmean of x \n 6.896296 \n\n# do US adults sleep more than 8 hours a night? [result: no]\nt.test(sleep, mu = 8, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -9.1966, df = 134, p-value = 1\nalternative hypothesis: true mean is greater than 8\n95 percent confidence interval:\n 6.69752     Inf\nsample estimates:\nmean of x \n 6.896296 \n\n# do US adults sleep less than 8 hours a night? [result: yes]\nt.test(sleep, mu = 8, alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -9.1966, df = 134, p-value = 3.164e-16\nalternative hypothesis: true mean is less than 8\n95 percent confidence interval:\n     -Inf 7.095073\nsample estimates:\nmean of x \n 6.896296 \n\n\n\n\n\n\n\n\nChoosing your alternative wisely\n\n\n\nIt can be tricky to pick alternatives in directional tests. Suppose you want to test whether adults sleep less than 8 hours. Should you use an upper-sided or a lower-sided test?\nConsider the difference in interpretation between the upper-sided (\\(p \\approx 1\\)) and lower-sided (\\(p \\approx 0\\)) tests above:\n\n[upper sided test fails to reject] the data do not provide evidence that U.S. adults don’t sleep less then 8 hours a night\n[lower sided test rejects] the data provide evidence that U.S. adults don’t sleep more than 8 hours a night, and favor instead the alternative that they sleep less than 8 hours a night\n\nBoth answers are consistent with adults sleeping less than 8 hours, but they frame the question in different ways, and the results are not logically equivalent: the former is a double negative and basically inconclusive; the latter is a stronger conclusion.\nIf you want to answer the question whether adults sleep less than 8 hours, the better way to do it is with a lower-sided test, so that you potentially find evidence for the claim of interest (should you reject the upper-sided test) rather than potentially fail to find evidence against the claim of interest.\nThe best rule of thumb for choosing the null and alternative in a direction test is: the null should be the claim you hope to refute, and the alternative should be the claim you hope to support with evidence."
  },
  {
    "objectID": "content/labs/lab7-moretests.html#reporting-test-results",
    "href": "content/labs/lab7-moretests.html#reporting-test-results",
    "title": "Lab 7: More hypothesis testing",
    "section": "Reporting test results",
    "text": "Reporting test results\nHere you’ll practice reporting test results. You’ll work with a group on just one of the questions below, and your objective is to write a complete report of the test outcome:\n\nhypotheses tested\ntest conclusion, interpreted in context\ntest statistic, degrees of freedom, \\(p\\)-value\nconfidence interval and point estimate, interpreted in context\n\nThese elements should be summarized together in complete sentences; see the lecture slides for a complete example. Be sure to consider how to frame the hypotheses appropriately to answer the question (see remark above on choosing alternatives).\n\nDo US adults sleep 7.5 hours per night on average?\nDo US adults sleep less than 7.5 hours per night on average?\nDo US adults sleep more than 7.5 hours per night on average?\nDo US adults sleep more than 6.5 hours per night on average?\n\n\n\n\n\n\n\nYour turn\n\n\n\nWork with your group to answer one of the four questions above. Use significance level \\(\\alpha = 0.05\\) to make a decision.\n\n# 1. Do US adults sleep 7.5 hours per night on average?\n\n# 2. Do US adults sleep less than 7.5 hours per night on average?\n\n# 3. Do US adults sleep more than 7.5 hours per night on average?\n\n# 4. Do US adults sleep more than 6.5 hours per night on average?\n\nWrite a complete report of the test results."
  },
  {
    "objectID": "content/labs/lab7-moretests.html#changing-the-confidence-level",
    "href": "content/labs/lab7-moretests.html#changing-the-confidence-level",
    "title": "Lab 7: More hypothesis testing",
    "section": "Changing the confidence level",
    "text": "Changing the confidence level\nYou will have noticed by now that t.test produces a confidence interval. The interval calculated is designed to match the test in the following sense:\n\na lower-sided test produces a lower confidence bound \\(\\bar{x} - c\\times SE(\\bar{x})\\)\nan upper-sided test produces an upper confidence bound \\(\\bar{x} + c\\times SE(\\bar{x})\\)\na two-sided test produces the usual interval \\(\\bar{x} \\pm c\\times SE(\\bar{x})\\)\n\nIn each case, the critical value \\(c\\) is chosen to provide a specific coverage. It will not be the same in each formula above, even for the same coverage level.\nThe test rejects at level \\(\\alpha\\) just in case the interval excludes the null value \\(\\mu_0\\). To change the confidence level, add the argument conf.level = ... to the t.test function:\n\n# a 99% interval\nt.test(sleep, mu = 8, alternative = 'two.sided', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -9.1966, df = 134, p-value = 6.328e-16\nalternative hypothesis: true mean is not equal to 8\n99 percent confidence interval:\n 6.582703 7.209890\nsample estimates:\nmean of x \n 6.896296 \n\n# an 80% interval\nt.test(sleep, mu = 8, alternative = 'two.sided', conf.level = 0.8)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -9.1966, df = 134, p-value = 6.328e-16\nalternative hypothesis: true mean is not equal to 8\n80 percent confidence interval:\n 6.741733 7.050860\nsample estimates:\nmean of x \n 6.896296 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCompute an upper 87% confidence bound for the mean nightly hours of sleep for U.S. adults. Interpret the interval in context.\n\n# fill in the '...' parts to obtain an 87% upper confidence bound\nt.test(sleep, \n       mu = ...,\n       alternative = ...,\n       conf.level = ...)"
  },
  {
    "objectID": "content/labs/lab7-moretests.html#exploring-decision-errors",
    "href": "content/labs/lab7-moretests.html#exploring-decision-errors",
    "title": "Lab 7: More hypothesis testing",
    "section": "Exploring decision errors",
    "text": "Exploring decision errors\nThere are two ways to make an error in a hypothesis test.\n\nWe can explore type I error rates by testing a hypothesis known to be true; we can explore type II error rates by testing a hypothesis known to be false. To do so, let’s again pretend that the following 3,179 NHANES survey responses are a population.\n\nload('data/nhanes.RData')\n\n# \"true\" population mean\npop_mean &lt;- mean(nhanes$TotChol)\n\n\nType I errors\nA type I error is rejecting a true null hypothesis. To explore the rate of type I errors, let’s test the following:\n\\[H_0: \\mu = 5.043\\] \\[H_A: \\mu \\neq 5.043\\]\n5.043 is the true population mean, so in point of fact \\(H_0\\) is true and we should not reject; any rejections are therefore type I errors. We’ll all generate a sample from 3,179 observations of total HDL cholesterol. Each of us will obtain a different sample. Using our respective samples, we’ll each test whether the population mean is 5.043 and see how many of us produce an erroneous conclusion. This will provide a sense of the error rate.\nTo start, run these commands:\n\n# draw a sample \nsamp &lt;- sample(nhanes$TotChol, size = 20)\n\n# test a true null\nt.test(samp, mu = pop_mean, alternative = 'two.sided')\n\nRecall that test decisions depend on the rule: reject if \\(p &lt; \\alpha\\). Let’s tally type I errors for different significance levels \\(\\alpha\\):\n\n\n\nSignificance level\nError frequency\n\n\n\n\n\\(\\alpha = 0.2\\)\n\n\n\n\\(\\alpha = 0.1\\)\n\n\n\n\\(\\alpha = 0.05\\)\n\n\n\n\\(\\alpha = 0.02\\)\n\n\n\n\nWe should see that the type I error rate is approximately equal to the significance level. The significance level is, in fact, a cap on type I error.\n\n\nType II errors\nNow let’s test whether the population mean is some \\(\\mu_0 \\neq 5.043\\). In this case, \\(H_0\\) will be false, and a correct decision is to reject \\(H_0\\). Any failures to reject will be considered type II errors.\nTry running the following.\n\n# draw a sample \nsamp &lt;- sample(nhanes$TotChol, size = 20)\n\n# test a false null\nt.test(samp, \n       mu = 4.2, # change this for exercise\n       alternative = 'two.sided')\n\nThe error rate will depend on how far the null hypothesis is from the truth. Presumably, if we test that \\(\\mu = 5.044\\), we’ll have a high type II error rate because the null value is so close to the truth; this hypothesis would be very hard to refute. If, on the other hand, we test \\(\\mu = 0\\), we whould have a low type II error rate because the null value is far from the truth. Let’s use significance level \\(\\alpha = 0.05\\) throughout and tally errors for several different null values:\n\n\n\nNull value \\(\\mu_0\\)\nError frequency\n\n\n\n\n4.2\n\n\n\n4.6\n\n\n\n4.9\n\n\n\n5.1\n\n\n\n5.4\n\n\n\n5.7\n\n\n\n\nNotice that the type II error is quite high for null values near the true mean; this is because the test prioritizes avoiding type I errors, and as a result has little power to detect such alternatives."
  },
  {
    "objectID": "content/labs/lab4-sampling.html",
    "href": "content/labs/lab4-sampling.html",
    "title": "Lab 4: Sampling variability",
    "section": "",
    "text": "In this lab you’ll explore sampling variation. There are two learning goals:\nWe’ll use 3,179 responses to the 2009-2010 NHANES and consider specifically the total cholesterol variable. We will treat these observations as a population, and simulate the effect of sampling on summary statistics by generating small to moderate subcollections of observations drawn at random without replacement.\nThe dataset has been stored as a separate file nhanes.RData. Load the dataset, extract the variable of interest, and preview the first handful of observations using the commands below.\n# load nhanes dataset\nload('data/nhanes.RData')\n\n# extract total cholesterol column\ncholesterol &lt;- nhanes$TotChol\n\n# view summary\nstr(cholesterol)\n\n num [1:3179] 3.49 3.49 3.49 6.7 5.82 5.82 5.82 4.99 4.24 6.41 ...\nSince we are pretending that these 3,179 respondents form a population, we can examine the population distribution of the cholesterol variable and also calculate population statistics.\n# numeric summary\nsummary(cholesterol)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.330   4.240   4.970   5.043   5.690  13.650 \n\n# compute population mean and sd\npop_mean &lt;- mean(cholesterol)\npop_sd &lt;- sd(cholesterol)\n\n# construct histogram\nhist(cholesterol, breaks = 30)\nabline(v = pop_mean, col = 4, lty = 2)"
  },
  {
    "objectID": "content/labs/lab4-sampling.html#exploring-sampling-variability",
    "href": "content/labs/lab4-sampling.html#exploring-sampling-variability",
    "title": "Lab 4: Sampling variability",
    "section": "Exploring sampling variability",
    "text": "Exploring sampling variability\nThe commands below will draw a sample, compute the mean and standard deviation, and compare them with the corresponding population values.\n\ndoes the histogram look similar to the population distribution?\ndo the sample statistics closely align with the population values?\n\n\n# draw a sample -- try running this line a few times\nsample(cholesterol, size = 50, replace = F)\n\n# now store a sample\nsamp &lt;- sample(cholesterol, size = 50)\n\n# calculate mean and sd\nsamp_mean &lt;- mean(samp)\nsamp_sd &lt;- sd(samp)\nsamp_mean\nsamp_sd\n\n# estimation error\nsamp_mean - pop_mean\nsamp_sd - pop_sd\n\n# make a histogram\nhist(samp, breaks = 10)\n\n# add lines at sample mean and population mean\nabline(v = samp_mean, col = 2) # red line\nabline(v = pop_mean, col = 4, lty = 2) # blue line\n\nBecause you are drawing a random sample, results will differ each time you run the above commands. Likewise, results will differ from your groupmates.\n\n\n\n\n\n\nYour turn\n\n\n\nRun the lines above and compare results with your group. Manually enter your means and standard deviations in a vector. Repeat and add entries until you have at least 6 sample means and 6 standard deviations. Discuss:\n\nDoes it appear that the sample statistics tend to be close to the population values?\nDo the sample statistics vary much across samples? How might you measure this using your simulated means and standard deviations?\n\n\n# make vectors of 6 or more means and standard deviations\nsamp_means &lt;- ...\nsamp_sds &lt;- ...\n\n# calculate errors\nsamp_means - pop_mean\nsamp_sds - pop_sd\n\n# how would you measure variability?"
  },
  {
    "objectID": "content/labs/lab4-sampling.html#effect-of-sample-size",
    "href": "content/labs/lab4-sampling.html#effect-of-sample-size",
    "title": "Lab 4: Sampling variability",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nNow let’s explore how sample size affects sampling variation of summary statistics.\nFirst let’s examine how the frequency distribution of a single sample changes when we increase the sample size. Notice that the histogram of the larger sample more closely matches the population distribution.\n\n# population distribution\nhist(cholesterol, breaks = 30)\n\n\n\n# small sample size (try running a few times)\nhist(sample(cholesterol, size = 50), breaks = 15)\n\n\n\n# large sample size (try running a few times)\nhist(sample(cholesterol, size = 1000), breaks = 20)\n\n\n\n\nWe should therefore expect that summary statistics more closely approximate population values for larger samples. Our measure of sampling variability reflects this expectation. Recall that the theoretical standard deviation of the sample mean is: \\[ SD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\n\\quad\\left(\\frac{\\text{population SD}}{\\sqrt{\\text{sample size}}}\\right)\\]\nThis will diminish as \\(n\\) increases, indicating less sampling variation for larger samples. Let’s explore that empirically.\nThe commands below simulate nsim samples of size n and calculate the mean of each sample. Don’t worry about understanding the code that creates samp_means; your job is to use the results to measure sampling variability. First run this without any changes.\n\n# number of samples to simulate\nnsim &lt;- 1000\n\n# this generates nsim sample means from samples of size 10\nsamp_means_10 &lt;- sapply(1:nsim,\n                     function(i){\n                       mean(sample(cholesterol, size = 10))\n                     })\n\n# repeat, but for samples of size 100\nsamp_means_100 &lt;- sapply(1:nsim,\n                     function(i){\n                       mean(sample(cholesterol, size = 100))\n                     })\n\n\n# inspect\nstr(samp_means_10)\n\n num [1:1000] 5.32 4.52 5.42 4.55 4.47 ...\n\nstr(samp_means_100)\n\n num [1:1000] 5.02 4.96 5.16 4.92 5.12 ...\n\n\nEffectively, you’ve just simulated a sampling distribution for the mean cholesterol of a sample of size 10 by generating means from lots of random samples. Notice already the additional variability for the smaller sample size — the means seem to deviate more often by a larger amount from the population value.\n\n\n\n\n\n\nYour turn\n\n\n\nNow try using your simulated means to measure the sampling variability of the mean; compare this with the theoretical standard deviation.\nDo this for both the \\(n = 10\\) and \\(n = 100\\) cases. What changes?\n\n# calculate standard deviation of simulated means\nsim_sd_10 &lt;- ...\n\n# calculate theoretical standard deviation\ntheory_sd_10 &lt;- ...\n\n# calculate average error\navg_error_10 &lt;- ...\n\n# calculate standard deviation of simulated means\nsim_sd_100 &lt;- ...\n\n# calculate theoretical standard deviation\ntheory_sd_100 &lt;- ...\n\n# calculate average error\navg_error_100 &lt;- ...\n\n\n\nTake note of the fact that in practice, it would not be possible to simulate sampling distributions in this way, because you’d lack data for a complete population. In practice, you’ll only have one sample, and will need to use this to estimate the sampling variability based on theory.\nThe simulations above don’t provide an actionable method for estimating sampling variability, but are rather an exercise to aid in understanding exactly what theoretical estimates of sampling variability are designed to measure."
  },
  {
    "objectID": "content/labs/lab2-descriptive.html",
    "href": "content/labs/lab2-descriptive.html",
    "title": "Lab 2: Descriptive statistics",
    "section": "",
    "text": "By now you have seen basic descriptive and graphical summaries. But what determines when some descriptive statistics are “better” to use than others?\nThis lab has two objectives:\n\nTeach you to compute descriptive statistics and prepare graphical summaries for a single variable in R\nLearn when and why to use certain descriptive statistics in place of others\n\nWe will focus on how outliers, or observations far from center, affect different descriptive measures. In statistics, the term for this is robustness: a robust statistic is less affected by the presence of outliers.\n\nAccessing data\nFor this lab we’ll use the FAMuSS dataset, as in lecture. This dataset is stored in the oibiostat package, which is an R package companion to the Vu and Harrington textbook.\n\n# load openintro biostat package\nlibrary(oibiostat)\n\n# load famuss data\ndata(famuss)\n\n# open data documentation\n?famuss\n\nThe command above makes the dataset available in your environment as a data frame named famuss in which the columns represent variables and the rows represent observations. You can see the first few rows using head():\n\nhead(famuss)\n\n  ndrm.ch drm.ch    sex age      race height weight actn3.r577x    bmi\n1      40     40 Female  27 Caucasian   65.0    199          CC 33.112\n2      25      0   Male  36 Caucasian   71.7    189          CT 25.845\n3      40      0 Female  24 Caucasian   65.0    134          CT 22.296\n4     125      0 Female  40 Caucasian   68.0    171          CT 25.998\n5      40     20 Female  32 Caucasian   61.0    118          CC 22.293\n6      75      0 Female  24  Hispanic   62.2    120          CT 21.805\n\n\nYou can extract a vector of the observations for any particular variable from the dataframe as follows: famuss$[variable name]. We will be performing calculations one column at a time, so you’ll need to be able to extract and store a column as a new R object.\n\n# extract the age variable\nfamuss$age\n\n# extract the bmi variable\nfamuss$bmi\n\n# store the age column as a vector\nage &lt;- famuss$age\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n# try it yourself: pick a variable to extract and store\n\n\n\n\n\nGraphical and tabular summaries\nOur most basic summaries are frequency distributions. For categorical variables, these are simply observation counts by category; for numeric variables, values must be binned and then tabulated.\nFor categorical variables (i.e., character vectors or factors):\n\ntable() will tabulate the number of occurrences of unique values in a vector\nfor a categorical variable, plot() will create a barplot of the frequency distribution\n\nFor numeric variables:\n\nhist() will create a histogram\n\n\n## categorical summaries\nrace &lt;- famuss$race\n\n# frequency distribution\ntable(race)\n\n# barplot of frequency distribution\nplot(race)\n## quantitative summaries\nage &lt;- famuss$age\n\n# histogram; 'breaks = ...' controls the binning\nhist(age, breaks = 20)\n\nA quick word about functions. The inputs to functions are called arguments. The histogram function hist() requires a data argument, in this case age, to make the plot. However, the function also has several optional arguments that control things like labels, binning, axis limits, and so on. The breaks = 20 part of the last command is an example of an optional argument. Most functions in R have optional arguments that control their behavior.\n\n\n\n\n\n\nYour turn\n\n\n\nTry making a table, barplot, and histogram on your own with some of the other variables.\nWhen generating the histogram, adjust the number of bins using the breaks = ... argument. Experiment to see how the shape of the distribution appears at various binning resolutions; then pick a number of breaks that you feel reflects the data best.\n\n# make a table of the frequency distribution of genotypes\n\n# make a barplot of the frequency distribution of genotypes\n\n# make a histogram of dominant arm change; play with the binning!\n\n\n\n\n\nNumerical summaries\nIn class we discussed several descriptive statistics for numeric variables.\n\nmeasures of center: mean, median\npercentiles: quartiles, min, max\n\nThese statistics are so commonly used that they have their own functions in R.\n\n# the median gets its own function\nmedian(age)\n\n# ditto mean\nmean(age)\n\n# ... and minimum and maximum\nmin(age)\nmax(age)\n\n# 30th percentile of age (\"quantile\" is another term for percentile)\nquantile(age, probs = 0.3)\n\n# 30th *and* 60th percentile of age\nquantile(age, probs = c(0.3, 0.6))\n\nThe five-number summary is the collection of the percentiles that give the minimum, quartiles, and maximum.\n\n\n\n\n\n\nYour turn\n\n\n\nTry computing the five-number summary using a variable of your choice. Be sure you pick a numeric and not a categorical variable.\nPay attention to how the probs = ... argument to the quantile() function can be used to calculate multiple percentiles at once. Use this feature to calculate all five numbers in the five-number summary with one command.\n\n# choose a *quantiative* variable from the dataset\n\n# compute the five-number summary using quantile()\n\n# compare the mean and the median. are they close?\n\n\n\nAlternatively, the summary() command will do the work for you.\n\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   17.0    20.0    22.0    24.4    27.0    40.0 \n\n\n\n\nExploring robustness\nStatistics based on percentiles are in general insensitive to outliers, unless there’s a large group of outlying observations. In this sense they are robust statistics.\nAn easy way to see this is to consider the median (middle value or 50th percentile). The maximum observation could be arbitrarily large without changing the middle value, so the median will be the same whether the largest value is 10 or 10,000. The mean, by contrast, does not share this property.\n\n# make up some observations between 1 and 100\nx &lt;- sample(1:100, size = 20)\nx\n\n# median of made up observations, plus 101\nmedian(c(x, 101))\n\n# median of made up observations, plus 1M\nmedian(c(x, 1000000))\n\n# same comparison, but with mean\nmean(c(x, 101))\nmean(c(x, 1000000))\n\nSo in the presence of outliers, the median will capture the center of the distribution of values more accurately. In fact, even if the distribution is simply skewed, the mean will shift away from center.\n\n# distribution of ages is right-skewed\nhist(age)\n\n# plot the mean and median on top of the histogram\nhist(age)\nabline(v = mean(age), col = 2)\nabline(v = median(age), col = 4)\n\n\n\n\nThis difference is useful — the comparison of median and mean can indicate the direction and amount of skewness present.\n\n# mean &gt; median ---&gt; right-skewed\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   17.0    20.0    22.0    24.4    27.0    40.0 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate the numeric summary for percent change in dominant arm strength and see if you can determine the direction and magnitude of skewness based on this summary only.\nThe amount of skewness is a bit subjective — so just discuss and make a determination about whether the difference between median and mean seems sizeable.\n\n# check the numeric summary for percent change in dominant arm strength\n# can you tell the direction of skewness?? does it seem very skewed??"
  },
  {
    "objectID": "content/week4-sampling.html#todays-agenda",
    "href": "content/week4-sampling.html#todays-agenda",
    "title": "Sampling variability",
    "section": "Today’s agenda",
    "text": "Today’s agenda\nToday we’ll focus on understanding sampling variability. This is a foundation for the development of inferential statistics.\n\nReading quiz [2pm section] [4pm section]\n[lecture/lab] Effect of sampling variability on summary statistics\n[lecture/lab] Effect of sample size on sampling variability of summary statistics\nThe normal model"
  },
  {
    "objectID": "content/week4-sampling.html#inferential-statistics",
    "href": "content/week4-sampling.html#inferential-statistics",
    "title": "Sampling variability",
    "section": "Inferential statistics",
    "text": "Inferential statistics\nConsider this descriptive finding from the FAMuSS study:\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nInferential statistics will allow us to address questions like:\n\nShould we expect these differences to persist among the general population?\n\nTo do so we will need to articulate how samples relate to populations."
  },
  {
    "objectID": "content/week4-sampling.html#sampling-variability",
    "href": "content/week4-sampling.html#sampling-variability",
    "title": "Sampling variability",
    "section": "Sampling variability",
    "text": "Sampling variability\nDifferent samples inevitably produce different outcomes. This is sampling variation:\n\nIf we re-run the study with new participants we’ll get different results\n\nThe basis for statistical inference is distinguishing sampling variation from systematic variation.\nWhat we really want to know in the FAMuSS study:\n\nIs the CC/TT/CT difference systematic in the population or an artefact of the sample?\n\nIn other words…\n\nwas the result due to chance?\nor was it genuine?"
  },
  {
    "objectID": "content/week4-sampling.html#random-sampling",
    "href": "content/week4-sampling.html#random-sampling",
    "title": "Sampling variability",
    "section": "Random sampling",
    "text": "Random sampling\nIf we assume study units are sampled at random from a broader population, we can quantify how much summary statistics are expected to change from sample to sample.\n\n\n\n\nA study population is a collection of all study units of interest.\nA sample is a subcollection from a population:\n\nrandom if study units have a known chance of inclusion in the sample\nnonrandom or convenience otherwise\n\n\n\nIn STAT218 we’ll limit attention to simple random samples: each study unit in the population has an equal chance of inclusion in the sample."
  },
  {
    "objectID": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "href": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "title": "Sampling variability",
    "section": "A pretend population: NHANES data",
    "text": "A pretend population: NHANES data\nThe National Health and Nutrition Esamination Survey (NHANES) is an annual CDC program to collect health and nutrition data on the non-institutionalized civilian resident population of the United States. Here are a few variables:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubj.id\nGender\nAge\nPoverty\nPulse\nBPSys1\nBPDia1\nTotChol\nSleepHrsNight\n\n\n\n\n1\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n2\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n3\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n5\nfemale\n49\n1.91\n86\n118\n82\n6.7\n8\n\n\n\n\n\nI’ve selected 3,179 responses from the 2009-2010 survey; let’s pretend the corresponding individuals form a population of interest."
  },
  {
    "objectID": "content/week4-sampling.html#population-distribution-of-a-variable",
    "href": "content/week4-sampling.html#population-distribution-of-a-variable",
    "title": "Sampling variability",
    "section": "Population distribution of a variable",
    "text": "Population distribution of a variable\nConsider the TotChol variable: total HDL cholesterol in mmol/L. It has a certain frequency distribution among the population that we’ll call its population distribution.\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\nIf we draw a random sample of 50 individuals…\n\nhow closely will the sample align with the population distribution?\nhow much will alignment change if we select a new sample?"
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability",
    "href": "content/week4-sampling.html#simulating-sampling-variability",
    "title": "Sampling variability",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\nOpen the class activity lab4-sampling. The first part of this lab will load the NHANES data and provide you with a command for extracting a sample.\nYour task:\n\nHave each person in your group extract a sample.\nCalculate the mean and standard deviation.\nMake a histogram.\nCompare your results to the population.\nCompare your results to each other.\n\nAfter you’ve had a chance to try in groups, we’ll compare across the class."
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability-1",
    "href": "content/week4-sampling.html#simulating-sampling-variability-1",
    "title": "Sampling variability",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\n\n\n\n\n\n\n\n\nYou should have observed something a bit like this.\nThese are 16 random samples with the sample mean indicated by the blue dashed line and the population mean indicated by the red solid line.\n\nfrequency distributions differ a lot\nsample means vary a little\nmost means are close to 5\nmost standard deviations are near 1"
  },
  {
    "objectID": "content/week4-sampling.html#point-estimation",
    "href": "content/week4-sampling.html#point-estimation",
    "title": "Sampling variability",
    "section": "Point estimation",
    "text": "Point estimation\nIt should seem plausible that the sample mean and standard deviation provide good estimates of the corresponding population quantities.\nWe call them point estimates of population parameters.\n\n\n\nParameter name\nParameter notation\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)\n\n\n\nNow we can more formally describe statistical inference:\n\na population parameter is any numeric characteristic of a population distribution\nan inference is a conclusion about the value of a population parameter based on point estimates and their sampling variability\n\nWe will focus initially on inferences about the mean \\(\\mu\\) based on the point estimate \\(\\bar{x}\\)."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability",
    "href": "content/week4-sampling.html#measuring-sampling-variability",
    "title": "Sampling variability",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nIf we had means calculated from a large number of samples, we could make a frequency distribution for the values of the sample mean. This is called a sampling distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample\n1\n2\n3\n4\n5\n\n\nmean\n4.957\n5.039\n5.24\n5.24\n4.864\n\n\n\n\n\nCould measure the sampling variability using any measure of spread.\n\nstandard deviation: 0.1513356\n\nOn average, the sample mean varies about the population mean by 0.15 mmol/L across simple random samples."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability-1",
    "href": "content/week4-sampling.html#measuring-sampling-variability-1",
    "title": "Sampling variability",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nTheory indicates the standard deviation of the sample mean under random sampling is: \\[\nSD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{population SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor TotChol, the theoretical standard deviation is \\(SD(\\bar{x}) = \\frac{1.0747}{\\sqrt{50}} =\\) 0.1519822.\nWe can estimate this quantity by replacing \\(\\sigma\\) with the point estimate \\(s_x\\), resulting in a standard error (estimated standard deviation): \\[SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\\]"
  },
  {
    "objectID": "content/week4-sampling.html#example-with-one-sample",
    "href": "content/week4-sampling.html#example-with-one-sample",
    "title": "Sampling variability",
    "section": "Example with one sample",
    "text": "Example with one sample\nThe simulations we’ve done so far have been a means of understanding just what a standard error is meant to capture; these are not a practicable method for measuring sampling variation.\nIn practice we’d simply compute a point estimate and standard error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.031\n0.1396\n\n\n\n\n\n\nThe estimated mean total HDL cholesterol among the population is 5.031 mmol/L.\nThe point estimate is expected to deviate by 0.1396 mmol/L on average from the population mean."
  },
  {
    "objectID": "content/week4-sampling.html#effect-of-sample-size",
    "href": "content/week4-sampling.html#effect-of-sample-size",
    "title": "Sampling variability",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nThe formula for the theoretical standard deviation of \\(\\bar{x}\\) suggests that sampling variability diminishes with sample size. For example:\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\n10\n50\n100\n1000\n10000\n\n\nSD\n0.3398\n0.152\n0.1075\n0.03398\n0.01075\n\n\n\n\n\nThe second section of lab4-sampling explores this. With your group:\n\nStart with a sample size of 10.\nHave each person draw a sample and compute the mean (or draw a series of samples and compute the mean each time).\nCompare and observe how big the differences between your means are.\nRepeat with a sample size of 1000.\n\nYou should see much less sampling variability after increasing \\(n\\)."
  },
  {
    "objectID": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "href": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "title": "Sampling variability",
    "section": "Visualizing effect of sample size",
    "text": "Visualizing effect of sample size\nGenerating a large number (10K) of samples allows us to approximate the sampling distribution for a variety of sample sizes.\n\n\nspread diminishes with sample size\nless variability among estimates from larger samples\n\nSo estimates are more accurate with more data, assuming data are from a random sample."
  },
  {
    "objectID": "content/week4-sampling.html#normal-model",
    "href": "content/week4-sampling.html#normal-model",
    "title": "Sampling variability",
    "section": "Normal model",
    "text": "Normal model\nNotice that each simulated sampling distribution has produced a unimodal, symmetric, bell-shaped histogram.\n\n\nThe normal model is a theoretical frequency distribution characterized by two parameters:\n\na mean (center)\na standard deviation (spread)\n\nTheory dictates that the sampling distribution of the sample mean is well-approximated by a normal model under simple random sampling.\n\n\n\n\n\n\n\n\nBased on discussion thus far, what do you think the model parameters might be?\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/hw/hw1.html",
    "href": "content/hw/hw1.html",
    "title": "Homework 1: Study designs",
    "section": "",
    "text": "Instructions: type up your answers and submit your work electronically. Questions with a learning outcome indicated in brackets will be evaluated for credit; other questions are provided for additional practice. You are expected to answer all questions."
  },
  {
    "objectID": "content/hw/hw1.html#education-and-injury-prevention",
    "href": "content/hw/hw1.html#education-and-injury-prevention",
    "title": "Homework 1: Study designs",
    "section": "Education and injury prevention",
    "text": "Education and injury prevention\nSuppose you wish to study the efficacy of physical activity and exercise education as a means of sports injury prevention. Your research question is, “does knowledge about injury prevention during physical activity and exercise reduce the risk of injury?”. Imagine that ASI offers a short injury prevention training.\nYour initial study proposal is as follows:\n\n150 Cal Poly students will be selected for the study: 75 from among those who completed the injury prevention training voluntarily, and 75 from among those who did not. Participants will be followed for a year to determine how many in each group experience an injury related to physical activity at any point during the study period. At the end of the study, injury rates will be compared between those that participated in the injury prevention training and those that did not.\n\n\n[L2] Is this an observational study or an experiment? Explain your answer.\n[L1] How would you go about selecting study participants at random? Propose a specific means of identifying and contacting students to participate in the study.\n[L2] Imagine you found that those who completed the training are less likely to experience injury. Are these results potentially subject to confounding? If you answer yes, give a hypothetical example of a confounding factor; if you answer no, explain why confounding is not a concern.\n[L1] If you answered that it is an observational study, propose an experiment that would address the same question. If you answered that it is an experiment, propose an observational study that would address the same question.\n[L2] Suppose the alternate study you proposed in (4) indicated that those who completed the training are less likely to experience injury. Are these results potentially subject to confounding? If so, give a hypothetical example of a confounding factor; if not, explain why confounding is not a concern.\nIs the original proposal a retrospective or prospective study? Explain your answer.\nIf you answered that it is retrospective, determine an alternate study to investigate the same research question that is instead prospective; if you answered that it is prospective, determine an alternate study that is retrospective. Write a short proposal similar to the above for your alternate study."
  },
  {
    "objectID": "content/hw/hw1.html#peanut-allergies",
    "href": "content/hw/hw1.html#peanut-allergies",
    "title": "Homework 1: Study designs",
    "section": "Peanut allergies",
    "text": "Peanut allergies\nConsider the Learning Early About Peanut allergy (LEAP) study discussed in class and in your reading:\n\nFor the LEAP study, 640 infants in the United Kingdom were enrolled with risk factors for peanut allergies (eczema or egg allergy); 530 passed a skin test at the start of the study showing no peanut allergy. Each infant was randomly assigned to peanut consumption (6g peanut protein per day) or peanut avoidance (no peanut consumption) groups. At 5 years of age, an allergy test was administered to each study participant; the rates of peanut allergy were compared between the two groups.\n\n\n[L1] Propose a retrospective observational study to investigate the research question, “is early peanut exposure associated with a lower risk of developing peanut allergies?” Include a specific description of how you might enroll participants."
  },
  {
    "objectID": "content/hw/hw2-soln.html",
    "href": "content/hw/hw2-soln.html",
    "title": "Homework 2: Descriptive statistics",
    "section": "",
    "text": "Instructions: type up your answers and submit your work electronically via Gradescope. Questions with a learning outcome indicated in brackets will be evaluated for credit; other questions are provided for additional practice. You are expected to answer all questions. Note that an R project with datasets and prompts is provided on the class posit.cloud workspace. Please do not submit R codes; show only output or graphics relevant to answering the question.\n\n[L3] The oibiostat::frog dataset contains measurements on samples of frog egg clutches collected at various study sites in early 2013 to investigate the effect of altitude on relative investment in egg size versus clutch size (number of eggs). Visualize the frequency distributions of clutch volume, egg size, and clutch size. For each variable, describe the shape and modality of the distribution and calculate appropriate measures of spread and center.\n\n\n\n\n\n\nClutch volume is right-skewed and unimodal with a few potential outliers over 2000. Students should choose median to measure center in light of skew and IQR in light of outliers.\n\n\n\n\n\n\n\n\n\nmedian\niqr\n\n\n\n\n831.8\n486.9\n\n\n\n\n\nClutch size is unimodal and symmetric, though students might perceive a slight right skew, which is an acceptable answer. Any measures of center and spread are acceptable.\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\niqr\nsd\n\n\n\n\n721.3\n707.9\n301.6\n237\n\n\n\n\n\nEgg size is bimodal and neither symmetric nor skewed. The only acceptable measure of center is mode (for each mode), which can be visually located at 1.9 and 2.3 (or thereabouts, depending on the binning resolution used). As there are no obvious outliers, any measure of spread is acceptable.\n\n\n\n\n\n\n\n\n\niqr\nsd\n\n\n\n\n0.341\n0.1749\n\n\n\n\n\n\nVu and Harrington exercise 1.8. Below are some observations from a study collecting data to analyze smoking habits of UK residents.\n\nWhat does each row of this table represent?\nHow many participants were included in the study?\nFor each variable, indicate whether it is numerical or categorical. If numerical, indicate whether it is continuous or discrete; if categorical, indicate if it is nominal or ordinal.\n\n\n\n\n[L2] Vu and Harrington exercise 1.17. The following scatterplot shows life expectancies and percentages of internet users for 208 countries.\n\nDescribe the relationship between life expectancy and percentage of internet users. Specifically: is there an apparent association, and if so, is it positive or negative and linear or nonlinear?\nState a possible confounding variable that might explain this relationship and describe how the confounder might relate to both the percentage of internet users and the life expectancy of a country.\nAre these data experimental or observational? And if you had to guess, were they obtained by a random or nonrandom sample, and why?\n\n\n\nAnswers:\n\nThere is a positive and nonlinear association between percentage of internet users and life expectancy.\nAny development indicators are acceptable answers: more developed countries have better infrastructure, including internet access, and also higher life expectancies.\nObservational. Data are likely from a nonrandom sample, because the sample most likely comprises all countries for which these data are available.\n\n\nVu and Harrington exercise 1.28. For each of the distributions (a), (b), and (c), describe the shape and modality and identify the matching boxplot from (1), (2), and (3).\n\n\n\nVu and Harrington exercise 1.36.\n\n\n\n[L3] The oibiostat::yrbss dataset contains measurements on a small collection of variables from 13,583 survey responses collected as part of the CDC’s Youth Risk Behavior Surveillance System (YRBSS) from 1991-2013. The objective of the survey program is to track behaviors with potential negative physical and mental health impacts among adolescents.\n\nSummarize the racial composition of survey respondents by academic grade. Produce both a contingency table and a proportional bar plot. Make sure to choose the correct (row or column) normalization for your bar plot so that it shows the racial composition by grade (not the grade composition by race). Are there apparent differences in racial composition across grades?\nMake a bar plot showing the frequency distribution of hours of sleep on school nights. Based on the summary, what is the typical amount of sleep respondents get on school nights?\nProduce a tabular or graphical summary that addresses the question: do older students sleep more on school nights than younger students?\nVisualize the frequency distribution of the number of days per week that survey participants are physically active. Describe the distribution and indicate whether the variable is discrete or continuous. Is the mean an appropriate measure of center for this data? Why or why not?\nProduce side-by-side boxplots visualizing the number of days per week that survey participants are physically active by grade. Based on the graphical summary, do there appear to be differences in physical activity by grade? Explain.\n\n\nAnswers:\n\nThe contingency table and plot are shown below. Students need only produce these plots; no commentary is necessary. However, if table is cut off or organized poorly, which will happen if race is put in columns due to label length, students should be asked to revise and change the orientation of the table for readability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n9\n10\n11\n12\nother\n\n\n\n\nAmerican Indian or Alaska Native\n97\n75\n69\n79\n1\n\n\nAsian\n119\n112\n149\n170\n1\n\n\nBlack or African American\n898\n730\n753\n839\n5\n\n\nNative Hawaiian or Other Pacific Islander\n67\n56\n62\n71\n1\n\n\nWhite\n1620\n1529\n1533\n1721\n9\n\n\n\n\n\n\n\n\n\nMost students seem to get 7 hours per night. This is the most frequent category.\n\n\n\n\n\n\n\nStudents may do this either using age (treated as categorical), or using grade as a proxy for age. Both possible plots are shown below. Older students do indeed seem to get less sleep, reflected by the decreasing proportions of students in each grade/age getting over 6 hours per night.\n\n\n\n\n\n\n\n\n\n\nNumber of days per week of physical activity is a discrete variable. Students should choose an appropriate number of breaks so that there are no gaps in the histogram. The distribution is bimodal and symmetric; because of the shape, mean is a poor choice for measuring the center of the distribution, as it will locate the least frequent values.\n\n\n\n\n\n\n\nThe plot is shown below. Ignoring the ‘other’ category, which comprises only a handful of respondents, days of physical activity do not seem to differ by grade substantially, except possibly in grade 12 – medians are the same across grades and interquartile ranges are similar for all grades except 12.\n\n\n\n\n\n\n\n[L3] Vu and Harrington exercise 1.39. Trait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. People with high trait anger have rage and fury more often, more intensely, and with long-laster episodes than people with low trait anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease; 12,986 participants were recruited for a study examining this hypothesis. Participants were followed for five years. The following table shows data for the participants identified as having normal blood pressure (normotensives).\n\nWhat percentage of participants have moderate anger scores?\nWhat percentage of participants who experienced a CHD event have moderate anger scores?\nWhat percentage of participants with high trait anger scores experienced a CHD event?\nWhat percentage of participants with low trait anger scores experienced a CHD event?\nWhat is the ratio of the percentages in (c) and (d)? (This is called a “relative risk” of CHD events.) Based on this, does it appear that the risk of a CHD event is higher in the high trait anger group?\nProduce a proportional bar plot that substantiates your answer in (e).\n\n\n\nStudents should give answers in complete sentences.\n\n56.2580344% of participants have moderate anger scores.\n57.8947368% of participants who experienced a CHD event have moderate anger scores.\n4.2654028% of participants with high anger scores experienced a CHD event.\n1.7041801% of participants with low anger scores experienced a CHD event.\nThe percentage of participants who experienced a CHD event was 2.5029062 times higher in the high anger group compared with the low anger group.\nA proportional bar plot of the percentage of participants who experienced a CHD event by trait anger group does the trick."
  },
  {
    "objectID": "content/week1-studydesigns.html#what-is-a-study",
    "href": "content/week1-studydesigns.html#what-is-a-study",
    "title": "Study designs",
    "section": "What is a study?",
    "text": "What is a study?\nA study is an effort to collect data in order to answer one or more research questions.\nUltimately, you’re in this class to learn how to answer questions (or evaluate the answers of others) in a sound way, so we will start with the basics of study design because:\n\nstudies must be well-matched to research questions to provide good answers\nhow data are obtained is just as important as how the resulting data are analyzed\nno analysis, no matter how sophisticated will rescue a poorly conceived study\n\n[Terminology] A study unit is the smallest object or entity that is measured in a study; also called experimental unit or observational unit."
  },
  {
    "objectID": "content/week1-studydesigns.html#two-types-of-studies",
    "href": "content/week1-studydesigns.html#two-types-of-studies",
    "title": "Study designs",
    "section": "Two types of studies",
    "text": "Two types of studies\nObservational studies collect data from an existing situation without intervention.\n\nAim is to detect associations and patterns\nCan’t be used to establish causal links (more on this later)\n\nExperiments collect data from a situation in which one or more interventions have been introduced by the investigator.\n\nAim is to draw conclusions about the effect of one or more interventions\nStronger form of scientific evidence than an observational study\n\nOften, observational studies are used to explore/generate hypotheses prior to designing an experiment."
  },
  {
    "objectID": "content/week1-studydesigns.html#comparing-study-types",
    "href": "content/week1-studydesigns.html#comparing-study-types",
    "title": "Study designs",
    "section": "Comparing study types",
    "text": "Comparing study types\nEither type of study can be used to address a question.\n\n\n\n\n\n\n\n\nQuestion\nObservational study\nExperiment\n\n\n\n\nAre diet and mood related?\nConduct surveys on diet, lifestyle, and affect\nRecruit study participants, assign diets, measure affect\n\n\nIs vaping safer than smoking?\nFollow groups of vapers and smokers over time and record health outcomes\nAmong a group of smokers, assign some to switch to vaping; compare health outcomes over time\n\n\nDo insecticide applications affect soil microbes?\nAnalyze soil samples from farms using different insecticides\n[Your turn]\n\n\n\nIn each example, which approach would you choose if you were the researcher? What are the pros and cons?"
  },
  {
    "objectID": "content/week1-studydesigns.html#why-does-intervention-matter",
    "href": "content/week1-studydesigns.html#why-does-intervention-matter",
    "title": "Study designs",
    "section": "Why does intervention matter?",
    "text": "Why does intervention matter?\nIn an experiment, the researcher intentionally intervenes in a way that they expect to change outcomes.\n\nintervention confers a degree of control over study conditions\n\n\nIn an observational study, there is no (intentional) intervention.\n\nwithout intervention the researcher has no control over study conditions\n\n\nControl over conditions allows a researcher to study causal effects resulting from interventions. This can best be understood in terms of a phenomenon known as confounding."
  },
  {
    "objectID": "content/week1-studydesigns.html#confounding",
    "href": "content/week1-studydesigns.html#confounding",
    "title": "Study designs",
    "section": "Confounding",
    "text": "Confounding\nConfounding occurs when two conditions cannot be differentiated.\nThis happens when either:\n\n[omission] one study condition is not measured\n\na researcher finds that dog owners live longer, but failed to measure how much exercise each subject gets\nis it the dog, or the daily walks?\n\n[conflation] two study conditions are always observed simultaneously\n\na researcher finds that people on diet 1 tend to be overweight, but all study participants on diet 1 are from the U.S., and all study participants on diet 2 are from Canada\nis it the diet, or the country?"
  },
  {
    "objectID": "content/week1-studydesigns.html#consequences-of-confounding",
    "href": "content/week1-studydesigns.html#consequences-of-confounding",
    "title": "Study designs",
    "section": "Consequences of confounding",
    "text": "Consequences of confounding\nUsually, researchers are concerned with confounding by omission.\n\n\nWhen an unobserved condition is associated with both the observed condition and the outcome one may observe:\n\nan artificial association between the study condition and outcome\na distorted association between the study condition and outcome\n\n\n\n\n\n\nflowchart TD\n  A(unobserved condition) --- B(study condition) & C(outcome) \n\n\n\n\n\n\n\nThis is very common in observational studies, because you can’t measure every study condition."
  },
  {
    "objectID": "content/week1-studydesigns.html#example-of-confounding",
    "href": "content/week1-studydesigns.html#example-of-confounding",
    "title": "Study designs",
    "section": "Example of confounding",
    "text": "Example of confounding\n\n\n[left] Higher state educational expenditure looks associated with lower SAT scores\n[center] But increased expenditure looks associated with more test takers\n[right] And higher percentages of test takers are associated with lower averages\n\nIf you ignored how many students take the test, you’d conclude that expenditure is counterproductive"
  },
  {
    "objectID": "content/week1-studydesigns.html#antidote-randomization",
    "href": "content/week1-studydesigns.html#antidote-randomization",
    "title": "Study designs",
    "section": "Antidote: randomization",
    "text": "Antidote: randomization\nThe ability to control study conditions allows researchers to randomize interventions among study subjects:\n\nallocated in such a way that all subjects are equally likely to receive any intervention\n\n\nRandomization eliminates confounding by isolating the condition(s) of interest:\n\ninterventions are independent of extraneous conditions ⟹ no association possible\nif outcomes differ systematically according to the intervention, you can be certain that it is not an artifact\n\n\n\n\n\n\nflowchart TD\n  A(unobserved condition) x-.-x B(study condition)\n  A --- C(outcome)\n\n\n\n\n\n\n\nExtraneous conditions may still be associated with outcomes and add noise — well-thought-out experiments find ways to reduce this."
  },
  {
    "objectID": "content/week1-studydesigns.html#practical-consequences",
    "href": "content/week1-studydesigns.html#practical-consequences",
    "title": "Study designs",
    "section": "Practical consequences",
    "text": "Practical consequences\nThe ability to randomize interventions in experiments means:\n\nobserved associations are independent of extraneous factors\nresults can support causal inferences\n\nThe absence of randomization in observational studies means:\n\nconfounding is always possible\nresults may be misleading\n\nUsually, observational studies are considered weaker forms of scientific evidence than experiments, with some caveats:\n\na large volume of independent observational studies pointing to the same conclusions may be more convincing than a few narrow experiments\nobservational studies may provide better evidence under real world conditions than available experimental data"
  },
  {
    "objectID": "content/week1-studydesigns.html#common-study-designs",
    "href": "content/week1-studydesigns.html#common-study-designs",
    "title": "Study designs",
    "section": "Common study designs",
    "text": "Common study designs\nStudies can be designed to investigate outcomes in a variety of ways, regardless of whether study conditions include intervention. Many study designs originate in clinical medicine — the following are among the most common.\n\n\nIn prospective studies participants are recruited before developing a condition\n\n\nIn retrospective studies participants are recruited after developing a condition\n\n\n\nThese terms usually refer to observational studies; experiments are typically prospective in nature, since one recruits participants before administering an intervention."
  },
  {
    "objectID": "content/week1-studydesigns.html#example-study",
    "href": "content/week1-studydesigns.html#example-study",
    "title": "Study designs",
    "section": "Example study",
    "text": "Example study\nImagine following this year’s Cal Poly freshmen (a “cohort”) and comparing four-year graduation (outcome) rates between those that participated in a new orientation activity and those that didn’t. This would be:\n\nprospective if an observational study\n\n\n\nan experiment if students were assigned to participate or not\n\nCan you change the example so that it’s a retrospective study?"
  },
  {
    "objectID": "content/week1-studydesigns.html#experimental-designs",
    "href": "content/week1-studydesigns.html#experimental-designs",
    "title": "Study designs",
    "section": "Experimental designs",
    "text": "Experimental designs\nA treatment is an experimental intervention; the design of an experiment refers to how treatments are allocated to study units.\nThe most basic design is:\n\n[balanced] each treatment is replicated an equal number of times\n[randomized] treatments are allocated completely at random to study units\n[no crossover] each study unit receives exactly one treatment\n\nWe’ll call this a completely randomized design. It’s the only kind of experimental design we’re going to consider in STAT218.\nThere are many other designs that we won’t discuss (but see STAT313); these are all about improving experimental efficiency by controlling extraneous variation."
  },
  {
    "objectID": "content/week1-studydesigns.html#blinding",
    "href": "content/week1-studydesigns.html#blinding",
    "title": "Study designs",
    "section": "Blinding",
    "text": "Blinding\nThe terms single-blind and double-blind refer to experiments with human subjects, typically in clinical studies.\nIn a single-blind study, participants are unaware of which treatment they receive.\n\nExample: in a case-control study, one group of patients is given a new medicine and the other is given a saline solution administered under identical conditions\nIdea: controls for placebo effect\n\nIn a double-blind study, both participants and investigators are unaware of which treatments are administered.\n\nExample: same as above, but now the person administering the doses is unaware of which are medicine and which are saline\nIdea: controls for unconscious influence on outcome by researchers (e.g., attend more carefully and compassionately to patients receiving medicine out of concern for side-effects)"
  },
  {
    "objectID": "content/week1-studydesigns.html#data-collection",
    "href": "content/week1-studydesigns.html#data-collection",
    "title": "Study designs",
    "section": "Data collection",
    "text": "Data collection\nWe will discuss data collection in more depth later, but the way that study units are obtained, independently of the type of study or design, determines the breadth of conclusions supported by the results, depending on whether study units chosen are representative of a larger collection.\nThe gold standard is a simple random sample of units selected from a larger collection:\n\nevery unit in the larger collection has an equal chance of being selected\neasier said than done — failure to obtain a random sample is one of the most common study flaws\n\nFor now we will set the issue of sampling aside."
  },
  {
    "objectID": "content/week1-studydesigns.html#considerations-in-creating-a-study",
    "href": "content/week1-studydesigns.html#considerations-in-creating-a-study",
    "title": "Study designs",
    "section": "Considerations in creating a study",
    "text": "Considerations in creating a study\n\nThe research question should be motivated by science, not statistics.\nYou may have to work within limitations:\n\nThe design that best aligns with the question may not be possible or practical.\nAn experiment might give the strongest evidence but not be feasible or ethical.\n\nA plan should be in place for all of the following:\n\nhow to obtain/recruit study units/participants\nhow many to obtain/recruit and how to ensure that target is met\nprotocol for allocating treatment\ncollecting data and measuring outcomes\ncontingencies for dropout/failure to ensure the study yields results\ndata handling and data analysis\nhow results will be used"
  },
  {
    "objectID": "content/week1-studydesigns.html#study-design-in-stat218",
    "href": "content/week1-studydesigns.html#study-design-in-stat218",
    "title": "Study designs",
    "section": "Study design in STAT218",
    "text": "Study design in STAT218\nIn STAT218 you are expected to be fluent with study design concepts, but of course won’t actually carry out an end-to-end study.\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each"
  },
  {
    "objectID": "content/week1-studydesigns.html#case-study-leap",
    "href": "content/week1-studydesigns.html#case-study-leap",
    "title": "Study designs",
    "section": "Case study: LEAP",
    "text": "Case study: LEAP\nLearning early about peanut allergy (LEAP) study:\n\n640 infants in UK with eczema, egg allergy, or both enrolled; 530 passed a skin test showing no peanut allergy\neach infant randomly assigned to peanut consumption and peanut avoidance groups\n\npeanut consumption: fed 6g peanut protein daily until 5 years old\npeanut avoidance: no peanut consumption until 5 years old\n\nat 5 years old, oral food challenge (OFC) allergy test administered\n\nPASS: no allergy detected\nFAIL: allergy detected\n\n\nWhat kind of study is this? Experiment or observational study? Retrospective, prospective, or neither?"
  },
  {
    "objectID": "content/week1-studydesigns.html#case-study-leap-1",
    "href": "content/week1-studydesigns.html#case-study-leap-1",
    "title": "Study designs",
    "section": "Case study: LEAP",
    "text": "Case study: LEAP\n\n\n\n\n\nTabular summary of LEAP results.\n\n\n\n\n\n\nGraphical summary of LEAP results.\n\n\n\n\n\nDoes peanut avoidance/consumption appear associated with developing an allergy?\nIf so, does the study provide evidence that early exposure likely reduces the chance of developing an allergy by 5 years of age?\nWhat calculation could you perform to summarize the difference between study groups?\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-datatypes.html#todays-agenda",
    "href": "content/week2-datatypes.html#todays-agenda",
    "title": "Introduction to data",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\nLoose ends from last time: LEAP case study; experimental design basics\nData basics: data semantics and data types\nLab: data types and data structures in R"
  },
  {
    "objectID": "content/week2-datatypes.html#case-study-leap",
    "href": "content/week2-datatypes.html#case-study-leap",
    "title": "Introduction to data",
    "section": "Case study: LEAP",
    "text": "Case study: LEAP\nLearning early about peanut allergy (LEAP) study:\n\n640 infants in UK with eczema, egg allergy, or both enrolled; 530 passed a skin test showing no peanut allergy\neach infant randomly assigned to peanut consumption and peanut avoidance groups\n\npeanut consumption: fed 6g peanut protein daily until 5 years old\npeanut avoidance: no peanut consumption until 5 years old\n\nat 5 years old, oral food challenge (OFC) allergy test administered\n\nPASS: no allergy detected\nFAIL: allergy detected\n\n\nReview questions from last time:\n\nExperiment or observational study?\nRetrospective, prospective, or neither?"
  },
  {
    "objectID": "content/week2-datatypes.html#case-study-leap-1",
    "href": "content/week2-datatypes.html#case-study-leap-1",
    "title": "Introduction to data",
    "section": "Case study: LEAP",
    "text": "Case study: LEAP\n \nA greater proportion of children in the avoidance group developed an allergy (11.8% specifically). Given the type of study, which is a more accurate interpretation of this result?\n\nCompared with consumption, peanut avoidance early in life was associated with more frequent occurrence of peanut allergies among children in the study.\nCompared with consumption, peanut avoidance early in life led to more frequent occurrence of peanut allergies among children in the study."
  },
  {
    "objectID": "content/week2-datatypes.html#experimental-design",
    "href": "content/week2-datatypes.html#experimental-design",
    "title": "Introduction to data",
    "section": "Experimental design",
    "text": "Experimental design\nThe LEAP study exemplifies the simple kind of design we will consider for much of STAT218:\n\ntwo “treatments” (peanut consumption and peanut avoidance)\ntreatments are allocated completely at random among all study units\n\nThe design of an experiment refers to how treatments (experimental interventions) are allocated to study units.\n\nthe number of replicates of a treatment refers to the number of study units receiving that treatment\na design is balanced if all treatments have equal numbers of replicates\ntreatment allocation should always be randomized; different designs use different randomization schemes\n\nCheck your understanding: is the experimental design of the LEAP study balanced?"
  },
  {
    "objectID": "content/week2-datatypes.html#data-semantics",
    "href": "content/week2-datatypes.html#data-semantics",
    "title": "Introduction to data",
    "section": "Data semantics",
    "text": "Data semantics\n\nData are a set of measurements.\nA variable is any measured attribute of study units.\nAn observation is a measurement of one or more variables taken on one particular study unit.\n\nIt is usually expedient to arrange data values in a table in which each row is an observation and each column is a variable:"
  },
  {
    "objectID": "content/week2-datatypes.html#leap-example",
    "href": "content/week2-datatypes.html#leap-example",
    "title": "Introduction to data",
    "section": "LEAP example",
    "text": "LEAP example\nA table showing the observations and variables for the LEAP study would look like this:\n\n\n\n\n\n\n\n\n\n\nparticipant.ID\ntreatment.group\nofc.test.result\n\n\n\n\nLEAP_100522\nPeanut Consumption\nPASS OFC\n\n\nLEAP_103358\nPeanut Consumption\nPASS OFC\n\n\nLEAP_105069\nPeanut Avoidance\nPASS OFC\n\n\nLEAP_105328\nPeanut Consumption\nPASS OFC\n\n\n\n\n\nThe table you saw earlier was a summary of the data (not the data itself):\n\n\n\n\n\n\n\n\n\n\n \nFAIL OFC\nPASS OFC\n\n\n\n\nPeanut Avoidance\n36\n227\n\n\nPeanut Consumption\n5\n262"
  },
  {
    "objectID": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "href": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "title": "Introduction to data",
    "section": "Numeric and categorical variables",
    "text": "Numeric and categorical variables\nVariables are classified according to their values. The broadest distinction made is between numeric and categorical variables.\n\nA variable is numeric if its value is a number\nA variable is categorical if its value is a category, usually recorded as a name or label\n\nFor example:\n\nthe value of sex can be male or female, so it is categorical\nwhereas age (in years) can be any positive integer, so it is numeric\n\nCheck your understanding: in the LEAP study…\n\ntreatment group is [numeric/categorical]\nOFC test result is [numeric/categorical]"
  },
  {
    "objectID": "content/week2-datatypes.html#variable-subtypes",
    "href": "content/week2-datatypes.html#variable-subtypes",
    "title": "Introduction to data",
    "section": "Variable subtypes",
    "text": "Variable subtypes\nFurther distinctions are made based on the type of number or type of category used to measure an attribute. Can you match the subtypes to the variables at right?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nhispanic\ngrade\nweight\n\n\n\n\n15\nnot\n10\n78.02\n\n\n18\nhispanic\n12\n78.47\n\n\n17\nnot\n11\n95.26\n\n\n18\nnot\n12\n95.26\n\n\n\n\n\n\n\n\na numerical variable is discrete if there are ‘gaps’ between its possible values\na numerical variable is continuous if there are no such gaps\na categorical variable is nominal if its levels are not ordered\na categorical variable is ordinal if its levels are ordered"
  },
  {
    "objectID": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "href": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "title": "Introduction to data",
    "section": "Many ways to measure attributes",
    "text": "Many ways to measure attributes\nVariable type (or subtype) is not an inherent quality — attributes can often be measured in many different ways.\nFor instance, age might be measured as either a discrete, continuous, or ordinal variable, depending on the situation:\n\n\n\nAge (years)\nAge (minutes)\nAge (brackets)\n\n\n\n\n12\n6307518.45\n10-18\n\n\n8\n4209187.18\n5-10\n\n\n21\n11258103.08\n18-30\n\n\n\n\nNumeric variables can always be represented as categorical, but not the other way around."
  },
  {
    "objectID": "content/week2-datatypes.html#your-turn-identifying-data-types-i",
    "href": "content/week2-datatypes.html#your-turn-identifying-data-types-i",
    "title": "Introduction to data",
    "section": "Your turn: identifying data types I",
    "text": "Your turn: identifying data types I\nClassify each variable as nominal, ordinal, discrete, or continuous:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n33.3\n0\nFemale\n19\nCaucasian\n65.7\n129\nCT\n21.01\n\n\n71.4\n0\nFemale\n18\nOther\n67\n148\nCT\n23.18\n\n\n37.5\n0\nFemale\n21\nCaucasian\n66.7\n183\nCC\n28.92\n\n\n50\n0\nFemale\n28\nAsian\n61\n112\nCC\n21.16\n\n\n\n\n\nData are from an observational study investigating demographic, physiological, and genetic characteristics associated with muscle strength.\n\nndrm.ch and drm.ch are change in strength in nondominant and dominant arms before and after training\nactn3.r577x gives genotype at a particular location within the ACTN3 gene (so-called “sports gene”)"
  },
  {
    "objectID": "content/week2-datatypes.html#your-turn-identifying-data-types-ii",
    "href": "content/week2-datatypes.html#your-turn-identifying-data-types-ii",
    "title": "Introduction to data",
    "section": "Your turn: identifying data types II",
    "text": "Your turn: identifying data types II\nRecall the kimchi/prediabetes study from last time:\n\nA total of 21 participants with prediabetes were enrolled. During the first 8 weeks, they consumed either fresh (1-day-old) or fermented (10-day-old) kimchi. After a 4-week washout period, they switched to the other type of kimchi for the next 8 weeks. Consumption of both types of kimchi significantly decreased body weight, body mass index, and waist circumference. Fermented kimchi decreased insulin resistance, and increased insulin sensitivity … Systolic and diastolic blood pressure (BP) decreased significantly in the fermented kimchi group. The percentages of participants who showed improved glucose tolerance were 9.5 and 33.3% in the fresh and fermented kimchi groups, respectively.\n\n\nlist the minimal collection of variables measured\ndetermine a likely type for each variable\ninvent a few example observations to sketch what the data table might look like"
  },
  {
    "objectID": "content/week2-datatypes.html#describing-studies",
    "href": "content/week2-datatypes.html#describing-studies",
    "title": "Introduction to data",
    "section": "Describing studies",
    "text": "Describing studies\nLet’s put some pieces together. The “essential information” to report when explaining/presenting a study comprises:\n\nResearch question or hypothesis\nType of study\nNumber and type of study units\nIf applicable, experimental design\nObservations and variables measured"
  },
  {
    "objectID": "content/week2-datatypes.html#lab-data-types-and-structures-in-r",
    "href": "content/week2-datatypes.html#lab-data-types-and-structures-in-r",
    "title": "Introduction to data",
    "section": "Lab: data types and structures in R",
    "text": "Lab: data types and structures in R\nThe simplest data structure in R is a vector: a collection of values of a single “type”. This lab will introduce you to data types, vectors, and a few other common data structures.\n\n\n\n\nThe variable types we just discussed map pretty neatly (but not perfectly) onto the main “data types” in R:\n\nnumeric ➜ integer, numeric\ncategorical ➜ character, logical\n\nYour objectives in this lab are:\n\nlearn to perform simple calculations\nlearn to recognize data types and data structures"
  },
  {
    "objectID": "content/week2-datatypes.html#up-next-descriptive-statistics",
    "href": "content/week2-datatypes.html#up-next-descriptive-statistics",
    "title": "Introduction to data",
    "section": "Up next: descriptive statistics",
    "text": "Up next: descriptive statistics\nSo far we’ve discussed study designs and the basics of data.\nDescriptive statistics refers to techniques for summarizing data:\n\nnumerical summaries\ntables\ngraphical summaries\n\nSuch summaries are essential first steps in any data analysis, as they convey what was directly observed in the study.\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week1-handout-designs.html",
    "href": "content/week1-handout-designs.html",
    "title": "Distinguishing study designs",
    "section": "",
    "text": "Recall that the difference between an observational study and an experiment hinges on whether researchers intentionally intervene on the system of study (experiment) or passively record outcomes (observational study). Additionally, the following common study designs were discussed:\n\nIn prospective (“looking ahead”) studies participants are recruited before developing a condition based on exposure; results look ahead to the proportions in each exposure group that develop the condition\nIn retrospective (“looking back”) studies participants are recruited after developing a condition; results look back on the proportions in each group that met certain criteria\n\nIn this activity you’ll read abstracts from a few published studies and determine, in consultation with your group, what kind of study is described in the abstract. You do not need to consider the examples in order — start with the ones that look most interesting.\n\nExample 1: long COVID\nLong COVID is a multi-systemic and often debilitating condition that develops in at least 10% of patients following a COVID infection. The following is an excerpt of the abstract from a recent study seeking to identify symptoms and risk factors associated with long COVID and published in Nature Medicine:\n\nWe undertook a … study using a UK-based primary care database, Clinical Practice Research Datalink Aurum, to determine symptoms that are associated with confirmed SARS-CoV-2 infection beyond 12 weeks in non-hospitalized adults and the risk factors associated with developing persistent symptoms. We selected 486,149 adults with confirmed SARS-CoV-2 infection … Outcomes included 115 individual symptoms, as well as long COVID, defined as a composite outcome of 33 symptoms by the World Health Organization clinical case definition … Among the patients infected with SARS-CoV-2, risk factors for long COVID included female sex, belonging to an ethnic minority, socioeconomic deprivation, smoking, obesity and a wide range of comorbidities. The risk of developing long COVID was also found to be increased along a gradient of decreasing age.\n\nSubramanian et al. (2022). Symptoms and risk factors for long COVID in non-hospitalized adults. Nature medicine, 28(8), 1706-1714.\nDiscuss the following questions with your group:\n\nIs this an observational study or an experiment?\nIs this study retrospective, prospective, or neither?\n\n\n\nExample 2: selenium exposure and Mediterranean diet\nThe following is from the abstract of a study investigating dietary mitigation of selenium exposure:\n\nSelenium is a trace element found in many chemical forms. Selenium and its species have nutritional and toxicologic properties, some of which may play a role in the etiology of neurological disease. We hypothesized that adherence to the Mediterranean-Dietary Approach to Stop Hypertension Intervention for Neurodegenerative Delay (MIND) diet could influence intake and endogenous concentrations of selenium and selenium species, thus contributing to the beneficial effects of this dietary pattern. We carried out a cross-sectional study of 137 non-smoking blood donors (75 females and 62 males) from the Reggio Emilia province, Northern Italy. We assessed MIND diet adherence using a semiquantitative food frequency questionnaire. We assessed selenium exposure through dietary intake and measurement of urinary and serum concentrations, including speciation of selenium compound in serum … Adherence to the MIND diet was positively associated with dietary selenium intake and urinary selenium excretion, whereas it was inversely associated with serum concentrations of overall selenium and organic selenium … Our results suggest that greater adherence to the MIND diet is non-linearly associated with lower circulating concentrations of selenium and of 2 potentially neurotoxic species of this element, selenoprotein P and selenate. This may explain why adherence to the MIND dietary pattern may reduce cognitive decline.\n\nUrbano, T., et al. (2023). Adherence to the Mediterranean-DASH Intervention for Neurodegenerative Delay (MIND) diet and exposure to selenium species: A cross-sectional study. Nutrition Research.\nDiscuss the following questions with your group:\n\nDoes the abstract describe an observational study or an experiment?\nIs this study prospective, retrospective, or neither?\nConsider the finding that MIND adherence is associated with lower circulating concentrations of selenium. Does this provide evidence that adoption of the MIND diet is likely to reduce selenium concentrations? Why or why not?\n\n\n\nExample 3: fermented kimchi and glucose metabolism\nThe following is from an abstract of a study investigating possible benefits of kimchi consumption among prediabetic individuals:\n\nWith the increased incidence of diabetes mellitus, the importance of early intervention in prediabetes has been emphasized … We hypothesized that kimchi and its fermented form would have beneficial effects on glucose metabolism in patients with prediabetes. A total of 21 participants with prediabetes were enrolled. During the first 8 weeks, they consumed either fresh (1-day-old) or fermented (10-day-old) kimchi. After a 4-week washout period, they switched to the other type of kimchi for the next 8 weeks. Consumption of both types of kimchi significantly decreased body weight, body mass index, and waist circumference. Fermented kimchi decreased insulin resistance, and increased insulin sensitivity … Systolic and diastolic blood pressure (BP) decreased significantly in the fermented kimchi group. The percentages of participants who showed improved glucose tolerance were 9.5 and 33.3% in the fresh and fermented kimchi groups, respectively.\n\nAn, S. Y., et al. (2013). Beneficial effects of fresh and fermented kimchi in prediabetic individuals. Annals of Nutrition and Metabolism, 63(1-2), 111-119.\nDiscuss the following questions with your group:\n\nDoes the abstract describe an observational study or an experiment?\nIs this study retrospective, prospective, or neither?\nConsider the finding that a third of participants consuming fermented kimchi showed improved glucose tolerance. Does this provide evidence that consuming fermented kimchi is likely to improve glucose tolerance? Would you answer differently if the study had found that number were 8 in 10?\n\n\n\nExample 4: Moderna vaccine efficacy\nFrom the journal publication reporting clinical trial results for the Moderna COVID vaccine:\n\nVaccines are needed to prevent coronavirus disease 2019 (Covid-19) and to protect persons who are at high risk for complications. The mRNA-1273 vaccine is a lipid nanoparticle–encapsulated mRNA-based vaccine that encodes the prefusion stabilized full-length spike protein of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes Covid-19. This [study] was conducted at 99 centers across the United States … The trial enrolled 30,420 volunteers who were randomly assigned in a 1:1 ratio to receive either vaccine or placebo (15,210 participants in each group) … Symptomatic Covid-19 illness was confirmed in 185 participants in the placebo group and in 11 participants in the mRNA-1273 group; vaccine efficacy was 94.1%. \n\nBaden, L. R., et al. (2021). Efficacy and safety of the mRNA-1273 SARS-CoV-2 vaccine. New England journal of medicine, 384(5), 403-416.\nDiscuss the following questions with your group:\n\nIs this an observational study or an experiment?\nIs this study prospective, retrospective, or neither?\nConsider the finding that 11 symptomatic illnesses were observed in the vaccine group and 185 were observed in the placebo group. Does this provide evidence that the vaccine reduces the likelihood of symptomatic illness?"
  },
  {
    "objectID": "content/week4-intervals.html#todays-agenda",
    "href": "content/week4-intervals.html#todays-agenda",
    "title": "Interval estimation",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nHW2 remarks/discussion\n[Lecture] A basic interval estimate for the mean\n[Lecture/lab] Exploring interval coverage\n[Lecture/lab] Comparing normal and \\(t\\) models"
  },
  {
    "objectID": "content/week4-intervals.html#from-last-time",
    "href": "content/week4-intervals.html#from-last-time",
    "title": "Interval estimation",
    "section": "From last time",
    "text": "From last time\n\n\nUnder simple random sampling:\n\nthe sample mean provides a good point estimate of the population mean\nits theoretical sampling variability is given by the standard deviation \\(\\frac{\\sigma}{\\sqrt{n}} = \\frac{\\text{population SD}}{\\sqrt{\\text{sample size}}}\\)\nits estimated sampling variability is given by the standard error \\(\\frac{s_x}{\\sqrt{n}} = \\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n \nmean\nsd\n\n\n\n\nsample\n5.031\n0.9873\n\n\npopulation\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\nSo in the example: the estimated mean total HDL cholesterol among the population is 5.031 mmol/L and this point estimate is expected to deviate by 0.1396 mmol/L from the population mean on average across samples."
  },
  {
    "objectID": "content/week4-intervals.html#interval-estimation",
    "href": "content/week4-intervals.html#interval-estimation",
    "title": "Interval estimation",
    "section": "Interval estimation",
    "text": "Interval estimation\nA point estimate for the mean provides a guess at the exact value of the parameter; an interval estimate is a range of plausible values.\nIn general, an interval estimate is constructed from two main ingredients:\n\npoint estimate\nstandard error\n\nAnd one secret ingredient:\n\na model for the sampling distribution of the point estimate\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]"
  },
  {
    "objectID": "content/week4-intervals.html#precision-and-coverage",
    "href": "content/week4-intervals.html#precision-and-coverage",
    "title": "Interval estimation",
    "section": "Precision and coverage",
    "text": "Precision and coverage\n\n\nIntervals have two main and attributes:\n\nprecision refers to how wide or narrow the interval is\ncoverage refers to how often, under random sampling, the interval captures the parameter of interest\n\n\nThese properties are inversely related:\n\nif I say mean cholesterol is between 0 and 50 I’m almost certainly right, but the estimate is useless\nif I say mean cholesterol is between 5.0299 and 5.0301, I’ve made a very precise guess, but I’m likely wrong (think about sampling variability)\n\n\n\n\nAn accurate interval should maintain high coverage while achieving practically useful precision. This isn’t always possible!"
  },
  {
    "objectID": "content/week4-intervals.html#an-interval-for-the-mean",
    "href": "content/week4-intervals.html#an-interval-for-the-mean",
    "title": "Interval estimation",
    "section": "An interval for the mean",
    "text": "An interval for the mean\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.031 \\pm 2\\times 0.1396 = (4.75, 5.31)\\]\n\nIn R:\n\nc(lwr = mean(samp) - 2*sd(samp)/sqrt(50), \n  upr = mean(samp) + 2*sd(samp)/sqrt(50))\n\n     lwr      upr \n4.751348 5.309852 \n\n\n\n\nThe precision is evident from the interval width (0.5611). But what about coverage?"
  },
  {
    "objectID": "content/week4-intervals.html#exploring-interval-coverage",
    "href": "content/week4-intervals.html#exploring-interval-coverage",
    "title": "Interval estimation",
    "section": "Exploring interval coverage",
    "text": "Exploring interval coverage\nLet’s carry on pretending that the NHANES data comprise a population.\nThe first section of lab5-intervals contains some simple commands to draw a sample and calculate an interval estimate.\n\nEach of you will generate an interval based on a different sample\nWe’ll tally how many of you obtained intervals capturing the population mean\n\nOur tally will give an approximate idea of the coverage."
  },
  {
    "objectID": "content/week4-intervals.html#more-simulation",
    "href": "content/week4-intervals.html#more-simulation",
    "title": "Interval estimation",
    "section": "More simulation",
    "text": "More simulation\n\n\nArtificially simulating a larger number of intervals provides a slightly better approximation of coverage.\n\nat right, 100 intervals\n97% cover the population mean (vertical dashed line)\n\nWhat do you expect would happen to coverage if, for the same samples…\n\na wider margin of error (say, \\(3\\times SE\\)) were used?\na narrower margin of error (say, \\(1\\times SE\\)) were used?"
  },
  {
    "objectID": "content/week4-intervals.html#so-why-2-standard-errors",
    "href": "content/week4-intervals.html#so-why-2-standard-errors",
    "title": "Interval estimation",
    "section": "So why 2 standard errors?",
    "text": "So why 2 standard errors?\n\n\nThe margin of error of \\(2\\times SE\\) comes from the so-called “empirical rule”.\n\nunder the normal model, 95% of values are within 2SD of center\nso for 95% of samples, the sample mean is within 2SD of the population mean\n\nSo in theory, according to the normal model, \\(\\bar{x} \\pm 2\\times SD\\) achieves 95% coverage.\n\n\n\n\nBut we are using standard error (SE), not standard deviation (SD). Do we still get the same coverage using the normal model?"
  },
  {
    "objectID": "content/week4-intervals.html#normal-model-coverage",
    "href": "content/week4-intervals.html#normal-model-coverage",
    "title": "Interval estimation",
    "section": "Normal model coverage",
    "text": "Normal model coverage\n\n\nAt right, the misses are compared between intervals calculated with SD (left) and SE (right) using the multiplier from the normal model on the same 10,000 simulated datasets with sample size \\(n = 15\\).\n\nSE misses more often\nso the normal model produces under-coverage\n\n\n\n\n\n\n\n\n\n\ntype\ncoverage\n\n\n\n\nsd\n0.954\n\n\nse\n0.9294\n\n\n\n\n\nWhat do you think: the multiplier should be [smaller/larger] to ensure 95% coverage."
  },
  {
    "objectID": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "href": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "title": "Interval estimation",
    "section": "A closer look at the normal model",
    "text": "A closer look at the normal model\nAn alternate but equivalent way to understand the normal model for the sampling distribution of \\(\\bar{x}\\) is in terms of deviations. The following are equivalent:\n\nThe expression \\(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\\) measures the number of standard deviations from center."
  },
  {
    "objectID": "content/week4-intervals.html#simulating-deviations",
    "href": "content/week4-intervals.html#simulating-deviations",
    "title": "Interval estimation",
    "section": "Simulating deviations",
    "text": "Simulating deviations\nAnother way to check normal model coverage is to use deviations:\n\nSimulate many samples\nCompute scaled deviations\nTally how many scaled deviations are between -2 and 2\n\nThe proportion of samples for which the scaled deviation is between -2 and 2 approximates the coverage.\nWe’ll try it in the next part of the lab5-intervals. Hypotheses:\n\ndeviations scaled by SD should be between -2 and 2 95% of the time\ndeviations scaled by SE should be between -2 and 2 [more/less] than 95% of the time"
  },
  {
    "objectID": "content/week4-intervals.html#the-t-model",
    "href": "content/week4-intervals.html#the-t-model",
    "title": "Interval estimation",
    "section": "The \\(t\\) model",
    "text": "The \\(t\\) model\nWe’re actually using \\(\\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\\) to construct intervals, because we don’t know \\(\\sigma\\).\nThese deviations are better approximated by a \\(t\\) model, which adjusts the normal model for the extra uncertainty that comes from estimating the standard deviation.\n\n\nThe difference between models depends mainly on sample size:\n\nbehaves almost exactly the same for moderate to large samples\nlarger deviations from center for small samples\nleads to larger multipliers for computing margin of error\n\n\n\n\n\nComparison of \\(t\\) model with normal model for various degrees of freedom."
  },
  {
    "objectID": "content/week4-intervals.html#model-specification",
    "href": "content/week4-intervals.html#model-specification",
    "title": "Interval estimation",
    "section": "Model specification",
    "text": "Model specification\nThe \\(t\\) model is characterized by its degrees of freedom.\n\nfor interval estimates for the mean, \\(n - 1\\) is used\ndepending on the degrees of freedom (i.e., sample size), a different multiplier is applied to the standard error to obtain the margin of error\n\nThe multiplier is called a critical value, and can be found in R via:\n\n# pseudo code -- replace coverage with desired level, e.g., 0.95\nqt((1 - coverage)/2, df = (n - 1), lower.tail = F)\n\n\nchosen to ensure a specified nominal coverage level (usually 95%)\nhigher nominal coverage levels utilize larger critical values, producing wider intervals"
  },
  {
    "objectID": "content/week4-intervals.html#model-validation",
    "href": "content/week4-intervals.html#model-validation",
    "title": "Interval estimation",
    "section": "Model validation",
    "text": "Model validation\nUsing the \\(t\\) model should produce coverage closer to the nominal level compared with the normal model. Let’s check through simulation.\n\n\nAt right, misses are compared between intervals using SE and critical values from the normal model (left) and \\(t\\) model (right) constructed on the same 10,000 simulated datasets with sample size \\(n = 10\\).\n\n\n\n\n\n\n\n\n\nmodel\ncoverage\n\n\n\n\nnormal\n0.9219\n\n\nt\n0.9461\n\n\n\n\n\nThe \\(t\\) model produces coverage much closer to the nominal level."
  },
  {
    "objectID": "content/week4-intervals.html#calculations",
    "href": "content/week4-intervals.html#calculations",
    "title": "Interval estimation",
    "section": "Calculations",
    "text": "Calculations\nSo, to sum up, the general formula for an interval for a population mean is: \\[\\bar{x} \\pm c \\times SE(\\bar{x}) \\quad\\text{where}\\quad SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\]\n\n\nRules of thumb:\n\nfor moderate to large samples, use the normal model\n\n\\(c = 1\\) for 68% coverage\n\\(c = 2\\) for 95% coverage\n\\(c = 3\\) for 99.7% coverage\n\nfor small sample sizes, use the \\(t\\) model\nwhen in doubt, use the \\(t\\) model\n\n\nExact critical values in R:\n\n# normal critical value\nc &lt;- qnorm((1 - coverage)/2, lower.tail = F)\n\n# t critical value\nc &lt;- qt((1 - coverage)/2, df = n - 1, lower.tail = F)\n\nInterval calculation:\n\n# pseudo code\nmean(data_vec) + c(-1, 1)*c*sd(data_vec)/sqrt(n)"
  },
  {
    "objectID": "content/week4-intervals.html#interpretation",
    "href": "content/week4-intervals.html#interpretation",
    "title": "Interval estimation",
    "section": "Interpretation",
    "text": "Interpretation\nAs we’ve seen, coverage pertains to how often an interval of a particular form captures the population parameter of interest across samples of a fixed size. Loosely speaking, this represents how often you’d be right if you were to fully replicate your study ad infinitum.\nThis leads to the following interpretation:\n\nWith [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units].\n\nFor this reason, statisticians call interval estimates confidence intervals."
  },
  {
    "objectID": "content/week4-intervals.html#revisiting-initial-example",
    "href": "content/week4-intervals.html#revisiting-initial-example",
    "title": "Interval estimation",
    "section": "Revisiting initial example",
    "text": "Revisiting initial example\n\n\nSo in the example we began with:\n\n# calculate 95% interval\nmean(samp) + c(-1, 1)*2*sd(samp)/sqrt(50)\n\n[1] 4.751348 5.309852\n\n\nWith 95% confidence, the mean total HDL cholesterol is estimated to be between 4.751 and 5.31 mmol/L.\nRemember, “95% confidence” refers to coverage under sampling variation.\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.031\n0.1396\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week1-welcome.html#about-me",
    "href": "content/week1-welcome.html#about-me",
    "title": "Welcome to STAT218",
    "section": "About me",
    "text": "About me\nHi I’m Trevor. You can call me Trevor, or, if you prefer, Dr. Ruiz.\n\nBorn in Oregon, grew up in Maryland\nBA Philosophy, MS & PhD Statistics\nResearch interests: methods for large and complex scientific data\nHobbies: juggling, dance, classical guitar\nHave a cat named Mona"
  },
  {
    "objectID": "content/week1-welcome.html#why-am-i-a-statistician",
    "href": "content/week1-welcome.html#why-am-i-a-statistician",
    "title": "Welcome to STAT218",
    "section": "Why am I a statistician?",
    "text": "Why am I a statistician?\nShort answer: I like variety.\nLong answer:\n\nas a philosophy student I was interested in how we support claims of empirical fact through scientific investigation\nusually involves data as quantitative evidence\nstatistics is the science of evaluating quantitative evidence\n\nSilly answer: I couldn’t come up with anything better to do."
  },
  {
    "objectID": "content/week1-welcome.html#about-you",
    "href": "content/week1-welcome.html#about-you",
    "title": "Welcome to STAT218",
    "section": "About you",
    "text": "About you\nBy the numbers…"
  },
  {
    "objectID": "content/week1-welcome.html#about-you-1",
    "href": "content/week1-welcome.html#about-you-1",
    "title": "Welcome to STAT218",
    "section": "About you",
    "text": "About you\nBy show of hands…\n\n\nFirst statistics class ever?\nLast statistics class you expect to take?\nYes to 1 and 2?\nExpect to use statistics for your degree coursework?\nExpect to use statistics for research or senior project?\nRequired for your major?\nConsidering a statistics or data science minor?"
  },
  {
    "objectID": "content/week1-welcome.html#uncertainty",
    "href": "content/week1-welcome.html#uncertainty",
    "title": "Welcome to STAT218",
    "section": "Uncertainty",
    "text": "Uncertainty\nLife is full of uncertainty, and this can make a lot of questions hard to answer:\n\nImagine you’re a medical care provider. Which therapy should you prescribe for a patient given their prognosis? You don’t know precisely how each one will turn out, so how do you decide?\nFlu shots don’t always prevent people from getting the flu; many people who are innoculated still become ill, sometimes fatally. Your doctor asks if you want the shot. Should you get it?\nYou’re a trainer devising a fitness regimen for a client. You’ve decided on the basic elements, but what order should you recommend? Will it make any difference with respect to their goals? How will you know?\nYou’re considering trying a nutritional supplement for weight loss. Some people that try it experience severe side effects, but not too many. So, is it safe? It works sometimes, but not all the time. So, is it effective?"
  },
  {
    "objectID": "content/week1-welcome.html#statistics-and-uncertainty",
    "href": "content/week1-welcome.html#statistics-and-uncertainty",
    "title": "Welcome to STAT218",
    "section": "Statistics and uncertainty",
    "text": "Statistics and uncertainty\nKey problem: similar situations do not always result in the same outcome.\nStatistical thinking: uncertainty is measurable.\nWhat statistics can offer:\n\nprinciples for designing studies and collecting data in order to capture outcome variability\ndata analytic tools to distinguish random from systematic variability\nheuristics to make inferences that account for uncertainty"
  },
  {
    "objectID": "content/week1-welcome.html#course-goal-and-scope",
    "href": "content/week1-welcome.html#course-goal-and-scope",
    "title": "Welcome to STAT218",
    "section": "Course goal and scope",
    "text": "Course goal and scope\nThe overarching goal of 218 is to introduce you to statistics in a hands-on way that is relevant to your major.\nSo we will focus on:\n\nstatistical thinking, study design, and data analysis\nclassical methods, mostly developed 1900-1940\ncase studies from life sciences\napplication, not theory"
  },
  {
    "objectID": "content/week1-welcome.html#materials",
    "href": "content/week1-welcome.html#materials",
    "title": "Welcome to STAT218",
    "section": "Materials",
    "text": "Materials\nWeb. All materials are hosted/linked on the course website. I won’t be using Canvas.\nBooks. Readings will be assigned from three textbooks, all available at no cost:\n\nVu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences.\nVan Belle, Fisher, Heagerty, and Lumley (2004). Biostatistics: a methodology for the health sciences.\nDouglas et al. (2023). An Introduction to R.\n\nComputing. Hosted online via posit.cloud; more details in a bit.\nOther. You’ll need a laptop, tablet with keyboard, or access to a computer. MS Word strongly recommended but not strictly necessary."
  },
  {
    "objectID": "content/week1-welcome.html#printing-slides",
    "href": "content/week1-welcome.html#printing-slides",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nOpen menu from lower left"
  },
  {
    "objectID": "content/week1-welcome.html#printing-slides-1",
    "href": "content/week1-welcome.html#printing-slides-1",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nNavigate to tools"
  },
  {
    "objectID": "content/week1-welcome.html#printing-slides-2",
    "href": "content/week1-welcome.html#printing-slides-2",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nSelect PDF export mode"
  },
  {
    "objectID": "content/week1-welcome.html#printing-slides-3",
    "href": "content/week1-welcome.html#printing-slides-3",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n - I suggest landscape layout and either 1 slide per page or per page"
  },
  {
    "objectID": "content/week1-welcome.html#preparing-for-class",
    "href": "content/week1-welcome.html#preparing-for-class",
    "title": "Welcome to STAT218",
    "section": "Preparing for class",
    "text": "Preparing for class\nEvery class meeting you should arrive having:\n\nprepared reading notes or annotations of assigned reading\nobtained a copy of the slides for taking notes\nobtained a copy of activity or lab handouts\n\nLet’s do this now for next class."
  },
  {
    "objectID": "content/week1-welcome.html#patterns",
    "href": "content/week1-welcome.html#patterns",
    "title": "Welcome to STAT218",
    "section": "Patterns",
    "text": "Patterns\nClass meeting pattern:\n\nreading quiz (not for credit)\nshort lecture, with discussion\n5min break\nlab activity in groups\ncase study presentation, with discussion\n\nWeekly pattern:\n\nclass meeting agendas updated Mondays\nassignments distributed Thursdays\n\nQuarterly pattern:\n\n[two homeworks, one test] x 3"
  },
  {
    "objectID": "content/week1-welcome.html#tentative-schedule",
    "href": "content/week1-welcome.html#tentative-schedule",
    "title": "Welcome to STAT218",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\n\n\n\n\n\nWeek\nTopics\nAssignments\n\n\n\n\n1 (1/8/24)\nIntroduction to statistical thinking and study design\nHW1\n\n\n2 (1/15/24)\nData, data types, and data collection\nNone\n\n\n3 (1/22/24)\nFoundations for inference\nHW2\n\n\n4 (1/29/24)\nInterval estimation and hypothesis tests\nTest 1\n\n\n5 (2/5/24)\nTwo-sample inference; nonparametric alternatives\nHW3\n\n\n6 (2/12/24)\nComparing means with analysis of variance\nHW4\n\n\n7 (2/19/24)\nInference for categorical data: proportions\nTest 2\n\n\n8 (2/26/24)\nInference for categorical data: chi-square tests\nHW5\n\n\n9 (3/4/24)\nSimple linear regression: model framework and estimation\nHW6\n\n\n10 (3/11/24)\nSimple linear regression: inference\nTest 3\n\n\nFinals (3/18/24)\nN/A\nOral exam"
  },
  {
    "objectID": "content/week1-welcome.html#assessments",
    "href": "content/week1-welcome.html#assessments",
    "title": "Welcome to STAT218",
    "section": "Assessments",
    "text": "Assessments\nOn the syllabus, you’ll find 11 learning outcomes labelled [L1] — [L11].\n\nGraded questions on the homeworks are matched to these outcomes.\nTest questions are also matched to outcomes.\n\nGraded questions receive a binary assessment.\n\n(S) satisfactory: outcome attained\n(NI) needs improvement: outcome not yet attained\n\nYou can submit revisions to any assignment or test for reassessment at least once.\n\nTimelines and number of attempts differ by assignment type (homework or test)."
  },
  {
    "objectID": "content/week1-welcome.html#final-oral-exam",
    "href": "content/week1-welcome.html#final-oral-exam",
    "title": "Welcome to STAT218",
    "section": "Final oral exam",
    "text": "Final oral exam\nYou’ll be responsible for identifying a case study in your field of study that uses methods from the class:\n\nin pairs, otherwise private\n5min presentation\nshort Q&A\nscheduled during final exam period\n\nYour assessment for this will pertain only to [L11]: applying methods from the course to your major field of study."
  },
  {
    "objectID": "content/week1-welcome.html#grades",
    "href": "content/week1-welcome.html#grades",
    "title": "Welcome to STAT218",
    "section": "Grades",
    "text": "Grades\nAt the end of the quarter, you will receive for each outcome L1 — L11:\n\na score: proportion of questions that received a satisfactory assessment\nan evaluation: whether that outcome was fully met, partly met, or not met\n\nnot met: under 50%\npartly met: 50% — 80%\nfully met: 80% or better\n\n\nYour letter grade will be based on the tally of fully met and partly met outcomes."
  },
  {
    "objectID": "content/week1-welcome.html#letter-grades",
    "href": "content/week1-welcome.html#letter-grades",
    "title": "Welcome to STAT218",
    "section": "Letter grades",
    "text": "Letter grades\nDetails are in the syllabus, but:\n\nMust at partly or fully meet at least…\n\n6/11 to pass\n8/11 for B or better\n10/11 for A or better\n\nSubject to those thresholds, letter grade is determined by the number of outcomes fully met"
  },
  {
    "objectID": "content/week1-welcome.html#grading-example",
    "href": "content/week1-welcome.html#grading-example",
    "title": "Welcome to STAT218",
    "section": "Grading example",
    "text": "Grading example\n\n\n\n\n\nLearning outcome\nScore\nEvaluation\n\n\n\n\nL1\n83%\nFully met\n\n\nL2\n87%\nFully met\n\n\nL3\n100%\nFully met\n\n\nL4\n64%\nPartly met\n\n\nL6\n86%\nFully met\n\n\nL7\n91%\nFully met\n\n\nL8\n89%\nFully met\n\n\nL9\n57%\nPartly met\n\n\nL10\n41%\nNot met\n\n\nL11\n100%\nFully met\n\n\n\n\nYour grade might look something like this:\n[10 partly or fully met] + [7 fully met] ➞ B+"
  },
  {
    "objectID": "content/week1-welcome.html#select-policies",
    "href": "content/week1-welcome.html#select-policies",
    "title": "Welcome to STAT218",
    "section": "Select policies",
    "text": "Select policies\n\ncollaboration encouraged on homeworks; not allowed on tests\nattendance expected; more than two unexcused absences may negatively impact grade\nexpect to allocate 12-16hr/wk, including class meetings\ntwo free late assignment submissions, anytime and without notice, up to one week from deadline"
  },
  {
    "objectID": "content/week1-welcome.html#office-hours-email",
    "href": "content/week1-welcome.html#office-hours-email",
    "title": "Welcome to STAT218",
    "section": "Office hours & email",
    "text": "Office hours & email\nOffice hours are 12-2 M-W in 25-236\n\nSchedule 10min time slots via Calendly (see syllabus for link)\n\nUsually I get to email quickly, but allow at least 48 (weekday) hours\n\nIf you haven’t heard back in a week, that means I missed it — send me a reminder!"
  },
  {
    "objectID": "content/week1-welcome.html#working-groups",
    "href": "content/week1-welcome.html#working-groups",
    "title": "Welcome to STAT218",
    "section": "Working groups",
    "text": "Working groups\nFor labs and in class activities, you’ll work in groups. Let’s form those now.\nIcebreakers:\n\nName, major, class standing\nHometown\nOne personal interest"
  },
  {
    "objectID": "content/week1-welcome.html#introduction-to-posit.cloud",
    "href": "content/week1-welcome.html#introduction-to-posit.cloud",
    "title": "Welcome to STAT218",
    "section": "Introduction to posit.cloud",
    "text": "Introduction to posit.cloud\nYou’ll be learning some basics in R in this class. It’s not a programming class, so the goal is to learn simply to implement the methods we’re covering.\nWhy R?\n\nFree\nOpen source — rapid development for new methods\nWidely used\n\nLet’s take some time to introduce this environment.\nGo to: course webpage &gt; syllabus &gt; materials. Then look for:\n\nTake a moment to create an account."
  },
  {
    "objectID": "content/week1-welcome.html#an-example-project",
    "href": "content/week1-welcome.html#an-example-project",
    "title": "Welcome to STAT218",
    "section": "An example project",
    "text": "An example project\n\n\nMake sure the class workspace “stat218” is highlighted at left. If “Your Workspace” is highlighted, you won’t see the example assignment.\n\n\n\nClick on the example assignment, then wait.\n\nOnce everyone is ready, we’ll have a look at the example files together."
  },
  {
    "objectID": "content/week1-welcome.html#orientation-checklist",
    "href": "content/week1-welcome.html#orientation-checklist",
    "title": "Welcome to STAT218",
    "section": "Orientation checklist",
    "text": "Orientation checklist\n\nOverall layout of RStudio: file navigator, console, file editor.\nREADME.md — keep this open for reference as we go.\n\nAs an aside: whenever you see a file called README, you should read it.\n\nexample-script.R is an R script\n\nhow to execute lines of code\ndata import on L18\nmake and save changes\n\nexample-doc.qmd is a quarto doc\n\nYAML header\nhow to render\nmake and save changes\n\nFiles you should ignore"
  },
  {
    "objectID": "content/week1-welcome.html#for-next-time",
    "href": "content/week1-welcome.html#for-next-time",
    "title": "Welcome to STAT218",
    "section": "For next time",
    "text": "For next time\nYou’ll read about study designs. As you’re reading…\nConsider: there is broad agreement that smoking causes cancer, but not all smokers develop cancer.\n\nWhat then do we mean by “cause” when we say that smoking causes cancer?\nWhat do you imagine the evidence might be for this claim?\nHow would you design a study to test the hypothesis that smoking causes cancer?\n\nWhich of the designs mentioned in the reading seem potentially applicable?\nPick one design that seems best suited to the question. If you were to carry out a study according to that design, what would constitute evidence for or against the hypothesis?\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics for Life Sciences",
    "section": "",
    "text": "Announcements\n\n\n\n\nOutlines updated for week 7\nPlease complete feedback survey by Thursday 2/22: [2pm section] [4pm section]\n\n\n\nThis is the class website for Applied Statistics for Life Sciences (STAT 218) offered at Cal Poly in Winter 2024 for sections 05-8273 (TR 2:10pm – 4:00pm) and 06-8274 (TR 4:10pm – 6:00pm). Please note that this site does not pertain to other sections of STAT 218 (of which there are several).\nMaterial posted on this site is intended for use by currently enrolled students in the above-mentioned sections for educational purposes only, and is subject to copyright. Please do not reproduce any material on this site without permission."
  },
  {
    "objectID": "content/week6-power.html#todays-agenda",
    "href": "content/week6-power.html#todays-agenda",
    "title": "Power analyses",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz [2pm section] [4pm section]\nType II errors: review and further exploration\n[lecture/lab] Power analysis\n\nSample size calculations\nPost-hoc power analyses"
  },
  {
    "objectID": "content/week6-power.html#from-last-time",
    "href": "content/week6-power.html#from-last-time",
    "title": "Two sample inference",
    "section": "From last time",
    "text": "From last time\n\n\nSuppose that for the cloud data you’d performed a two-sided test: \\[H_0: \\mu_\\text{seeded} = \\mu_\\text{unseeded}\\] \\[H_A: \\mu_\\text{seeded} \\neq \\mu_\\text{unseeded}\\]\n\n\n\n    Welch Two Sample t-test\n\ndata:  Rainfall by Treatment\nt = 1.9982, df = 33.855, p-value = 0.05377\nalternative hypothesis: true difference in means between group Seeded and group Unseeded is not equal to 0\n95 percent confidence interval:\n  -4.764295 559.556603\nsample estimates:\n  mean in group Seeded mean in group Unseeded \n              441.9846               164.5885 \n\n\n\nAlmost below the significance threshold, but not quite.\n\nfail to reject null hypothesis of no difference\nbut point estimate for difference is \\(\\bar{x} - \\bar{y}\\) = 277.4\n\n\nDid we make a type II error? What is the likelihood your test would detect a difference of 277?"
  },
  {
    "objectID": "content/week6-power.html#power-analysis",
    "href": "content/week6-power.html#power-analysis",
    "title": "Power analyses",
    "section": "Power analysis",
    "text": "Power analysis\n\nIf a test lacks power, failure to reject \\(H_0\\) is not a particularly strong result. How much data do you need to collect in order to detect a difference of \\(\\delta\\)?\n\n\n\nThe statistical power of a test captures how often it detects a specified alternative.\n\ndefined as \\(\\beta = (1 - \\text{type II error rate})\\)\nmeasures how often the test correctly rejects\nvalue depends on…\n\nmagnitude of difference between null value and true value of parameter\nsignificance level\nsample size\n\n\n\n\npower.t.test(power = 0.95, \n             delta = 0.5, \n             sig.level = 0.05, \n             type = 'two.sample',\n             alternative = 'two.sided')\n\n\n     Two-sample t test power calculation \n\n              n = 104.928\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\\(\\Rightarrow\\) need 105 observations in each group to detect a difference of 0.5 standard deviations at level 0.05 with type II error rate 5% or less\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week6-power.html#statistical-power",
    "href": "content/week6-power.html#statistical-power",
    "title": "Power analyses",
    "section": "Statistical power",
    "text": "Statistical power\nThe power of a test refers to its true rejection rates across alternatives and is defined as: \\[\\beta(\\delta) = \\underbrace{(1 - \\text{type II error rate}_\\delta )}_\\text{correct decision rate when null is false}\\]\nPower is often interpreted as a detection rate for a specified alternative \\(\\delta\\):\n\nhigh type II error \\(\\longrightarrow\\) low power \\(\\longrightarrow\\) low detection rate\nlow type II error \\(\\longrightarrow\\) high power \\(\\longrightarrow\\) high detection rate\n\n\nIn general tests have low power for alternatives close to the null value (where “close” is relative to sampling variability).\n\nTheory allows a direct calculation of power, given sample size, significance level, population standard deviation, and population difference in means."
  },
  {
    "objectID": "content/week6-power.html#a-thought-experiment",
    "href": "content/week6-power.html#a-thought-experiment",
    "title": "Power analyses",
    "section": "A thought experiment",
    "text": "A thought experiment\n\n\nSuppose that for the cloud data you’d performed a two-sided test: \\[H_0: \\mu_\\text{seeded} = \\mu_\\text{unseeded}\\] \\[H_A: \\mu_\\text{seeded} \\neq \\mu_\\text{unseeded}\\]\n\n\n\n    Welch Two Sample t-test\n\ndata:  Rainfall by Treatment\nt = 1.9982, df = 33.855, p-value = 0.05377\nalternative hypothesis: true difference in means between group Seeded and group Unseeded is not equal to 0\n95 percent confidence interval:\n  -4.764295 559.556603\nsample estimates:\n  mean in group Seeded mean in group Unseeded \n              441.9846               164.5885 \n\n\nAlmost below the significance threshold but not quite.\n\n\nThe data do not provide sufficient evidence to reject the null hypothesis that seeding has no effect relative to the alternative of an increase or decrease in mean rainfall due to seeding (T = 1.998 on 33.86 degrees of freedom, p = 0.05377).\n\nThe point estimate for the difference is 277.4 acre-feet.\n\nThe test says this observed difference could plausibly be due to sampling variation\nBut is it also plausible that our test result is wrong if the difference is real?"
  },
  {
    "objectID": "content/week6-power.html#factors-influencing-power",
    "href": "content/week6-power.html#factors-influencing-power",
    "title": "Power analyses",
    "section": "Factors influencing power",
    "text": "Factors influencing power\nType II error rates, and therefore statistical power, depend on several factors.\n\n\n\nCondition\nIncrease power if…\n\n\n\n\nMagnitude of difference \\(\\delta\\)\nlarger\n\n\nSignificance level \\(\\alpha\\)\nlarger\n\n\nSample sizes\nlarger\n\n\nPopulation variability \\(\\sigma\\)\nsmaller"
  },
  {
    "objectID": "content/week6-power.html#type-ii-error-rates",
    "href": "content/week6-power.html#type-ii-error-rates",
    "title": "Power analyses",
    "section": "Type II error rates",
    "text": "Type II error rates\n\nRecall: a type II error is failing to reject a false null hypothesis.\n\nIn the context of two-sample inference a type II error occurs when:\n\nthe true difference is \\(\\delta \\neq 0\\)\nwe test and fail to reject \\(H_0: \\delta \\neq 0\\)\n\nThe type II error rate depends on both known and unknown factors:\n\n[unknown] magnitude of \\(\\delta\\)\n[unknown] population variability \\(\\sigma\\)\n[known] significance level\n[known] sample sizes\n\nWhat was the type II error rate for the cloud seeding test?"
  },
  {
    "objectID": "content/week6-power.html#simulating-type-ii-errors",
    "href": "content/week6-power.html#simulating-type-ii-errors",
    "title": "Power analyses",
    "section": "Simulating type II errors",
    "text": "Simulating type II errors\n\n\nSummary stats for cloud data:\n\n\n\n\n\n\n\n\n\n\n\nTreatment\nmean\nsd\nn\n\n\n\n\nSeeded\n442\n650.8\n26\n\n\nUnseeded\n164.6\n278.4\n26\n\n\n\n\n\nWe can approximate the type II error rate by:\n\nsimulating datasets with matching summary statistics\nperforming two-sided tests of no difference\ncomputing the proportion of fail-to-reject decisions\n\n\ntype2sim(delta = 277, n = 26, sd = 650, alpha = 0.05)\n\n[1] 0.689\n\n\n\\(\\Rightarrow\\) if the true difference were exactly as estimated, our test result would be incorrect nearly 70% of the time!\n\nWhat would happen to the error rate if…\n\nthe true difference delta were bigger?\nthe significance level alpha were smaller?\nthe sample size n was larger?\nthe variability of rainfall sd were less?"
  },
  {
    "objectID": "content/week6-power.html#simulating-type-ii-errors-1",
    "href": "content/week6-power.html#simulating-type-ii-errors-1",
    "title": "Power analyses",
    "section": "Simulating type II errors",
    "text": "Simulating type II errors\n\n\nOpen the lab and use the simulation function type2sim to fill in the table by changing arguments accordingly.\n\ntry a few magnitudes of difference for each scenario\nrepeat runs for each setting once or twice to confirm effect\n\n\n\n\n\nFactor\nChange\nEffect on error rate\n\n\n\n\ntrue difference in means\nlarger\n\n\n\ntrue difference in means\nsmaller\n\n\n\npopulation variability\nlarger\n\n\n\npopulation variability\nsmaller\n\n\n\nsample size\nlarger\n\n\n\nsample size\nsmaller\n\n\n\nsignificance level\nlarger\n\n\n\nsignificance level\nsmaller\n\n\n\n\n\n\n\nBased on your explorations, do you think our original test decision was erroneous?"
  },
  {
    "objectID": "content/week6-power.html#factors-affecting-power",
    "href": "content/week6-power.html#factors-affecting-power",
    "title": "Power analyses",
    "section": "Factors affecting power",
    "text": "Factors affecting power\n\nPower depends on all the same factors as type II error rates\n\n\n\n\n\n\n\n\n\n\nFactor\nChange\nEffect on error rate\nEffect on power\n\n\n\n\ntrue difference in means\nlarger\n\n\n\n\ntrue difference in means\nsmaller\n\n\n\n\npopulation variability\nlarger\n\n\n\n\npopulation variability\nsmaller\n\n\n\n\nsample size\nlarger\n\n\n\n\nsample size\nsmaller\n\n\n\n\nsignificance level\nlarger\n\n\n\n\nsignificance level\nsmaller"
  },
  {
    "objectID": "content/week6-power.html#extra-stuff",
    "href": "content/week6-power.html#extra-stuff",
    "title": "Power analyses",
    "section": "extra stuff",
    "text": "extra stuff\n\n\n\nWhat does it mean to ‘detect’ a difference?\n\nWe say that a test detects a difference of \\(\\delta \\neq 0\\) if:\n\nthe true difference is \\(\\delta\\) and\nthe test rejects a null hypothesis excluding \\(\\delta\\) in favor of an alternative including the true value \\(\\delta\\)\n\n\n\n\n\nIf a test fails to detect such a difference, the test has made a type II error.\n\nThe power of a test refers to its detection rate.\n\nLow type II error \\(\\longrightarrow\\) high detection rate\nHigh type II error \\(\\longrightarrow\\) low detection rate\n\n\n\n\n\n\n\nImportant\n\n\nIf a test has low power, failure to reject \\(H_0\\) is expected, and therefore a weak result."
  },
  {
    "objectID": "content/week6-power.html#two-power-related-questions",
    "href": "content/week6-power.html#two-power-related-questions",
    "title": "Power analyses",
    "section": "Two power-related questions",
    "text": "Two power-related questions\n\n[post hoc analysis] How much power would the test I conducted have if the true difference and population variability are exactly equal to my estimates?\n[study design] How much data do I need to collect to detect a difference of \\(\\delta\\) using a particular test and assuming a specified amount of population variability?"
  },
  {
    "objectID": "content/week6-power.html#two-common-power-analyses",
    "href": "content/week6-power.html#two-common-power-analyses",
    "title": "Power analyses",
    "section": "Two common power analyses",
    "text": "Two common power analyses\n\n\nPost hoc analysis: how much power does the test I conducted have if the true difference is exactly equal to my estimate?\nHelps to interpret negative results:\n\nlow power \\(\\rightarrow\\) failure to reject was likely\nhigh power \\(\\rightarrow\\) failure to reject was not likely\n\n\n\n\n\n\n\nDon’t over-interpret post-hoc analyses\n\n\nFailure to reject using a well-powered test does not confirm the null hypothesis.\n\n\n\n\nSample size determination: how much data do I need to collect to detect a difference of \\(\\delta\\) using a particular test?\nHelps avoid two potential issues:\n\ntoo little data \\(\\rightarrow\\) study not likely to yield significant results\ntoo much data \\(\\rightarrow\\) study is too likely to yield significant results"
  },
  {
    "objectID": "content/week6-power.html#post-hoc-analysis",
    "href": "content/week6-power.html#post-hoc-analysis",
    "title": "Power analyses",
    "section": "Post-hoc analysis",
    "text": "Post-hoc analysis\n\nCan we estimate the power of a test we already performed?\n\n\n\nFeasible if we assume (a) a population standard deviation and (b) test conditions are met.\nFor the cloud seeding test:\n\npower.t.test(delta = 250, # magnitude of difference\n             sd = 650, # largest population SD\n             n = 26, # smallest sample size\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n\n\n     Two-sample t test power calculation \n\n              n = 26\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.2743235\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nFor a conservative estimate, use:\n\nsmallest of the two sample sizes\nlargest of the two standard deviations\nsmaller difference than observed\n\n\n\\(\\Longrightarrow\\) our test would only reject in favor of a difference of the observed magnitude about 27% of the time\n\nFailure to reject doesn’t strongly rule out the alternative."
  },
  {
    "objectID": "content/week6-power.html#sample-size-calculatioon",
    "href": "content/week6-power.html#sample-size-calculatioon",
    "title": "Power analyses",
    "section": "Sample size calculatioon",
    "text": "Sample size calculatioon\n\nIf you were (re)designing the study, how much data should you collect to detect a specified effect size?\n\n\n\nTo detect a difference of 250 or more due to cloud seeding with power 0.9:\n\npower.t.test(power = 0.9, # target power level\n             delta = 250, # smallest difference\n             sd = 650, # largest population SD\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n\n\n     Two-sample t test power calculation \n\n              n = 143.0276\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nFor a conservative estimate, use:\n\noverestimate of the larger of the two standard deviations\nminimum difference of interest\n\n\n\\(\\Longrightarrow\\) we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time"
  },
  {
    "objectID": "content/week6-power.html#your-turn",
    "href": "content/week6-power.html#your-turn",
    "title": "Power analyses",
    "section": "Your turn",
    "text": "Your turn\n\n\nConsider testing whether body temperature differs by sex.\nSummary stats and test result:\n\n\n\n\n\n\n\n\n\n\n\nsex\nmean\nsd\nn\n\n\n\n\nfemale\n98.66\n0.9929\n19\n\n\nmale\n98.17\n0.7876\n20\n\n\n\n\n\n\nt.test(body.temp ~ sex, data = temps)\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 1.7118, df = 34.329, p-value = 0.09595\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.09204497  1.07783444\nsample estimates:\nmean in group female   mean in group male \n            98.65789             98.16500 \n\n\n\nConduct a post-hoc power analysis to answer the following questions:\n\nWhat is the power of the test if the true difference and population variability are exactly as estimated?\nWhat is the power of the test if the true difference is 0.5 but we underestimated the population variability?\nHow much power would we have gained using a directional test?\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week6-power.html#your-turn-post-hoc-analysis",
    "href": "content/week6-power.html#your-turn-post-hoc-analysis",
    "title": "Power analyses",
    "section": "Your turn: post-hoc analysis",
    "text": "Your turn: post-hoc analysis\n\n\nConsider testing whether body temperature differs by sex.\nSummary stats and test result:\n\n\n\n\n\n\n\n\n\n\n\nsex\nmean\nsd\nn\n\n\n\n\nfemale\n98.66\n0.9929\n19\n\n\nmale\n98.17\n0.7876\n20\n\n\n\n\n\n\nt.test(body.temp ~ sex, data = temps)\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 1.7118, df = 34.329, p-value = 0.09595\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.09204497  1.07783444\nsample estimates:\nmean in group female   mean in group male \n            98.65789             98.16500 \n\n\n\nAssume the true difference is actually 0.5 °F. Determine the power of the test above when:\n\nPopulation SD is the smaller of the two groups\nPopulation SD is the larger of the two groups\nA one-sided test is used instead\n\n\nBased on your answers, do you think the negative test result rules out the alternative?"
  },
  {
    "objectID": "content/week6-power.html#the-equal-variance-t-test",
    "href": "content/week6-power.html#the-equal-variance-t-test",
    "title": "Power analyses",
    "section": "The equal-variance \\(t\\)-test",
    "text": "The equal-variance \\(t\\)-test\nIf it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power by using a different standard error:\n\\[SE_\\text{pooled}(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{\\color{red}{s_p^2}}{n_x} + \\frac{\\color{red}{s_p^2}}{n_y}}\n\\quad\\text{where}\\quad \\color{red}{s_p} = \\underbrace{\\sqrt{\\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\\text{weighted average of } s_x^2 \\;\\&\\; s_y^2}\\]\nIn the case of the body temperature data, \\(s_p\\) = 0.8934. Check:\n\nHow much power do we gain if we assume a common SD of 0.89?\nDoes it change the outcome of the test (add var.equal = T)?\n\n\nProduces minimal gains and inflates type I error if not warranted, so better avoided unless you have a small sample size"
  },
  {
    "objectID": "content/week6-power.html#your-turn-sample-size-calculation",
    "href": "content/week6-power.html#your-turn-sample-size-calculation",
    "title": "Power analyses",
    "section": "Your turn: sample size calculation",
    "text": "Your turn: sample size calculation\nSuppose you are designing a follow-up study and wish to detect a difference of 0.4 °F at least 70% of the time. You know women have slightly higher body temperatures than men on average.\n\n\n\n\n\n\n\n\nKnown direction?\nPopulation SD\nMinimum \\(n\\)\n\n\n\n\nNo\nlarger of prior estimates\n\n\n\nNo\n1.2 times larger than larger of prior estimates\n\n\n\nYes\nlarger of prior estimates\n\n\n\nYes\n1.2 times larger than larger of prior estimates\n\n\n\n\n\nIf it costs $10 per participant to run the study, what’s the best power achievable within a $2K budget for the target detection magnitude?"
  },
  {
    "objectID": "content/week6-power.html#rank-based-inference",
    "href": "content/week6-power.html#rank-based-inference",
    "title": "Power analyses",
    "section": "Rank-based inference",
    "text": "Rank-based inference\nWhat do you do if the test assumptions aren’t met?\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week6-power.html#power-curve",
    "href": "content/week6-power.html#power-curve",
    "title": "Power analyses",
    "section": "Power curve",
    "text": "Power curve\n\nPower is usually construed as a curve depending on the true difference.\n\n\n\nPower curve for the test exactly as performed with the cloud seeding data:\n\n\n\n\n\n\nAll other attributes of the test are fixed to approximate the test performed:\n\nsample size \\(n = 26\\)\nsignificance level \\(\\alpha = 0.05\\)\npopulation standard deviation \\(\\sigma = 650\\) (larger of two group estimates)"
  },
  {
    "objectID": "content/week6-power.html#power-curves",
    "href": "content/week6-power.html#power-curves",
    "title": "Power analyses",
    "section": "Power curves",
    "text": "Power curves\n\nPower is usually construed as a curve depending on the true difference.\n\n\n\nPower curve for the test exactly as performed with the cloud seeding data:\n\n\n\n\n\n\n\n\n\n\nAll other attributes of the test are fixed to approximate the test performed:\n\nsample size \\(n = 26\\)\nsignificance level \\(\\alpha = 0.05\\)\npopulation standard deviation \\(\\sigma = 650\\) (larger of two group estimates)"
  },
  {
    "objectID": "content/week6-power.html#powersample-size-curves",
    "href": "content/week6-power.html#powersample-size-curves",
    "title": "Power analyses",
    "section": "Power/sample size curves",
    "text": "Power/sample size curves"
  },
  {
    "objectID": "content/week6-power.html#power-vs.-sample-size-curves",
    "href": "content/week6-power.html#power-vs.-sample-size-curves",
    "title": "Power analyses",
    "section": "Power vs. sample size curves",
    "text": "Power vs. sample size curves\n\n\nMinimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:\n\n\n\n\n\n\n\n\n\nAssumes \\(\\sigma = 1.2\\) for a conservative estimate.\n\nThe best power achievable within budget for the target detection range is 0.7593159.\n\nincreasing power to 0.8 will require n = 112 per group\n\n$240 over budget\n\nincreasing power to 0.9 will require n = 155 per group\n\n$1050 over budget\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week6-power.html#sample-size-calculation",
    "href": "content/week6-power.html#sample-size-calculation",
    "title": "Power analyses",
    "section": "Sample size calculation",
    "text": "Sample size calculation\n\nIf you were (re)designing the study, how much data should you collect to detect a specified effect size?\n\n\n\nTo detect a difference of 250 or more due to cloud seeding with power 0.9:\n\npower.t.test(power = 0.9, # target power level\n             delta = 250, # smallest difference\n             sd = 650, # largest population SD\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n\n\n     Two-sample t test power calculation \n\n              n = 143.0276\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nFor a conservative estimate, use:\n\noverestimate of the larger of the two standard deviations\nminimum difference of interest\n\n\n\\(\\Longrightarrow\\) we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time"
  },
  {
    "objectID": "content/week6-power.html#power-curve-for-body-temps",
    "href": "content/week6-power.html#power-curve-for-body-temps",
    "title": "Power analyses",
    "section": "Power curve for body temps",
    "text": "Power curve for body temps\n\n\nAssuming we underestimated the population standard deviation a bit, the power curve for a one-sided test would look like this:\n\n\n\n\n\n\n\n\n\n\nAssumptions:\n\nn = 19 per group\n\\(\\sigma = 1.2\\) per group\nsignificance level \\(\\alpha = 0.05\\)\none-sided test\n\n\nFairly low power for alternatives near the estimated difference (dashed line), so failure to reject doesn’t strongly rule out the alternative."
  },
  {
    "objectID": "content/labs/lab9-power.html",
    "href": "content/labs/lab9-power.html",
    "title": "Lab 9: Power analysis",
    "section": "",
    "text": "The objective of this lab is to explore statistical power. Your goals are to:\nWe won’t use much data for this lab, but the cloud seeding and body temperature data will be referenced in passing. Run these lines to load the datasets:\nlibrary(Sleuth3)\nlibrary(tidyverse)\ncloud &lt;- case0301\nload('data/temps.RData')\nIn the first part of the lab, we’ll aim to simply understand type II errors a little better. This will be achieved through simulation. In the second part of the lab, we’ll translate some of the insights from the first part into performing basic power analyses."
  },
  {
    "objectID": "content/labs/lab9-power.html#simulating-type-ii-errors",
    "href": "content/labs/lab9-power.html#simulating-type-ii-errors",
    "title": "Lab 9: Power analysis",
    "section": "Simulating type II errors",
    "text": "Simulating type II errors\n\ntype2sim &lt;- function(delta, n, sd, alpha, nsim = 1000){\n  # simulate nsim tests by...\n  sim.pvals &lt;- sapply(1:nsim, function(i){\n    # draw sample with true group difference of delta\n    samp &lt;- data.frame(variable = c(rnorm(n, mean = 0, sd = sd), rnorm(n, mean = delta, sd = sd)),\n                       group = rep(1:2, each = n))\n    # perform test and compute p value\n    pval &lt;- t.test(variable ~ group, data = samp, mu = 0, alternative = 'two.sided')$p.value\n    return(pval)\n  })\n  # compute proportion of tests that failed to reject (made a type ii error)\n  err.rate &lt;- mean(sim.pvals &gt; alpha)\n  return(err.rate)\n}\n\nsave(type2sim, file = 'type2sim.RData')\n\nYour first task in this lab is to explore the impact of several factors on type II error rates for two-sample inference on means using simulation. These factors are:\n\ntrue difference in means (\\(\\delta\\))\npopulation variability (\\(\\sigma\\))\nsample size per group (\\(n\\))\nsignificance level (\\(\\alpha\\))\n\nWe will base our simulations on the two-sided test for the cloud dataset.\n\n# two-sided test\ncloud.test.out &lt;- t.test(Rainfall ~ Treatment, data = cloud, mu = 0, alternative = 'two.sided')\n\n# estimated difference in means\ndiff(cloud.test.out$estimate)\n\nmean in group Unseeded \n             -277.3962 \n\n# population SDs (don't worry about syntax here)\ncloud |&gt; group_by(Treatment) |&gt; summarize(sd(Rainfall))\n\n# A tibble: 2 × 2\n  Treatment `sd(Rainfall)`\n  &lt;fct&gt;              &lt;dbl&gt;\n1 Seeded              651.\n2 Unseeded            278.\n\n\nThe following function will simulate 1000 datasets assuming:\n\na true difference in means of delta\na sample size per group of n\na population standard deviation per group of sd\n\nFor each dataset, a two-sided test is computed at level alpha. The function returns the proportion of tests that failed to reject.\n\n# load simulation function\nload('type2sim.RData')\n\n# simulation-based estimate of type 2 error\ntype2sim(delta = 277, n = 26, sd = 650, alpha = 0.05)\n\n[1] 0.674\n\n\nThis approximates the type II error rate for the test.\n\n\n\n\n\n\nYour turn\n\n\n\nYour objective is to explore the impact of population and test conditions on type II error rates. You’ll do this by changing the arguments of the type2sim() function.\nCarry out each change in turn and record the results. Repeat your runs a few times, and try a few different magnitudes each time.\n\n# store 'baseline' type ii error rate\nestimated.error &lt;- type2sim(delta = 277, n = 26, sd = 650, alpha = 0.05)\n\n# increase delta\ntype2sim(delta = ..., n = 26, sd = 650, alpha = 0.05) - estimated.error\n\n# decrease delta\ntype2sim(delta = ..., n = 26, sd = 650, alpha = 0.05) - estimated.error\n\n# increase sd\ntype2sim(delta = 277, n = 26, sd = ..., alpha = 0.05) - estimated.error\n\n# decrease sd\ntype2sim(delta = 277, n = 26, sd = ..., alpha = 0.05) - estimated.error\n\n# increase sample size\ntype2sim(delta = 277, n = ..., sd = 650, alpha = 0.05) - estimated.error\n\n# decrease sample size\ntype2sim(delta = 277, n = ..., sd = 650, alpha = 0.05) - estimated.error\n\n# increase significance level\ntype2sim(delta = 277, n = 26, sd = 650, alpha = ...) - estimated.error\n\n# decrease significance level\ntype2sim(delta = 277, n = 26, sd = 650, alpha = ...) - estimated.error"
  },
  {
    "objectID": "content/labs/lab9-power.html#power-analyses",
    "href": "content/labs/lab9-power.html#power-analyses",
    "title": "Lab 9: Power analysis",
    "section": "Power analyses",
    "text": "Power analyses\nThe statistical power of a test is the “complement” of a type II error rate: \\[\\beta = 1 - \\text{type II error}\\] It depends on all of the same factors you explored above that affect type II error rates. Perhaps most importantly, the power of a test is specific to an alternative. In the case of two-sample inference, we can think of power as a function of the true difference.\nIn R, theory-based power calculations for common tests are implemented in a user-friendly way. An estimate of the power for the cloud seeding test can be calculated as follows:\n\n# minimal arguments\npower.t.test(delta = 277, n = 26, sd = 650)\n\n# verbose\npower.t.test(delta = 277, n = 26, sd = 650, sig.level = 0.05, type = 'two.sample', alternative = 'two.sided')\n\nIt is common to visualize power curves that show statistical power as a function of the alternative value of the parameter of interest, keeping all other population and test attributes fixed. For example, again using conditions similar to the cloud seeding test:\n\n# plot power curve for cloud seeding test \ncurve(power.t.test(delta = x, n = 26, sd = 650)$power, \n      from = 0, to = 1000, xlab = 'true difference', ylab = 'power')\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nExperiment with the test conditions and examine their impact on the power curve. Try:\n\nincreasing/decreasing sample size\nincreasing/decreasing population standard deviation\nincreasing/decreasing significance level\n\n\n# plot power curve for cloud seeding test \ncurve(power.t.test(delta = x, n = 26, sd = 650, sig.level = 0.05)$power, \n      from = 0, to = 1000, xlab = 'true difference', ylab = 'power')\n\n\n\n\n\n\n\nPost hoc power analysis\nWhat we’ve done above in estimating the power of a test we’ve already performed by assuming population conditions based on estimates and fixing test conditions is an example of a post-hoc power analysis. The goal of a post-hoc analysis is to estimate the power of a test already performed. This is usually only done if the test produces a negative result (i.e., fails to reject \\(H_0\\)).\nLet’s consider a different example and have you carry out your own post-hoc analysis. The following is a test of whether body temperature differs by sex:\n\n# summary stats (don't worry about syntax here)\ntemps |&gt; group_by(sex) |&gt; summarize(mean = mean(body.temp), sd = sd(body.temp))\n\n# A tibble: 2 × 3\n  sex     mean    sd\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 female  98.7 0.993\n2 male    98.2 0.788\n\n# two-sided test\nt.test(body.temp ~ sex, data = temps, mu = 0, alternative = 'two.sided')\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 1.7118, df = 34.329, p-value = 0.09595\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.09204497  1.07783444\nsample estimates:\nmean in group female   mean in group male \n            98.65789             98.16500 \n\n\nThe hypotheses for this test are: \\[H_0: \\mu_M = \\mu_F\\] \\[H_A: \\mu_M \\neq \\mu_F\\]\nNotice the moderate \\(p\\) value: the test fails to reject \\(H_0\\), but just barely. This type of result is usually what prompts a post-hoc analysis. The goal is to answer the question: how strongly does the result rule out the alternative?\n\n\n\n\n\n\nYour turn\n\n\n\nUse power.t.test with conditions based on the sample estimates. Assume the true difference is actually 0.5 °F. Determine the power of the test above when:\n\nPopulation SD is the smaller of the two groups\nPopulation SD is the larger of the two groups\nA one-sided test is used instead\n\n\n# power with smaller sd\npower.t.test(n = ..., delta = ..., sd = ..., sig.level = ..., alternative = ...)\n\n# power with larger sd\npower.t.test(n = ..., delta = ..., sd = ..., sig.level = ..., alternative = ...)\n\n# power for one-sided test\npower.t.test(n = ..., delta = ..., sd = ..., sig.level = ..., alternative = ...)\n\nBased on your answers, discuss with your group/neighbor whether you think the negative test result rules out the alternative hypothesis of a difference by sex.\n\n\n\n\nPower analysis for study design\nThe other kind of power analysis relates to study design, and seeks to answer the question: how much data do I need to collect to achieve a specified power?\nThe same function power.t.test() can be used with a power argument instead of an n argument to calculate the sample size that achieves a particular power for a particular difference, under assumptions about population variability and test conditions.\nFor the cloud seeding test, if we wished to detect a difference of at least 250 with power 0.9 using the same test and supposing that the population variability is about what we observed in the previous study, the power calculation for sample size determination would look as follows:\n\npower.t.test(power = 0.9, delta = 250, sd = 650, sig.level = 0.05, alternative = 'two.sided')\n\n\n     Two-sample t test power calculation \n\n              n = 143.0276\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThus, we would need 144 observations in each group.\n\n\n\n\n\n\nYour turn\n\n\n\nContinuing with the example of body temperatures, suppose you are designing a follow-up study and wish for the study to be sensitive to temperature differences of 0.4 °F.\nSuppose also that you have a $2K budget and data acquisition costs are estimated at $10 per study subject. Your task is to determine the best power achievable within budget and assess the feasibility of the study objectives.\nDo a sample size power calculation to determine how much data you would need to collect to achieve power 0.7 if…\n\nYou use a two-sided test and assume population standard deviation is as estimated above.\nYou use a two-sided test and assume population standard deviation is 1.2 times larger than estimated above.\nYou use a one-sided test and assume population standard deviation is as estimated above.\nYou use a one-sided test and assume population standard deviation is 1.2 times larger than estimated above.\n\n\n# 1. two sided test, population sd = 1\npower.t.test(power = ..., delta = ..., sd = ..., sig.level = ..., alternative = ...)\n\n# 2. two sided test, population sd = 1.2\npower.t.test(power = ..., delta = ..., sd = ..., sig.level = ..., alternative = ...)\n\n# 3. one sided test, population sd = 1\npower.t.test(power = ..., delta = ..., sd = ..., sig.level = ..., alternative = ...)\n\n# 4. one sided test, population sd = 1.2\npower.t.test(power = ..., delta = ..., sd = ..., sig.level = ..., alternative = ...)"
  },
  {
    "objectID": "content/week6-twosample.html",
    "href": "content/week6-twosample.html",
    "title": "Two sample inference",
    "section": "",
    "text": "Reading quiz [2pm section] [4pm section]\n[lecture/lab] Two-sample inference for population means\n\nPaired data\nIndependent data\n\n[if time] Introduction to power analysis"
  },
  {
    "objectID": "content/hw/hw3.html",
    "href": "content/hw/hw3.html",
    "title": "Homework 3: One- and two-sample inference",
    "section": "",
    "text": "Instructions: type up your answers and submit your work electronically via Gradescope. Questions with a learning outcome indicated in brackets will be evaluated for credit; other questions are provided for additional practice. You are expected to answer all questions. Note that an R project with datasets and prompts is provided on the class posit.cloud workspace. Please do not submit R codes; show only output or graphics relevant to answering the question. Please skip the question matching step in Gradescope.\n\nVu and Harrington exercise 4.4. The 2010 General Social Survey asked the question: “For how many days during the past 30 days was your mental health, which includes stress, depression, and problems with emotions, not good?” Based on responses from 1,151 US residents, the survey reported a 95% confidence interval of 3.40 to 4.24 days in 2010.\n\nInterpret this interval in context of the data.\nWhat does “95% confident” mean? Explain in the context of the application.\nIf a new survey were to be done with 500 Americans, would the standard error of the estimate be larger, smaller, or about the same? Assume the standard deviation has remained constant since 2010.\n\n[L4] Vu and Harrington exercise 4.6. Thanksgiving spending, Part I. The 2009 holiday retail season, which kicked off on November 27, 2009 (the day after Thanksgiving), had been marked by somewhat lower self-reported consumer spending than was seen during the comparable period in 2008. To get an estimate of consumer spending, 436 randomly sampled American adults were surveyed. Daily consumer spending for the six-day period after Thanksgiving, spanning the Black Friday weekend and Cyber Monday, averaged $84.71. A 95% confidence interval based on this sample is ($80.31, $89.11). Determine whether the following statements are true or false, and explain your reasoning.\n\nWe are 95% confident that the average spending of these 436 American adults is between $80.31 and $89.11.\nThis confidence interval is not valid since the distribution of spending in the sample is right skewed.\n95% of random samples have a sample mean between $80.31 and $89.11.\nWe are 95% confident that the average spending of all American adults is between $80.31 and $89.11.\nA 90% confidence interval would be narrower than the 95% confidence interval.\nThe margin of error is 4.4.\n\n\n\n\nVu and Harrington exercise 4.21. Testing for fibromyalgia. A patient named Diana was diagnosed with fibromyalgia, a long-term syn- drome of body pain, and was prescribed anti-depressants. Being the skeptic that she is, Diana didn’t initially believe that anti-depressants would help her symptoms. However after a couple months of being on the med- ication she decides that the anti-depressants are working, because she feels like her symptoms are in fact getting better.\n\nWrite the hypotheses in words for Diana’s skeptical position when she started taking the anti-depressants.\nWhat is a Type 1 Error in this context?\nWhat is a Type 2 Error in this context?\n\nVu and Harrington exercise 4.24. True or false. Determine if the following statements are true or false, and explain your reasoning. If false, state how it could be corrected.\n\nIf a given value (for example, the null hypothesized value of a parameter) is within a 95% confidence interval, it will also be within a 99% confidence interval.\nDecreasing the significance level \\(\\alpha\\) will increase the probability of making a Type 1 Error.\nSuppose the null hypothesis is \\(H_0: \\mu = 5\\) and we fail to reject \\(H_0\\). Under this scenario, the true population mean is 5.\nIf the alternative hypothesis is true, then the probability of making a Type 2 Error and the power of a test add up to 1.\nWith large sample sizes, even small differences between the null value and the true value of the parameter, a difference often called the effect size, will be identified as statistically significant.\n\n[L3, L5] Cancer rates and sunspot activity. The dataset cancer contains skin cancer rates per 100,000 people in Connecticut each year from 1938 to 1972. Each year is also classified as following a period of higher than average or lower than average sunspot activity.\n\nPlot the skin cancer rates over time.\nBecause rates are generally increasing over time, the variable delta measures the deviation of skin cancer rate from the trend. Test the hypothesis that higher-than-average sunspot activity is associated with higher-than-expected skin cancer rates relative to the long term trend. Write a full narrative summary of the test result: state the conclusion; interpret the test result in context; provide supporting statistics; and report and interpret interval and point estimates. Do not provide any R output.\nIdentify, in words, the population parameter of interest in the test from part (b).\n\n\n\n[L4, L5] Comparing self-fertilization with cross-fertilization. The dataset plants contains measurements of plant heights in inches for 15 pairs of plants of the same age; one plant in the pair was grown from a seed from a cross-fertilized flower, and the other was grown from a seed from a self-fertilized flower.\n\nCalculate and interpret an interval estimate for the difference in mean height between plants produced from cross-fertilization and plants produced from self-fertilization.\nTest the hypothesis that plants produced from cross-fertilization are taller on average than those produced from self-fertilization. Write a full narrative summary of the test result: state the conclusion; interpret the test result in context; provide supporting statistics; and report and interpret interval and point estimates. Do not provide any R output.\n\n\n\n[L4, L5] Tuberculuosis and survival in guinea pigs. The dataset tubercle contains survival times in days of guinea pigs that were randomly assigned to receive a dose of tubercle bacilli, the bacterial pathogen that causes tuberculosis, or to a control group.\n\nTest the hypothesis that exposure to tubercle bacilli causes a decrease in survival time. Interpret the result of the test in context and provide supporting statistics.\nEstimate the decrease in survival time using a 99% confidence interval. Interpret the interval in context.\n\n\n\n[L5] Medical marijuana. To investigate the capacity of marijuana to reduce the side effects of cancer chemotherapy, researchers performed a double-blind, randomized, crossover trial. Fifteen cancer patients on chemotherapy were randomly assigned to receive either a marijuana treatment or a placebo treatment after their first three sessions of chemotherapy. They were then crossed over to the opposite treatment for their next 3 sessions. The dataset marijuana conatins the resulting measurements.\n\nTest the hypothesis that marijuana treatments cause a reduction in vomiting and retching episodes for cancer patients receiving chemotherapy. Write a full narrative summary of the test results. Do not include any R output.\nAssume that you found evidence that medical marijuana caused a reduction in part (a), regardless of your actual result. Would it be advisable to assure a patient that medical marijuana will reduce their vomiting and retching episodes? Why or why not?\n\n\n\nVu and Harrington exercise 5.37. A large farm wants to try out a new type of fertilizer to evaluate whether it will improve the farm’s corn production. The land is broken into plots that produce an average of 1,215 pounds of corn with a standard deviation of 94 pounds per plot. The owner is interested in detecting any average difference of at least 40 pounds per plot.\n\nHow many plots of land would be needed for the experiment if the desired power level is 90%? Assume fertilizer treatments (current and new) are allocated in equal proportion among the plots.\nExplain in words the meaning of the power level guarantee.\nIf the experiment will cost $250 per plot and the owner wants to limit the total cost to $10,000, what average difference will they be able to detect at the desired power level?"
  },
  {
    "objectID": "content/week6-power.html",
    "href": "content/week6-power.html",
    "title": "Power analyses",
    "section": "",
    "text": "Reading quiz [2pm section] [4pm section]\nType II errors: review and further exploration\n[lecture/lab] Power analysis\n\nSample size calculations\nPost-hoc power analyses"
  },
  {
    "objectID": "content/week7-nonparametric.html#todays-agenda",
    "href": "content/week7-nonparametric.html#todays-agenda",
    "title": "Nonparametric inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] One- and two-sample inference using ranks\n[lecture] Permutation tests\n[lab] Three applications of nonparametric tests"
  },
  {
    "objectID": "content/week7-nonparametric.html#from-before",
    "href": "content/week7-nonparametric.html#from-before",
    "title": "Nonparametric inference",
    "section": "From before",
    "text": "From before\nRecall the basis for our inferential procedures (intervals/tests) so far:\n\nthe \\(t\\) model approximates the sampling distribution of the \\(T\\) statistic(s)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an example of a parametric approach.\n\nthe t model is parametric distribution\nwe assume it describes the frequency of values of \\(T\\) under random sampling\n\nrequires “sufficiently large” sample(s) or normal population distribution(s)"
  },
  {
    "objectID": "content/week7-nonparametric.html#assumptions-underlying-t-tests",
    "href": "content/week7-nonparametric.html#assumptions-underlying-t-tests",
    "title": "Nonparametric inference",
    "section": "Assumptions underlying \\(t\\) tests",
    "text": "Assumptions underlying \\(t\\) tests"
  },
  {
    "objectID": "content/week7-nonparametric.html#nonparametric-procedures",
    "href": "content/week7-nonparametric.html#nonparametric-procedures",
    "title": "Nonparametric inference",
    "section": "Nonparametric procedures",
    "text": "Nonparametric procedures\nNonparametric procedures are distribution-free in the sense that they make no assumptions about the specific form of the underlying population distribution.\n\nThis does not mean nonparametric methods are free of any assumptions whatsoever\n\nNonparametrics can provide useful alternatives in special circumstances:\n\nparametric assumptions not plausible\ninferences about population parameters besides means\nsmall-sample settings\n‘narrow’ inferences limited in scope to observed data rather than a population\n\nWe will cover two nonparametric approaches to one- and two-sample inference:\n\nRank-based procedures\nPermutation procedures"
  },
  {
    "objectID": "content/week7-nonparametric.html#rank-based-procedures",
    "href": "content/week7-nonparametric.html#rank-based-procedures",
    "title": "Nonparametric inference",
    "section": "Rank-based procedures",
    "text": "Rank-based procedures\n\nRank-based procedures are inferential procedures that leverage sorted data and distributional symmetry or group exchangeability assumptions to test for location shifts.\n\nConsider the DDT data (15 measurements of DDT in kale in ppm) again. Sorted:\n\n\n2.79, 2.93, 3.06, 3.07, 3.08, 3.18, 3.22, 3.22, 3.33, 3.34, 3.34, 3.38, 3.56, 3.78 and 4.64\n\n\nSuppose you wish to test whether the ‘center’ is 3ppm. If you assume the population distribution is symmetric about c…\n\nHow many observations would you expect to be smaller than c?\nHow many observations are actually smaller than 3?\nBased on your answers to 1-2, do you think it is likely that c = 3?"
  },
  {
    "objectID": "content/week7-nonparametric.html#t-tests-are-parametric",
    "href": "content/week7-nonparametric.html#t-tests-are-parametric",
    "title": "Nonparametric inference",
    "section": "\\(t\\) tests are parametric",
    "text": "\\(t\\) tests are parametric\nRecall the basis for our inferential procedures (intervals/tests) so far:\n\nthe \\(t\\) model approximates the sampling distribution of the \\(T\\) statistic(s)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an example of a parametric approach: it relies on a distributional assumption.\n\nthe t model is parametric distribution\n\\(T\\) exactly follows this model only when underlying population distribution(s) are normal"
  },
  {
    "objectID": "content/week7-nonparametric.html#parametric-inference",
    "href": "content/week7-nonparametric.html#parametric-inference",
    "title": "Nonparametric inference",
    "section": "Parametric inference",
    "text": "Parametric inference\nRecall the basis for our inferential procedures (intervals/tests) so far:\n\nthe \\(t\\) model approximates the sampling distribution of the \\(T\\) statistic(s)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an example of a parametric approach: it relies on a distributional assumption.\n\nthe t model is parametric distribution\n\\(T\\) exactly follows this model only when underlying population distribution(s) are normal\n\nWhat should be done if the assumption is not plausible?"
  },
  {
    "objectID": "content/week7-nonparametric.html#stuff",
    "href": "content/week7-nonparametric.html#stuff",
    "title": "Nonparametric inference",
    "section": "stuff",
    "text": "stuff\n\n\n [1] -4.6 -2.4 -1.8 -1.4 -1.3 -1.3 -1.2 -1.0 -0.8  0.0\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  ddt\nV = 111.5, p-value = 0.9984\nalternative hypothesis: true location is less than 3\n95 percent confidence interval:\n  -Inf 3.425\nsample estimates:\n(pseudo)median \n       3.26001 \n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-nonparametric.html#signed-ranks",
    "href": "content/week7-nonparametric.html#signed-ranks",
    "title": "Nonparametric inference",
    "section": "Signed ranks",
    "text": "Signed ranks\n\n\n\n\n\n\n\n\n\n\n\nobs.num\nddt\nddt.c\nsigned.rank\n\n\n\n\n3\n3.06\n0.06\n1\n\n\n2\n2.93\n-0.07\n-2.5\n\n\n4\n3.07\n0.07\n2.5\n\n\n5\n3.08\n0.08\n4\n\n\n6\n3.18\n0.18\n5\n\n\n1\n2.79\n-0.21\n-6\n\n\n7\n3.22\n0.22\n7.5\n\n\n8\n3.22\n0.22\n7.5\n\n\n9\n3.33\n0.33\n9\n\n\n10\n3.34\n0.34\n10.5\n\n\n11\n3.34\n0.34\n10.5\n\n\n12\n3.38\n0.38\n12\n\n\n13\n3.56\n0.56\n13\n\n\n14\n3.78\n0.78\n14\n\n\n15\n4.64\n1.64\n15"
  },
  {
    "objectID": "content/week7-nonparametric.html#signed-rank-test",
    "href": "content/week7-nonparametric.html#signed-rank-test",
    "title": "Nonparametric inference",
    "section": "Signed rank test",
    "text": "Signed rank test\n\nThe signed rank test is an alternative to the one-sample t test that assumes the population distribution is symmetric to test hypotheses about its center c.\n\n\n\nHypotheses: \\[H_0: c = c_0 \\quad\\text{vs.}\\quad H_A: c \\mathrel{\\substack{&lt;\\\\\\neq\\\\ &gt;}} c_0\\] Procedure:\n\n[center] Calculate deviations \\(d_i = x_i - c_0\\)\n[rank] Sort and rank the absolute deviations \\(|d_i|\\)\n\naverage ranks in case of ties\ndrop zeros\n\n[sum] Add up the ‘signed ranks’ \\(\\text{sign}(d_i) \\times R_i\\)\n\n\nThis produces the test statistic:\n\\[V = \\sum_{i = 1}^n \\text{sign}(d_i) \\times R_i\\]\nSampling distribution found using:\n\n(exact) combinatorics\n(approximate) probability theory\n\nReject if \\(V\\) is sufficiently large in the direction of the alternative."
  },
  {
    "objectID": "content/week7-nonparametric.html#illustration",
    "href": "content/week7-nonparametric.html#illustration",
    "title": "Nonparametric inference",
    "section": "Illustration",
    "text": "Illustration\n\n\nSigned rank calculations:\n\n\n# A tibble: 15 × 4\n   obs.num   ddt   ddt.c signed.rank\n     &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1      10  3.06  0.0600         1  \n 2       2  2.93 -0.0700        -2.5\n 3      11  3.07  0.0700         2.5\n 4      13  3.08  0.0800         4  \n 5       7  3.18  0.180          5  \n 6       1  2.79 -0.21          -6  \n 7       3  3.22  0.220          7.5\n 8       5  3.22  0.220          7.5\n 9       8  3.33  0.33           9  \n10       9  3.34  0.34          10.5\n11      15  3.34  0.34          10.5\n12       6  3.38  0.38          12  \n13      12  3.56  0.56          13  \n14       4  3.78  0.78          14  \n15      14  4.64  1.64          15  \n\n\n\n\n\n\n\n\n\nsign\nV\n\n\n\n\nnegative\n-8.5\n\n\npositive\n111.5\n\n\nboth\n103"
  },
  {
    "objectID": "content/week7-nonparametric.html#illustration-ddt-data",
    "href": "content/week7-nonparametric.html#illustration-ddt-data",
    "title": "Nonparametric inference",
    "section": "Illustration: DDT data",
    "text": "Illustration: DDT data\n\n\nSigned rank calculations:\n\n\n# A tibble: 15 × 4\n   obs.num   ddt   ddt.c signed.rank\n     &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1      10  3.06  0.0600         1  \n 2       2  2.93 -0.0700        -2.5\n 3      11  3.07  0.0700         2.5\n 4      13  3.08  0.0800         4  \n 5       7  3.18  0.180          5  \n 6       1  2.79 -0.21          -6  \n 7       3  3.22  0.220          7.5\n 8       5  3.22  0.220          7.5\n 9       8  3.33  0.33           9  \n10       9  3.34  0.34          10.5\n11      15  3.34  0.34          10.5\n12       6  3.38  0.38          12  \n13      12  3.56  0.56          13  \n14       4  3.78  0.78          14  \n15      14  4.64  1.64          15  \n\n\nProcedure:\n\nCenter observations\nRank absolute deviations\nSum signed ranks\n\n\nHypotheses:\n\\[H_0: c = 3 \\quad\\text{vs}\\quad H_A: c &gt; 3\\]\nTest statistic:\n\n\n\n\n\n\n\n\n\nsign\nsum\n\n\n\n\nnegative\n-8.5\n\n\npositive\n111.5\n\n\n\n103\n\n\n\n\n\nThere are 32768 possible sign combinations; of these, only about 0.18% give a larger value of \\(V\\).\n\n\\(p = 0.0018 &lt; 0.05 \\;\\Rightarrow\\; \\text{reject } H_0\\)"
  },
  {
    "objectID": "content/week7-nonparametric.html#signed-rank-test-in-r",
    "href": "content/week7-nonparametric.html#signed-rank-test-in-r",
    "title": "Nonparametric inference",
    "section": "Signed rank test in R",
    "text": "Signed rank test in R\nThe implementation in R looks and functions much like t.test:\n\n\n\n# signed rank test\nwilcox.test(ddt, mu = 3, \n            alternative = 'greater',\n            conf.int = T)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  ddt\nV = 111.5, p-value = 0.001876\nalternative hypothesis: true location is greater than 3\n95 percent confidence interval:\n 3.135002      Inf\nsample estimates:\n(pseudo)median \n       3.26001 \n\n\n\nreports the sum of positive signed ranks\n\\(p\\) value is approximate with ties or zeros\npseudo-median (\\(\\neq\\) median) is a measure of center\n\n\nThe technically accurate interpretation is given in terms of “center”:\n\nThe data provide strong evidence against the null hypothesis that the center of the distribution of DDT in kale is 3ppm in favor of the alternative hypothesis that the center exceeds 3ppm (signed rank test, p = 0.001876)."
  },
  {
    "objectID": "content/week7-nonparametric.html#paired-differences",
    "href": "content/week7-nonparametric.html#paired-differences",
    "title": "Nonparametric inference",
    "section": "Paired differences",
    "text": "Paired differences\n\nFor paired differences, the signed rank test tests for a difference in location between the groups.\n\nThe test assumes that both population distributions are symmetric and tests the hypotheses:\n\\[H_0: c_1 = c_2 \\quad\\text{vs}\\quad H_A: c_1 \\mathrel{\\substack{&lt;\\\\\\neq\\\\ &gt;}} c_2\\]\n\n\\(c_1\\) is the center of group/population 1\n\\(c_2\\) is the center of group/population 2"
  },
  {
    "objectID": "content/week7-nonparametric.html#rank-sum-test",
    "href": "content/week7-nonparametric.html#rank-sum-test",
    "title": "Nonparametric inference",
    "section": "Rank sum test",
    "text": "Rank sum test\n\nThe rank-sum test is a two-sample rank-based inference procedure that tests for a difference in location from independent samples.\n\nKey idea: if observations in both groups come from the same population distribution, then they should be exchangeable (i.e., groupings don’t matter).\n\n\nAssumptions:\n\nobservations are independent\nthe populations differ only by location\n\nHypotheses:\n\\[H_0: c_1 = c_2\\quad\\text{vs}\\quad H_A: c_1 \\mathrel{\\substack{&lt;\\\\\\neq\\\\&gt;}} c_2\\]\n\nProcedure:\n\n[pool] Combine observations from both groups\n[rank] Sort and rank pooled observations\n[sum] Add up ranks in the smaller group\n\nReject if the sum is larger/smaller than expected."
  },
  {
    "objectID": "content/week7-nonparametric.html#rank-sum-test-in-r",
    "href": "content/week7-nonparametric.html#rank-sum-test-in-r",
    "title": "Nonparametric inference",
    "section": "Rank sum test in R",
    "text": "Rank sum test in R\nThe wilcox.test function also implements the rank sum test.\n\n\n\nwilcox.test(delta ~ sunspot.activity, data = cancer, \n            mu = 0, alternative = 'greater', \n            conf.int = T)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  delta by sunspot.activity\nW = 270, p-value = 8.182e-06\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 0.2693557       Inf\nsample estimates:\ndifference in location \n              0.455014 \n\n\n\n\\(p\\)-value gives the percentage of possible rank allocations among the groups for which the rank sum is at least as favorable to \\(H_A\\)\n\ncomputed by combinatorics\nsubject to minima depending on sample sizes\n\n\n\n\nThe data provide strong evidence against the hypothesis that the distribution of deviations from expected cancer rates in Connecticut does not differ according to sunspot activity in favor of a positive location shift associated with high sunspot activity (rank sum test, \\(p &lt; 0.0001\\))."
  },
  {
    "objectID": "content/tests/test1-soln.html",
    "href": "content/tests/test1-soln.html",
    "title": "Test 1",
    "section": "",
    "text": "[L2] Which of the following is an example of an experimental intervention?\n\nContacting prospective study participants during recruitment\nContacting study participants for follow-up surveys\nOffering study participants an incentive, such as a gift card\nAllocating different stimuli to study participants\n\nAnswer: D\n[L1] Suppose you wish to conduct a study involving a survey of California residents. Which of the following sampling schemes would identify a random sample from the target population?\n\nPhysically visit CVS stores around the state of California and survey shoppers at random\nObtain a list of all residential addresses, use a computer to draw 5,000 at random, and mail your survey to each selected address\nObtain a list of all residential addresses, use a computer to draw 5,000 at random, and recruit volunteers to physically visit each selected address\nGenerate a large number of random phone numbers with California area codes and send text messages to every generated number.\n\nAnswer: B, C\n[L1] Suppose you read a study wherein 300 people with a self-reported history of substance abuse and 200 people without any such history are surveyed to determine how many experienced adverse events in childhood, and the study finds that a much higher proportion of those with substance abuse histories report adverse childhood experiences than those without substance abuse histories. Identify the study type.\n\nRetrospective\nProspective\nExperiment\nCohort study\n\nAnswer: A\n\nThe next two questions are based on the figure below, which shows deaths among residents in Virginia in 1940 by age bracket and demographic.\n\n\n\n\n\n\n\n\n\n\n[L3] Which age group has the highest death rate?\n\n50-54\n55-59\n60-64\n65-69\n70-74\ncan’t tell\n\nAnswer: F\n[L3] For which age group do urban women account for the highest proportion of deaths?\n\n50-54\n55-59\n60-64\n65-69\n70-74\ncan’t tell\n\nAnswer: D\n[L4] A 95% confidence interval for the diameter in inches of black cherry trees estimated from 31 observations made on felled trees is (12.097, 14.399). Select the correct interpretation of this estimate:\n\nWith 95% probability, the diameter of black cherry trees is between 12.097 and 14.399.\nWith 95% confidence, the diameter of black cherry trees is between 12.097 and 14.399 inches.\nWith 95% confidence, the mean diameter of black cherry trees is between 12.097 and 14.399.\nWith 95% confidence, the mean diameter of black cherry trees is between 12.097 and 14.399 inches.\n\nAnswer: D\n[L4] From the interval in the previous question, determine the point estimate for the population mean.\n\n2.3\n13.2\n14.5\n12.1\ncan’t tell\n\nAnswer: B\n\nThe next three questions relate to the following data on in-state tuition in dollars from a sample of 50 public and private colleges. Note that 1e+04 indicates ten thousand dollars, 2e+04 indicates twenty thousand, and so on.\n\n\n\n\n\n\n\n\n\n\n[L3] Describe the shape of the distribution.\n\nLeft-skewed\nRight-skewed\nSymmetric\nNone of the above\n\nAnswer: B\n[L3] The sample mean is 17781 dollars. Identify below the median.\n\n13852\n17501\n19307\n8000\nnone of the above\n\nAnswer: A\n[L3] Which measure of spread should be used to describe this data?\n\nStandard deviation\nInterquartile range\nRange\nNone of the above\n\nAnswer: B"
  },
  {
    "objectID": "content/tests/test1-soln.html#part-i-concepts",
    "href": "content/tests/test1-soln.html#part-i-concepts",
    "title": "Test 1",
    "section": "",
    "text": "[L2] Which of the following is an example of an experimental intervention?\n\nContacting prospective study participants during recruitment\nContacting study participants for follow-up surveys\nOffering study participants an incentive, such as a gift card\nAllocating different stimuli to study participants\n\nAnswer: D\n[L1] Suppose you wish to conduct a study involving a survey of California residents. Which of the following sampling schemes would identify a random sample from the target population?\n\nPhysically visit CVS stores around the state of California and survey shoppers at random\nObtain a list of all residential addresses, use a computer to draw 5,000 at random, and mail your survey to each selected address\nObtain a list of all residential addresses, use a computer to draw 5,000 at random, and recruit volunteers to physically visit each selected address\nGenerate a large number of random phone numbers with California area codes and send text messages to every generated number.\n\nAnswer: B, C\n[L1] Suppose you read a study wherein 300 people with a self-reported history of substance abuse and 200 people without any such history are surveyed to determine how many experienced adverse events in childhood, and the study finds that a much higher proportion of those with substance abuse histories report adverse childhood experiences than those without substance abuse histories. Identify the study type.\n\nRetrospective\nProspective\nExperiment\nCohort study\n\nAnswer: A\n\nThe next two questions are based on the figure below, which shows deaths among residents in Virginia in 1940 by age bracket and demographic.\n\n\n\n\n\n\n\n\n\n\n[L3] Which age group has the highest death rate?\n\n50-54\n55-59\n60-64\n65-69\n70-74\ncan’t tell\n\nAnswer: F\n[L3] For which age group do urban women account for the highest proportion of deaths?\n\n50-54\n55-59\n60-64\n65-69\n70-74\ncan’t tell\n\nAnswer: D\n[L4] A 95% confidence interval for the diameter in inches of black cherry trees estimated from 31 observations made on felled trees is (12.097, 14.399). Select the correct interpretation of this estimate:\n\nWith 95% probability, the diameter of black cherry trees is between 12.097 and 14.399.\nWith 95% confidence, the diameter of black cherry trees is between 12.097 and 14.399 inches.\nWith 95% confidence, the mean diameter of black cherry trees is between 12.097 and 14.399.\nWith 95% confidence, the mean diameter of black cherry trees is between 12.097 and 14.399 inches.\n\nAnswer: D\n[L4] From the interval in the previous question, determine the point estimate for the population mean.\n\n2.3\n13.2\n14.5\n12.1\ncan’t tell\n\nAnswer: B\n\nThe next three questions relate to the following data on in-state tuition in dollars from a sample of 50 public and private colleges. Note that 1e+04 indicates ten thousand dollars, 2e+04 indicates twenty thousand, and so on.\n\n\n\n\n\n\n\n\n\n\n[L3] Describe the shape of the distribution.\n\nLeft-skewed\nRight-skewed\nSymmetric\nNone of the above\n\nAnswer: B\n[L3] The sample mean is 17781 dollars. Identify below the median.\n\n13852\n17501\n19307\n8000\nnone of the above\n\nAnswer: A\n[L3] Which measure of spread should be used to describe this data?\n\nStandard deviation\nInterquartile range\nRange\nNone of the above\n\nAnswer: B"
  },
  {
    "objectID": "content/tests/test1-soln.html#part-ii-applications",
    "href": "content/tests/test1-soln.html#part-ii-applications",
    "title": "Test 1",
    "section": "Part II: applications",
    "text": "Part II: applications\n\nDiets and chick weights\nThe following data come from a study investigating the early growth of chicks on different diets. In the study, 47 chicks were randomly assigned one of four diets at birth and researchers measured body weight in grams daily. The data below show body weights at 18 days since birth for each chick. The question of interest is: which diet is best?\n\n# read in data\nchick &lt;- read.csv('data/chick.csv')\n\n# preview\nhead(chick)\n\n\n[L2] Is this observational or experimental data? Explain your reasoning.\n\nExperimental. Researchers allocated diets at random to the chicks, which is an experimental intervention in the relevant sense.\n\n[L3] Produce a visualization that compares body weight distributions by diet. For which diet have chicks grown the most? The least? Explain the statistic(s) or features of the distribution you used to make this determination.\n\n\nboxplot(weight ~ diet, data = chick, xlab = '', ylab = 'weight (g)')\n\n\n\n\n\n\n\n\nBased on the boxplots, chicks grew the most on diet 3 and the least on diet 1. This is most easily seen by comparing medians (central bar in each box), but also reflected in the other quartiles.\n\n[L3] Calculate point estimates and standard errors for the mean body weight at 18 days after birth on each diet.\n\n\nlibrary(tidyverse)\nchick |&gt;\n  group_by(diet) |&gt;\n  summarize(avg.weight = mean(weight),\n            se = sd(weight)/sqrt(n())) |&gt;\n  pander::pander()\n\n\n\n\n\n\n\n\n\ndiet\navg.weight\nse\n\n\n\n\ndiet 1\n158.9\n11.94\n\n\ndiet 2\n187.7\n20.03\n\n\ndiet 3\n233.1\n18.21\n\n\ndiet 4\n202.9\n10.61\n\n\n\n\n\n\n[L2] Assume that in the previous question you found that chicks on diet 3 grew the most, regardless of your actual answer. Can you conclude that diet 3 caused the fastest growth? Explain why or why not.\n\nYes. Since the diets were randomly allocated, no additional factors can be confounded with diet. Therefore, estimated differences in growth are due to the effect of diet.\n\n[L4] Calculate a 95% confidence interval for the mean body weight of chicks on diet 3 at 18 days after birth. Interpret the interval in context.\n\nWith 95% confidence, the mean body weight of a chick on diet 3 at 18 days after birth is estimated to be between 191.9043713 and 274.2956287 grams.\n\n\nGSS data\nThe General Social Survey (GSS) is an effort to measure behaviors, opinions, and demographics of Americans and has been conducted annually since 1972. The data below are observations of a small collection of variables for 500 respondents from across several survey years.\n\n# import GSS data\ngss &lt;- read.csv('data/gss.csv')\n\n# preview\nhead(gss)\n\n\n[L3] Make a histogram of the weekly hours worked and describe the shape and modality. Choose an appropriate number of breaks.\n\n\nhist(gss$weekly.hours.worked, breaks = 20,\n     main = '', xlab = 'weekly hours', ylab = 'frequency')\n\n\n\n\n\n\n\n\nThe breaks chosen should make apparent the relatively high frequency of ~40 hour weeks compared with all other hours.\n\n[L3] What is the 20th percentile of weekly hours worked among respondents? What is the 80th percentile?\n\n\nquantile(gss$weekly.hours.worked, probs = c(0.2, 0.8)) |&gt; \n  pander::pander()\n\n\n\n\n\n\n\n\n20%\n80%\n\n\n\n\n30.8\n50\n\n\n\n\n\n\n[L3] Compute a point estimate of the mean hours worked. Report the estimate and its standard error.\n\nThe mean weekly hours worked is estimated to be 41.382 (SE = 0.6627689).\n\n[L4] Produce and interpret an 85% confidence interval for the mean weekly hours worked.\n\nWith 85% confidence, the mean weekly hours worked is estimated to be between 40.4264524 and 42.3375476.\n\n[L3] Use an appropriate graphical summary to assess whether mean hours worked seems to differ by class. Explain the plot you produce and interpret any patterns observed.\n\n\nboxplot(weekly.hours.worked ~ class, \n        data = gss, \n        range = 2,\n        xlab = '')\n\n\n\n\n\n\n\n\nThe boxplot shows some slight differences by class. The nost notable difference is lower class compared with all others, as the interquartile range for lower class hours worked is almost entirely below the central value of 40. Most likely, class definitions include hours worked either directly or indirectly (e.g., through accounting for income), so the observed differences are not especially straightforward to interpret or necessarily meaningful.\n\n[L3] Make any additional bivariate comparison addressing a question of your choice. State the question in non-technical terms, produce a visualization that conveys the comparison, and interpret any patterns observed."
  },
  {
    "objectID": "content/tests/test2.html",
    "href": "content/tests/test2.html",
    "title": "Test 1",
    "section": "",
    "text": "Instructions\nYou have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers. Please use the word document provided (download from the class website) and write in your answers below each prompt. You should submit your work via Gradescope; please skip the page matching step and do not match pages to parts of the outline.\nThe test comprises two parts: concepts, containing short multiple-choice questions; and applications, which requires some data analysis. Revisions will be allowed for the applications part, but not for the concepts part.\nex0332"
  },
  {
    "objectID": "content/week7-nonparametric.html#minimum-p-values",
    "href": "content/week7-nonparametric.html#minimum-p-values",
    "title": "Nonparametric inference",
    "section": "Minimum \\(p\\) values",
    "text": "Minimum \\(p\\) values\nThe \\(p\\) value for the test is the proportion of all possible sign combinations that yield a \\(V\\) larger/smaller than observed.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfewer sign combinations for few observations\nsmallest possible \\(p\\) value depends on sample size\ndon’t expect small p-values in small-\\(n\\) settings"
  },
  {
    "objectID": "content/week7-nonparametric.html#paired-difference-example",
    "href": "content/week7-nonparametric.html#paired-difference-example",
    "title": "Nonparametric inference",
    "section": "Paired difference example",
    "text": "Paired difference example\n\n\n\nIs drug 2 more effective than drug 1?\n\nFirst few observations of sleep data:\n\n\n\n\n\n\n\n\n\n\n\nID\ndrug1\ndrug2\ndiff\n\n\n\n\n1\n0.7\n1.9\n-1.2\n\n\n2\n-1.6\n0.8\n-2.4\n\n\n3\n-0.2\n1.1\n-1.3\n\n\n4\n-1.2\n0.1\n-1.3\n\n\n\n\n\nThe signed rank test will test for a difference in location, assuming that the pairwise differences are symmetric about some central point.\n\nHypotheses:\n\\[\\begin{cases} H_0: c_\\text{drug1} = c_\\text{drug2} \\\\\nH_A: c_\\text{drug1} &lt; c_\\text{drug2}\n\\end{cases}\\]\n\nwilcox.test(sleep.diffs, mu = 0,\n            alternative = 'less',\n            conf.int = T)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  sleep.diffs\nV = 0, p-value = 0.004545\nalternative hypothesis: true location is less than 0\n95 percent confidence interval:\n      -Inf -1.149967\nsample estimates:\n(pseudo)median \n     -1.400031 \n\n\nAside: what does \\(V = 0\\) mean?"
  },
  {
    "objectID": "content/week7-nonparametric.html#checking-assumptions",
    "href": "content/week7-nonparametric.html#checking-assumptions",
    "title": "Nonparametric inference",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\nFor the signed rank test, the underlying distribution is assumed to be symmetric.\n\nCheck the symmetry assumption graphically using histograms.\n\nIt can be tricky with small sample sizes; don’t be too sensitive to asymmetries. Both of these are acceptable."
  },
  {
    "objectID": "content/week7-nonparametric.html#your-turn",
    "href": "content/week7-nonparametric.html#your-turn",
    "title": "Nonparametric inference",
    "section": "Your turn",
    "text": "Your turn\n\nIs the rank sum test appropriate for comparing centers?\n\n\n\nOut-of-state tuition costs from 26 public and 26 private universities.\n\n\n\n\n\n\n\n\n\n\nDeviations from expected cancer rates in CT in years with high and low sunspot activity."
  },
  {
    "objectID": "content/week7-nonparametric.html#example-tuition",
    "href": "content/week7-nonparametric.html#example-tuition",
    "title": "Nonparametric inference",
    "section": "Example: tuition",
    "text": "Example: tuition\n\n\nBelow are out-of-state tuition costs from 26 public and 26 private universities.\n\n\n\n\n\n\n\n\n\n\nBelow are deviations from expected cancer rates in years with high and low sunspot activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-nonparametric.html#examples",
    "href": "content/week7-nonparametric.html#examples",
    "title": "Nonparametric inference",
    "section": "Examples",
    "text": "Examples\n\nIs the rank sum test appropriate for comparing centers?\n\n\n\nOut-of-state tuition costs from 26 public and 26 private universities.\n\n\n\n\n\n\n\n\n\n\nDeviations from expected cancer rates in CT in years with high and low sunspot activity."
  },
  {
    "objectID": "content/week7-nonparametric.html#in-practice",
    "href": "content/week7-nonparametric.html#in-practice",
    "title": "Nonparametric inference",
    "section": "In practice",
    "text": "In practice\n\nIs the rank sum test appropriate for comparing centers?\n\n\n\nOut-of-state tuition costs from 26 public and 26 private universities.\n\n\n\n\n\n\n\n\n\n\nDeviations from expected cancer rates in CT in years with high and low sunspot activity."
  },
  {
    "objectID": "content/week7-nonparametric.html#calibrate-your-eyes",
    "href": "content/week7-nonparametric.html#calibrate-your-eyes",
    "title": "Nonparametric inference",
    "section": "Calibrate your eyes",
    "text": "Calibrate your eyes\n\nIs the rank sum test appropriate for comparing centers?\n\n\n\nOut-of-state tuition costs from 26 public and 26 private universities.\n\n\n\n\n\n\n\n\n\n\nDeviations from expected cancer rates in CT in years with high and low sunspot activity."
  },
  {
    "objectID": "content/week7-nonparametric.html#checking-assumptions-1",
    "href": "content/week7-nonparametric.html#checking-assumptions-1",
    "title": "Nonparametric inference",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\nThe assumption for the rank sum test is that the distributions differ only by location.\n\nIf this is true, histograms should have the same shape and modes.\n\nIn all of these cases, there is a (true) difference in location.\n\nif shape differs too much, the rank sum test should not be used\ncan be hard to tell with small samples"
  },
  {
    "objectID": "content/week7-nonparametric.html#use-heuristics-for-rank-procedures",
    "href": "content/week7-nonparametric.html#use-heuristics-for-rank-procedures",
    "title": "Nonparametric inference",
    "section": "Use heuristics for rank procedures",
    "text": "Use heuristics for rank procedures\nConsider a rank-based procedure if…\n\nsample sizes are small\nyou want to test for a location shift\n\nOnce you’ve decided to consider a rank test…\n\nDetermine test type (one-sample, paired, two-sample)\nCheck symmetry/similarity assumption graphically\nPerform test"
  },
  {
    "objectID": "content/week7-nonparametric.html#permutation-tests",
    "href": "content/week7-nonparametric.html#permutation-tests",
    "title": "Nonparametric inference",
    "section": "Permutation tests",
    "text": "Permutation tests\n\nPermutation tests are two-sample procedures that provide highly flexible alternatives to parametric tests based on exchangeability.\n\nKey idea: if there is no difference between groups/populations, observations are exchangeable between groups.\n\n\nProcedure:\n\nRandomly shuffle group assignments among the observations.\nCalculate any group comparison.\nRepeat many times.\nCalculate proportion of shufflings for which the comparison is at least as large as the observed comparison in the direction of the alternative\n\n\nPermutation tests for differences in means are implemented by permTS().\n\npermTS(OutOfState ~ Type, data = ex0332, \n       alternative = 'greater')\n\n\n    Permutation Test using Asymptotic Approximation\n\ndata:  OutOfState by Type\nZ = 3.0644, p-value = 0.001091\nalternative hypothesis: true mean Type=Private - mean Type=Public is greater than 0\nsample estimates:\nmean Type=Private - mean Type=Public \n                            12219.08"
  },
  {
    "objectID": "content/week7-nonparametric.html#when-to-use-permutation-tests",
    "href": "content/week7-nonparametric.html#when-to-use-permutation-tests",
    "title": "Nonparametric inference",
    "section": "When to use permutation tests",
    "text": "When to use permutation tests\nThere are three main situations where permutation tests are recommended over \\(t\\) tests:\n\nSmall sample sizes\nYou wish to make inferences about an unusual parameter (e.g. median) or function (e.g. ratio of means)\nYou wish to make “narrow” inference about the likelihood of observing the group difference you obtained given the exact observations in your samples.\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-nonparametric.html#lab-two-end-to-end-analyses",
    "href": "content/week7-nonparametric.html#lab-two-end-to-end-analyses",
    "title": "Nonparametric inference",
    "section": "Lab: two end-to-end analyses",
    "text": "Lab: two end-to-end analyses\n\n\nCholesterol data:\n\n\n\n\n\n\n\n\n\nDiet\nCholesterol\n\n\n\n\nCORNFLK\n4.61\n\n\nOATBRAN\n3.84\n\n\nCORNFLK\n6.42\n\n\nOATBRAN\n5.57\n\n\n\n\n\nZinc and dietary supplements:\n\n\n\n\n\n\n\n\n\nGroup\nZinc\n\n\n\n\nA\n1.31\n\n\nA\n1.45\n\n\nA\n1.12\n\n\nA\n1.16\n\n\n\n\n\n\nQuestions:\n\nAre zinc concentrations (mg/ml) in the blood of rats lower among those that received a dietary supplement (group A) compared with those that did not receive the supplement group (B)?\nIs there a difference in cholesterol associated with consuming oat bran compared with consuming corn flakes?\n\nYour task, for each dataset:\n\nDetermine the appropriate nonparametric test\nPerform the test\nPrepare a written interpretation of the result\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-nonparametric.html#lab-three-end-to-end-analyses",
    "href": "content/week7-nonparametric.html#lab-three-end-to-end-analyses",
    "title": "Nonparametric inference",
    "section": "Lab: three end-to-end analyses",
    "text": "Lab: three end-to-end analyses\n\n\nCholesterol data:\n\n\n\n\n\n\n\n\n\nDiet\nCholesterol\n\n\n\n\nCORNFLK\n4.61\n\n\nOATBRAN\n3.84\n\n\n\n\n\nZinc and dietary supplements:\n\n\n\n\n\n\n\n\n\nGroup\nZinc\n\n\n\n\nA\n1.31\n\n\nA\n1.45\n\n\n\n\n\nSleep data:\n\n\n\n\n\n\n\n\n\n\n\nID\n1\n2\ndiff\n\n\n\n\n1\n0.7\n1.9\n-1.2\n\n\n2\n-1.6\n0.8\n-2.4\n\n\n\n\n\n\nQuestions:\n\nAre zinc concentrations (mg/ml) in the blood of rats lower among those that received a dietary supplement (group A) compared with those that did not receive the supplement group (B)?\nIs there a difference in cholesterol associated with consuming oat bran compared with consuming corn flakes?\nDoes drug 2 produce more than 1 hour of additional sleep compared with drug 1?\n\nYour task, for each dataset, is to (a) determine and perform the appropriate nonparametric test and (b) interpret the result accurately.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/labs/lab10-nonparametric.html",
    "href": "content/labs/lab10-nonparametric.html",
    "title": "Lab 10: nonparametric inference",
    "section": "",
    "text": "The goal of this lab is to learn how and when to implement nonparametric alternatives to one- and two-sample \\(t\\) tests. There are three alternatives we will consider:\nWe will illustrate the implementation of these tests using the following datasets:\n# load example datasets\nddt &lt;- MASS::DDT\nsleep &lt;- read.csv('data/sleep.csv')\nload('data/cancer.RData')\nTo determine whether the rank procedures are appropriate, histograms should be inspected for:\nExamples of these histograms are shown below for each dataset.\n# for one-sample inference, check histogram for symmetry\nhist(ddt, breaks = 5)\n\n\n\n\n\n\n\n# for paired inference, check histogram *of differences* for symmetry\nhist(sleep$diff, breaks = 10)\n\n\n\n\n\n\n\n# for two-sample inference, check groupwise histograms for similar shape\npar(mfrow = c(1, 2))\nhist(cancer$delta[cancer$sunspot.activity == 'High'], main = 'high activity', xlab = 'delta')\nhist(cancer$delta[cancer$sunspot.activity == 'Low'], main = 'low activity', xlab = 'delta')\nNotice that due to the small sample sizes, symmetry/similarity is hard to assess. As a result, you should only look for obviously asymmetric/different shapes. Each of these is acceptable.\nThe signed rank test and rank sum tests are implemented using wilcox.test().\n# signed rank test\nwilcox.test(ddt, mu = 3, alternative = 'greater')\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  ddt\nV = 111.5, p-value = 0.001876\nalternative hypothesis: true location is greater than 3\n\n# signed rank test, paired differences\nwilcox.test(sleep$diff, mu = 0, alternative = 'less')\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  sleep$diff\nV = 0, p-value = 0.004545\nalternative hypothesis: true location is less than 0\n\n# rank sum test\nwilcox.test(delta ~ sunspot.activity, data = cancer, alternative = 'greater')\n\n\n    Wilcoxon rank sum exact test\n\ndata:  delta by sunspot.activity\nW = 270, p-value = 8.182e-06\nalternative hypothesis: true location shift is greater than 0\nPermutation tests, by contrast, are implemented using permTS(). Note that these are only applicable for two-sample inference with independent (i.e., not paired) data.\n# permutation test\nlibrary(perm)\npermTS(delta ~ sunspot.activity, data = cancer, alternative = 'greater')\n\n\n    Permutation Test using Asymptotic Approximation\n\ndata:  delta by sunspot.activity\nZ = 3.8143, p-value = 6.829e-05\nalternative hypothesis: true mean sunspot.activity=High - mean sunspot.activity=Low is greater than 0\nsample estimates:\nmean sunspot.activity=High - mean sunspot.activity=Low \n                                             0.4433333\nPermutation tests provide an alternative nonparametric inference procedure with minimal assumptions."
  },
  {
    "objectID": "content/labs/lab10-nonparametric.html#sleep-data-revisited",
    "href": "content/labs/lab10-nonparametric.html#sleep-data-revisited",
    "title": "Lab 10: nonparametric inference",
    "section": "Sleep data revisited",
    "text": "Sleep data revisited\nTo start, let’s extend the sleep data example above by trying something a little different than usual. Notice that the estimated difference in means is -1.58, and every subject experienced more extra sleep on drug 2.\n\n\n\n\n\n\nYour turn\n\n\n\nCan you work out how to test, using a rank procedure, whether drug 2 is associated with at least one more hour of extra sleep than drug 1?\nWrite the hypotheses, perform the test, and interpret the result.\n\n# use rank procedure to test whether difference exceeds 1 hour\n\n\n\nThis is not a scenario specific to nonparametric methods, but just an exercise in testing a less typical set of hypotheses with paired data.\nOnce you’ve determined and performed your test, write a short report interpreting the result."
  },
  {
    "objectID": "content/labs/lab10-nonparametric.html#cholesterol-and-cereal",
    "href": "content/labs/lab10-nonparametric.html#cholesterol-and-cereal",
    "title": "Lab 10: nonparametric inference",
    "section": "Cholesterol and cereal",
    "text": "Cholesterol and cereal\nYour goal is to test whether cereal is associated with a difference in cholesterol using a nonparametric method. The data are as follows:\n\n# load cholesterol data\ncholesterol &lt;- read.csv('data/cholesterol.csv')\nhead(cholesterol, 4)\n\n     Diet Cholesterol\n1 CORNFLK        4.61\n2 OATBRAN        3.84\n3 CORNFLK        6.42\n4 OATBRAN        5.57\n\n\nDiets were randomly allocated to 28 participants in equal proportion; there are 14 subjects in each diet group.\n\n\n\n\n\n\nYour turn\n\n\n\nTest whether oat bran significantly lowers cholesterol relative to corn flakes using a nonparametric method.\nUse an appropriate graphic to check whether the assumptions for a rank procedure are appropriate; if so, use the appropriate rank procedure; if not, use a permutation test.\n\n# check assumptions\n\n# determine and perform appropriate test\n\nOnce you’ve determined and performed your test, write a short report interpreting the result."
  },
  {
    "objectID": "content/labs/lab10-nonparametric.html#zinc-and-dietary-supplements",
    "href": "content/labs/lab10-nonparametric.html#zinc-and-dietary-supplements",
    "title": "Lab 10: nonparametric inference",
    "section": "Zinc and dietary supplements",
    "text": "Zinc and dietary supplements\nYour goal is to test whether taking a dietary supplement lowers zinc concentrations among rats using the following data.\n\n# load cholesterol data\nzinc &lt;- Sleuth3::ex0125\nhead(zinc, 3)\n\n  Group Zinc\n1     A 1.31\n2     A 1.45\n3     A 1.12\n\n\nThe data come from an experiment in which 39 rats were randomly assigned a dietary supplement (group A) or no dietary supplement (group B). After a period of time, the zinc concentration in each rat’s blood was measured.\n\n\n\n\n\n\nYour turn\n\n\n\nTest whether the dietary supplement lowers zinc concentrations among rats using a nonparametric method.\nUse an appropriate graphic to check whether the assumptions for a rank procedure are appropriate; if so, use the appropriate rank procedure; if not, use a permutation test.\n\n# check assumptions\n\n# determine and perform appropriate test\n\nOnce you’ve determined and performed your test, write a short report interpreting the result."
  },
  {
    "objectID": "content/week7-nonparametric.html",
    "href": "content/week7-nonparametric.html",
    "title": "Nonparametric inference",
    "section": "",
    "text": "[lecture] One- and two-sample inference using ranks\n[lecture] Permutation tests\n[lab] Three applications of nonparametric tests"
  },
  {
    "objectID": "content/week7-nonparametric.html#remark",
    "href": "content/week7-nonparametric.html#remark",
    "title": "Nonparametric inference",
    "section": "Remark",
    "text": "Remark\n\nAvoid over-interpreting one summary/graphic by checking alternatives\n\n\n\n\n\n\n\n\n\n\n\n\n\ncommon error: “republicans had larger households”"
  },
  {
    "objectID": "content/week7-anova.html#todays-agenda",
    "href": "content/week7-anova.html#todays-agenda",
    "title": "Analysis of Variance",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture/discussion] inference for several means with ANOVA\n[lab] ANOVA in R\npractice problem diet restriction and longevity"
  },
  {
    "objectID": "content/week7-anova.html#chicks",
    "href": "content/week7-anova.html#chicks",
    "title": "Analysis of Variance",
    "section": "Chicks",
    "text": "Chicks\n\n\nOn the test, you considered this data on chick weights by diet:\n\n\n\n\n\n\n\n\n\nAre observed differences sufficiently large to conclude diets cause growth differences?\n\nHere we have four means to compare rather than just two.\nPoint estimates (which you computed) are:\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nse\n\n\n\n\n1\n170.4\n13.45\n\n\n2\n205.6\n22.22\n\n\n3\n258.9\n20.63\n\n\n4\n233.9\n12.52\n\n\n\n\n\n\nDo the population means differ by diet? How do you test this?"
  },
  {
    "objectID": "content/week7-anova.html#univariate",
    "href": "content/week7-anova.html#univariate",
    "title": "Analysis of Variance",
    "section": "Univariate",
    "text": "Univariate\nOne option is to make confidence intervals:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#ad-hoc-approach",
    "href": "content/week7-anova.html#ad-hoc-approach",
    "title": "Analysis of Variance",
    "section": "Ad hoc approach",
    "text": "Ad hoc approach\n\n\nOne option is to make confidence intervals:\n\n\n\n\n\n\n\n\n\nWhich intervals don’t overlap?\n\n2 not different from any others\n1 is different from 3 and 4\n\n\nNot satisfactory, because intervals are not simultaneous:\n\neach interval has 95% coverage individually\nbut together as a collection coverage is lower: all four intervals capture all four means less than 95% of the time\n\n\nThere is extra uncertainty when means are estimated jointly.\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#an-ad-hoc-approach",
    "href": "content/week7-anova.html#an-ad-hoc-approach",
    "title": "Analysis of Variance",
    "section": "An ad hoc approach",
    "text": "An ad hoc approach\n\n\nOne option is to make confidence intervals and check for overlap:\n\n\n\n\n\n\n\n\n\nIdea: if any of the intervals don’t overlap, then there’s a difference.\n\ne.g., diet 1 differs from diets 3 and 4\n\n\nNot satisfactory, because interval coverage is not simultaneous:\n\nintervals won’t always cover or not cover all four means at the same time\nso the “joint” coverage is less than 95%\n\n\nThere is extra uncertainty due to the multiplicity; inferences must account for this.\n\nStrategy:\n\nfirst test for any group differences\nthen determine which groups differ"
  },
  {
    "objectID": "content/week7-anova.html#inference-for-comparing-several-means",
    "href": "content/week7-anova.html#inference-for-comparing-several-means",
    "title": "Analysis of Variance",
    "section": "Inference for comparing several means",
    "text": "Inference for comparing several means\n\nCould the means differ by random chance?\n\nLet’s consider how to test the hypothesis that all means are equal:\n\\[H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\qquad (\\mu_i = \\text{mean weight on diet } i)\\] \\[H_A: \\mu_i \\neq \\mu_j \\quad\\text{for at least one pair of means}\\]\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#an-idea-for-joint-inference",
    "href": "content/week7-anova.html#an-idea-for-joint-inference",
    "title": "Analysis of Variance",
    "section": "An idea for joint inference",
    "text": "An idea for joint inference\n\nCould the means differ by random chance?\n\nLet \\(\\mu_i = \\text{mean weight on diet } i\\). Consider testing the hypotheses:\n\\[\\begin{align*}\n&H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\quad &(\\text{all means are the same}) \\\\\n&H_A: \\mu_i \\neq \\mu_j \\quad &(\\text{at least one pair differs})\n\\end{align*}\\]\nConsider this hypothetical scenario:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe means differ more at right, but is it “enough”?\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#a-hypothesis-to-test",
    "href": "content/week7-anova.html#a-hypothesis-to-test",
    "title": "Analysis of Variance",
    "section": "A hypothesis to test",
    "text": "A hypothesis to test\nLet \\(\\mu_i = \\text{mean weight on diet } i\\). Consider testing the hypotheses:\n\\[\\begin{align*}\n&H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\quad &(\\text{all means are the same}) \\\\\n&H_A: \\mu_i \\neq \\mu_j \\quad &(\\text{at least one pair differs})\n\\end{align*}\\]\nConsider this hypothetical scenario:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe means differ more at right, but is it “enough”?\n\n\n\nHow variable are the means? How variable are the groups (i.e., error bars)?"
  },
  {
    "objectID": "content/week7-anova.html#an-approach",
    "href": "content/week7-anova.html#an-approach",
    "title": "Analysis of Variance",
    "section": "An approach",
    "text": "An approach\n\nIf the means vary a lot relative to the observations in each group we should reject \\(H_0\\)\n\nIdea: compare variability between groups to variability within groups.\n\n\nNotation:\n\n\\(\\bar{x}\\): “grand” mean of all observations\n\\(\\bar{x}_i\\): mean of observations in group \\(i\\)\n\\(s_i\\): SD of observations in group \\(i\\)\n\\(k\\) groups\n\\(n\\) total observations\n\\(n_i\\) observations per group\n\n\nMeasures of variability:\n\\[MSG = \\frac{1}{k - 1}\\sum_i n_i(\\bar{x}_i - \\bar{x})^2 \\quad(\\text{between groups})\\] \\[MSE = \\frac{1}{n - k}\\sum_i (n_i - 1)s_i^2 \\quad(\\text{within groups})\\] Ratio:\n\\[F = \\frac{MSG}{MSE} \\quad\\left(\\frac{\\text{variation between means}}{\\text{variation within groups}}\\right)\\]\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#the-f-statistic",
    "href": "content/week7-anova.html#the-f-statistic",
    "title": "Analysis of Variance",
    "section": "The \\(F\\) statistic",
    "text": "The \\(F\\) statistic\n\nIf the means vary a lot relative to the observations in each group we should reject \\(H_0\\)\n\n\n\nNotation:\n\n\\(\\bar{x}\\): “grand” mean of all observations\n\\(\\bar{x}_i\\): mean of observations in group \\(i\\)\n\\(s_i\\): SD of observations in group \\(i\\)\n\\(k\\) groups\n\\(n\\) total observations\n\\(n_i\\) observations per group\n\nAssumption: variability is the same within each group.\n\nMeasures of variability:\n\\[\\color{red}{MSG} = \\frac{1}{k - 1}\\sum_i n_i(\\bar{x}_i - \\bar{x})^2 \\quad(\\color{red}{\\text{group}})\\] \\[\\color{blue}{MSE} = \\frac{1}{n - k}\\sum_i (n_i - 1)s_i^2 \\quad(\\color{blue}{\\text{error}})\\] Ratio:\n\\[F = \\frac{\\color{red}{MSG}}{\\color{blue}{MSE}} \\quad\\left(\\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\right)\\]"
  },
  {
    "objectID": "content/week7-anova.html#example-calculation",
    "href": "content/week7-anova.html#example-calculation",
    "title": "Analysis of Variance",
    "section": "Example calculation",
    "text": "Example calculation\n\n\nGrouped summaries:\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nsd\nn\n\n\n\n\n1\n170.4\n55.44\n17\n\n\n2\n205.6\n70.25\n10\n\n\n3\n258.9\n65.24\n10\n\n\n4\n233.9\n37.57\n9\n\n\n\n\n\nUngrouped summaries:\n\n\n\n\n\n\n\n\n\nmean\nn\n\n\n\n\n209.7\n46\n\n\n\n\n\n\n\\[\\begin{align*}\nMSG &= \\frac{1}{k - 1}\\sum_i n_i(\\bar{x}_i - \\bar{x})^2 \\\\\n&= \\hspace{10cm}\\\\\\\\\n&= 18627 \\\\\\\\\nMSE &= \\frac{1}{n - k}\\sum_i (n_i - 1)s_i^2 \\\\\n&= \\hspace{10cm} \\\\\\\\\n&= 3409.3\\\\\\\\\nF &= \\frac{MSG}{MSE} = \\hspace{5cm} = 5.4636\n\\end{align*}\\]"
  },
  {
    "objectID": "content/week7-anova.html#interpreting-the-f-statistic",
    "href": "content/week7-anova.html#interpreting-the-f-statistic",
    "title": "Analysis of Variance",
    "section": "Interpreting the \\(F\\) statistic",
    "text": "Interpreting the \\(F\\) statistic\n\nF = 5.4636 means the proportion of variation in weight attributable to diets is 5.46 times greater than the proportion of variation attributable to chicks.\n\nThe significance of this result is measured by a \\(p\\) value:\n\nif there is in fact no difference in means, then 0.291% of samples (i.e., 2 in 1000) would produce more diet-to-diet variability than what we observed.\n\nThe \\(p\\)-value is based on an \\(F\\) model for the sampling distribution. This is a parametric model.\n\nparameters are numerator and denominator degrees of freedom\nassumes underlying population distributions for each group are well-approximated by a normal model"
  },
  {
    "objectID": "content/week7-anova.html#analysis-of-variance-table",
    "href": "content/week7-anova.html#analysis-of-variance-table",
    "title": "Analysis of Variance",
    "section": "Analysis of variance table",
    "text": "Analysis of variance table\nThe results of an analysis of variance are traditionally displayed in a table.\n\n\n\n\n\n\n\n\n\n\nSource\ndegrees of freedom\nSum of squares\nMean square\nF statistic\n\n\n\n\nGroup\n\\(k - 1\\)\nSSG\n\\(MSG = \\frac{SSG}{k - 1}\\)\n\\(\\frac{MSG}{MSE}\\)\n\n\nError\n\\(n - k\\)\nSSE\n\\(MSE = \\frac{SSG}{n - k}\\)\n\n\n\n\n\nthe sum of square terms are ‘raw’ measures of variability\nthe mean square terms are averages adjusted for the amount of data available to estimate variability due to each source\n\nFormally, the ANOVA model says \\((n - 1)s^2 = SSG + SSE\\)."
  },
  {
    "objectID": "content/week7-anova.html#performing-calculations-in-r",
    "href": "content/week7-anova.html#performing-calculations-in-r",
    "title": "Analysis of Variance",
    "section": "Performing calculations in R",
    "text": "Performing calculations in R\n\n\n\nAnalysis of Variance Table\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nDiet\n3\n55881\n18627\n5.464\n0.002909\n\n\nResiduals\n42\n143190\n3409\nNA\nNA"
  },
  {
    "objectID": "content/week7-anova.html#partitioning-variation",
    "href": "content/week7-anova.html#partitioning-variation",
    "title": "Analysis of Variance",
    "section": "Partitioning variation",
    "text": "Partitioning variation\n\n\n\n\n\n\n\n\n\n\n\n\nTwo sources of variability:\n\ngroup variability between diets (systematic)\nerror variability between chicks (random)\n\nA model:\n\\[\\color{grey}{\\text{total variation}} = \\color{red}{\\text{group variation}} + \\color{blue}{\\text{error variation}}\\]\n\n\nAn idea: use the ratio \\(\\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\) to measure how much means differ.\n\nPartitioning variation and taking ratios is called the “analysis of variance”"
  },
  {
    "objectID": "content/week7-anova.html#analysis-of-variance",
    "href": "content/week7-anova.html#analysis-of-variance",
    "title": "Analysis of Variance",
    "section": "Analysis of variance",
    "text": "Analysis of variance\n\nThe analysis of variance is based on a type of summary called an \\(F\\) statistic.\n\n\\(F\\) statistics measure ratios of variance components. In our case, we will use: \\[F = \\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\]\n\na large \\(F\\) indicates that grouping accounts for relatively more variation\n\na small \\(F\\) indicates that grouping accounts for relatively little variation\n\n\nTo implement this idea, we need measures of group and error variation"
  },
  {
    "objectID": "content/week7-anova.html#anova-in-r",
    "href": "content/week7-anova.html#anova-in-r",
    "title": "Analysis of Variance",
    "section": "ANOVA in R",
    "text": "ANOVA in R\n\n\nThere are two steps to performing an ANOVA in R:\n\nFit an ANOVA model\nGenerate the table\n\n\n# fit anova model\nfit &lt;- aov(weight ~ Diet, data = chicks)\n\n# generate table\nsummary(fit)\n\n\n\nThe data provide sufficiently strong evidence to reject the null hypothesis that diets do not cause differences in growth in favor of the alternative that at least two diets differ significantly (F = 5.464 on 3 and 42 df, p = 0.0029).\n\n\n\n\n\n\nAnalysis of Variance Model\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nDiet\n3\n55881\n18627\n5.464\n0.002909\n\n\nResiduals\n42\n143190\n3409\nNA\nNA"
  },
  {
    "objectID": "content/week7-anova.html#summing-up",
    "href": "content/week7-anova.html#summing-up",
    "title": "Analysis of Variance",
    "section": "Summing up",
    "text": "Summing up\n\nThe ANOVA setup (for us) is comparing \\(k\\) population means.\n\nHypotheses:\n\\[\\begin{align*}\n&H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\quad &(\\text{all means are the same}) \\\\\n&H_A: \\mu_i \\neq \\mu_j \\quad &(\\text{at least one pair differs})\n\\end{align*}\\]\nAssumptions:\n\npopulation standard deviations are the same for every group\ngroupwise population distributions follow a normal model\n\nModel and approach:\n\npartition total variation into group and error components \\((SST = SSG + SSE)\\)\nreject \\(H_0\\) if group variation is sufficiently large relative to error variation"
  },
  {
    "objectID": "content/week7-anova.html#another-example",
    "href": "content/week7-anova.html#another-example",
    "title": "Analysis of Variance",
    "section": "Another example",
    "text": "Another example\n\n\nWeight change was measured for 72 young female anorexia patients randomly allocated to three treatment groups:\n\ncognitive behavioral therapy (CBT)\nfamily treatment (FT)\na control (Cont)\n\nGrouped summary statistics:\n\n\n\n\n\n\n\n\n\n\n\nTreat\npost - pre\nsd\nn\n\n\n\n\nCBT\n3.007\n7.309\n29\n\n\nCont\n-0.45\n7.989\n26\n\n\nFT\n7.265\n7.157\n17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#another-example-treating-anorexia",
    "href": "content/week7-anova.html#another-example-treating-anorexia",
    "title": "Analysis of Variance",
    "section": "Another example: treating anorexia",
    "text": "Another example: treating anorexia\n\n\nWeight change was measured for 72 young female anorexia patients randomly allocated to three treatment groups:\n\ncognitive behavioral therapy (CBT)\nfamily treatment (FT)\na control (Cont)\n\nGrouped summary statistics:\n\n\n\n\n\n\n\n\n\n\n\nTreat\npost - pre\nsd\nn\n\n\n\n\nCBT\n3.007\n7.309\n29\n\n\nCont\n-0.45\n7.989\n26\n\n\nFT\n7.265\n7.157\n17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWere any of the treatments more effective than others?"
  },
  {
    "objectID": "content/week7-anova.html#another-example-treating-anorexia-1",
    "href": "content/week7-anova.html#another-example-treating-anorexia-1",
    "title": "Analysis of Variance",
    "section": "Another example: treating anorexia",
    "text": "Another example: treating anorexia\n\n# fit anova model\nfit &lt;- aov(change ~ Treat, data = anorexia)\n\n# generate table\nsummary(fit)\n\n\n\n\nAnalysis of Variance Model\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nTreat\n2\n614.6\n307.3\n5.422\n0.006499\n\n\nResiduals\n69\n3911\n56.68\nNA\nNA\n\n\n\n\n\n\nThe data provide sufficiently strong evidence to reject the null hypothesis of no effect of therapeutic treatment on mean weight change among young women with anorexia (F = 5.422 on 2 and 69 degrees of freedom, p = 0.0065)."
  },
  {
    "objectID": "content/week7-anova.html#checking-assumptions",
    "href": "content/week7-anova.html#checking-assumptions",
    "title": "Analysis of Variance",
    "section": "Checking assumptions",
    "text": "Checking assumptions\nANOVA assumes that populations are normal with the same standard deviation across groups. The sample SD’s can be compared for any big differences.\n\n\nAnorexia treatment data:\n\n\n\n\n\n\n\n\n\n\n\nTreat\npost - pre\nsd\nn\n\n\n\n\nCBT\n3.007\n7.309\n29\n\n\nCont\n-0.45\n7.989\n26\n\n\nFT\n7.265\n7.157\n17\n\n\n\n\n\n\nall SD’s are fairly close to 7\n\n\nANOVA is fairly robust to violations of the normality and equal variance assumptions. Consequences of each:\n\nnon-normality: type I error rate is not exact\nunequal variances: loss of power\n\nIn practice there’s not much need to worry about these unless the assumptions are very obviously untenable (e.g., binary or highly discrete variable of interest)."
  },
  {
    "objectID": "content/week7-anova.html#following-up-on-last-time",
    "href": "content/week7-anova.html#following-up-on-last-time",
    "title": "Analysis of Variance",
    "section": "Following up on last time",
    "text": "Following up on last time\n\n\n\n\n\n\n\n\n\n\nCase\nProblem type\nHypotheses\nTest\n\\(p\\)-value\n\n\n\n\nCholesterol\ntwo-sample\n\\(\\begin{cases}H_0: c_\\text{OB} = c_\\text{CF} \\\\ H_A: c_\\text{OB} \\neq c_\\text{CF}\\end{cases}\\)\nrank sum\n0.2063\n\n\nZinc\ntwo-sample\n\\(\\begin{cases}H_0: c_\\text{A} = c_\\text{B} \\\\ H_A: c_\\text{A} &lt; c_\\text{B}\\end{cases}\\)\npermutation\n0.1351\n\n\nSleep\npaired differences\n\\(\\begin{cases}H_0: c_\\text{drug1} - c_\\text{drug2}  = -1 \\\\ H_A: c_\\text{drug1} - c_\\text{drug2} &lt; -1 \\end{cases}\\)\nsigned rank\n0.04609\n\n\n\nR commands:\n\n# cholesterol\nwilcox.test(Cholesterol ~ Diet, data = cholesterol, mu = 0, alternative = 'two.sided')\n\n# zinc\npermTS(Zinc ~ Group, data = zinc, mu = 0, alternative = 'less')\n\n# sleep\nwilcox.test(sleep$diff, mu = -1, alternative = 'less')"
  },
  {
    "objectID": "content/week7-anova.html#lab",
    "href": "content/week7-anova.html#lab",
    "title": "Analysis of Variance",
    "section": "Lab",
    "text": "Lab\n\n\nLab 11 demonstrates the basic process of performing an analysis of variance:\n\nPrepare a graphical display of the data\nCalculate grouped summaries\nFit an ANOVA model and construct an ANOVA table\n\n\nHere is a basic template for these steps\n\n# graphical display: boxplot\nboxplot([VARIABLE] ~ [GROUPS], data = [DATASET])\n\n# grouped summaries\n[DATASET] |&gt;\n  group_by([GROUPS]) |&gt;\n  summarize([SUMMARY1 NAME] = fn1([VARIABLE]),\n            [SUMMARY2 NAME] = fn2([VARIABLE]))\n\n# fit ANOVA model\nfit &lt;- aov([VARIABLE] ~ [GROUPS], data = [DATASET])\n\n# generate ANOVA table\nsummary(fit)"
  },
  {
    "objectID": "content/week7-anova.html#why-permutation",
    "href": "content/week7-anova.html#why-permutation",
    "title": "Analysis of Variance",
    "section": "Why permutation?",
    "text": "Why permutation?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rank sum test assumes population distributions differ only in location\n\nThese histograms suggest:\n\ndifferent sperad\ndifferent shape\n\nIf assumptions aren’t met, the permutation test should be used instead."
  },
  {
    "objectID": "content/week7-anova.html#zinc-example-why-permutation",
    "href": "content/week7-anova.html#zinc-example-why-permutation",
    "title": "Analysis of Variance",
    "section": "Zinc example: why permutation?",
    "text": "Zinc example: why permutation?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rank sum test assumes population distributions differ only in location\n\nThese histograms suggest:\n\ndifferent sperad\ndifferent shape\n\nIf assumptions aren’t met, the permutation test should be used instead."
  },
  {
    "objectID": "content/week7-anova.html#practice-problem",
    "href": "content/week7-anova.html#practice-problem",
    "title": "Analysis of Variance",
    "section": "Practice problem",
    "text": "Practice problem\n\n\nFemale mice were randomly assigned to six treatment groups to investigate whether restricting dietary intake increases life expectancy:\n\n[NP] mice ate unlimited amount of nonpurified, standard diet\n[N/N85] normal diet before weaning and normal diet after weaning (85 kcal/wk)\n[N/R50] normal diet before weaning and reduced calorie diet after weaning (50 kcal/wk)\n[N/R40] normal diet before weaning and reduced diet after weaning (40 Kcal/wk)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiet\nlife.mean\nlife.sd\nn\n\n\n\n\nNP\n27.4\n6.134\n49\n\n\nN/N85\n32.69\n5.125\n57\n\n\nN/R50\n42.3\n7.768\n71\n\n\nN/R40\n45.12\n6.703\n60\n\n\n\n\n\n\nTest whether diet restriction has an effect on longevity. Make a boxplot of the data, write the hypotheses you are testing, produce an ANOVA table, and write a narrative summary of the test result.\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#lab-anova-in-r",
    "href": "content/week7-anova.html#lab-anova-in-r",
    "title": "Analysis of Variance",
    "section": "Lab: ANOVA in R",
    "text": "Lab: ANOVA in R\n\n\nLab 11 demonstrates the basic process of performing an analysis of variance:\n\nPrepare a graphical display of the data\nCalculate grouped summaries\nFit an ANOVA model and construct an ANOVA table\n\n\nHere is a basic template for these steps\n\n# graphical display: boxplot\nboxplot([VARIABLE] ~ [GROUPS], data = [DATASET])\n\n# grouped summaries\n[DATASET] |&gt;\n  group_by([GROUPS]) |&gt;\n  summarize([SUMMARY1 NAME] = fn1([VARIABLE]),\n            [SUMMARY2 NAME] = fn2([VARIABLE]))\n\n# fit ANOVA model\nfit &lt;- aov([VARIABLE] ~ [GROUPS], data = [DATASET])\n\n# generate ANOVA table\nsummary(fit)\n\n\n\nYour task is simply to recreate examples from lecture by executing provided commands."
  },
  {
    "objectID": "content/labs/lab11-anova.html",
    "href": "content/labs/lab11-anova.html",
    "title": "Lab 11: analysis of variance",
    "section": "",
    "text": "The goal of this lab is to learn how to implement analysis of variance in R. In its most basic form, this comprises three steps:\nBoxplots are commonly used as graphical displays, which you already know how to make. We will include that step but focus on graphical summaries and ANOVA calculations. You’ll learn a few new commands, some of which come from the widely-used tidyverse package:"
  },
  {
    "objectID": "content/labs/lab11-anova.html#performing-anova-in-r",
    "href": "content/labs/lab11-anova.html#performing-anova-in-r",
    "title": "Lab 11: analysis of variance",
    "section": "Performing ANOVA in R",
    "text": "Performing ANOVA in R\nHere we’ll reproduce an analysis of variance using the anorexia data. Data come from a study of young anorexic women that measured weight change before and after therapeutic treatment for two therapies, cognitive behavioral therapy (CBT) and family treatment (FT), and a control. Treatment groups were randomly allocated among the study participants, so inferences can be made about the causal effects of the therapies.\n\nlibrary(tidyverse)\nanorexia &lt;- read_csv('data/anorexia.csv')\nhead(anorexia)\n\nTo perform an analysis of variance, we need to identify the variable of interest and the grouping variable. These are:\n\n[variable] change, which gives the change in weight after treatment in pounds\n[grouping] Treat, which indicates the treatment group\n\nAs an aside, notice that the raw data are actually before-treatment and after-treatment weight measurements. So there is pairing in the data! We won’t really account for this explicitly, and will just treat change as the sole variable of interest.\n\nStep 1: graphical summary\nIt’s always a good idea to plot data before any analysis. This helps identify any unexpected patterns. Boxplots are a fairly standard summary for analysis of variance – after all, we are comparing distributions of a numerical variable (here, weight change) across several categories (here, treatment groups), so this on some level can be thought of as a numeric/categorical comparison.\n\n# boxplot\nboxplot(change ~ Treat, data = anorexia,\n        xlab = 'treatment group', ylab = 'weight change (lbs)')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nSometimes you’ll prefer to see the treatment groups in a particular order. This can be accomplished by manually specifying the order, from left to right, in which you want each ‘level’ to appear. This is illustrated below\n\n# rearrange order of treatment groups\nboxplot(change ~ factor(Treat, levels = c('Cont', 'FT', 'CBT')), data = anorexia,\n        xlab = 'treatment group', ylab = 'weight change (lbs)')\n\nTry changing the order of treatment groups so that, from left to right, the plot shows FT, then CBT, then the control group.\n\n\n\n\nStep 2: grouped summaries\nA next step is to calculate means, standard deviations, and sample sizes for each group. This provides you with point estimates (group means), a way to check the equal-variance assumption (comparing standard deviations), and an understanding of how balanced the treatment groups are (sample size).\nYou already have a bit of experience computing numerical summaries like means and standard deviations. In principle, you could divide up the observations by group, and then calculate each statistic separately. However, there is an easier way:\n\n# grouped summaries: mean, standard deviation, sample size\nanorexia |&gt;\n  group_by(Treat) |&gt;\n  summarize(group.mean = mean(change),\n            group.sd = sd(change),\n            group.n = n())\n\n# A tibble: 3 × 4\n  Treat group.mean group.sd group.n\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 CBT        3.01      7.31      29\n2 Cont      -0.450     7.99      26\n3 FT         7.26      7.16      17\n\n\nThis syntax will look a little unfamiliar. The symbol |&gt; is called the ‘pipe’ operator, and it passes the result of one command as the first argument to the next. Read it as “then”: take anorexia then group by treatment then summarize.\nThe summarize step has a series of lines like group.mean = mean(change). These specify a name for the summary on the left, and the actual calculation, in terms of variables in the dataframe, on the right. They are of the form: summary name = calculation. You could use this to compute other statistics too, if you wish.\n\n\n\n\n\n\nYour turn\n\n\n\nModify the example above to also compute a median for each group.\n\nanorexia |&gt;\n  group_by(Treat) |&gt;\n  summarize(group.mean = mean(change),\n            group.sd = sd(change),\n            group.n = n(),\n            ...) # add a summary here\n\n\n\n\n\nStep 3: ANOVA model and table\nThe purpose of the graphical and group summaries is to inspect the data before conducting inference. This affords you an opportunity to consider whether model assumptions (normality and equal variance within each group) are reasonable, and to notice anything unusual that might require further scrutiny (outliers, unexpected patterns, etc.).\nAnalysis of variance models can be fit in a variety of ways in R, but we will use aov(). The syntax is quite similar to what you use to make a boxplot. It requires two inputs:\n\na formula specifying the variable of interest and grouping factor\na dataframe where the variable names in the formula can be found\n\n\n# fit anova model\nfit &lt;- aov(change ~ Treat, data = anorexia)\n\n# generate table\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)   \nTreat        2    615  307.32   5.422 0.0065 **\nResiduals   69   3911   56.68                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry inspecting the fitted model fit. It will show you the sums of squares (raw variability estimates) and corresponding degrees of freedom, along with a “residual standard error”, which gives MSE. The term “residual” is used in place of “error”.\n\n# inspect fitted anova model\nfit\n\nCall:\n   aov(formula = change ~ Treat, data = anorexia)\n\nTerms:\n                   Treat Residuals\nSum of Squares   614.644  3910.742\nDeg. of Freedom        2        69\n\nResidual standard error: 7.528441\nEstimated effects may be unbalanced\n\n\n\nMatch each number in the output to the terms in the ANOVA table.\nWhat calculations are performed to obtain the MSG and F terms? (Don’t actually do the calculations, just write what they would be.)"
  },
  {
    "objectID": "content/labs/lab11-anova.html#practice-problem",
    "href": "content/labs/lab11-anova.html#practice-problem",
    "title": "Lab 11: analysis of variance",
    "section": "Practice problem",
    "text": "Practice problem\n\nWeindruch, R., Walford, R.L., Fligiel, S. and Guthrie D. (1986). The Retardation of Aging in Mice by Dietary Restriction: Longevity, Cancer, Immunity and Lifetime Energy Intake, Journal of Nutrition 116(4):641–54.\n\nFemale mice were randomly assigned to six treatment groups to investigate whether restricting dietary intake increases life expectancy:\n\n[NP] mice ate unlimited amount of nonpurified, standard diet\n[N/N85] normal diet before weaning and normal diet after weaning (85 kcal/wk)\n[N/R50] normal diet before weaning and reduced calorie diet after weaning (50 kcal/wk)\n[N/R40] normal diet before weaning and reduced diet after weaning (40 Kcal/wk)\n\nTest whether diet restriction has an effect on longevity by carrying out the following:\n\nMake a boxplot of the data, produce group summaries, write the hypotheses you are testing, and produce an ANOVA table.\nWrite a narrative summary of the test result.\n\n\n# read in data and preview\nlongevity &lt;- read.csv('data/longevity.csv')\nhead(longevity)\n\n  Lifetime Diet\n1     35.5   NP\n2     35.4   NP\n3     34.9   NP\n4     34.8   NP\n5     33.8   NP\n6     33.5   NP\n\n# visualize the data -- make a boxplot\n\n# calculate grouped summaries\n\n# fit anova model and produce table"
  },
  {
    "objectID": "content/week7-anova.html",
    "href": "content/week7-anova.html",
    "title": "Analysis of Variance",
    "section": "",
    "text": "Case\nProblem type\nHypotheses\nTest\n\\(p\\)-value\n\n\n\n\nCholesterol\ntwo-sample\n\\(\\begin{cases}H_0: c_\\text{OB} = c_\\text{CF} \\\\ H_A: c_\\text{OB} \\neq c_\\text{CF}\\end{cases}\\)\nrank sum\n0.2063\n\n\nZinc\ntwo-sample\n\\(\\begin{cases}H_0: c_\\text{A} = c_\\text{B} \\\\ H_A: c_\\text{A} &lt; c_\\text{B}\\end{cases}\\)\npermutation\n0.1351\n\n\nSleep\npaired differences\n\\(\\begin{cases}H_0: c_\\text{drug1} - c_\\text{drug2}  = -1 \\\\ H_A: c_\\text{drug1} - c_\\text{drug2} &lt; -1 \\end{cases}\\)\nsigned rank\n0.04609\n\n\n\nR commands:\n\n# cholesterol\nwilcox.test(Cholesterol ~ Diet, data = cholesterol, mu = 0, alternative = 'two.sided')\n\n# zinc\npermTS(Zinc ~ Group, data = zinc, mu = 0, alternative = 'less')\n\n# sleep\nwilcox.test(sleep$diff, mu = -1, alternative = 'less')"
  }
]