[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics for Life Sciences",
    "section": "",
    "text": "Announcements\n\n\n\nRecently posted:\n\nscores and response summaries for PS9 & PS10\noutlines for weeks 9-10 (note NO class meeting Monday 6/3)\nfinal project guidelines\n\nUpcoming assignments:\n\nproblem set 11 due Wednesday 5/29; late submissions until Friday 5/31 5pm\ntest 3 corrections due Friday 5/31 5pm\nproblem set 12 due Monday 6/3; late submissions until Wednesday 6/5 5pm\n\n\n\n\nCourse information\nRead the [course syllabus] for detailed information on content, materials, learning outcomes, assessments, and course policies.\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment]\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class.\n\nCompleting assignments:\nOne set of practice problems is included at the end of each lab; these problem sets are your homework assignments. You will often have some time to work on them during class, and they will be due by the following class period. To complete these assignments:\n\nReview the prompts included with the lab.\nDo your work (calculations, making plots, etc.) in the lab script provided in Posit Cloud.\nFollow the link that appears as [problem set N] with the class meeting outline for the period in which the problem set was assigned. This will direct you to a form where you’ll fill out select answers. Refer to your work in Posit Cloud as you complete the form.\n\nSome general remarks:\n\nproblem sets are due one hour before the next class meeting\nlate submissions are accepted until 5pm two days after the due date\nscore summaries will be posted once all deadlines pass\nonce scores are posted, you can see your individual responses using the link that you used to access the form\n\n\n\nWeek 1 (4/1/24)\nAcademic holiday 4/1/24\nIntroduction to statistical thinking and study designs\nWednesday class meeting\n\n[reading] Vu and Harrington 1.1\n[lecture] course introduction; study designs\n[activity] distinguishing types of studies\n[problem set 1] due Monday 4/8; late submissions until Wednesday 4/10 5pm\n[problem set 1 corrections] due by 5pm Friday 4/12\n\nResponse summary [PS1] [PS1 corrections]\n\n\nWeek 2 (4/8/24)\nData types and descriptive statistics\nMonday class meeting\n\nreading quiz [12pm section] [2pm section]\n[reading] Vu and Harrington 1.2\n[lecture] data types\n[lab] R basics\n[problem set 2] due Wednesday 4/10; late submissions until Friday 4/12 5pm\n\nResponse summary [PS2]\nWednesday class meeting\n\n[reading] Vu and Harrington 1.4 - 1.5\n[lecture] descriptive statistics\n[lab] descriptive statistics in R\n[problem set 3] due Monday 4/15; late submissions until Wednesday 4/17 5pm\n\nResponse summary [PS3]\n\n\nWeek 3 (4/15/24)\nDescriptive statistics and graphical summaries\nMonday class meeting\n\n[reading quiz] Vu and Harrington 1.6\n[lecture] descriptive statistics for relationships between two variables\n[lab] bivariate summaries in R\n[problem set 4] due Wednesday 4/17; late submissions until Friday 4/19 5pm\n\nResponse summary [PS4]\nWednesday class meeting\n\n[reading] review course notes and PS1, PS2, PS3 in detail\n[review] recap and Q&A\n[test 1 practice problems] in groups with short solution presentations\n[R cheatsheet] for easy reference\n\nTest 1 available Wednesday 4/17 5pm and due Friday 4/19 5:00pm PDT [prompts] [submission] [upload R script]\n\n\nWeek 4 (4/22/24)\nFoundations for inference\nMonday class meeting\n\n[reading] Vu and Harrington 4.1\n[lecture] point estimation, sampling variability, and interval estimation\n[lab] point and interval estimation for a population mean\n[problem set 5] due Wednesday 4/24; late submissions until Friday 4/26 5pm\n\nResponse summary [PS5]\nWednesday class meeting\n\n[reading quiz] Vu and Harrington 3.3.1, 3.3.2, and 3.3.3; and 4.2\n[lecture] constructing and interpreting confidence intervals\n[lab] computing confidence intervals\n\nTest 1 corrections due Friday 4/26 5pm [submit corrections]\n\n\nWeek 5 (4/29/24)\nOne-sample inference for numerical data\nMonday class meeting\n\n[reading] Vu and Harrington 4.3.1 & 4.3.2\n[lecture] the \\(t\\)-test for a population mean\n[lab] computing test statistics, critical values, and \\(p\\)-values\n[problem set 6] due Wednesday 5/1; late submissions until Friday 5/3 5pm\n\nResponse summary [PS6]\nWednesday class meeting\n\n[reading] Vu and Harrington 4.3.3 & 4.3.4\n[lecture] directional tests\n[lab] directional tests\n[problem set 7] due Monday 5/6; late submissions until Wednesday 5/8 5pm\n\nResponse summary [PS7]\n\n\nWeek 6 (5/6/24)\nTwo-sample inference for numerical data\nPlease complete this short [midquarter feedback survey] by Friday 5/10. Responses are anonymous.\nMonday class meeting\n\n[reading] Vu and Harrington 5.3\n[lecture] two-sample inference\n[lab] two-sample t tests in R\n[problem set 8] due Wednesday 5/8; late submissions until Friday 5/10\n\nResponse summary [PS8]\nWednesday class meeting\n\n[reading] Vu and Harrington 5.4\n[lecture] decision errors; statistical power\n[test 2 practice problems] test 2 prep\n[R cheatsheet] for easy reference\n\nTest 2 available Wednesday 5/8 5pm and due Friday 5/10 5pm [prompts] [submission form] [upload R script] [submit corrections]\n\n\nWeek 7 (5/13/24)\nAnalysis of variance\nMonday class meeting\n\n[reading] Vu and Harrington 5.5.1 & 5.5.2\n[lecture] Introduction to analysis of variance\n[lab] fitting ANOVA models in R\n\nNO Wednesday class meeting\n\n[reading] van Belle et al. 8.4 and 8.5 up to 8.5.4\n[self-paced activity] nonparametric inferences for one- and two-sample problems\n[problem set 9] due Monday 5/20; late submissions until Wednesday 5/22 5pm\n\nResponse summary [PS9]\n\n\nWeek 8 (5/20/24)\nPost hoc inference in ANOVA; inference for a population proportion\nMonday class meeting\n\n[reading] Vu and Harrington 5.5.3 & 5.5.4\n[lecture] post hoc inference in ANOVA\n[lab] pairwise comparisons in R\n[problem set 10] due Wednesday 5/22; late submissions until Friday 5/24 5pm\n\nResponse summary [PS10]\nWednesday class meeting\n\n[reading] Vu and Harrington 8.1 & 8.2\n[lecture] inference for population proportions\n[lab] tests and intervals for proportions in R\n\nTest 3 available Wednesday 5/22 5pm PDT and due Friday 5/24 5:00pm PDT [prompts] [submission] [upload R script]\n\n\nWeek 9 (5/27/24)\nAcademic holiday 5/27/24; Tuesday follows Monday schedule\nAnalysis of two-way contingency tables\nTuesday class meeting\n\n[reading] Vu and Harrington 8.3 (excluding 8.3.5)\n[lecture] tests of association in two-way tables\n[lab] \\(\\chi^2\\) tests in R\n[problem set 11] due Wednesday 5/29; late submissions until Friday 5/31 5pm\n\nWednesday class meeting\n\n[reading] Vu and Harrington 8.5\n[lecture] inference for odds ratios and relative risk\n[lab] measures of association in R\n[problem set 12] due Monday 6/3; late submissions until Wednesday 6/5 5pm\n\n\n\nWeek 10 (6/3/24)\nSimple linear regression\nNO Monday class meeting\n\n[reading] Vu and Harrington 6.1 & 6.2\n[activity] simple linear regression warm-up\n[problem set 13] due Wednesday 6/5; late submissions until Friday 6/7 5pm\n\nWednesday class meeting\n\n[reading] Vu and Harrington 6.4\n[lecture] inference in simple linear regression\n[lab] estimating the age of the universe\n[miscellany] scheduling oral exam times\n\nTest 4 due Friday 6/7 5:00pm PDT\n\n\nFinals week (6/10/24)\nOral exams to be held during scheduled exam time\nScheduled exam times:\n\n[12pm section] Wednesday 6/12 10:10am – 1:00pm\n[2pm section] Monday 6/10 1:10pm – 4:00pm\n\n[project guidelines]"
  },
  {
    "objectID": "content/syllabus.html",
    "href": "content/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Statistics plays a crucial role in the sciences: statistical techniques provide a means of weighing quantitative evidence derived from observation and experimentation while accounting for uncertainty. Statistical thinking and data analysis also facilitate discovery, exploration, and hypothesis generation. This class aims to provide a hands-on introduction to common statistical methods used almost universally across the sciences — descriptive and graphical techniques, inferential methods for comparing population means, analysis of categorical data and contingency tables, and linear regression — while drawing on examples from the life sciences to help illuminate the potential for application in students’ chosen field(s) of study and providing basic training in the use of statistical software.\n\n\nCourse information\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nClass meetings will comprise a mixture of lecture, lab activities, class activities, and discussion.\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment].\nThese times are partitioned into 15 minute intervals that you can schedule via the appointment link above; this system is intended to minimize waiting times and guarantee one-on-one availability. Slots can be scheduled anywhere from 7 calendar days to 10 minutes in advance. While drop-ins are welcome, I can’t guarantee availability outside of scheduled times.\nCatalog Description: Data collection and experimental design, descriptive statistics, confidence intervals, parametric and non parametric one and two-sample hypothesis tests, analysis of variance, correlation, simple linear regression, chi-square tests. Applications of statistics to the life sciences. Substantial use of statistical software. Prerequisite: MATH 96; or MATH 115; or appropriate Math Placement Level. Fulfills GE Area B4 (GE Area B1 for students on the 2019-20 or earlier catalogs); a grade of C- or better is required in one course in this GE area.\n\n\nMaterials\nYou’ll need an internet-connected laptop or tablet (a keyboard is necessary since we will do some web-hosted computation and you will be expected to type assignments). You should expect to bring your laptop or tablet to every class meeting.\nComputing: use of R/RStudio will be hosted online via a posit.cloud workspace [link to join]. To access the workspace, you’ll need to create a posit.cloud account and purchase a $5/month student plan.\nTextbook: Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences, First edition. A PDF and tablet-friendly version are available for free online at the link above. This will be our primary reference and we will cover chapters 1 – 2, 4 – 6, and 8.\nCourse notes: course notes will be posted as slides on the course website.\nOther references:\n\nVan Belle, Fisher, Heagerty, and Lumley (2004). Biostatistics: a methodology for the health sciences. Wiley. A PDF can be obtained through the Kennedy Library via the link above. This text provides a thorough introduction to biostatistics (statistics for life sciences) and is an excellent reference for more depth of coverage. Select readings will be assigned from this book.\nDouglas et al. (2023). An Introduction to R. This online book covers a variety of introductory topics pertaining to R/RStudio: installation, packages, files and directories, objects, functions, data types, data structures, graphics, basic statistics, markdown, and version control. Select readings will be assigned from this book.\n\n\n\nLearning outcomes\nThis course aims to support you in developing the following abilities.\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques\n[L4] construct and interpret confidence intervals for means and differences between means for independent and paired samples\n[L5] conduct parametric and non-parametric two-sample hypothesis tests for means\n[L6] construct and interpret a confidence interval for a single proportion\n[L7] conduct Chi-square goodness-of-fit tests and tests for independence\n[L8] distinguish between case-control and cohort studies and compute relative-risk and odds in the appropriate settings\n[L9] perform analysis of variance tests and post-hoc comparisons for completely randomized designs\n[L10] use simple linear regression to describe relationships between variables\n[L11] apply one or more methods from the course to your major field of study\n\nEmphasis is placed on conceptual fluency, application, and interpretation. In addition, you will learn to perform simple statistical analyses in R and can expect to develop a basic familiarity with the software; however, as this is not a programming class, the R environment will not be discussed in any detail and you will only learn to use a handful of commands.\n\n\nAssessments\nAttainment of learning outcomes will be measured by performance on homework assignments, tests, and a short project with an oral assessment in lieu of a final exam.\n\nHomework assignments will be given at the end of every class meeting and will comprise two practice problems due by the next class meeting. These are your opportunity to practice applying course concepts and methods covered in class and will help you to keep current with the pace and content of the lectures.\nTests will be given every 2-3 weeks and will comprise roughly 10-20 problems each. These are your opportunity to demonstrate that you’ve synthesized course material and achieved learning outcomes, and you will have approximately 48 hours to complete each test. One round of revisions will be allowed for each test in which you can make up full credit for any problems answered incorrectly in your initial attempt.\nA project with an oral assessment will be given in place of a final exam. However, you will need to be available in person during the scheduled final exam time, as this is when the oral assessment will take place.\n\nEvery assessed problem will be matched to one of the learning outcomes L1-L10. All submitted work will be assessed on a question-by-question basis as satisfactory (S) or needing improvement (NI) according to whether responses are fully correct. The percentage of problems matched to a particular learning outcome for which you receive a satisfactory assessment provides a measure of your attainment of that learning outcome. These percentages form a basis for determining your course grade (see below).\nDue to limited resources we will only provide qualitative feedback on a small subset of assessed questions, and only when an assessment of NI is made. As such, it is your responsibility to seek the feedback you need to correct your understanding where needed via class engagement, office hours, peer consultation, further study, and [tutoring resources].\n\n\nLetter grades\nStudents will receive a score for each learning outcome representing the (possibly weighted) proportion of questions matched with that outcome that received a satisfactory assessment across all assignments. The outcome will be assessed as follows:\n\n‘fully met’ if the proportion is at least 0.8\n‘partly met’ if the proportion is between 0.5 and 0.8\n‘unmet’ otherwise\n\nYou will receive periodic email summaries of your progress on each learning outcome. To receive a passing grade in the class, at least six outcomes must be either partly or fully met. Subject to this condition, letter grades are then defined as follows:\n\n\n\nGrade\nNumber of fully met outcomes\n\n\n\n\nA\n10\n\n\nA-\n9\n\n\nB+\n8\n\n\nB\n7\n\n\nB-\n6\n\n\nC+\n5\n\n\nC\n4\n\n\nC-\n3\n\n\nD+\n2\n\n\nD\n1\n\n\nD-\n0\n\n\n\nPlease note that these definitions are tentative and potentially subject to change; however, I will not make the grading requirements more stringent under any circumstances.\nPlease also note that failure to adhere to course policies may result in a lower letter grade than would otherwise be assigned.\n\n\nTentative schedule\nSubject to change.\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings (V&H)\nAssessments\n\n\n\n\n1 (4/1/24)\nIntroduction to statistical thinking and study design\n1.1\n\n\n\n2 (4/8/24)\nData, data types, and data collection\n1.2\n\n\n\n3 (4/15/24)\nDescriptive statistics and graphical summaries\n1.4, 1.5, 1.6\nTest 1 [L1, L2, L3]\n\n\n4 (4/22/24)\nFoundations for inference\n4.1, 4.2\n\n\n\n5 (4/29/24)\nOne-sample inference for numerical data\n4.3, 5.1\n\n\n\n6 (5/6/24)\nTwo-sample inference for numerical data\n5.2, 5.3, 5.4\nTest 2 [L4, L5]\n\n\n7 (5/13/24)\nNonparametric tests; analysis of variance\n5.5\n\n\n\n8 (5/20/24)\nPost-hoc inference in ANOVA; intro to categorical data analysis\n8.1\nTest 3 [L6, L9]\n\n\n9 (5/27/24)\nCategorical data analysis and contingency tables\n8.3, 8.5.1, 8.5.3\n\n\n\n10 (6/3/24)\nSimple linear regression\n6.1, 6.2, 6.4, 6.5\nTest 4 [L7, L8, L10]\n\n\nFinals (6/10/24)\nN/A\nN/A\nOral project assessment [L11]\n\n\n\n\n\nCourse policies\n\nTime commitment\nSTAT218 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including lectures, reading, assignments, and study time. Some variability in workload by week should be expected, and most students will need to budget a few extra hours each week. However, students can expect to be able to meet course expectations with a time commitment of 12-16 hours per week. Considering that class meetings account for four hours per week, students should anticipate devoting 8-12 hours outside of class. If you are spending considerably more time than this on a regular basis, please let me know.\n\n\nAttendance\nRegular attendance is essential for success in the course and required per University policy. Each student may miss two class meetings without notice but additional absences should be excusable and students should notify the instructor. Unexcused absences may negatively impact course grades.\n\n\nDeadlines and extensions\nA one-hour grace period is applied to all deadlines. Work submitted more than one hour after a deadline is considered late. Policies regarding late work are as follows:\n\nYou may turn in as many as four homework assignments up to 48 hours late without penalty at any time during the quarter and without notice. Subsequently, late work may incur a penalty in final grade calculations.\nLate submissions are not allowed for tests. You are expected to plan ahead in order to meet test deadlines; I recommend putting the dates in your calendar at the beginning of the quarter.\nExceptions may be granted for significant and unforeseen challenges (medical absences, family emergencies, and the like).\n\nExtensions may be arranged as needed if warranted by the circumstances and should be requested by email. When requesting an extension, you should explain why it is needed; it is at my discretion to grant the extension or not based on the reason provided. Extensions must be arranged at least 24 hours in advance of the original deadline; requests made after this time will not be considered as a general rule.\nThese policies are intended to provide you with some flexibility to work around unforeseen circumstances while maintaining accountability for completing coursework in a timely manner. That said, if any circumstances arise that the policies do not accommodate well, please let me know and I will do my best to work with you to keep you on track in the course.\n\n\nAcademic integrity\nYou are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR). Particularly important course policies related to academic integrity are discussed below.\nCollaboration. Collaboration among enrolled students is allowed and encouraged on homework assignments subject to the condition that every collaborator must make material contributions. Material contributions might include participation in group discussions, critique or presentation of a proposed solution, comparing numerical answers, and the like. However, group submissions are not allowed and you are expected to write up your own work. Copying the work of another student outright, knowingly allowing another student to copy your work, or submitting a copy of a shared set of answers is not acceptable and amounts to a violation of University policy on academic integrity. The best way to adhere to this policy and ensure your collaborations are productive is to:\n\nattempt problems individually before consulting others\nwrite up your own solutions in private\n\nCollaboration is not permitted on tests and will result in loss of credit.\nUse of AI. Learning to use AI effectively and responsibly for problem-solving in an academic context is a skill unto itself. Submitting problem prompts directly to ChatGPT will, most of the time, return superfluous, tangential, and erroneous answers that do not meet assessment criteria for satisfactory work. Furthermore, even when AI-generated material is technically accurate, outputs rarely conform to the examples set forth in class or the solution strategies that you have been taught.\nSo in the best-case scenario, AI-generated material might be useful but only if you expend additional effort refine the prompts you use and subsequently to parse, understand, and integrate outputs with class content. In the worst-case scenario, AI-generated material will be wrong or irrelevant and simply confuse you. Considering you are learning material that is new to you, you will most likely not be able to distinguish correct from incorrect outputs – if you could, you would have had no need to query in the first place – and it will therefore be difficult if not impossible to use AI effectively. Thus, using AI is more likely to hinder than to help your learning, and for this reason I do not recommend it.\nShould you choose to use AI you must use it as an aid only and not as a substitute for doing your own work. You will be responsible for using it thoughtfully and judiciously. That means critically assessing any outputs and continuing to prepare work to be submitted in your own words and using your own analyses. Submitting AI-generated outputs directly is never acceptable — doing so amounts to falsely representing material that you did not create as your own work and is a violation of University academic integrity policy. I will respond to such violations as follows.\n\nsome AI-generated content detected: loss of credit and warning\nflagrant AI plagiarism, first offense: loss of credit and report to OSRR\nflagrant AI plagiarism, second offense: automatic course failure and report to OSRR\n\nIf you are unsure about where the line is between acceptable and unacceptable use in any particular situation, please discuss the situation with me – I’d much rather help you learn to navigate the issue without the use of penalties wherever possible.\n\n\nAssessments and final grades\nI make every effort to provide consistent, fair, and accurate evaluation of student work. Please notify me of any suspected errors or discrepancies in evaluation promptly on an assignment-by-assignment basis (i.e., not at the end of the quarter) to guarantee consideration. Final (letter) grades will only be changed in the case of clerical errors. Attempting to negotiate scores or final grades is not appropriate.\nPer University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. If you feel your grade is unfairly assigned at the end of the course, you have the right to appeal it according to the procedure outlined here.\n\n\nCommunication and email\nStudents are encouraged to use face-to-face means of communication (class meetings and office hours) when possible. Every effort is made to respond to email within 48 weekday hours; please be aware that a message sent Thursday or Friday may not receive a reply until Monday or Tuesday. Time-sensitive messages should be identified as such in the subject line. For non-time-sensitive messages, please wait one week before sending a reminder.\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nCopyright and distribution of course materials\nStudents are not permitted to share or distribute any course materials without the written consent of the instructor. This includes, in particular, uploading materials or prepared solutions to online services and sharing materials or prepared solutions with students who may take the course in a future term. Transgressions of this policy compromise the effectiveness of instruction and assessment and do a disservice to current and future students."
  },
  {
    "objectID": "content/week2-descriptive.html#todays-agenda",
    "href": "content/week2-descriptive.html#todays-agenda",
    "title": "Descriptive statistics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] frequency distributions; measures of spread and center\n[lab] descriptive statistics and simple graphics in R"
  },
  {
    "objectID": "content/week2-descriptive.html#last-time",
    "href": "content/week2-descriptive.html#last-time",
    "title": "Descriptive statistics",
    "section": "Last time",
    "text": "Last time\n\n\n\nData semantics\n\n\ncategorical data: ordinal (ordered) or nominal (unordered)\nnumeric data: continuous (no ‘gaps’) or discrete (‘gaps’)\n\n\nData types and data structures in R\n\n\nbasic types: numeric, character, logical, integer\na vector is a collection of values of one type\na data frame is a type-heterogeneous list of vectors of equal length\n\n\nVectors can store observations of one variable:\n\n# 4 observations of age\nages &lt;- c(18, 22, 18, 12)\nages\n\n[1] 18 22 18 12\n\n\nData frames can store observations of many variables:\n\n# 3 observations of 2 variables\ndata.frame(subject.id = c(11, 2, 31),\n           age = c(24, 31, 17),\n           sex = c('m', 'm', 'f'))\n\n  subject.id age sex\n1         11  24   m\n2          2  31   m\n3         31  17   f\n\n\n\n\nTechniques for summarizing data depend on the data type"
  },
  {
    "objectID": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "href": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "title": "Descriptive statistics",
    "section": "What are descriptive statistics?",
    "text": "What are descriptive statistics?\nWe learned last time that a statistic is a data summary, i.e., any function of a set of observations.\nDescriptive statistics refers to analysis of sample characteristics using summary statistics.\n\nthese are data analyses that uses statistics interpreted on face value\nin contrast to inferential statistics, which uses statistics interpreted relative to a broader population\n\nDescriptive statistics can be either numerical or graphical; we’ll discuss both."
  },
  {
    "objectID": "content/week2-descriptive.html#dataset-famuss-study",
    "href": "content/week2-descriptive.html#dataset-famuss-study",
    "title": "Descriptive statistics",
    "section": "Dataset: FAMuSS study",
    "text": "Dataset: FAMuSS study\nObservational study of 595 individuals comparing change in arm strength before and after resistance training between genotypes for a region of interest on the ACTN3 gene.\n\nPescatello, L. S., et al. (2013). Highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or FAMuSS study. BioMed research international.\n\n\n\n\nExample data rows\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n125\n0\nFemale\n40\nCaucasian\n68\n171\nCT\n26"
  },
  {
    "objectID": "content/week2-descriptive.html#categorical-frequency-distributions",
    "href": "content/week2-descriptive.html#categorical-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Categorical frequency distributions",
    "text": "Categorical frequency distributions\nFor categorical variables, the frequency distribution is simply an observation count by category. For example:\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\ngenotype\n\n\n\n\n494\nTT\n\n\n510\nTT\n\n\n216\nCT\n\n\n19\nTT\n\n\n278\nCT\n\n\n86\nTT\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n173\n261\n161"
  },
  {
    "objectID": "content/week2-descriptive.html#numeric-frequency-distributions",
    "href": "content/week2-descriptive.html#numeric-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Numeric frequency distributions",
    "text": "Numeric frequency distributions\nFrequency distributions of numeric variables are observation counts by range; a plot of a numeric frequency distribution is called a histogram.\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\nbmi\n\n\n\n\n194\n22.3\n\n\n141\n20.76\n\n\n313\n23.48\n\n\n522\n29.29\n\n\n504\n42.28\n\n\n273\n20.34\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\n\n(10,20]\n(20,30]\n(30,40]\n(40,50]\n\n\n\n\n69\n461\n58\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe operation of dividing a numeric variable into interval ranges is called binning."
  },
  {
    "objectID": "content/week2-descriptive.html#histograms-and-binning",
    "href": "content/week2-descriptive.html#histograms-and-binning",
    "title": "Descriptive statistics",
    "section": "Histograms and binning",
    "text": "Histograms and binning\nBinning has a big effect on the visual impression. Which one captures the shape best?"
  },
  {
    "objectID": "content/week2-descriptive.html#shapes",
    "href": "content/week2-descriptive.html#shapes",
    "title": "Descriptive statistics",
    "section": "Shapes",
    "text": "Shapes\nFor numeric variables, the histogram reveals the shape of the distribution:\n\nsymmetric if it shows left-right symmetry about a central value\nskewed if it stretches farther in one direction from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#modes",
    "href": "content/week2-descriptive.html#modes",
    "title": "Descriptive statistics",
    "section": "Modes",
    "text": "Modes\nHistograms also reveal the number of modes or local peaks of frequency distributions.\n\nuniform if there are zero peaks\nunimodal if there is one peak\nbimodal if there are two peaks\nmultimodal if there are two or more peaks"
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nConsider four variables from the FAMuSS study. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nHere are some made-up data. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#descriptive-measures",
    "href": "content/week2-descriptive.html#descriptive-measures",
    "title": "Descriptive statistics",
    "section": "Descriptive measures",
    "text": "Descriptive measures\nA descriptive measure is a summary statistic that captures a particular feature of the frequency distribution of a numeric variable.\nCommonly, measures capture either location or spread.\n\n\nMeasures of location:\n\nmean\nmedian\nmode\npercentiles/quantiles\n\n\nMeasures of spread:\n\nrange (min and max)\ninterquartile range\naverage deviation\nvariance\nstandard deviation\n\n\n\nIt is common practice to report multiple measures."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-center",
    "href": "content/week2-descriptive.html#measures-of-center",
    "title": "Descriptive statistics",
    "section": "Measures of center",
    "text": "Measures of center\nA measure of center is a statistic that reflects the typical value of a variable.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23"
  },
  {
    "objectID": "content/week2-descriptive.html#quick-example",
    "href": "content/week2-descriptive.html#quick-example",
    "title": "Descriptive statistics",
    "section": "Quick example",
    "text": "Quick example\nConsider the first 8 observations of change in nondominant arm strength from the FAMuSS study data:\n\n\n40, 25, 40, 125, 40, 75, 100 and 57.1\n\n\nCompute the mean, median, and mode."
  },
  {
    "objectID": "content/week2-descriptive.html#comparing-measures-of-center",
    "href": "content/week2-descriptive.html#comparing-measures-of-center",
    "title": "Descriptive statistics",
    "section": "Comparing measures of center",
    "text": "Comparing measures of center\nEach statistic is a little different, but often they roughly agree; for example, all are between 20 and 25, which seems to capture the typical BMI well enough.\n\nHow do you think the frequency distribution affects which one is “best”?"
  },
  {
    "objectID": "content/week2-descriptive.html#means-medians-and-skewness",
    "href": "content/week2-descriptive.html#means-medians-and-skewness",
    "title": "Descriptive statistics",
    "section": "Means, medians, and skewness",
    "text": "Means, medians, and skewness\nThe mean and median both get ‘pulled’ in the direction of skewness, but the mean is more sensitive:\n\nComparing means and medians captures information about skewness present since:\n\nmean \\(&gt;\\) median: right skew\nmean \\(&lt;\\) median: left skew\nmean \\(\\approx\\) median: symmetric"
  },
  {
    "objectID": "content/week2-descriptive.html#when-to-use-modes",
    "href": "content/week2-descriptive.html#when-to-use-modes",
    "title": "Descriptive statistics",
    "section": "When to use mode(s)",
    "text": "When to use mode(s)\nMode is rarely used unless extreme skewness or multiple modes are present; below are two examples."
  },
  {
    "objectID": "content/week2-descriptive.html#percentiles",
    "href": "content/week2-descriptive.html#percentiles",
    "title": "Descriptive statistics",
    "section": "Percentiles",
    "text": "Percentiles\nA percentile is a value with specified proportions of data lying both above and below that value.\n\nmeasure of location (but not center)\ndefined with reference to the percentage of data below\n\nFor example, the 20th percentile is the value with 20% of observations below and 80% of observations above. Suppose we have 5 observations:\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n19\n20\n21\n25\n31\n\n\nrank\n1\n2\n3\n4\n5\n\n\n\n\n\nThe 20th percentile is not unique! In fact any number between 19 and 20 is a 20th percentile since it would satisfy:\n\n20% below (19)\n80% above (20, 21, 25, 31)"
  },
  {
    "objectID": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "href": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "title": "Descriptive statistics",
    "section": "Cumulative frequency distribution",
    "text": "Cumulative frequency distribution\nThe cumulative frequency distribution is a data summary showing percentiles. Think of it as percentile (y) against value (x).\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of some specific values:\n\nabout 40% of the subjects are 20 or younger\nabout 80% of the subjects are 24 or younger\n\nYour turn:\n\nRoughly what percentage of subjects are 22 or younger?\nAbout what age is the 10th percentile?"
  },
  {
    "objectID": "content/week2-descriptive.html#common-percentiles",
    "href": "content/week2-descriptive.html#common-percentiles",
    "title": "Descriptive statistics",
    "section": "Common percentiles",
    "text": "Common percentiles\n\n\nThe five-number summary is a collection of five percentiles that succinctly describe the frequency distribution:\n\n\n\nStatistic name\nMeaning\n\n\n\n\nminimum\n0th percentile\n\n\nfirst quartile\n25th percentile\n\n\nmedian\n50th percentile\n\n\nthird quartile\n75th percentile\n\n\nmaximum\n100th percentile\n\n\n\n\nBoxplots provide a graphical display of the five-number summary."
  },
  {
    "objectID": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "href": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "title": "Descriptive statistics",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\nNotice how the two displays align, and also how they differ. The histogram shows shape in greater detail, but the boxplot is much more compact."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-spread",
    "href": "content/week2-descriptive.html#measures-of-spread",
    "title": "Descriptive statistics",
    "section": "Measures of spread",
    "text": "Measures of spread\nThe spread of observations refers to how concentrated or diffuse the values are.\n\nTwo ways to understand and measure spread:\n\nranges of values capturing much of the distribution\ndeviations of values from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#range-based-measures",
    "href": "content/week2-descriptive.html#range-based-measures",
    "title": "Descriptive statistics",
    "section": "Range-based measures",
    "text": "Range-based measures\nA simple way to understand and measure spread is based on ranges. Consider more ages, sorted and ranked:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\nrank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\nThe range is the minimum and maximum values: \\[\\text{range} = (\\text{min}, \\text{max}) = (16, 34)\\]\nThe interquartile range (IQR) is the difference [75th percentile] - [25th percentile] \\[\\text{IQR} = 29 - 19 = 10\\] When might you prefer IQR to range? Can you think of an example?"
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures",
    "href": "content/week2-descriptive.html#deviation-based-measures",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe average deviation is defined as the average of the absolute values of the deviations from the mean: \\[\\frac{8 + 6 + 5 + 4 + 3 + 2 + 1 + 2 + 4 + 5 + 6 + 10}{12}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#mathematical-notations",
    "href": "content/week2-descriptive.html#mathematical-notations",
    "title": "Descriptive statistics",
    "section": "Mathematical notations",
    "text": "Mathematical notations\nFollowing the convention from before, write a set of \\(n\\) observations as \\(x_1, x_2, \\dots, x_n\\).\n\n\nThe mean of the observations is written: \\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]\nThe average deviation is: \\[\\frac{1}{n} \\sum_i |x_i - \\bar{x}|\\]\n\nThe variance is: \\[s_x^2 = \\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2\\]\nThe standard deviation is: \\[s_x = \\sqrt{\\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#interpretations",
    "href": "content/week2-descriptive.html#interpretations",
    "title": "Descriptive statistics",
    "section": "Interpretations",
    "text": "Interpretations\nListed from largest to smallest, here are each of the measures of spread for the 12 ages:\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin\nmax\niqr\nvariance\nst.dev\navg.dev\n\n\n\n\n16\n34\n8.5\n30.55\n5.527\n4.667\n\n\n\n\n\nThe interpretations differ between these statistics:\n\n[range] all of the data lies on an between 16 and 34 years old on an interval 18 years in width\n[IQR] the middle half of the data lies on an interval 8.5 years in width\n[average deviation] the average distance from the mean is 4.67 years\n[variance] the average squared distance from the mean is 30.55 years\\(^2\\)\n[standard deviation] the average squared distance from the mean, rescaled to years, is 5.53 years\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-descriptive.html#lab-robustness",
    "href": "content/week2-descriptive.html#lab-robustness",
    "title": "Descriptive statistics",
    "section": "Lab: robustness",
    "text": "Lab: robustness\nFor this lab we’ll continue to work with the FAMuSS data as we have throughout lecture.\nThis lab has two objectives:\n\nTeach you to compute descriptive statistics and prepare graphical summaries for a single variable in R\nLearn when and why to use certain descriptive statistics in place of others\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab1-rbasics.html",
    "href": "content/lab1-rbasics.html",
    "title": "Lab 1: R Basics",
    "section": "",
    "text": "This lab is intended to introduce you to the basics in R that you will need for this class. Most of our analyses will consist of just a few steps:\n\nload a dataset\nidentify and select variable(s) of interest\nperform one or more calculations using variable(s) of interest as inputs\n\nWe will illustrate this process so that you can get used to the mechanics and familiarize yourself with how different data types appear in R.\n\nHow to do this lab\nI’ve provided you with a project on Posit Cloud containing data files and a script (a script is a plain text file containing R commands). The script contains all commands shown in this document, and some blank areas for you to fill in, with comment lines (the ones starting with #) to help you navigate.\nYou should refer back to this document for instructions and context, and fill in the script as you go:\n\nrun the codes provided as you read through the narrative in this document and inspect the results\nin the ‘your turn’ sections, refer to the prompt in this document and use the example commands provided immediately beforehand to determine which command to write\nwrite in your commands in the script the space below the corresponding comment, not in the console (otherwise you’ll have a hard time keeping track of your work)\n\nYour goal is to complete all of the “your turn” parts in the script. Two practice problems are given at the end of the lab as homework for you to complete on your own before next class.\n\n\nHow to use this lab\nThis lab (and the lab activities in general) are designed to provide you with a set of examples to learn initially in class and then follow on your own later when doing the homework problems given at the end of the lab.\nIf you can do the examples and ‘your turn’ activities in class, all you’ll need to do to complete the homeworks is copy commands from those examples and activities and adjust some small details (variable names, dataset names, etc.).\nIf you later need to figure out how to do something in R for a homework problem or test, all you’ll need to do is refer back to the labs.\n\n\nPackages in R\nA “package” is a bundle of functions, datasets, and other objects that can be imported into R for use in your working environment. Many scripts begin by loading packages that will be used throughout the script. Packages are loaded using the command library(&lt;PACKAGE NAME&gt;) where &lt;PACKAGE NAME&gt; is replaced by the actual name of the package. For example:\n\nlibrary(tidyverse)\n\nPackages do need to be installed before they can be loaded. One of the nice things about using Posit Cloud is that I can manage all of these installs for you. However, if you ever wish to install and use a package that’s not available (or if you use R on your own machine), you can install a package using the command install.packages(\"&lt;PACKAGE NAME&gt;\") after replacing &lt;PACKAGE NAME&gt; with the actual name of the package (but keeping the quotation marks!).\n\n\nLoading a dataset\nThere are several ways to load datasets in R. The strategy we’ll use most often is to load an .RData file, but you will encounter a few others here and there.\n\n# load nhanes data\nload('data/nhanes.RData')\n\nThis command looks for a file called nhanes.RData in a directory folder named data and reads the file.\nNotice that once you run the command, an object called nhanes appears in the “Environment” tab in the upper right hand panel of your RStudio window.\nIf you click the little blue carrot next to nhanes in the environment tab, you will then see a list of variables contained in the dataset. You can also see the first few rows of the dataset using head(...).\n\n# first few rows\nhead(nhanes)\n\n# A tibble: 6 × 9\n  subj.id gender   age poverty pulse bpsys1 bpdia1 totchol sleephrsnight\n    &lt;int&gt; &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;         &lt;int&gt;\n1       1 male      34    1.36    70    114     88    3.49             4\n2       2 male      34    1.36    70    114     88    3.49             4\n3       3 male      34    1.36    70    114     88    3.49             4\n4       5 female    49    1.91    86    118     82    6.7              8\n5       8 female    45    5       62    106     62    5.82             8\n6       9 female    45    5       62    106     62    5.82             8\n\n\nThis kind of object in R is called a data frame. Data frames are displayed in a tabular layout, like a spreadsheet. While data frames should be arranged so that observations are shown in rows and variables in columns, this is not guaranteed, so you should be in the habit of checking to make sure the layout is sensible; otherwise, you might accidentally perform bogus calculations and analyses.\nBeyond providing a sanity check, inspecting the data frame will show you three key pieces of information besides the values of the first few observations of each variable.\n\nData dimensions: how many observations (rows) and how many variables (columns)\nVariable names: subj.id, gender, age, etc.\nData types:\n\nint for integer (numerical data type)\nfct for factor (categorical data type)\nnum for numeric (numerical data type)\nchr for character (categorical data type)\n\n\nSo, for example, seeing that pulse is of data type int tells you that pulse is a discrete numerical variable. It also tells you what name to use to refer to the variable in subsequent R commands.\n\n\n\n\n\n\nYour turn\n\n\n\nThere is another data file in the data directory called famuss.RData. Load this into the environment, preview the first few observations, and check the variable names and data types.\n\n# load famuss dataset\n\n# preview first few rows\n\nTo check your understanding:\n\nhow many observations and variables?\nidentify a categorical variable\nwhat kind of variable is bmi?\n\n\n\n\n\nSelecting variables\nThe variable names in a dataset can be used to retrieve or refer to specific variables. For example, try running this command:\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# preview first few values\nhead(total.cholesterol)\n\n[1] 3.49 3.49 3.49 6.70 5.82 5.82\n\n\nThat command did the following:\n\nextracted the totchol column of nhanes (the nhanes$totchol part)\nassigned the result a new name total.cholesterol (the &lt;- part)\n\nAssignment (&lt;-) is a very important concept in R – you can store the result of any calculation as an object with a name of your choosing.\nYou’ll notice that total.cholesterol looks a bit different than the data frame in terms of its appearance. This is because it’s not a data frame but rather a different kind of object called a vector: a collection of values of the same data type.\n\n\n\n\n\n\nYour turn\n\n\n\nExtract the change in nondominant arm strength variable from the FAMuSS dataset, and store it as a vector called strength.\n\n# store the change in nondominant arm strength variable as a vector called 'strength'\n\n# preview the first few values\n\n\n\n\n\nPerforming calculations\nExtracting and storing variables as vectors isn’t strictly necessary, but does make it easier to perform many calculations. While you’re a beginner, I’d recommend using this strategy.\n\nNumeric summaries\nMost simple summary statistics can be calculated using simple functions in R that take a single vector argument. For example, to calculate the average, minimum, and maximum total cholesterol among the respondents in the sample:\n\n# average total cholesterol\nmean(total.cholesterol)\n\n[1] 5.042938\n\n# minimum\nmin(total.cholesterol)\n\n[1] 2.33\n\n# maximum\nmax(total.cholesterol)\n\n[1] 13.65\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nFind the average percent change in nondominant arm strength of participants in the FAMuSS study sample using the strength vector you created before.\n\n# compute mean change in nondominant arm strength\n\n\n\n\n\nCategorical summaries\nMost data summaries for categorical variables proceed from counts of the number of observations in each category. These counts can be obtained by passing a vector of observations to table(...):\n\n# retreive sex variable\nsex &lt;- nhanes$gender\n\n# counts\ntable(sex)\n\nsex\nfemale   male \n  1588   1591 \n\n\nTo obtain the proportion of observations in each category – the counts divided by the total number of observations – pass the table to the proportions(...) function:\n\n# proportions\ntable(sex) |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nThe character string |&gt; is a bit of syntax that you could read verbally as ‘then’: first make a table, then obtain proportions. It’s known as the pipe operator, because it ‘pipes’ the result of the command on its left into the command on its right.\nTo see another example of the pipe operator in action, you could rewrite the previous command as a chain of three steps:\n\n# same as above\nsex |&gt; table() |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nYou could interpret this as follows: start with sex, pass that to table(), then pass the result to proportions.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the FAMuSS dataset, calculate the genotype frequencies in the sample (i.e., find the proportion of observations of each genotype).\n\n# retrieve genotype\n\n# counts\n\n# proportions\n\n\n\nWhile the analyses you’ll learn will get more complex than computing summary statistics, the mechanics of performing the computations in R will be analogous to what you just did: executing a one-line command with a vector input.\n\n\n\nPutting together the pieces\nReflect for a moment on what you just did: you wrote a few lines of code to import a dataset, extract a variable, and compute a statistic. If you filled in the script as instructed, you now have a record of the commands you executed that you can use to retrace your steps.\nIn fact, anyone with your script and the data files (including future you) could easily reproduce your work. Reproducibility is a pillar of data-driven science; by storing analyses in the form of executable scripts, researchers can easily create and share records of their work.\nWe could put the steps above together in just a few lines as if it were a short script. Typical style is to provide line-by-line comments explaining what the commands do.\n\n# import nhanes data\nload('data/nhanes.RData')\n\n# inspect data\nhead(nhanes)\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# compute average total cholesterol\nmean(total.cholesterol)\n\n# extract sex\nsex &lt;- nhanes$gender\n\n# proportions of men and women in sample\ntable(sex) |&gt; proportions()\n\n\n\n\n\n\n\nYour turn\n\n\n\nFollow the example above and combine the previous exercises into a few lines of code with appropriate line comments.\n\n# load famuss dataset\n\n# inspect data\n\n# extract nondominant change in arm strength\n\n# compute average change in strength\n\n# extract genotype\n\n# compute genotype frequencies (proportions)\n\n\n\nIf this was all entirely new to you, congratulations on writing your first lines of code!\n\n\nExtras\n\nReading CSV files\nOften data are stored in spreadsheets, which can be easily converted to comma-separated values or CSV files (extension .csv). These are plain-text files that are a bit more lightweight than an Excel spreadsheet.\nR can read CSV (as well as other) files. The read.csv(...) function will parse the file and produce a data frame. The result can be assigned a name and stored as an object in the environment.\n\n# parse a csv file\nread.csv('data/gss.csv')\n\n# store the result in the environment\ngss &lt;- read.csv('data/gss.csv')\n\nMost of the time in class we’ll load .RData files or obtain datasets through packages (more on this later), but if you use R outside of class you may find it more common to manage data input via .csv files.\n\n\nMore about R\nWhile you will learn new commands going forward, we won’t go much more in depth with R than what you just saw. However, if you’re interested in understanding the above concepts in greater detail, or learning about R as a programming environment, see An Introduction to R.\n\n\n\nPractice problems\nDue before the next class meeting.\n\nThe census dataset contains a sample of data for 377 individuals included in the 2000 U.S. census. Load and inspect the dataset, and determine:\n\nthe youngest and oldest individual in the sample\nthe average total personal income\nthe average total family income\nhow many variables are in the dataset, not including census year and FIPS code\nhow many categorical variables are in the dataset, not including FIPS code\n\n\n\nThe cdc.samp dataset in the oibiostat package contains a sample of data for 60 individuals surveyed by the CDC’s Behavioral Risk Factors Surveillance System (BRFSS). Use the provided commands to load the dataset, and then inspect it the usual way. Notice that several of the variables are 1’s and 0’s. Use the command ?cdc.samp to view the data documentation.\n\nWhat do the values (1’s and 0’s) mean in the exerany variable?\nWhat proportion of the sample are men? What proportion are women?\nFor each general health category, find the proportion of respondents who rated themselves in that category.\nHow many of the respondents have health coverage? (Hint: sum(x) will add up the values in a vector x; adding up a collection of 1’s and 0’s is equivalent to counting the number of 1’s.) What percentage of the respondents have health coverage?"
  },
  {
    "objectID": "content/week1-studies.html#todays-agenda",
    "href": "content/week1-studies.html#todays-agenda",
    "title": "Welcome to STAT218",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nCourse logistics\n[lecture] Study designs\n[activity, if time] Distinguishing study types"
  },
  {
    "objectID": "content/week1-studies.html#icebreakers",
    "href": "content/week1-studies.html#icebreakers",
    "title": "Welcome to STAT218",
    "section": "Icebreakers",
    "text": "Icebreakers\nBy show of hands…\n\n\nFirst statistics class ever?\nLast statistics class ever?\nExpect to take STAT313?\nExpect to use statistics for your degree coursework or senior project?\nConsidering a statistics or data science minor?"
  },
  {
    "objectID": "content/week1-studies.html#class-composition",
    "href": "content/week1-studies.html#class-composition",
    "title": "Welcome to STAT218",
    "section": "Class composition",
    "text": "Class composition\nBy the numbers…"
  },
  {
    "objectID": "content/week1-studies.html#statistics-and-uncertainty",
    "href": "content/week1-studies.html#statistics-and-uncertainty",
    "title": "Welcome to STAT218",
    "section": "Statistics and uncertainty",
    "text": "Statistics and uncertainty\n\nLife is full of uncertainty, and this can make a lot of questions hard to answer, because similar situations do not always result in the same outcome.\n\nStatistical thinking: uncertainty is measurable.\nWhat statistics can offer:\n\nprinciples for designing studies and collecting data in order to capture outcome variability\ndata analytic tools to distinguish random from systematic variability\nheuristics to make inferences that account for uncertainty"
  },
  {
    "objectID": "content/week1-studies.html#course-goal-and-scope",
    "href": "content/week1-studies.html#course-goal-and-scope",
    "title": "Welcome to STAT218",
    "section": "Course goal and scope",
    "text": "Course goal and scope\nThe overarching goal of 218 is to introduce you to statistics in a hands-on way that is relevant to your major.\nSo we will focus on:\n\nstatistical thinking, study design, and data analysis\nclassical methods, mostly developed 1900-1940\ncase studies from life sciences"
  },
  {
    "objectID": "content/week1-studies.html#materials",
    "href": "content/week1-studies.html#materials",
    "title": "Welcome to STAT218",
    "section": "Materials",
    "text": "Materials\nComputer/tablet. You’ll need a laptop (preferred) or tablet with keyboard (workable).\nCourse website. All materials are hosted/linked on the course website. I won’t be using Canvas.\nTextbook. Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences. I suggest a $5-15 donation.\nStatistical software. R/RStudio hosted online via posit.cloud workspace. You will need to create an account and purchase a $5/month student subscription."
  },
  {
    "objectID": "content/week1-studies.html#class-meetings",
    "href": "content/week1-studies.html#class-meetings",
    "title": "Welcome to STAT218",
    "section": "Class meetings",
    "text": "Class meetings\nClass meetings will usually consist of a reading quiz, a lecture, a break, and a lab.\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class."
  },
  {
    "objectID": "content/week1-studies.html#assignments",
    "href": "content/week1-studies.html#assignments",
    "title": "Welcome to STAT218",
    "section": "Assignments",
    "text": "Assignments\nYou will have three categories of assignments:\n\nhomework problems: two per class due by next class\ntests: every 2-3 weeks, distributed Wednesday, due Friday\na project: find and present a case study\n\nDeadline policies:\n\none-hour grace period on all deadlines\nfour homework problem sets can be turned in up to 48 hours late without notice\nbesides free lates, extensions must be arranged 24 hours in advance of the deadline"
  },
  {
    "objectID": "content/week1-studies.html#grades",
    "href": "content/week1-studies.html#grades",
    "title": "Welcome to STAT218",
    "section": "Grades",
    "text": "Grades\nEvery graded question/problem is matched to one or more of the 11 course learning outcomes.\n\nQuestions/problems are evaluated as satisfactory (S), needs improvement (NI), or missing (M).\nFor each outcome, the percentage of questions/problems awarded a satisfactory mark is used to determine whether that outcome is fully met, partly met, or not met:\n\nfully met: 80% or more of matched questions satisfactory\npartly met: 50% – 80% of matched questions satisfactory\nnot met: less than 50% of matched questions satisfactory\n\n\nYour course grade is based on how many learning outcomes are fully met. To pass, you must partly or fully meet at least 6 outcomes; for a C-, you must fully meet at least 3 outcomes."
  },
  {
    "objectID": "content/week1-studies.html#important-policies",
    "href": "content/week1-studies.html#important-policies",
    "title": "Welcome to STAT218",
    "section": "Important policies",
    "text": "Important policies\n\nextensions must be confirmed (not simply requested) 24 hours in advance\ncollaboration on homework is encouraged, but everyone involved needs to…\n\nmake a contribution\nwrite up their own work\n\nsubmitting AI-generated content in place of your own work is not acceptable\n\nresponsible use is okay, but not recommended (GPT outputs are misleading)\npenalties for AI plagiarism depend on precedent and severity\n\n\n\n\n\nMinor offense\nMajor offense\nPenalty\n\n\n\n\nFirst\n\nloss of credit and warning\n\n\nSecond\nFirst\nloss of credit and OSRR report\n\n\nThird\nSecond\ncourse failure and second OSRR report"
  },
  {
    "objectID": "content/week1-studies.html#what-is-a-study",
    "href": "content/week1-studies.html#what-is-a-study",
    "title": "Welcome to STAT218",
    "section": "What is a study?",
    "text": "What is a study?\nA study is an effort to collect data in order to answer one or more research questions.\n\nstudies must be well-matched to research questions to provide good answers\nhow data are obtained is just as important as how the resulting data are analyzed\nno analysis, no matter how sophisticated will rescue a poorly conceived study\n\nA study unit is the smallest object or entity that is measured in a study; also called experimental unit or observational unit."
  },
  {
    "objectID": "content/week1-studies.html#two-types-of-studies",
    "href": "content/week1-studies.html#two-types-of-studies",
    "title": "Welcome to STAT218",
    "section": "Two types of studies",
    "text": "Two types of studies\nObservational studies collect data from an existing situation without intervention.\n\nAim is to detect associations and patterns\nCan’t be used to establish causal links\n\nExperiments collect data from a situation in which one or more interventions have been introduced by the investigator.\n\nAim is to draw conclusions about the causal effect of interventions\nStronger form of scientific evidence than an observational study\n\nOften, observational studies are used to explore/generate hypotheses prior to designing an experiment."
  },
  {
    "objectID": "content/week1-studies.html#comparing-study-types",
    "href": "content/week1-studies.html#comparing-study-types",
    "title": "Welcome to STAT218",
    "section": "Comparing study types",
    "text": "Comparing study types\nEither type of study can be used to address a question.\n\n\n\n\n\n\n\n\nQuestion\nObservational study\nExperiment\n\n\n\n\nAre diet and mood related?\nConduct surveys on diet, lifestyle, and affect\nRecruit study participants, assign diets, measure affect\n\n\nIs vaping safer than smoking?\nFollow groups of vapers and smokers over time and record health outcomes\nAmong a group of smokers, assign some to switch to vaping; compare health outcomes over time\n\n\nDo insecticide applications affect soil microbes?\nAnalyze soil samples from farms using different insecticides\n[Your turn]\n\n\n\nCan you think of pros and cons for each study type?"
  },
  {
    "objectID": "content/week1-studies.html#why-does-intervention-matter",
    "href": "content/week1-studies.html#why-does-intervention-matter",
    "title": "Welcome to STAT218",
    "section": "Why does intervention matter?",
    "text": "Why does intervention matter?\nControl over conditions allows a researcher to study causal effects resulting from interventions. This is not possible in observational studies due to the potential for confounding.\n\n\nConfounding: an unobserved condition is associated with both the study condition and the outcome.\n\nFailure to measure and account for confounders potentially distorts observed associations\nExample: a study finds that dog owners live longer, but doesn’t measure exercise; so it might just be the daily walks.\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved variable) --- B(study variable) & C(study outcome) \n\n\n\n\n\n\n\n\nThis is very common in observational studies, because you can’t measure every study condition."
  },
  {
    "objectID": "content/week1-studies.html#antidote-randomization",
    "href": "content/week1-studies.html#antidote-randomization",
    "title": "Welcome to STAT218",
    "section": "Antidote: randomization",
    "text": "Antidote: randomization\nThe ability to control study conditions allows researchers to randomly allocate interventions among study subjects.\n\nRandomization eliminates confounding by isolating the condition(s) of interest:\n\ninterventions are independent of extraneous conditions ⟹ no association possible\nif outcomes differ systematically according to the intervention, you can be certain that it is not an artifact\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved condition) x-.-x B(study condition)\n  A --- C(outcome)"
  },
  {
    "objectID": "content/week1-studies.html#practical-consequences",
    "href": "content/week1-studies.html#practical-consequences",
    "title": "Welcome to STAT218",
    "section": "Practical consequences",
    "text": "Practical consequences\nThe ability to randomize interventions in experiments means:\n\nobserved associations are independent of extraneous factors\nresults can support causal inferences\n\nThe absence of randomization in observational studies means:\n\nconfounding is always possible\nresults may be misleading"
  },
  {
    "objectID": "content/week1-studies.html#experimental-designs",
    "href": "content/week1-studies.html#experimental-designs",
    "title": "Welcome to STAT218",
    "section": "Experimental designs",
    "text": "Experimental designs\nA treatment is an experimental intervention; the design of an experiment refers to how treatments are allocated to study units.\nThe most basic design is:\n\n[balanced] each treatment is replicated an equal number of times\n[randomized] treatments are allocated completely at random to study units\n[no crossover] each study unit receives exactly one treatment\n\nWe’ll call this a completely randomized design. It’s the only kind of experimental design we’re going to consider in STAT218.\nThere are many other designs that we won’t discuss (but see STAT313); these are all about improving experimental efficiency by controlling extraneous variation."
  },
  {
    "objectID": "content/week1-studies.html#data-collection",
    "href": "content/week1-studies.html#data-collection",
    "title": "Welcome to STAT218",
    "section": "Data collection",
    "text": "Data collection\nStudy units should be chosen so as to represent a larger collection.\n\n\n\n\nA study population is a collection of all study units of interest.\nA sample is a subcollection from a population:\n\nrandom if study units have a known chance of inclusion in the sample\nnonrandom or convenience otherwise\n\n\n\nThe gold standard is the simple random sample: each study unit in the population has an equal chance of inclusion in the sample."
  },
  {
    "objectID": "content/week1-studies.html#leap-study",
    "href": "content/week1-studies.html#leap-study",
    "title": "Welcome to STAT218",
    "section": "LEAP Study",
    "text": "LEAP Study\n\n\nLearning early about peanut allergy (LEAP) study:\n\n640 infants in UK with eczema or egg allergy but no peanut allergy enrolled\neach infant randomly assigned to peanut consumption and peanut avoidance groups\n\npeanut consumption: fed 6g peanut protein daily until 5 years old\npeanut avoidance: no peanut consumption until 5 years old\n\nat 5 years old, oral food challenge (OFC) allergy test administered\n13.3% of the avoidance group developed allergies, compared with 1.9% of the consumption group\n\n\n\n\n\nStudy characteristics\n\n\nStudy type: experiment\nStudy population: UK infants with eczema or egg allergy but no peanut allergy\nSample: 640 infants from population\nStudy design: completely randomized design\nTreatments: peanut consumption; peanut avoidance\nStudy outcome: development of peanut allergy by 5 years of age\n\n\n\n\n\n\nStudy results\n\n\nModerated peanut consumption causes a reduction in the likelihood of developing an allergy."
  },
  {
    "objectID": "content/week1-studies.html#checklist-for-next-time",
    "href": "content/week1-studies.html#checklist-for-next-time",
    "title": "Welcome to STAT218",
    "section": "Checklist for next time",
    "text": "Checklist for next time\n\nObtain a copy of the textbook.\nCreate a posit.cloud account and purchase a student subscription. Ensure you can access the stat218-s24 workspace.\nComplete practice problems and reading before class.\nWrite down one question about the reading.\nPrint a paper or virtual copy of the slides."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account",
    "href": "content/week1-studies.html#posit-cloud-account",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nGo to: course webpage &gt; syllabus &gt; materials. Then look for the link to join the class workspace:"
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-1",
    "href": "content/week1-studies.html#posit-cloud-account-1",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nFollow prompts to create an account. Use your Cal Poly email."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-2",
    "href": "content/week1-studies.html#posit-cloud-account-2",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nOnce your email is verified, return to posit.cloud (or click the link in the syllabus again), and join the class workspace."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-3",
    "href": "content/week1-studies.html#posit-cloud-account-3",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nUpgrade your account to the student plan. Input payment details."
  },
  {
    "objectID": "content/week1-studies.html#printing-slides",
    "href": "content/week1-studies.html#printing-slides",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nOpen menu from lower left"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-1",
    "href": "content/week1-studies.html#printing-slides-1",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nNavigate to tools"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-2",
    "href": "content/week1-studies.html#printing-slides-2",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nSelect PDF export mode"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-3",
    "href": "content/week1-studies.html#printing-slides-3",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\n\n\nThen print from browser to PDF\n\n\nI suggest landscape layout and either 1, 2 or 4 slides per page\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-datatypes.html#todays-agenda",
    "href": "content/week2-datatypes.html#todays-agenda",
    "title": "Data semantics and data types",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\n[lecture] data semantics and data types\n[lab] R basics"
  },
  {
    "objectID": "content/week2-datatypes.html#data-semantics",
    "href": "content/week2-datatypes.html#data-semantics",
    "title": "Data semantics and data types",
    "section": "Data semantics",
    "text": "Data semantics\n\nData are a set of measurements.\nA variable is any measured attribute of study units.\nAn observation is a measurement of one or more variables taken on one particular study unit.\n\nIt is usually expedient to arrange data values in a table in which each row is an observation and each column is a variable:"
  },
  {
    "objectID": "content/week2-datatypes.html#leap-example",
    "href": "content/week2-datatypes.html#leap-example",
    "title": "Data semantics and data types",
    "section": "LEAP example",
    "text": "LEAP example\nA table showing the observations and variables for the LEAP study would look like this:\n\n\n\n\n\n\n\n\n\n\nparticipant.ID\ntreatment.group\nofc.test.result\n\n\n\n\nLEAP_100522\nPeanut Consumption\nPASS OFC\n\n\nLEAP_103358\nPeanut Consumption\nPASS OFC\n\n\nLEAP_105069\nPeanut Avoidance\nPASS OFC\n\n\nLEAP_105328\nPeanut Consumption\nPASS OFC\n\n\n\n\n\nThe table you saw in the reading was a summary of the data (not the data itself):\n\n\n\n\n\n\n\n\n\n\n \nFAIL OFC\nPASS OFC\n\n\n\n\nPeanut Avoidance\n36\n227\n\n\nPeanut Consumption\n5\n262"
  },
  {
    "objectID": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "href": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "title": "Data semantics and data types",
    "section": "Numeric and categorical variables",
    "text": "Numeric and categorical variables\nVariables are classified according to their values. Values can be one of two different types:\n\nA variable is numeric if its value is a number\nA variable is categorical if its value is a category, usually recorded as a name or label\n\nFor example:\n\nthe value of sex can be male or female, so it is categorical\nwhereas age (in years) can be any positive integer, so it is numeric"
  },
  {
    "objectID": "content/week2-datatypes.html#variable-subtypes",
    "href": "content/week2-datatypes.html#variable-subtypes",
    "title": "Data semantics and data types",
    "section": "Variable subtypes",
    "text": "Variable subtypes\nFurther distinctions are made based on the type of number or type of category used to measure an attribute. Can you match the subtypes to the variables at right?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nhispanic\ngrade\nweight\n\n\n\n\n15\nnot\n10\n78.02\n\n\n18\nhispanic\n12\n78.47\n\n\n17\nnot\n11\n95.26\n\n\n18\nnot\n12\n95.26\n\n\n\n\n\n\n\n\na numerical variable is discrete if there are ‘gaps’ between its possible values\na numerical variable is continuous if there are no such gaps\na categorical variable is nominal if its levels are not ordered\na categorical variable is ordinal if its levels are ordered"
  },
  {
    "objectID": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "href": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "title": "Data semantics and data types",
    "section": "Many ways to measure attributes",
    "text": "Many ways to measure attributes\nVariable type (or subtype) is not an inherent quality — attributes can often be measured in many different ways.\nFor instance, age might be measured as either a discrete, continuous, or ordinal variable, depending on the situation:\n\n\n\nAge (years)\nAge (minutes)\nAge (brackets)\n\n\n\n\n12\n6307518.45\n10-18\n\n\n8\n4209187.18\n5-10\n\n\n21\n11258103.08\n18-30\n\n\n\n\nNumeric variables can always be represented as categorical, but not the other way around."
  },
  {
    "objectID": "content/week2-datatypes.html#your-turn",
    "href": "content/week2-datatypes.html#your-turn",
    "title": "Data semantics and data types",
    "section": "Your turn",
    "text": "Your turn\nClassify each variable as nominal, ordinal, discrete, or continuous:\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ngenotype\nsex\nage\nrace\nbmi\n\n\n\n\n33.3\nCT\nFemale\n19\nCaucasian\n21.01\n\n\n71.4\nCT\nFemale\n18\nOther\n23.18\n\n\n37.5\nCC\nFemale\n21\nCaucasian\n28.92\n\n\n50\nCC\nFemale\n28\nAsian\n21.16\n\n\n\n\n\nData are from an observational study investigating demographic, physiological, and genetic characteristics associated with muscle strength.\n\nndrm.ch is change in strength in nondominant arm after resistance training\ngenotype indicates genotype at a particular location within the ACTN3 gene"
  },
  {
    "objectID": "content/week2-datatypes.html#common-summary-statistics",
    "href": "content/week2-datatypes.html#common-summary-statistics",
    "title": "Data semantics and data types",
    "section": "Common summary statistics",
    "text": "Common summary statistics\n\nA statistic is a data summary: in mathematical terms, a function of several observations\n\n\n\nFor numeric variables, the most common summary statistic is the average value:\n\\[\\text{average} = \\frac{\\text{sum of values}}{\\text{# observations}}\\]\nFor example, the average percent change in nondominant arm strength was 53.291%.\n\nFor categorical variables, the most common summary statistic is a proportion:\n\\[\\text{proportion}_i = \\frac{\\text{# observations in category } i}{\\text{# observations}}\\]\nFor example:\n\n\n\nGenotype proportions\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n0.2908\n0.4387\n0.2706"
  },
  {
    "objectID": "content/week2-datatypes.html#descriptive-analyses",
    "href": "content/week2-datatypes.html#descriptive-analyses",
    "title": "Data semantics and data types",
    "section": "Descriptive analyses",
    "text": "Descriptive analyses\nSometimes, a few clever summary statistics can be used to answer a research question.\n\nHow much does the average change in arm strength differ by genotype, if at all?\n\nComputing per-genotype averages provides an answer:\n\n\n\n\n\n\n\n\n\n\n\ngenotype\navg.change\nn.obs\nprop.obs\n\n\n\n\nTT\n58.08\n161\n0.2706\n\n\nCT\n53.25\n261\n0.4387\n\n\nCC\n48.89\n173\n0.2908\n\n\n\n\n\nNumber of observations and proportions are included because they provide information about genotype frequencies in the sample.\n\nconveys how many individuals were measured\nalso provides an estimate of genotype frequencies in the population"
  },
  {
    "objectID": "content/week2-datatypes.html#common-mathematical-notation",
    "href": "content/week2-datatypes.html#common-mathematical-notation",
    "title": "Data semantics and data types",
    "section": "Common mathematical notation",
    "text": "Common mathematical notation\nWhile we won’t use mathematical expressions too often in STAT218, it’s useful to be aware of some common notations.\nTypically, a set of observations is written as:\n\\[x_1, x_2, \\dots, x_n\\]\n\n\\(x\\) represents the variable (e.g., genotype, age, percent change, etc.)\nsubscript indexes observations: \\(x_i\\) is the \\(i\\)th observation\n\\(n\\) is the total number of observations\n\nThe sum of the observations is written \\(\\sum_i x_i\\), where the symbol \\(\\sum\\) stands for ‘summation’. This is useful for writing the formula for computing an average:\n\\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]"
  },
  {
    "objectID": "content/week2-datatypes.html#lab-data-basics-in-r",
    "href": "content/week2-datatypes.html#lab-data-basics-in-r",
    "title": "Data semantics and data types",
    "section": "Lab: data basics in R",
    "text": "Lab: data basics in R\nThe variable types we just discussed map pretty neatly (but not perfectly) onto the main “data types” in R:\n\nnumeric ➜ integer, numeric\ncategorical ➜ character, factor, logical\n\nThe primary way data are arranged in R is in a data frame. This lab will show you how to load, inspect, and use data frames.\nYour objectives in this lab are:\n\nlearn to load and inspect datasets\nlearn to recognize data types\nlearn to perform simple calculations (averages, etc.)"
  },
  {
    "objectID": "content/week2-datatypes.html#opening-the-lab-activity",
    "href": "content/week2-datatypes.html#opening-the-lab-activity",
    "title": "Data semantics and data types",
    "section": "Opening the lab activity",
    "text": "Opening the lab activity\nNavigate to posit.cloud. Then:\n\n\n\n\n\n\nMake sure the class workspace “stat218-s24” is highlighted at left. If “Your Workspace” is highlighted, you won’t see the example assignment.\nClick on the lab1-rbasics, then wait.\n\nOnce everyone is ready, we’ll have a look at the example files together.\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week1-activity-studydesigns.html",
    "href": "content/week1-activity-studydesigns.html",
    "title": "Distinguishing study designs",
    "section": "",
    "text": "Recall that the difference between an observational study and an experiment hinges on whether researchers intentionally intervene on the system of study (experiment) or passively record outcomes (observational study).\nIn this activity you’ll read abstracts from a few published studies and determine what kind of study is described in the abstract. You do not need to consider the examples in order — start with the ones that look most interesting.\nFor each example, identify the following:\n\nstudy type\nstudy population\nsample characteristics\nstudy outcome(s)\n\n\nExample 1: selenium exposure and Mediterranean diet\nThe following is from the abstract of a study investigating dietary mitigation of selenium exposure:\n\nSelenium is a trace element found in many chemical forms. Selenium and its species have nutritional and toxicologic properties, some of which may play a role in the etiology of neurological disease. We hypothesized that adherence to the Mediterranean-Dietary Approach to Stop Hypertension Intervention for Neurodegenerative Delay (MIND) diet could influence intake and endogenous concentrations of selenium and selenium species, thus contributing to the beneficial effects of this dietary pattern. We carried out a cross-sectional study of 137 non-smoking blood donors (75 females and 62 males) from the Reggio Emilia province, Northern Italy. We assessed MIND diet adherence using a semiquantitative food frequency questionnaire. We assessed selenium exposure through dietary intake and measurement of urinary and serum concentrations, including speciation of selenium compound in serum … Adherence to the MIND diet was positively associated with dietary selenium intake and urinary selenium excretion, whereas it was inversely associated with serum concentrations of overall selenium and organic selenium … Our results suggest that greater adherence to the MIND diet is non-linearly associated with lower circulating concentrations of selenium and of 2 potentially neurotoxic species of this element, selenoprotein P and selenate. This may explain why adherence to the MIND dietary pattern may reduce cognitive decline.\n\nUrbano, T., et al. (2023). Adherence to the Mediterranean-DASH Intervention for Neurodegenerative Delay (MIND) diet and exposure to selenium species: A cross-sectional study. Nutrition Research.\n\n\nExample 2: fermented kimchi and glucose metabolism\nThe following is from an abstract of a study investigating possible benefits of kimchi consumption among prediabetic individuals:\n\nWith the increased incidence of diabetes mellitus, the importance of early intervention in prediabetes has been emphasized … We hypothesized that kimchi and its fermented form would have beneficial effects on glucose metabolism in patients with prediabetes. A total of 21 participants with prediabetes were enrolled. During the first 8 weeks, they consumed either fresh (1-day-old) or fermented (10-day-old) kimchi. After a 4-week washout period, they switched to the other type of kimchi for the next 8 weeks. Consumption of both types of kimchi significantly decreased body weight, body mass index, and waist circumference. Fermented kimchi decreased insulin resistance, and increased insulin sensitivity … Systolic and diastolic blood pressure (BP) decreased significantly in the fermented kimchi group. The percentages of participants who showed improved glucose tolerance were 9.5 and 33.3% in the fresh and fermented kimchi groups, respectively.\n\nAn, S. Y., et al. (2013). Beneficial effects of fresh and fermented kimchi in prediabetic individuals. Annals of Nutrition and Metabolism, 63(1-2), 111-119."
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures-1",
    "href": "content/week2-descriptive.html#deviation-based-measures-1",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe variance is the average squared deviation from the mean (but divided by one less than the sample size): \\[\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}\\]\nThe standard deviation is the square root of the variance: \\[\\sqrt{\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-location",
    "href": "content/week2-descriptive.html#measures-of-location",
    "title": "Descriptive statistics",
    "section": "Measures of location",
    "text": "Measures of location\nOften location is specified by the “center” of a frequency distribution.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23"
  },
  {
    "objectID": "content/lab2-descriptive.html",
    "href": "content/lab2-descriptive.html",
    "title": "Lab 2: Descriptive statistics and simple graphics",
    "section": "",
    "text": "The objectives of this lab are to learn to:\n\nmake basic statistical graphics for visualizing frequency distributions\ncompute common measures of location and spread\ndiscern appropriate measures of location and spread based on presence of outliers and skewness\n\nWe’ll use the FAMuSS dataset, as in lecture.\n\nlibrary(tidyverse)\n\n# load famuss dataset \nload('data/famuss.RData')\n\n# inspect data frame\nhead(famuss)\n\n# A tibble: 6 × 9\n  ndrm.ch drm.ch sex      age race      height weight genotype   bmi\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;\n1      40     40 Female    27 Caucasian   65      199 CC        33.1\n2      25      0 Male      36 Caucasian   71.7    189 CT        25.8\n3      40      0 Female    24 Caucasian   65      134 CT        22.3\n4     125      0 Female    40 Caucasian   68      171 CT        26.0\n5      40     20 Female    32 Caucasian   61      118 CC        22.3\n6      75      0 Female    24 Hispanic    62.2    120 CT        21.8\n\n\nAs a quick refresher, you can extract a vector of the observations for any particular variable from the dataframe as follows: famuss$[variable name].\nWhile not strictly necessary, I recommend retrieving and storing the variable(s) you’ll use as separate objects, at least while you’re still a beginner. For example:\n\n# extract the age variable\nfamuss$age\n\n# store the age column as a vector\nage &lt;- famuss$age\n\n\nBasic statistical graphics\n\nCategorical variables\nFor categorical variables, as you saw in the last lab, table(...) will tabulate counts of the number of occurrences of each unique values of a categorical variable. The result can be passed to barplot(...) for a simple barplot to visualize the frequency distribution:\n\n# retrieve genotype\ngenotype &lt;- famuss$genotype\n\n# make a table, generate a barplot\ntable(genotype) |&gt; barplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nMake a barplot to visualize the frequency distribution of racial groups in the FAMuSS study.\n\n# retrieve race\n\n# make a table, generate a barplot\n\n\n\n\n\nNumerical variables\nIf a numeric variable is discrete without too many values, the frequency distribution could be visualized without any binning as a barplot. However, this is not recommended because it will result in a plot that is not scaled properly.\nInstead, it is better to make a histogram with one bin per unique possible value; this will scale the axis properly. However, it is also acceptable to make a histogram with binning that will result in aggregating some values. Both are shown below.\nThe approximate number of bins, and thus the amount of aggregation, is controlled by the argument breaks = ...:\n\n# retrieve age\nage &lt;- famuss$age\n\n# effectively, a bar plot of ages\nhist(age, breaks = 25)\n\n\n\n\n\n\n\n# fewer bins\nhist(age, breaks = 10)\n\n\n\n\n\n\n\n\nFor continuous variables, binning is a necessity. The second plot is better, because it shows the shape more clearly without obscuring too much detail.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a histogram of percent change in dominant arm strength. Experiment to see how the shape of the distribution appears at various binning resolutions; then pick a number of breaks that you feel reflects the data best.\n\n# retrieve dominant arm percent change\n\n# make a histogram; find a binning that captures the shape well\n\n\n\nTo store a graphic as a separate file for use in other documents, find the ‘export’ icon in the plot panel and select the ‘Save as image’ option; then follow prompts. Try this for the plot you just made.\n\n\n\nDescriptive statistics\nIn class we discussed several descriptive statistics for numeric variables. These statistics are so commonly used that they have their own functions in R.\n\nMeasures of location\nThe following functions return common measures of location for numeric variables:\n\nmean(...) returns an average\nmedian(...) returns a median\nquantile(...) returns a quantile\nmin(...) and max(...) return a minimum and a maximum, respectively\n\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age (middle value)\nmedian(age)\n\n[1] 22\n\n# 25th percentile of age (\"quantile\" is another term for percentile)\nquantile(age, probs = 0.25)\n\n25% \n 20 \n\n# 25th *and* 75th percentile of age\nquantile(age, probs = c(0.25, 0.75))\n\n25% 75% \n 20  27 \n\n# minimum age\nmin(age)\n\n[1] 17\n\n# maximum age\nmax(age)\n\n[1] 40\n\n\nNotice how the probs = ... argument to the quantile() function, which specifies which percentile R will calculate, can be used to calculate multiple percentiles at once.\nIf you want to inspect all of the location measures above (the five-number summary plus the mean) the summary(...) function will do just that.\n\n# all common location measures\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   17.0    20.0    22.0    24.4    27.0    40.0 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry computing the location measures above for percent change in dominant arm strength. Compare the mean and median. What does the comparison tell you about the skewness of this variable? Is this consistent with the histogram from the previous ‘your turn’?\n\n# compute the five-number summary for change in dominant arm strength\n\n\n\n\n\nMeasures of spread\nThe following functions return common measures of spread for numeric variables:\n\nrange(...) returns the range (min, max)\nIQR(...) returns the interquartile range (middle 50% of data)\nvar(...) returns the variance (average squared deviations from mean)\nsd(...) returns the standard deviation (variance, on original scale)\n\n\n# age range\nrange(age)\n\n[1] 17 40\n\n# interquartile range of ages (width of interval containing middle 50% of data)\nIQR(age)\n\n[1] 7\n\n# variance of age\nvar(age)\n\n[1] 33.79966\n\n# standard deviation of age\nsd(age)\n\n[1] 5.813748\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCompute the standard deviation of percent change in dominant arm strength.\n\n# standard deviation of percent change in dominant arm strength\n\nInterpret the value in context.\n\n\n\n\nRobustness\nWhen would you use median instead of mean? IQR instead of standard deviation? The answer has to do with robustness, which in statistics means sensitivity to outliers or extreme values.\nTo explore this idea, recall first the actual mean and median ages for the participants in the FAMuSS study as well as the age range:\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age\nmedian(age)\n\n[1] 22\n\n# range\nrange(age)\n\n[1] 17 40\n\n\nNow let’s add an artificial outlier – a few hypothetical participant who are in their 80’s and 90’s – and compute the measures of location again:\n\n# average age\nmean(c(age, 96, 92, 87, 91))\n\n[1] 24.84975\n\n# median age\nmedian(c(age, 96, 92, 87, 91))\n\n[1] 22\n\n\nThe mean increases while the median does not. More broadly, statistics based on percentiles are in general insensitive to outliers, unless there’s a large group of outlying observations. In this sense they are robust statistics.\nA similar difference can be observed between deviation-based measures and interquartile range. The original measures were:\n\n# variance of ages\nvar(age)\n\n[1] 33.79966\n\n# interquartile range of ages\nIQR(age)\n\n[1] 7\n\n\nNow adding in our artificial outliers:\n\n# age variance\nvar(c(age, 96, 92, 87, 91))\n\n[1] 63.55598\n\n# age iqr\nIQR(c(age, 96, 92, 87, 91))\n\n[1] 7\n\n\n\n\n\nGrouped summaries\nWhat if you wish to find the mean percent change in dominant arm strength separately for each genotype?\nThe tidyverse package loaded at the outset has a pair of functions, group_by and summarize, that allow you to do this efficiently. The steps are:\n\nStart with the data frame famuss\ngroup by the genotype variable\nsummarize\n\n\n# average dominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch))\n\n# A tibble: 3 × 2\n  genotype avg.drm.ch\n  &lt;fct&gt;         &lt;dbl&gt;\n1 CC            10.7 \n2 CT             8.49\n3 TT            13.0 \n\n\nThe summarize function can actually compute multiple summaries: each argument should specify a name for the summary and the calculation to perform in the format &lt;NAME&gt; = &lt;FUNCTION&gt;(&lt;COLUMN NAME&gt;):\n\n# average dominant and nondominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch),\n            avg.ndrm.ch = mean(ndrm.ch))\n\n# A tibble: 3 × 3\n  genotype avg.drm.ch avg.ndrm.ch\n  &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 CC            10.7         48.9\n2 CT             8.49        53.2\n3 TT            13.0         58.1\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate the median percent change in dominant arm strength separately for each genotype by modifying the first example above.\n\n# median percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\nThese problems may look lengthy at face value, but the calculations are rather brief. A suggestion is: determine which calculations are required for each part and focus on doing those calculations first; then look back over your results to interpret them and answer the prompts.\n\n[L3] Use the census data again from the previous problem set and carry out the following descriptive summaries.\n\nMake a histogram of total personal income. Choose the binning so as to capture the shape well but not obscure too much detail. Are there outliers?\nCompute the mean and five-number summary of total personal income. Which measure of location is most appropriate and why?\nCompute the interquartile range and standard deviation of total personal income and interpret them in context. Which measure is more appropriate and why?\nCompute the median total personal income separately for men and women.\n\n\n\n[L3] Data from Chen, W., et al., Maternal investment increases with altitude in a frog on the Tibetan Plateau. Journal of Evolutionary Biology 26-12 (2013) includes measurements pertaining to egg clutches of several populations of frog at breeding ponds (sites) in the eastern Tibetan Plateau.\n\nHow many samples were collected at each site?\nCompute the frequency distribution of site altitudes among samples collected in the study.\nMake a barplot of the frequency distribution from (a). Are samples collected more or less uniformily across altitudes? If not, which altitudes are most represented in the sample?\nMake a histogram of clutch volumes. Describe the shape and number of modes.\nCalculate the mean and five-number summary of clutch volume.\nCalculate and interpret the standard deviation, variance, and interquartile range.\nCalculate the average clutch volume separately for each altitude. Does average clutch volume seem to differ by altitude?\n[optional] Devise a way to calculate the average absolute deviation."
  },
  {
    "objectID": "content/week2-descriptive.html",
    "href": "content/week2-descriptive.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "[lecture] frequency distributions; measures of spread and center\n[lab] descriptive statistics and simple graphics in R"
  },
  {
    "objectID": "content/week3-multivariate.html#todays-agenda",
    "href": "content/week3-multivariate.html#todays-agenda",
    "title": "Bivariate summaries",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\nLoose end: robustness\nBivariate numeric and graphical summaries\nLab: bivariate graphics in R"
  },
  {
    "objectID": "content/week3-multivariate.html#robustness",
    "href": "content/week3-multivariate.html#robustness",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\n\nPercentile-based measures of location and spread are less sensitive to outliers\n\nConsider adding an observation of 94 to our 12 ages from last time. This is an outlier.\n\n\n\n# append an outlier\nages_add &lt;- c(ages, 94)\n\n# means\nc(original = mean(ages), with.outlier = mean(ages_add))\n\n    original with.outlier \n    24.00000     29.38462 \n\n# medians\nc(original = median(ages), with.outlier = median(ages_add))\n\n    original with.outlier \n        23.5         25.0 \n\n# IQR\nc(original = IQR(ages), with.outlier = IQR(ages_add))\n\n    original with.outlier \n         8.5          9.0 \n\n# SD\nc(original = sd(ages), with.outlier = sd(ages_add))\n\n    original with.outlier \n    5.526794    20.122701 \n\n\n\nThe effect of this outlier on each statistic is:\n\nmean increases by 22.44%\nmedian increases by 6.38%\nIQR increases by 5.88%\nSD increases by 264.09%\n\nRobustness refers to sensitivity to outliers. Mean and SD are less robust than median and IQR."
  },
  {
    "objectID": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "href": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "title": "Bivariate summaries",
    "section": "Limitations of univariate summaries",
    "text": "Limitations of univariate summaries\n\nUnivariate summaries aim to capture the distribution of values of a single variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nboth unimodal, no obvious outliers\nheights symmetric\nweights right-skewed\nbut these observations actually come in pairs\n\n\n\nUnivariate summaries don’t reflect how the variables might be related."
  },
  {
    "objectID": "content/week3-multivariate.html#bivariate-summaries",
    "href": "content/week3-multivariate.html#bivariate-summaries",
    "title": "Bivariate summaries",
    "section": "Bivariate summaries",
    "text": "Bivariate summaries\n\nBivariate summaries aim to capture a relationship between two variables.\n\n\n\nA simple example is a scatterplot:\n\n\n\n\n\n\n\n\n\n\nEach point represents a pair of values \\((h, w)\\) for one study participant.\n\nReveals a relationship: taller participants tend to be heavier\nBut no longer shows individual distributions clearly\n\nNotice, though, that the marginal means (dashed red lines) still capture the center well."
  },
  {
    "objectID": "content/week3-multivariate.html#summary-types",
    "href": "content/week3-multivariate.html#summary-types",
    "title": "Bivariate summaries",
    "section": "Summary types",
    "text": "Summary types\n\nBivariate summary techniques differ depending on the data types of the variables.\n\n\n\n\n\n\n\n\nQuestion\nComparison type\n\n\n\n\nDid genotype frequencies differ by race or sex among study participants?\ncategorical/categorical\n\n\nWere differential changes in arm strength observed according to genotype?\nnumeric/categorical\n\n\nDid change in arm strength appear related in any way to body size among study participants?\nnumeric/numeric\n\n\nDid study participants experience similar or different changes in arm strength depending on arm dominance?\n??"
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical",
    "href": "content/week3-multivariate.html#categoricalcategorical",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\n\nA contingency table is a bivariate tabular summary of two categorical variables; it shows the frequency of each pair of values. Usually the marginal totals are also shown.\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n106\n149\n98\n353\n\n\nMale\n67\n112\n63\n242\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\nThere are multiple ways to convert to proportions by using different denominators, and these yield proportions with distinct interpretations:\n\ngrand total – frequency of genotype/sex combination\nrow total – genotype frequency by sex\ncolumn total – sex frequency by genotype"
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-1",
    "href": "content/week3-multivariate.html#categoricalcategorical-1",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid genotype frequencies differ by sex among study participants?\nFor this question, the row totals should be used to convert to proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n0.3003\n0.4221\n0.2776\n1\n\n\nMale\n0.2769\n0.4628\n0.2603\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are quite close, suggesting minimal sex differences."
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-2",
    "href": "content/week3-multivariate.html#categoricalcategorical-2",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid sex frequencies differ by genotype among study participants?\nFor this question, the column totals should be used to compute proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.6127\n0.5709\n0.6087\n\n\nMale\n0.3873\n0.4291\n0.3913\n\n\ntotal\n1\n1\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are close, suggesting minimal genotype differences."
  },
  {
    "objectID": "content/week3-multivariate.html#numericnumeric",
    "href": "content/week3-multivariate.html#numericnumeric",
    "title": "Bivariate summaries",
    "section": "Numeric/numeric",
    "text": "Numeric/numeric\nDid change in arm strength appear related in any way to body size among study participants?\n\nPairwise scatterplots indicate no apparent relationships."
  },
  {
    "objectID": "content/week3-multivariate.html#correlation",
    "href": "content/week3-multivariate.html#correlation",
    "title": "Bivariate summaries",
    "section": "Correlation",
    "text": "Correlation\nIn addition to graphical techniques, for numeric/numeric comparisons, there are also quantiative measures of relationship.\n\n\n\n\nCorrelation measures the strength of linear relationship, and is defined as: \\[r_{xy} = \\frac{1}{n - 1}\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\\]\n\n\\(r \\rightarrow 1\\): positive relationship\n\\(r \\rightarrow -1\\): negative relationship\n\\(r \\rightarrow 0\\): no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "href": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "title": "Bivariate summaries",
    "section": "Uncorrelated \\(\\neq\\) no relationship",
    "text": "Uncorrelated \\(\\neq\\) no relationship\nCorrelation only captures linear relationships. Always do a graphical check.\n\nCommon misconceptions:\n\nstronger correlation \\(\\longleftrightarrow\\) greater slope\nweaker correlation \\(\\longleftrightarrow\\) no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-correlations",
    "href": "content/week3-multivariate.html#interpreting-correlations",
    "title": "Bivariate summaries",
    "section": "Interpreting correlations",
    "text": "Interpreting correlations\nDid change in arm strength appear related in any way to body size among study participants?\nHere are the correlations corresponding to the plots we checked earlier.\n\n\n\n\n\n\n\n\n\n\n\n \nheight\nweight\nbmi\n\n\n\n\ndrm.ch\n-0.1104\n-0.1159\n-0.07267\n\n\nndrm.ch\n-0.265\n-0.2529\n-0.1436\n\n\n\n\n\nSo there aren’t any linear relationships here. A rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\\(|r| = 1\\): either a mistake or not real data"
  },
  {
    "objectID": "content/week3-multivariate.html#numericcategorical",
    "href": "content/week3-multivariate.html#numericcategorical",
    "title": "Bivariate summaries",
    "section": "Numeric/categorical",
    "text": "Numeric/categorical\nSide-by-side boxplots are usually a good option. Avoid stacked histograms.\nWere differential changes in arm strength observed according to genotype?\n\n\n\n\n\n\n\n\n\n\n\n\nLook for differences:\n\nlocation shift\nspread\ncenter\n\nWhat do you think? Any notable relationships?"
  },
  {
    "objectID": "content/week3-multivariate.html#robustness-1",
    "href": "content/week3-multivariate.html#robustness-1",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\nThe effect of the outlier on each measure is captured by the ratio \\(\\frac{\\text{measure with outlier}}{\\text{measure without outlier}}\\), which shows:\n\nthe IQR increases by 5.88%\nthe standard deviation increases by 264%"
  },
  {
    "objectID": "content/week3-multivariate.html#choosing-appropriate-measures-of-spread",
    "href": "content/week3-multivariate.html#choosing-appropriate-measures-of-spread",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures of spread",
    "text": "Choosing appropriate measures of spread"
  },
  {
    "objectID": "content/week3-multivariate.html#choosing-appropriate-measures",
    "href": "content/week3-multivariate.html#choosing-appropriate-measures",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures",
    "text": "Choosing appropriate measures\n\nWhen outliers are present, use percentile-based measures; otherwise, use mean and standard deviation or variance\n\n\nCheck your understanding: which measures are most appropriate for each variable above?"
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-scatterplots",
    "href": "content/week3-multivariate.html#interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots\n\nScatterplots show the presence or absence of an association.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf there is an association (i.e., discernible pattern), it can be:\n\nlinear or nonlinear\n\nlinear if scatter roughly follows a straight line, nonlinear otherwise\n\npositive or negative\n\npositive if scatter is increasing from left to right, negative otherwise\n\n\nThe plot at left is an example of a positive and (slightly) nonlinear relationship."
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-scatterplots-1",
    "href": "content/week3-multivariate.html#interpreting-scatterplots-1",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots"
  },
  {
    "objectID": "content/week3-multivariate.html#practice-interpreting-scatterplots",
    "href": "content/week3-multivariate.html#practice-interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Practice interpreting scatterplots",
    "text": "Practice interpreting scatterplots"
  },
  {
    "objectID": "content/week3-multivariate.html#data-transformations",
    "href": "content/week3-multivariate.html#data-transformations",
    "title": "Bivariate summaries",
    "section": "Data transformations",
    "text": "Data transformations\nSometimes a simple transformation can reveal a linear relationship on an alternate scale.\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, weight)\n\n[1] 0.5308787\n\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356"
  },
  {
    "objectID": "content/lab3-multivariate.html",
    "href": "content/lab3-multivariate.html",
    "title": "Lab 3: Bivariate summaries",
    "section": "",
    "text": "This lab covers descriptive summaries and graphics for two variables. The goal of the activity is to learn to produce joint summaries of two variables for identifying relationships, in particular:\n\ncontingency tables\nproportional stacked bar plots\nscatterplots\nside-by-side boxplots\n\nWe will use the FAMuSS dataset again.\n\nlibrary(tidyverse)\nload('data/famuss.RData')\n\n\nContingency tables\n\nCounts\nA contingency table is a summary of two categorical variables. Specifically, it is a two-way table in which:\n\nrows correspond to the values of one variable\ncolumns correspond to the values of the other variable\nentries are counts of the numbers of observations for each combination of values\n\nIn fact, a contingency table is a frequency distribution for two variables. It can be constructed in R by retrieving the two variables of interest and providing them as arguments to table().\n\n# retrieve variables of interest\ngenotype &lt;- famuss$genotype\nsex &lt;- famuss$sex\n\n# make a contingency table\ntable(sex, genotype)\n\n        genotype\nsex       CC  CT  TT\n  Female 106 149  98\n  Male    67 112  63\n\n\nThe order of the arguments determines the row/column orientation of the table.\nThe values show the observation counts for each combination, so, for example, the number 106 in the upper-leftmost cell indicates that 106 participants were women with genotype CC.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a contingency table of race and genotype, with race as the row variable and genotype as the column variable.\n\n# retrieve variables of interest\n\n# make a contingency table\n\n\n\n\n\nProportions\nTo convert a contingency table to proportions, one can use either the grand total, row totals, or column totals. The resulting proportions have different meanings:\n\ngrand total – combination frequencies\nrow totals – column variable frequencies by row variable value\ncolumn totals – row variable frequencies by column variable value\n\nIn the sex/genotype contingency table, these proportions would be computed and interpreted as follows:\n\n# combination frequencies (using grand total as denominator)\ntable(sex, genotype) |&gt; proportions(margin = NULL)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.1781513 0.2504202 0.1647059\n  Male   0.1126050 0.1882353 0.1058824\n\n# genotype frequencies by sex (using row margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 1)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.3002833 0.4220963 0.2776204\n  Male   0.2768595 0.4628099 0.2603306\n\n# sex frequencies by genotype (using column margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 2)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.6127168 0.5708812 0.6086957\n  Male   0.3872832 0.4291188 0.3913043\n\n\nNotice that the margin = ... argument controls which totals are used as denominators in computing proportions. The syntax is:\n\nmargin = NULL specifies grand total\nmargin = 1 uses row totals\nmargin = 2 uses column totals\n\nThe interpretation of entries depends on which type of proportion is comupted. For instance, looking at only the upper-leftmost entry in each of the three tables above:\n\n[first table] 17.82% of participants were women with genotype CC\n[second table] 30.03% of female participants had genotype CC\n[third table] 61.27% of participants with genotype CC were women\n\nIf you lose track of which margin was which, you can always check yourself by looking at which values sum to one: if all the values add up to one, grand totals were used; if rows sum to one, row totals were used; and if columns add up to one, column totals were used.\n\n\n\n\n\n\nYour turn\n\n\n\nConvert your contingency table of genotype and race from the previous ‘your turn’ to a table showing genotype frequencies by race.\n\n# genotype frequencies by race\n\nThen, interpret the entry for Hispanic/CT in context.\n\n\n\n\nGraphics\nThe latter two proportion tables – those constructed using row or column totals – can be visualized graphically using barplot(...):\n\n# stacked bar plot showing genotype frequencies by sex\ntable(genotype, sex) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nThis function is a little finnicky about the table orientation, so the most straightforward way to approach making the plot is to adhere to this rule of thumb:\n\nalways use margin = 2\nspecify the order of the variables as they are input to table(...) so that the grouping variable appears second\n\nSo, for instance, to visualize the sex frequencies by genotype, we would do the following:\n\n# stacked bar plot showing sex frequencies by genotype\ntable(sex, genotype) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nIf you see uneven bar heights, that means you have not configured the table correctly before passing it to barplot(...).\n\n\n\n\n\n\nYour turn\n\n\n\nMake a plot showing the genotype frequencies by race.\n\n# stacked bar plot showing genotype frequencies by race\n\nCheck your understanding:\n\nFor which group is the CC frequency highest?\nFor which group is the CC frequency lowest?\n\n\n\n\n\n\nScatterplots and correlation\nTaller people tend to be heavier. We should expect a relationship between weight and height. The example below shows a scatterplot of the two variables, and computes the correlation (measure of linear relationship).\n\n# retrieve height and weight columns\nheight &lt;- famuss$height\nweight &lt;- famuss$weight\n\n# basic scatterplot\nplot(height, weight)\n\n\n\n\n\n\n\n# correlation\ncor(weight, height)\n\n[1] 0.5308787\n\n\nA rough rule of thumb for interpreting correlations is as follows:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case, the correlation of 0.53 indicates a moderate positive relationship.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there a relationship between nondominant and dominant percent change in arm strength? Make a scatterplot and compute the correlation.\n\n# retrieve the percent change variables\n\n# construct a scatterplot\n\n# compute the correlation\n\n\n\n\n\nSide-by-side boxplots\nConsider one of the main questions for the study: were differences on the ACTN gene region associated with differential change in arm strength after resistance training?\nThis is a comparison between a categorical variable (genotype) and numeric variable (percent change in arm strength). For this type of comparison, a set of side-by-side boxplots is best:\n\n# side-by-side boxplots for non-dominant arm\nboxplot(ndrm.ch ~ genotype, data = famuss)\n\n\n\n\n\n\n\n# change the orientation \nboxplot(ndrm.ch ~ genotype, data = famuss, horizontal = T)\n\n\n\n\n\n\n\n\nThis graphics summarizes the frequency distribution of percent change in non-dominant arm strength separately for each genotype. It is essentially a grouped summary.\n\n\n\n\n\n\nYour turn\n\n\n\nConstruct side-by-side boxplots of percent change in dominant arm strength by genotype.\n\n# make side-by-side boxplots of percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\n\nTrait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease (CHD); 12,986 participants were recruited for a study examining this hypothesis and followed for five years. The anger dataset includes data for the 8557 participants identified as having normal blood pressure (normotensives) and records, for each participant, whether they are classified as having high, moderate, or low trait anger, and whether they experienced a CHD event during the study period.\n\nMake a contingency table showing trait anger and CHD event occurrence.\nFind the proportion of participants who experienced a CHD event in each trait anger group.\nConstruct a stacked bar plot to visualize the frequencies you found in (b).\nFind the ratio \\(\\frac{\\text{CHD frequency in the high anger group}}{\\text{CHD frequency in the low anger group}}\\). This is called the “relative risk” of a CHD event. (You can calculate this quantity by hand based on your result in (b).)\n\n\n\nUse the nhanes data from earlier to answer the following questions.\n\nMake a scatterplot of systolic blood pressure and diastolic blood pressure. Put systolic blood pressure on the y axis. Characterize the association, if any, as positive/negative/non-associated and linear/nonlinear.\nCompute and interpret the correlation between systolic blood pressure and diastolic blood pressure.\nDoes there appear to be a substantial difference in the distribution of total cholesterol by sex? Construct side-by-side barplots to answer this question."
  },
  {
    "objectID": "content/week3-multivariate.html#interpretation",
    "href": "content/week3-multivariate.html#interpretation",
    "title": "Bivariate summaries",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356\n\n\n\nApplying these rules of thumb:\n\n\\(|r| &lt; 0.3\\): minimal association\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong\n\nAnd:\n\n\\(r &gt; 0\\): positive association\n\\(r &lt; 0\\): negative association\n\nThe interpretation is:\nThere is a moderately strong positive linear relationship between height and log weight.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week3-bivariate.html#todays-agenda",
    "href": "content/week3-bivariate.html#todays-agenda",
    "title": "Bivariate summaries",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\nLoose end: robustness\nBivariate numeric and graphical summaries\nLab: bivariate graphics in R"
  },
  {
    "objectID": "content/week3-bivariate.html#robustness",
    "href": "content/week3-bivariate.html#robustness",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\n\nPercentile-based measures of location and spread are less sensitive to outliers\n\nConsider adding an observation of 94 to our 12 ages from last time. This is an outlier.\n\n\n\n# append an outlier\nages_add &lt;- c(ages, 94)\n\n# means\nc(original = mean(ages), with.outlier = mean(ages_add))\n\n    original with.outlier \n    24.00000     29.38462 \n\n# medians\nc(original = median(ages), with.outlier = median(ages_add))\n\n    original with.outlier \n        23.5         25.0 \n\n# IQR\nc(original = IQR(ages), with.outlier = IQR(ages_add))\n\n    original with.outlier \n         8.5          9.0 \n\n# SD\nc(original = sd(ages), with.outlier = sd(ages_add))\n\n    original with.outlier \n    5.526794    20.122701 \n\n\n\nThe effect of this outlier on each statistic is:\n\nmean increases by 22.44%\nmedian increases by 6.38%\nIQR increases by 5.88%\nSD increases by 264.09%\n\nRobustness refers to sensitivity to outliers. Mean and SD are less robust than median and IQR."
  },
  {
    "objectID": "content/week3-bivariate.html#choosing-appropriate-measures",
    "href": "content/week3-bivariate.html#choosing-appropriate-measures",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures",
    "text": "Choosing appropriate measures\n\nWhen outliers are present, use percentile-based measures; otherwise, use mean and standard deviation or variance\n\n\nCheck your understanding: which measures are most appropriate for each variable above?"
  },
  {
    "objectID": "content/week3-bivariate.html#limitations-of-univariate-summaries",
    "href": "content/week3-bivariate.html#limitations-of-univariate-summaries",
    "title": "Bivariate summaries",
    "section": "Limitations of univariate summaries",
    "text": "Limitations of univariate summaries\n\nUnivariate summaries aim to capture the distribution of values of a single variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nboth unimodal, no obvious outliers\nheights symmetric\nweights right-skewed\nbut these observations actually come in pairs\n\n\n\nUnivariate summaries don’t reflect how the variables might be related."
  },
  {
    "objectID": "content/week3-bivariate.html#bivariate-summaries",
    "href": "content/week3-bivariate.html#bivariate-summaries",
    "title": "Bivariate summaries",
    "section": "Bivariate summaries",
    "text": "Bivariate summaries\n\nBivariate summaries aim to capture a relationship between two variables.\n\n\n\nA simple example is a scatterplot:\n\n\n\n\n\n\n\n\n\n\nEach point represents a pair of values \\((h, w)\\) for one study participant.\n\nReveals a relationship: taller participants tend to be heavier\nBut no longer shows individual distributions clearly\n\nNotice, though, that the marginal means (dashed red lines) still capture the center well."
  },
  {
    "objectID": "content/week3-bivariate.html#summary-types",
    "href": "content/week3-bivariate.html#summary-types",
    "title": "Bivariate summaries",
    "section": "Summary types",
    "text": "Summary types\n\nBivariate summary techniques differ depending on the data types of the variables.\n\n\n\n\n\n\n\n\nQuestion\nComparison type\n\n\n\n\nDid genotype frequencies differ by race or sex among study participants?\ncategorical/categorical\n\n\nWere differential changes in arm strength observed according to genotype?\nnumeric/categorical\n\n\nDid change in arm strength appear related in any way to body size among study participants?\nnumeric/numeric\n\n\nDid study participants experience similar or different changes in arm strength depending on arm dominance?\n??"
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical",
    "href": "content/week3-bivariate.html#categoricalcategorical",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\n\nA contingency table is a bivariate tabular summary of two categorical variables; it shows the frequency of each pair of values. Usually the marginal totals are also shown.\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n106\n149\n98\n353\n\n\nMale\n67\n112\n63\n242\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\nThere are multiple ways to convert to proportions by using different denominators, and these yield proportions with distinct interpretations:\n\ngrand total – frequency of genotype/sex combination\nrow total – genotype frequency by sex\ncolumn total – sex frequency by genotype"
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical-1",
    "href": "content/week3-bivariate.html#categoricalcategorical-1",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid genotype frequencies differ by sex among study participants?\nFor this question, the row totals should be used to convert to proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n0.3003\n0.4221\n0.2776\n1\n\n\nMale\n0.2769\n0.4628\n0.2603\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are quite close, suggesting minimal sex differences."
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical-2",
    "href": "content/week3-bivariate.html#categoricalcategorical-2",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid sex frequencies differ by genotype among study participants?\nFor this question, the column totals should be used to compute proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.6127\n0.5709\n0.6087\n\n\nMale\n0.3873\n0.4291\n0.3913\n\n\ntotal\n1\n1\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are close, suggesting minimal genotype differences."
  },
  {
    "objectID": "content/week3-bivariate.html#numericcategorical",
    "href": "content/week3-bivariate.html#numericcategorical",
    "title": "Bivariate summaries",
    "section": "Numeric/categorical",
    "text": "Numeric/categorical\nSide-by-side boxplots are usually a good option. Avoid stacked histograms.\nWere differential changes in arm strength observed according to genotype?\n\n\n\n\n\n\n\n\n\n\n\n\nLook for differences:\n\nlocation shift\nspread\ncenter\n\nWhat do you think? Any notable relationships?"
  },
  {
    "objectID": "content/week3-bivariate.html#numericnumeric",
    "href": "content/week3-bivariate.html#numericnumeric",
    "title": "Bivariate summaries",
    "section": "Numeric/numeric",
    "text": "Numeric/numeric\nDid change in arm strength appear related in any way to body size among study participants?\n\nPairwise scatterplots indicate no apparent relationships."
  },
  {
    "objectID": "content/week3-bivariate.html#interpreting-scatterplots",
    "href": "content/week3-bivariate.html#interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots\n\nScatterplots show the presence or absence of an association.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf there is an association (i.e., discernible pattern), it can be:\n\nlinear or nonlinear\n\nlinear if scatter roughly follows a straight line, nonlinear otherwise\n\npositive or negative\n\npositive if scatter is increasing from left to right, negative otherwise\n\n\nThe plot at left is an example of a positive and (slightly) nonlinear relationship."
  },
  {
    "objectID": "content/week3-bivariate.html#practice-interpreting-scatterplots",
    "href": "content/week3-bivariate.html#practice-interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Practice interpreting scatterplots",
    "text": "Practice interpreting scatterplots"
  },
  {
    "objectID": "content/week3-bivariate.html#correlation",
    "href": "content/week3-bivariate.html#correlation",
    "title": "Bivariate summaries",
    "section": "Correlation",
    "text": "Correlation\nIn addition to graphical techniques, for numeric/numeric comparisons, there are also quantiative measures of relationship.\n\n\n\n\nCorrelation measures the strength of linear relationship, and is defined as: \\[r_{xy} = \\frac{1}{n - 1}\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\\]\n\n\\(r \\rightarrow 1\\): positive relationship\n\\(r \\rightarrow -1\\): negative relationship\n\\(r \\rightarrow 0\\): no relationship"
  },
  {
    "objectID": "content/week3-bivariate.html#interpreting-correlations",
    "href": "content/week3-bivariate.html#interpreting-correlations",
    "title": "Bivariate summaries",
    "section": "Interpreting correlations",
    "text": "Interpreting correlations\nDid change in arm strength appear related in any way to body size among study participants?\nHere are the correlations corresponding to the plots we checked earlier.\n\n\n\n\n\n\n\n\n\n\n\n \nheight\nweight\nbmi\n\n\n\n\ndrm.ch\n-0.1104\n-0.1159\n-0.07267\n\n\nndrm.ch\n-0.265\n-0.2529\n-0.1436\n\n\n\n\n\nSo there aren’t any linear relationships here. A rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\\(|r| = 1\\): either a mistake or not real data"
  },
  {
    "objectID": "content/week3-bivariate.html#data-transformations",
    "href": "content/week3-bivariate.html#data-transformations",
    "title": "Bivariate summaries",
    "section": "Data transformations",
    "text": "Data transformations\nSometimes a simple transformation can reveal a linear relationship on an alternate scale.\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, weight)\n\n[1] 0.5308787\n\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356"
  },
  {
    "objectID": "content/week3-bivariate.html#interpretation",
    "href": "content/week3-bivariate.html#interpretation",
    "title": "Bivariate summaries",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356\n\n\n\nApplying these rules of thumb:\n\n\\(|r| &lt; 0.3\\): minimal association\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong\n\nAnd:\n\n\\(r &gt; 0\\): positive association\n\\(r &lt; 0\\): negative association\n\nThe interpretation is:\nThere is a moderately strong positive linear relationship between height and log weight.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab3-bivariate.html",
    "href": "content/lab3-bivariate.html",
    "title": "Lab 3: Bivariate summaries",
    "section": "",
    "text": "This lab covers descriptive summaries and graphics for two variables. The goal of the activity is to learn to produce joint summaries of two variables for identifying relationships, in particular:\n\ncontingency tables\nproportional stacked bar plots\nscatterplots\nside-by-side boxplots\n\nWe will use the FAMuSS dataset again.\n\nlibrary(tidyverse)\nload('data/famuss.RData')\n\n\nContingency tables\n\nCounts\nA contingency table is a summary of two categorical variables. Specifically, it is a two-way table in which:\n\nrows correspond to the values of one variable\ncolumns correspond to the values of the other variable\nentries are counts of the numbers of observations for each combination of values\n\nIn fact, a contingency table is a frequency distribution for two variables. It can be constructed in R by retrieving the two variables of interest and providing them as arguments to table().\n\n# retrieve variables of interest\ngenotype &lt;- famuss$genotype\nsex &lt;- famuss$sex\n\n# make a contingency table\ntable(sex, genotype)\n\n        genotype\nsex       CC  CT  TT\n  Female 106 149  98\n  Male    67 112  63\n\n\nThe order of the arguments determines the row/column orientation of the table.\nThe values show the observation counts for each combination, so, for example, the number 106 in the upper-leftmost cell indicates that 106 participants were women with genotype CC.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a contingency table of race and genotype, with race as the row variable and genotype as the column variable.\n\n# retrieve variables of interest\n\n# make a contingency table\n\n\n\n\n\nProportions\nTo convert a contingency table to proportions, one can use either the grand total, row totals, or column totals. The resulting proportions have different meanings:\n\ngrand total – combination frequencies\nrow totals – column variable frequencies by row variable value\ncolumn totals – row variable frequencies by column variable value\n\nIn the sex/genotype contingency table, these proportions would be computed and interpreted as follows:\n\n# combination frequencies (using grand total as denominator)\ntable(sex, genotype) |&gt; proportions(margin = NULL)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.1781513 0.2504202 0.1647059\n  Male   0.1126050 0.1882353 0.1058824\n\n# genotype frequencies by sex (using row margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 1)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.3002833 0.4220963 0.2776204\n  Male   0.2768595 0.4628099 0.2603306\n\n# sex frequencies by genotype (using column margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 2)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.6127168 0.5708812 0.6086957\n  Male   0.3872832 0.4291188 0.3913043\n\n\nNotice that the margin = ... argument controls which totals are used as denominators in computing proportions. The syntax is:\n\nmargin = NULL specifies grand total\nmargin = 1 uses row totals\nmargin = 2 uses column totals\n\nThe interpretation of entries depends on which type of proportion is comupted. For instance, looking at only the upper-leftmost entry in each of the three tables above:\n\n[first table] 17.82% of participants were women with genotype CC\n[second table] 30.03% of female participants had genotype CC\n[third table] 61.27% of participants with genotype CC were women\n\nIf you lose track of which margin was which, you can always check yourself by looking at which values sum to one: if all the values add up to one, grand totals were used; if rows sum to one, row totals were used; and if columns add up to one, column totals were used.\n\n\n\n\n\n\nYour turn\n\n\n\nConvert your contingency table of genotype and race from the previous ‘your turn’ to a table showing genotype frequencies by race.\n\n# genotype frequencies by race\n\nThen, interpret the entry for Hispanic/CT in context.\n\n\n\n\nGraphics\nThe latter two proportion tables – those constructed using row or column totals – can be visualized graphically using barplot(...):\n\n# stacked bar plot showing genotype frequencies by sex\ntable(genotype, sex) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nThis function is a little finnicky about the table orientation, so the most straightforward way to approach making the plot is to adhere to this rule of thumb:\n\nalways use margin = 2\nspecify the order of the variables as they are input to table(...) so that the grouping variable appears second\n\nSo, for instance, to visualize the sex frequencies by genotype, we would do the following:\n\n# stacked bar plot showing sex frequencies by genotype\ntable(sex, genotype) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nIf you see uneven bar heights, that means you have not configured the table correctly before passing it to barplot(...).\n\n\n\n\n\n\nYour turn\n\n\n\nMake a plot showing the genotype frequencies by race.\n\n# stacked bar plot showing genotype frequencies by race\n\nCheck your understanding:\n\nFor which group is the CC frequency highest?\nFor which group is the CC frequency lowest?\n\n\n\n\n\n\nScatterplots and correlation\nTaller people tend to be heavier. We should expect a relationship between weight and height. The example below shows a scatterplot of the two variables, and computes the correlation (measure of linear relationship).\n\n# retrieve height and weight columns\nheight &lt;- famuss$height\nweight &lt;- famuss$weight\n\n# basic scatterplot\nplot(height, weight)\n\n\n\n\n\n\n\n# correlation\ncor(weight, height)\n\n[1] 0.5308787\n\n\nA rough rule of thumb for interpreting correlations is as follows:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case, the correlation of 0.53 indicates a moderate positive relationship.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there a relationship between nondominant and dominant percent change in arm strength? Make a scatterplot and compute the correlation.\n\n# retrieve the percent change variables\n\n# construct a scatterplot\n\n# compute the correlation\n\n\n\n\n\nSide-by-side boxplots\nConsider one of the main questions for the study: were differences on the ACTN gene region associated with differential change in arm strength after resistance training?\nThis is a comparison between a categorical variable (genotype) and numeric variable (percent change in arm strength). For this type of comparison, a set of side-by-side boxplots is best:\n\n# side-by-side boxplots for non-dominant arm\nboxplot(ndrm.ch ~ genotype, data = famuss)\n\n\n\n\n\n\n\n# change the orientation \nboxplot(ndrm.ch ~ genotype, data = famuss, horizontal = T)\n\n\n\n\n\n\n\n\nThis graphics summarizes the frequency distribution of percent change in non-dominant arm strength separately for each genotype. It is essentially a grouped summary.\n\n\n\n\n\n\nYour turn\n\n\n\nConstruct side-by-side boxplots of percent change in dominant arm strength by genotype.\n\n# make side-by-side boxplots of percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\n\nTrait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease (CHD); 12,986 participants were recruited for a study examining this hypothesis and followed for five years. The anger dataset includes data for the 8557 participants identified as having normal blood pressure (normotensives) and records, for each participant, whether they are classified as having high, moderate, or low trait anger, and whether they experienced a CHD event during the study period.\n\nMake a contingency table showing trait anger and CHD event occurrence.\nFind the proportion of participants who experienced a CHD event in each trait anger group.\nConstruct a stacked bar plot to visualize the frequencies you found in (b).\nFind the ratio \\(\\frac{\\text{CHD frequency in the high anger group}}{\\text{CHD frequency in the low anger group}}\\). This is called the “relative risk” of a CHD event. (You can calculate this quantity by hand based on your result in (b).)\n\n\n\nUse the nhanes data from earlier to answer the following questions.\n\nMake a scatterplot of systolic blood pressure and diastolic blood pressure. Put systolic blood pressure on the y axis. Characterize the association, if any, as positive/negative/non-associated and linear/nonlinear.\nCompute and interpret the correlation between systolic blood pressure and diastolic blood pressure.\nDoes there appear to be a substantial difference in the distribution of total cholesterol by sex? Construct side-by-side boxplots to answer this question."
  },
  {
    "objectID": "content/r-cheatsheet.html",
    "href": "content/r-cheatsheet.html",
    "title": "R Cheatsheet",
    "section": "",
    "text": "Note\n\n\n\nThis document is a work in progress and will be updated prior to each test.\n\n\n\nBasics\nLoading data:\n\n[.RData file] load('&lt;FILEPATH&gt;')\n[from package] data(&lt;DATASET NAME&gt;, package = '&lt;PACKAGE NAME&gt;')\n[reading a CSV file] read.csv('&lt;FILEPATH&gt;')\n\nViewing dataframes:\n\n[preview] head(&lt;NAME&gt;)\n[data viewer] view(&lt;NAME&gt;)\n[check structure] str(&lt;NAME&gt;)\n\nExtracting a variable from a dataframe:\n\nDATAFRAME$VARIABLE\n\n\n\nSummary statistics\nIf x is a vector of values of a numeric variable…\n\nmean(x) computes the average\nmedian(x) computes the median\nmin(x) and max(x) compute the minimum and maximum\nquantile(x, probs = &lt;PERCENTILE&gt;) computes the percentile\nsummary(x) computes the five-number summary, plus the mean\nrange(x) computes the range (min, max)\nIQR(x) computes the interquartile range\nvar(x) computes the variance\nsd(x) computes the standard deviation\n\nIf df is a dataframe with a numeric variable y and a categorical variable x…\n\ndf |&gt; group_by(x) |&gt; summarize(&lt;OUTPUT.NAME&gt; = &lt;FUNCTION&gt;(y)) computes the statistic specified by &lt;FUNCTION&gt; separately for each category of the variable x (requires tidyverse package)\n\nSee especially Lab 2: Descriptive statistics.\n\n\nTables\nIf x and y are a vectors of values of two categorical variables…\n\ntable(x) computes the frequency distribution (counts)\ntable(x) |&gt; proportions() computes the frequency distribution (proportions)\ntable(x, y) computes a contingency table\ntable(x, y) |&gt; proportions(margin = NULL) computes proportions using grand total\ntable(x, y) |&gt; proportions(margin = 1) computes proportions using row total (group by x)\ntable(x, y) |&gt; proportions(margin = 2) computes proportions using column total (group by y)\n\nSee especially Lab 1: R basics and Lab 3: Bivariate summaries.\n\n\nGraphics\nIf x and y are vectors of values of two numeric variables…\n\nhist(x, breaks = &lt;NUMBER OF BINS&gt;) generates a histogram\nboxplot(x) generates a boxplot\nplot(x, y) generates a scatterplot\n\nIf x and y are vectors of values of two categorical variables…\n\ntable(x) |&gt; barplot() generates a bar plot (counts)\ntable(x) |&gt; proportions() |&gt; barplot() generates a bar plot (proportions)\ntable(x, y) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by y\ntable(y, x) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by x\n\nIf x is a vector of values of a categorical variable and y is a vector of values of a numeric variable…\n\nboxplot(y ~ x) generates a boxplot with x on the x axis (vertical orientation)\nboxplot(y ~ x, horizontal = T) generates a boxplot with y on the x axis (horizontal orientation)\n\nSee especially Lab 2: Descriptive statistics and Lab 3: Bivariate summaries.\n\n\nOne- and two-sample inference\n\nDirect calculations for one-sample inference\nIf x is a vector of n values of a numeric variable…\n\nmean(x) + c(-1, 1)*qt(1 - alpha/2, df = n - 1)*sd(x)/sqrt(n) computes a \\((1 - \\alpha)\\times 100\\)% confidence interval\n\nsd(x)/sqrt(n) is the standard error for the estimate\nqt(1 - alpha/2, df = n - 1) is the critical value\nfor a 99% interval, use \\(\\alpha = 0.005\\): qt(0.995, df = n - 1)\nfor a 95% interval, use \\(\\alpha = 0.025\\): qt(0.975, df = n - 1)\nfor a 90% interval, use \\(\\alpha = 0.05\\): qt(0.95, df = n - 1)\n\ntstat &lt;- (mean(x) - mu_0)/(sd(x)/sqrt(n)) computes the T statistic for a hypothesis test of \\(H_0: \\mu = \\mu_0\\) against any of the three alternatives\n\nfor \\(H_A: \\mu \\neq \\mu_0\\): 2*pt(abs(tstat), lower.tail = F) computes a two-sided p-value\nfor \\(H_A: \\mu \\neq \\mu_0\\): qt(1 - alpha/2, df = n - 1) computes the critical value for a level \\(\\alpha\\) two-sided test\nfor \\(H_A: \\mu &gt; \\mu_0\\): pt(tstat, lower.tail = F) computes an upper-sided p-value\nfor \\(H_A: \\mu &gt; \\mu_0\\): qt(1 - alpha, df = n - 1) computes the critical value for a level \\(\\alpha\\) upper-sided test\nfor \\(H_A: \\mu &lt; \\mu_0\\): pt(tstat, lower.tail = T) computes a lower-sided p-value\nfor \\(H_A: \\mu &lt; \\mu_0\\): qt(alpha, df = n - 1) computes the critical value for a level \\(\\alpha\\) lower-sided test\n\nto compute quantiles or frequencies directly using the \\(t_{df}\\) model:\n\npt(q = &lt;QUANTILE&gt;, df = &lt;DEGREES OF FREEDOM&gt;) computes the frequency of values less than q for the \\(t_{df}\\) model\npt(q = &lt;QUANTILE&gt;, df = &lt;DEGREES OF FREEDOM&gt;, lower.tail = F) computes the frequency of values greater than q for the \\(t_{df}\\) model\nqt(p = &lt;PROPORTION&gt;, df = &lt;DEGREES OF FREEDOM&gt;) computes the \\(p\\)th quantile for the \\(t_{df}\\) model\n\n\nSee especially Lab 4: Point estimation, Lab 5: Intervals, and Lab 6: Hypothesis testing basics.\n\n\nUsing the t.test(...) function for one-sample inference\nIf x is a vector of values of a numeric variable then\nt.test(x, mu = mu_0, alternative = &lt;DIRECTION&gt;, conf.level = &lt;COVERAGE&gt;)\nperforms a one-sample t test at significance level 1 - &lt;COVERAGE&gt; where:\n\nmu_0 is the hypothetical value for the mean\ndirection can be 'less', 'greater', or 'two.sided' (in quotes)\ncoverage should be \\(1 - \\alpha\\): the ‘complement’ of the intended significance level \\(\\alpha\\) for the test\n\nOutputs are the test statistic, p-value, confidence interval, and point estimate.\nSee especially Lab 7: Directional tests.\n\n\nUsing the t.test(...) function for two-sample inference\nIf DATA is a dataframe with variables VARIABLE (numeric) and GROUP (categorical with two categories), where GROUP distinguishes two independent samples then\nt.test(VARIABLE ~ GROUP, data = DATA, mu = delta_0, \n       alternative = &lt;DIRECTION&gt;, conf.level = &lt;COVERAGE&gt;)\nperforms a two-sample t test at significance level 1 - &lt;COVERAGE&gt; where:\n\ndelta_0 is the hypothetical difference in means (often 0 but not always)\ndirection can be 'less', 'greater', or 'two.sided' (in quotes)\ncoverage should be \\(1 - \\alpha\\): the ‘complement’ of the intended significance level \\(\\alpha\\) for the test\n\nReturns test statistic, p-value, confidence interval for the difference, and point estimates.\nSee especially Lab 8: Two sample inference."
  },
  {
    "objectID": "content/week3-review.html#basics",
    "href": "content/week3-review.html#basics",
    "title": "Review session 1",
    "section": "Basics",
    "text": "Basics\nLoading data:\n\n[.RData file] load('&lt;FILEPATH&gt;')\n[from package] data(&lt;DATASET NAME&gt;, package = '&lt;PACKAGE NAME&gt;')\n[reading a CSV file] read.csv('&lt;FILEPATH&gt;')\n\nViewing dataframes:\n\n[preview] head(&lt;NAME&gt;)\n[data viewer] view(&lt;NAME&gt;)\n[check structure] str(&lt;NAME&gt;)\n\nExtracting a variable from a dataframe:\n\nDATAFRAME$VARIABLE"
  },
  {
    "objectID": "content/week3-review.html#summary-statistics",
    "href": "content/week3-review.html#summary-statistics",
    "title": "Review session 1",
    "section": "Summary statistics",
    "text": "Summary statistics\nIf x is a vector of values of a numeric variable…\n\n\n\nmean(x) computes the average\nmedian(x) computes the median\nmin(x) and max(x) compute the minimum and maximum\nquantile(x, probs = &lt;PERCENTILE&gt;) computes the percentile\nsummary(x) computes the five-number summary, plus the mean\n\n\n\nrange(x) computes the range (min, max)\nIQR(x) computes the interquartile range\nvar(x) computes the variance\nsd(x) computes the standard deviation\n\n\n\nSee especially Lab 2: Descriptive statistics."
  },
  {
    "objectID": "content/week3-review.html#tables",
    "href": "content/week3-review.html#tables",
    "title": "Review session 1",
    "section": "Tables",
    "text": "Tables\nIf x and y are a vectors of values of two categorical variables…\n\ntable(x) computes the frequency distribution (counts)\ntable(x) |&gt; proportions() computes the frequency distribution (proportions)\ntable(x, y) computes a contingency table\ntable(x, y) |&gt; proportions(margin = NULL) computes proportions using grand total\ntable(x, y) |&gt; proportions(margin = 1) computes proportions using row total (group by x)\ntable(x, y) |&gt; proportions(margin = 2) computes proportions using column total (group by y)\n\nSee especially Lab 3: Bivariate summaries."
  },
  {
    "objectID": "content/week3-review.html#graphics",
    "href": "content/week3-review.html#graphics",
    "title": "Review session 1",
    "section": "Graphics",
    "text": "Graphics\n\n\nIf x and y are vectors of values of two numeric variables…\n\nhist(x, breaks = &lt;NUMBER OF BINS&gt;) generates a histogram\nboxplot(x) generates a boxplot\nplot(x, y) generates a scatterplot\n\n\nIf x and y are vectors of values of two categorical variables…\n\ntable(x) |&gt; barplot() generates a bar plot (counts)\ntable(x) |&gt; proportions() |&gt; barplot() generates a bar plot (proportions)\ntable(x, y) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by y\ntable(y, x) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by x"
  },
  {
    "objectID": "content/test1.html",
    "href": "content/test1.html",
    "title": "Test 1",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 1 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 4/19. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/week3-review.html#study-design",
    "href": "content/week3-review.html#study-design",
    "title": "Review session 1",
    "section": "Study design",
    "text": "Study design"
  },
  {
    "objectID": "content/week3-review.html#data-types",
    "href": "content/week3-review.html#data-types",
    "title": "Review session 1",
    "section": "Data types",
    "text": "Data types"
  },
  {
    "objectID": "content/week3-review.html#descriptive-statistics",
    "href": "content/week3-review.html#descriptive-statistics",
    "title": "Review session 1",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics"
  },
  {
    "objectID": "content/test1.html#instructions",
    "href": "content/test1.html#instructions",
    "title": "Test 1",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 1 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 4/19. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test1.html#question-prompts",
    "href": "content/test1.html#question-prompts",
    "title": "Test 1",
    "section": "Question prompts",
    "text": "Question prompts\n\n[L2, L3] The yrbss dataset contains measurements on a small collection of variables from 10,587 survey responses collected as part of the CDC’s Youth Risk Behavior Surveillance System (YRBSS) from 1991-2013. The objective of the survey program is to track behaviors with potential negative physical and mental health impacts among adolescents. In this problem you’ll explore the amount of sleep that respondents get on school nights and the number of days per week respondents are physically active.\n\n[L2] Read briefly about the YRBSS on the CDC website: https://www.cdc.gov/healthyyouth/data/yrbs/overview.htm. Based on the overview and the description above, are the data observational or experimental?\n[L1] Based on the overview (link in part (a)), identify the study population.\n[LX] Load the dataset and identify the type of each variable. What kind of variable is sleep.hours?\n[LX] Examine the frequency distributions of age and grade level. Do any grades or ages seem over-represented in the sample?\n[L3] Make a bar plot showing the frequency distribution of hours of sleep on school nights. Based on the summary, what is the typical amount of sleep respondents get on school nights?\n[L3] Make a stacked bar plot showing levels of sleep by grade. Do older students sleep more on school nights than younger students?\n[L3] Visualize the frequency distribution of the number of days per week that survey participants are physically active. Describe the distribution and interpret any patterns.\n\n\n\n[L2, L3] Diet restriction and longevity. The longevity dataset contains data from a study in which 237 mice were randomly allocated to one of four diets at different levels of restriction: no restriction (NP), normal 85kCal diet before and after weaning (N/N85), normal diet before weaning and restricted 50kCal diet after weaning (N/R50), and normal diet before weaning and restricted 40kCal diet after weaning (N/R40). Researchers recorded the lifetime in months of each mouse in the study.\n\n[L2] Was this an experiment or observational study and why?\n[L3] Find the average lifetime of mice in each diet group.\n[L3] Find the standard deviation of lifetimes in each diet group.\n[L3] Make a plot comparing lifetimes by diet group.\n[L2] Write a short summary of the study results based on your work in (b)-(d). Indicate specifically whether there appears to be a relationship between dietary caloric intake and lifetime.\n\n\n\n[L3] Brain and body size. The mammals dataset contains average body weights (kg) and average brain weights (g) for 62 common species of mammal, as well as log-transformed versions of those weights.\n\n[L3] Make a scatterplot of log brain weights against log body weights and describe the apparent relationship, if any.\n[L3] Compute and interpret the correlation between log brain weight and log body weight.\n[L3] Make a histogram of brain weights (not on the log scale) with an appropriate number of bins. Describe the distribution. Are there outliers?\n[L3] Based on (c), compute an appropriate measure of center and spread for brain weight.\n[L3] Which species of mammal has the largest average brain weight? Inspect the data directly using view(...) to answer this question.\n[L3] Which species of mammal has the smallest \\(\\frac{\\text{brain weight}}{\\text{body weight}}\\) ratio? The largest? Inspect the data directly using view(...) to answer this question.\n\n\n\n[L1, L2] The following is an excerpt from the abstract of the study that reported the results of the Moderna Covid vaccine phase three clinical trial1: “Vaccines are needed to prevent coronavirus disease 2019 (Covid-19) and to protect persons who are at high risk for complications. The mRNA-1273 vaccine is a lipid nanoparticle–encapsulated mRNA-based vaccine that encodes the prefusion stabilized full-length spike protein of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes Covid-19. This [study] was conducted at 99 centers across the United States … The trial enrolled 30,420 [adult volunteers with no known history of SARS-CoV-2 infection and no circumstances that put them at high risk of infection or severe Covid-19 or both,] who were randomly assigned in a 1:1 ratio to receive either vaccine or placebo (15,210 participants in each group) … Symptomatic Covid-19 illness was confirmed in 185 participants in the placebo group and in 11 participants in the mRNA-1273 group; vaccine efficacy was 94.1%.”\n\n[L2] Is this an experiment or observational study? Explain.\n[L1] Identify the study population.\n[L1] Describe the study sample.\n[L1] What outcome(s) were measured in the study?\n[L3] The moderna dataset contains simulated observations according to the study description. Make a contingency table and use it to construct a table showing the proportions of volunteers infected and not infected in each group.\n[L3] Optional. Find the relative risk of illness in the vaccine group compared with the placebo group. Can you determine how efficacy is defined?\n\n\n\n[L1, L2, L3] The temps dataset contains physical data collected on a number of individuals. Explore the dataset and write a brief summary of your descriptive analysis. While open-ended, your analysis should include descriptions of the variables and their statistical properties, and descriptions of relationships between the variables. Include at least one graphic related to your summary. Your summary need not be exhaustive – in fact, it is better to pick 1-2 interesting findings and report those, rather than describe everything you tried."
  },
  {
    "objectID": "content/test1-practice.html",
    "href": "content/test1-practice.html",
    "title": "Extra practice problems",
    "section": "",
    "text": "The test will consist of four questions with multiple parts, and one less structured question in which your task is to explore and summarize a dataset based on descriptive statistics. You’ll have 48 hours to work on the test, and will submit your answers via a fillable form exactly as in the homework assignment. You can use any class resources (notes, textbook, lecture slides, past homeworks) but are required to work alone. You’ll be provided with a Posit cloud project in which to carry out your analyses, and will be expected to upload your script as a supporting document when you fill out the submission form. The practice problems below provide a sample of questions that approximate the test problems (but are slightly shorter)."
  },
  {
    "objectID": "content/test1-practice.html#test-information",
    "href": "content/test1-practice.html#test-information",
    "title": "Extra practice problems",
    "section": "",
    "text": "The test will consist of four questions with multiple parts, and one less structured question in which your task is to explore and summarize a dataset based on descriptive statistics. You’ll have 48 hours to work on the test, and will submit your answers via a fillable form exactly as in the homework assignment. You can use any class resources (notes, textbook, lecture slides, past homeworks) but are required to work alone. You’ll be provided with a Posit cloud project in which to carry out your analyses, and will be expected to upload your script as a supporting document when you fill out the submission form. The practice problems below provide a sample of questions that approximate the test problems (but are slightly shorter)."
  },
  {
    "objectID": "content/test1-practice.html#practice-problems",
    "href": "content/test1-practice.html#practice-problems",
    "title": "Extra practice problems",
    "section": "Practice problems",
    "text": "Practice problems\n\nAnother version of the frog dataset from earlier also includes measurements of egg size and body size. Use this dataset to practice visualizing and describing distributions of numeric variables.\n\nMake histograms of each of the four numeric variables, with appropriate numbers of bins, and describe the shape and number of modes.\nFor each variable, suggest an appropriate measure of center and measure spread and identify any measures that would not be appropriate.\nMake pairwise scatterplots of each of the four numeric variables and describe the association, if any. (Hint: try pairs(frog) for a more efficient way to generate these plots.)\nFor linear associations in (c), compute and interpret the correlation.\n\n\n\n# load and inspect data\nload('data/frog.RData')\nhead(frog)\n\n  site altitude clutch.size clutch.volume egg.size body.size\n1  040    3,462    181.9701      177.8279 1.949845  3.630781\n2  040    3,462    269.1535      257.0396 1.949845  3.630781\n3  040    3,462    158.4893      151.3561 1.949845  3.715352\n4  040    3,462    234.4229      223.8721 1.949845  3.801894\n5  040    3,462    245.4709      234.4229 1.949845  3.890451\n6  040    3,462    301.9952      288.4032 1.949845  3.890451\n\n# part a: histograms of each numeric variable; describe shape and modes\npar(mfrow = c(2, 2), mar = c(4, 4, 4, 1))\nhist(frog$clutch.size)\nhist(frog$clutch.volume)\nhist(frog$egg.size)\nhist(frog$body.size)\n\n\n\n\n\n\n\n# part c: pairwise scatterplots of clutch volume, egg size, body size, clutch size\npairs(frog)\n\n\n\n\n\n\n\n# part d: correlations for linear associations\ncor(frog$clutch.size, frog$clutch.volume)\n\n[1] 0.8077344\n\ncor(frog$clutch.volume, frog$egg.size)\n\n[1] 0.6462605\n\ncor(frog$body.size, frog$clutch.volume, use = 'complete.obs')\n\n[1] 0.6755435\n\ncor(frog$body.size, frog$clutch.size, use = 'complete.obs')\n\n[1] 0.6147564\n\n\n\nThe chick data data come from a study investigating the early growth of chicks on different diets. In the study, 47 chicks were randomly assigned one of four diets at birth and researchers measured body weight in grams daily. The data below show body weights at 18 days since birth for each chick. The question of interest is: which diet is best?\n\nIs this observational or experimental data? Explain your reasoning.\nProduce a visualization that compares body weight distributions by diet. For which diet have chicks grown the most? The least? Explain the statistic(s) or features of the distribution you used to make this determination.\nBased on your plot in (b), suggest a measure of center and measure of spread that would be appropriate for summarizing the data.\nCalculate the measures you suggested in (c) separately for each diet group.\nAssume that in the previous question you found that chicks on diet 3 grew the most, regardless of your actual answer. Can you conclude that diet 3 caused the fastest growth? Explain why or why not.\n\n\n\n# load and inspect data\nload('data/chick.RData')\nhead(chick)\n\n# A tibble: 6 × 3\n  chick.id weight diet  \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n1        1    171 diet 1\n2        2    187 diet 1\n3        3    187 diet 1\n4        4    154 diet 1\n5        5    199 diet 1\n6        6    160 diet 1\n\n# part b: visualize body weights by diet\nboxplot(weight ~ diet, data = chick)\n\n\n\n\n\n\n\n# part c-d: determine and compute appropriate measures of spread and center\nchick |&gt;\n  group_by(diet) |&gt;\n  summarize(avg.weight = mean(weight),\n            sd.weight = sd(weight))\n\n# A tibble: 4 × 3\n  diet   avg.weight sd.weight\n  &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 diet 1       159.      49.2\n2 diet 2       188.      63.3\n3 diet 3       233.      57.6\n4 diet 4       203.      33.6\n\n\n\nThe gss dataset contains observations for 500 respondents in the General Social Survey on a small number of demographic categorical variables. Use this to practice tabular and graphical summaries for categorical variables.\n\nFor each variable, determine whether the variable is nominal or ordinal.\nMake a contingency table of age bracket and whether participants have obtained a college degree.\nVisualize the relationship between age and having obtained a college degree.\nDoes the proportion of respondents with a college degree differ by sex?\nBy political party?\nBy socioeconomic class?\nMake one additional comparison of your choice and interpret the result.\n\n\n\n# load and inspect data\nload('data/gss.RData')\nhead(gss)\n\n# A tibble: 6 × 5\n  age     sex    college.degree political.party class        \n  &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;          &lt;fct&gt;           &lt;fct&gt;        \n1 (29,38] male   degree         ind             middle class \n2 (29,38] female no degree      rep             working class\n3 [18,29] male   degree         ind             working class\n4 (38,50] male   no degree      ind             working class\n5 (29,38] male   degree         rep             middle class \n6 (29,38] female no degree      rep             middle class \n\n# part b: contingency table of age and college degree\ntable(gss$college.degree, gss$age)\n\n           \n            [18,29] (29,38] (38,50] (50,87]\n  degree         36      44      60      34\n  no degree      99      74      64      89\n\n# part c: visualize relationship between age and college degree\ntable(gss$college.degree, gss$age) |&gt; \n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part d: does the proportion of respondents with a degree differ by sex?\ntable(gss$college.degree, gss$sex) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part e: by political party?\ntable(gss$college.degree, gss$political.party) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part f: by class?\ntable(gss$college.degree, gss$class) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part g: one additional comparison of your choosing\n\n\nLong COVID is a multi-systemic and often debilitating condition that develops in at least 10% of patients following a COVID infection. The following is an excerpt of the abstract from a recent study seeking to identify symptoms and risk factors associated with long COVID and published in Nature Medicine1: “We undertook a … study using a UK-based primary care database, Clinical Practice Research Datalink Aurum, to determine symptoms that are associated with confirmed SARS-CoV-2 infection beyond 12 weeks in non-hospitalized adults and the risk factors associated with developing persistent symptoms. We selected 486,149 adults with confirmed SARS-CoV-2 infection … Outcomes included 115 individual symptoms, as well as long COVID, defined as a composite outcome of 33 symptoms by the World Health Organization clinical case definition … Among the patients infected with SARS-CoV-2, risk factors for long COVID included female sex, belonging to an ethnic minority, socioeconomic deprivation, smoking, obesity and a wide range of comorbidities. The risk of developing long COVID was also found to be increased along a gradient of decreasing age.”\n\nIdentify the type of study.\nIdentify the study population.\nDescribe the sample.\nList the study outcomes of interest.\nIdentify any non-outcome variables."
  },
  {
    "objectID": "content/test1-practice.html#footnotes",
    "href": "content/test1-practice.html#footnotes",
    "title": "Extra practice problems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSubramanian et al. (2022). Symptoms and risk factors for long COVID in non-hospitalized adults. Nature medicine, 28(8), 1706-1714.↩︎"
  },
  {
    "objectID": "content/test1.html#footnotes",
    "href": "content/test1.html#footnotes",
    "title": "Test 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBaden, L. R., et al. (2021). Efficacy and safety of the mRNA-1273 SARS-CoV-2 vaccine. New England journal of medicine, 384(5), 403-416.↩︎"
  },
  {
    "objectID": "content/week3-review.html",
    "href": "content/week3-review.html",
    "title": "Review session 1",
    "section": "",
    "text": "[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques"
  },
  {
    "objectID": "content/week3-review.html#outcomes",
    "href": "content/week3-review.html#outcomes",
    "title": "Review session 1",
    "section": "Outcomes",
    "text": "Outcomes\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques"
  },
  {
    "objectID": "content/week3-review.html#lecturelab-recap",
    "href": "content/week3-review.html#lecturelab-recap",
    "title": "Review session 1",
    "section": "Lecture/lab recap",
    "text": "Lecture/lab recap\n\n\n\n\n\n\n\n\nWeek/day\nLecture\nLab\n\n\n\n\n1/W\nExperiments and observational studies\nReading abstracts\n\n\n2/M\nData vocabulary, proportions and means, common notation\nLoading data, extracting variables, computing means and proportions\n\n\n2/W\nMeasures of location/center and spread, simple graphics\nHistograms, barplots, summary statistics\n\n\n3/M\nGraphics and tables for two variables, interpreting relationships, correlation\nStacked bar plots, side-by-side boxplots, scatterplots"
  },
  {
    "objectID": "content/week3-review.html#assignment-recap",
    "href": "content/week3-review.html#assignment-recap",
    "title": "Review session 1",
    "section": "Assignment recap",
    "text": "Assignment recap\n\n\n\n\n\n\n\nProblem set\nTopics\n\n\n\n\nPS1\nInterpreting study descriptions\n\n\nPS2\nSummary statistics (mean, proportion) for one variable; identifying variable types\n\n\nPS3\nMeasures of location and spread for one variable; visualizing frequency distributions\n\n\nPS4\nVisualizing relationships between two variables (C/C, C/N, N/N)"
  },
  {
    "objectID": "content/week3-review.html#question-types-l2-study-design",
    "href": "content/week3-review.html#question-types-l2-study-design",
    "title": "Review session 1",
    "section": "Question types: [L2] study design",
    "text": "Question types: [L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#question-types-l3-descriptive-statistics",
    "href": "content/week3-review.html#question-types-l3-descriptive-statistics",
    "title": "Review session 1",
    "section": "Question types: [L3] descriptive statistics",
    "text": "Question types: [L3] descriptive statistics\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week3-review.html#questions-l2-study-design",
    "href": "content/week3-review.html#questions-l2-study-design",
    "title": "Review session 1",
    "section": "Questions: [L2] study design",
    "text": "Questions: [L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#questions-l3-descriptive-statistics",
    "href": "content/week3-review.html#questions-l3-descriptive-statistics",
    "title": "Review session 1",
    "section": "Questions: [L3] descriptive statistics",
    "text": "Questions: [L3] descriptive statistics\n\nCompute/interpret mean/median/percentiles/IQR/variance/SD\nCompute/interpret a grouped summary with one or more of the above measures\nDetermine appropriate measures of location/center/spread\nMake/interpret a table of proportions for values of a categorical variable\nMake/interpret a contingency table\nMake/interpret a histogram\nMake/interpret two-way tables of proportions\nMake/interpret stacked bar plots representing proportions\nMake/interpret side-by-side boxplots\nMake/interpret scatterplots\nCompute/interpret correlations"
  },
  {
    "objectID": "content/week3-review.html#questions-l1-l2-study-design",
    "href": "content/week3-review.html#questions-l1-l2-study-design",
    "title": "Review session 1",
    "section": "Questions: [L1, L2] study design",
    "text": "Questions: [L1, L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#commonly-missed-question-types",
    "href": "content/week3-review.html#commonly-missed-question-types",
    "title": "Review session 1",
    "section": "Commonly missed question types",
    "text": "Commonly missed question types\nIdentifying appropriate measures of center/spread (i.e., understanding robustness) Grouped summaries (getting syntax right) Determining"
  },
  {
    "objectID": "content/week3-review.html#key-concepts",
    "href": "content/week3-review.html#key-concepts",
    "title": "Review session 1",
    "section": "Key concepts",
    "text": "Key concepts\n\n[L1, L2] Experiments and observational studies\n[L1, L2] Samples and populations\n[L2] Variable types: categorical (nominal/ordinal) and numeric (discrete/continuous)\n[L3] Summary statistics: mean, median, percentile, IQR, SD, variance, correlation\n[L3] Distribution properties: skewness, outliers, modes\n[L3] Robustness of common summary statistics\n[L3] Relationships: positive/negative; linear/nonlinear"
  },
  {
    "objectID": "content/week3-review.html#about-the-test",
    "href": "content/week3-review.html#about-the-test",
    "title": "Review session 1",
    "section": "About the test",
    "text": "About the test\n\n4-5 questions\nExpect one challenge part, but otherwise very similar to homework problems and practice questions\n48 hours, open book, open note\nPosit cloud project + fillable form\nRevisions will be allowed to earn back credit\nNo collaboration\nDo your own analysis/writing (no AI plagiarism)\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#todays-agenda",
    "href": "content/week4-sampling.html#todays-agenda",
    "title": "Introduction to inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nTest 1 discussion\n[lecture] Inference vs. description, point estimation, interval estimation\n[lab] Exploring interval coverage"
  },
  {
    "objectID": "content/week4-sampling.html#inferential-statistics",
    "href": "content/week4-sampling.html#inferential-statistics",
    "title": "Introduction to inference",
    "section": "Inferential statistics",
    "text": "Inferential statistics\nDescriptive findings are statements about sample statistics. For instance:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nBy contrast, statistical inferences are statements about population statistics. For example:\n\nIndividuals with genotype TT exhibit the largest median percent change in strength."
  },
  {
    "objectID": "content/week4-sampling.html#sampling-variability",
    "href": "content/week4-sampling.html#sampling-variability",
    "title": "Introduction to inference",
    "section": "Sampling variability",
    "text": "Sampling variability\nA population model represents the distribution of values you’d see if you measured every individual in your population (a census).\nBut if you don’t measure every unit, results are subject to sampling variation: sample statistics change depending on which individuals you measure."
  },
  {
    "objectID": "content/week4-sampling.html#random-sampling",
    "href": "content/week4-sampling.html#random-sampling",
    "title": "Introduction to inference",
    "section": "Random sampling",
    "text": "Random sampling\n\nSampling establishes the link (or lack thereof) between a sample and a population.\n\n\n\n\n\nIn a simple random sample, units are chosen in such a way that every individual in the population has an equal probability of inclusion. For the SRS:\n\nsample statistics mirror population statistics (sample is representative)\nsampling variability depends only on population variability and sample size"
  },
  {
    "objectID": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "href": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "title": "Introduction to inference",
    "section": "A pretend population: NHANES data",
    "text": "A pretend population: NHANES data\nThe National Health and Nutrition Esamination Survey (NHANES) is an annual CDC program to collect health and nutrition data on the non-institutionalized civilian resident population of the United States. Here are a few variables:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubj.id\ngender\nage\npoverty\npulse\nbpsys1\nbpdia1\ntotchol\nsleephrsnight\n\n\n\n\n1\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n2\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n3\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n5\nfemale\n49\n1.91\n86\n118\n82\n6.7\n8\n\n\n\n\n\nI’ve selected 3,179 responses from the 2009-2010 survey; let’s pretend the corresponding individuals form a population of interest."
  },
  {
    "objectID": "content/week4-sampling.html#population-distribution-of-a-variable",
    "href": "content/week4-sampling.html#population-distribution-of-a-variable",
    "title": "Introduction to inference",
    "section": "Population distribution of a variable",
    "text": "Population distribution of a variable\nConsider the totchol variable: total HDL cholesterol in mmol/L. It has a certain frequency distribution among the population that we’ll call its population distribution.\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we draw a random sample of 50 individuals…\n\nhow closely will the sample align with the population distribution?\nhow much will alignment change if we select a new sample?"
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability",
    "href": "content/week4-sampling.html#simulating-sampling-variability",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\n\n\n\n\n\n\n\n\n\n\n\n\nThese are 20 random samples with the sample mean indicated by the dashed line and the population distribution and mean overlaid in red.\n\nsample size \\(n = 20\\)\nfrequency distributions differ a lot\nsample means differ some\n\nWe can actually measure this variability!"
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability-1",
    "href": "content/week4-sampling.html#simulating-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\nIf we had means calculated from a much larger number of samples, we could make a frequency distribution for the values of the sample mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample\n1\n2\n\\(\\cdots\\)\n10,000\n\n\nmean\n4.957\n5.039\n\\(\\cdots\\)\n5.24\n\n\n\n\nWe could then use the usual measures of center and spread to characterize the distribution of sample means.\n\nmean of \\(\\bar{x}\\): 5.0373228\nstandard deviation of \\(\\bar{x}\\): 0.2387869\n\n\nAcross 10,000 random samples of size 20, the typical sample mean was 5.04 and the root average squared distance of the sample mean from its typical value was 0.239."
  },
  {
    "objectID": "content/week4-sampling.html#point-estimation",
    "href": "content/week4-sampling.html#point-estimation",
    "title": "Introduction to inference",
    "section": "Point estimation",
    "text": "Point estimation\nSample statistics, viewed as guesses of population statistics, are called ‘point estimates’.\n\n\n\nParameter name\nParameter notation\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)\n\n\n\nNow we can more formally describe statistical inference:\n\na population parameter is any numeric characteristic of a population distribution\nan inference is a conclusion about the value of a population parameter based on point estimates and their sampling variability\n\nWe will focus initially on inferences about the mean \\(\\mu\\) based on the point estimate \\(\\bar{x}\\)."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability",
    "href": "content/week4-sampling.html#measuring-sampling-variability",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nIn practice \\(\\sigma\\) is not known so we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared error of the sample mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability-1",
    "href": "content/week4-sampling.html#measuring-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nTheory indicates the standard deviation of the sample mean under random sampling is: \\[\nSD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{population SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor totchol, the theoretical standard deviation is \\(SD(\\bar{x}) = \\frac{1.0747}{\\sqrt{50}} =\\) 0.1519822.\nWe can estimate this quantity by replacing \\(\\sigma\\) with the point estimate \\(s_x\\), resulting in a standard error (estimated standard deviation): \\[SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\\]"
  },
  {
    "objectID": "content/week4-sampling.html#example-with-one-sample",
    "href": "content/week4-sampling.html#example-with-one-sample",
    "title": "Introduction to inference",
    "section": "Example with one sample",
    "text": "Example with one sample\nThe simulations we’ve done so far have been a means of understanding just what a standard error is meant to capture; these are not a practicable method for measuring sampling variation.\nIn practice we’d simply compute a point estimate and standard error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.031\n0.1396\n\n\n\n\n\n\nThe estimated mean total HDL cholesterol among the population is 5.031 mmol/L.\nThe point estimate is expected to deviate by 0.1396 mmol/L on average from the population mean."
  },
  {
    "objectID": "content/week4-sampling.html#effect-of-sample-size",
    "href": "content/week4-sampling.html#effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\) is inversely proportional to sample size.\n\n\n\n\n\n\n\n\n\n\n\n\nAs sample size increases…\n\naccuracy remains the same\nestimates get more precise\nskewness vanishes"
  },
  {
    "objectID": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "href": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Visualizing effect of sample size",
    "text": "Visualizing effect of sample size\nGenerating a large number (10K) of samples allows us to approximate the sampling distribution for a variety of sample sizes.\n\n\nspread diminishes with sample size\nless variability among estimates from larger samples\n\nSo estimates are more accurate with more data, assuming data are from a random sample."
  },
  {
    "objectID": "content/week4-sampling.html#normal-model",
    "href": "content/week4-sampling.html#normal-model",
    "title": "Introduction to inference",
    "section": "Normal model",
    "text": "Normal model\nNotice that each simulated sampling distribution has produced a unimodal, symmetric, bell-shaped histogram.\n\n\nThe normal model is a theoretical frequency distribution characterized by two parameters:\n\na mean (center)\na standard deviation (spread)\n\nTheory dictates that the sampling distribution of the sample mean is well-approximated by a normal model under simple random sampling.\n\n\n\n\n\n\n\n\n\n\n\n\nBased on discussion thus far, what do you think the model parameters might be?\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#description-vs.-inference",
    "href": "content/week4-sampling.html#description-vs.-inference",
    "title": "Introduction to inference",
    "section": "Description vs. inference",
    "text": "Description vs. inference\n\nStatistical inferences are statements about population statistics based on samples.\n\nTo appreciate the meaning of this, consider the contrast with descriptive findings, which are statements about sample statistics:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nA descriptive finding is:\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nThe corresponding inference would be:\nThe median percent change in strength is highest among adults with genotype TT."
  },
  {
    "objectID": "content/week4-sampling.html#a-simpler-question",
    "href": "content/week4-sampling.html#a-simpler-question",
    "title": "Introduction to inference",
    "section": "A simpler question",
    "text": "A simpler question\n\n\nConsider total HDL cholesterol in mmol/L. We already know how to summarize the distribution of sample values:\n\n\n\n\n\n\n\n\n\nSample mean\nSample SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.043\n1.117"
  },
  {
    "objectID": "content/week4-sampling.html#population-model",
    "href": "content/week4-sampling.html#population-model",
    "title": "Introduction to inference",
    "section": "Population model",
    "text": "Population model\n\n\nConsider total HDL cholesterol in mmol/L. We already know how to summarize the distribution of sample values:\n\n\n\n\n\n\n\n\n\nSample mean\nSample SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.067\n1.126"
  },
  {
    "objectID": "content/week4-sampling.html#population-models",
    "href": "content/week4-sampling.html#population-models",
    "title": "Introduction to inference",
    "section": "Population models",
    "text": "Population models\n\nInference consists in using statistics of a random sample to ascertain population statistics under a population model.\n\nA population model represents the distribution of values you’d see if you measured every individual in the study population. We think of the sample values as a random draw.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Density is an alternative scale to frequency that is independent of population size.)"
  },
  {
    "objectID": "content/week4-sampling.html#sampling-variation",
    "href": "content/week4-sampling.html#sampling-variation",
    "title": "Introduction to inference",
    "section": "Sampling variation",
    "text": "Sampling variation\nA population model represents the distribution of values you’d see if you measured every individual in the study population (a census).\nThink of a sample as a random draw from the population:\n\ndifferent samples comprise different sets of individuals\nsample statistics depend on which individuals you measure"
  },
  {
    "objectID": "content/week4-sampling.html#sampling-distributions",
    "href": "content/week4-sampling.html#sampling-distributions",
    "title": "Introduction to inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nWhat we are simulating is known as a sampling distribution: the frequency of values of a statistic across all possible random samples.\n\n\n\n\n\n\n\n\n\n\n\n\nProvided data are from a random sample, the sample mean \\(\\bar{x}\\) has a sampling distribution with\n\nmean \\(\\color{red}{\\mu}\\) (population mean)\nstandard deviation \\(\\color{red}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\nregardless of its exact form.\n\n\nIn other words, across all random samples of a fixed size…\n\nThe average value of the sample mean is the population mean.\nThe average squared error (sample mean - population mean)\\(^2\\) is \\(\\frac{\\sigma^2}{n}\\)"
  },
  {
    "objectID": "content/week4-sampling.html#stuff",
    "href": "content/week4-sampling.html#stuff",
    "title": "Introduction to inference",
    "section": "Stuff",
    "text": "Stuff\nGenerating a large number (10K) of samples allows us to approximate the sampling distribution for a variety of sample sizes.\n\n\n\n\n\n\n\n\n\n\nspread diminishes with sample size\nless variability among estimates from larger samples\n\nSo estimates are more precise with more data.\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#standard-error",
    "href": "content/week4-sampling.html#standard-error",
    "title": "Introduction to inference",
    "section": "Standard error",
    "text": "Standard error\nIn practice we only have one sample so instead of a direct measure we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared distance of the sample mean from the population mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-sampling.html#point-estimates",
    "href": "content/week4-sampling.html#point-estimates",
    "title": "Introduction to inference",
    "section": "Point estimates",
    "text": "Point estimates\n\nSample statistics, viewed as guesses for the values of population statistics, are called ‘point estimates’.\n\nWe’ll focus on inferences involving the following:\n\n\n\nPopulation statistic\nParameter\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)"
  },
  {
    "objectID": "content/week4-sampling.html#standard-errors",
    "href": "content/week4-sampling.html#standard-errors",
    "title": "Introduction to inference",
    "section": "Standard errors",
    "text": "Standard errors\nIn practice we only have one sample so instead of a direct measure we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\n\n\n\n\n5.381\n1.073\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]"
  },
  {
    "objectID": "content/week4-sampling.html#reporting-point-estimates",
    "href": "content/week4-sampling.html#reporting-point-estimates",
    "title": "Introduction to inference",
    "section": "Reporting point estimates",
    "text": "Reporting point estimates\nIt is common style to report the value of a point estimate with a standard error given parenthetically.\n\n\nStatistics from full NHANES sample:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\n\n\n\n\n5.043\n1.075\n3179\n\n\n\n\n\n\n\nThe mean total cholesterol among the population is estimated to be 5.043 mmol/L (SE 0.019)\n\n\n\nThis style of report communicates:\n\nparameter of interest\nvalue of point estimate\nerror/variability of point estimate"
  },
  {
    "objectID": "content/week4-sampling.html#interval-estimation",
    "href": "content/week4-sampling.html#interval-estimation",
    "title": "Introduction to inference",
    "section": "Interval estimation",
    "text": "Interval estimation\n\nAn interval estimate is a range of plausible values for a population parameter.\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.043 \\pm 2\\times 0.0191 = (5.005, 5.081)\\]\n\nIn R:\n\navg.totchol &lt;- mean(totchol)\nse.totchol &lt;- sd(totchol)/sqrt(length(totchol))\navg.totchol + c(-2, 2)*se.totchol\n\n[1] 5.004817 5.081059\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#interval-construction",
    "href": "content/week4-sampling.html#interval-construction",
    "title": "Introduction to inference",
    "section": "Interval construction",
    "text": "Interval construction\nIn general, an interval estimate is constructed from two main ingredients:\n\npoint estimate\nstandard error\n\nAnd one secret ingredient:\n\na model for the sampling distribution of the point estimate\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]"
  },
  {
    "objectID": "content/week4-sampling.html#precision-and-coverage",
    "href": "content/week4-sampling.html#precision-and-coverage",
    "title": "Introduction to inference",
    "section": "Precision and coverage",
    "text": "Precision and coverage\n\n\nIntervals have two main and attributes:\n\nprecision refers to how wide or narrow the interval is\ncoverage refers to how often, under random sampling, the interval captures the parameter of interest\n\n\nThese properties are inversely related:\n\nif I say mean cholesterol is between 0 and 50 I’m almost certainly right, but the estimate is useless\nif I say mean cholesterol is between 5.0299 and 5.0301, I’ve made a very precise guess, but I’m likely wrong (think about sampling variability)\n\n\n\n\nAn accurate interval should maintain high coverage while achieving practically useful precision. This isn’t always possible!\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#inference-or-description",
    "href": "content/week4-sampling.html#inference-or-description",
    "title": "Introduction to inference",
    "section": "Inference or description?",
    "text": "Inference or description?\nSee if you can tell the difference:\n\nThe proportion of children who developed a peanut allergy was 0.133 in the avoidance group.\nThe proportion of children who develop peanut allergies is estimated to be 0.133 when peanut protein is avoided in infancy.\nThe average lifetime of mice on normal 85kCal diets is estimated to be 32.7 months.\nThe 57 mice on the normal 85kCal diet lived 32.7 months on average.\nThe relative risk of a CHD event in the high trait anger group compared with the low trait anger group was 2.5.\nThe relative risk of a CHD event among adults with high trait anger compared with adults with low trait anger is estimated to be 2.5."
  },
  {
    "objectID": "content/week4-sampling.html#a-difficulty",
    "href": "content/week4-sampling.html#a-difficulty",
    "title": "Introduction to inference",
    "section": "A difficulty",
    "text": "A difficulty\n\nDifferent samples yield different estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample means:\n\n\n\n\n\n\n\n\n\nsample.1\nsample.2\n\n\n\n\n5.093\n5.136\n\n\n\n\n\n\nestimates are close but not identical\nthe population mean can’t be both 5.093 and 5.136\nprobably neither estimate is exactly correct\n\nEstimation error and sample-to-sample variability are inherent to point estimation."
  },
  {
    "objectID": "content/week4-sampling.html#interpreting-standard-errors",
    "href": "content/week4-sampling.html#interpreting-standard-errors",
    "title": "Introduction to inference",
    "section": "Interpreting standard errors",
    "text": "Interpreting standard errors\nThe standard error is a point estimate of the (population) standard deviation of sample means across all possible random samples:\n\\[\nSE(\\bar{x}) \\text{ estimates } \\sqrt{\\text{average value of } (\\bar{x} - \\mu)^2}\n\\] Two phrasings for an interpretation:\n\nEstimated root average squared deviation of the sample mean from the population mean.\nEstimated root mean square error."
  },
  {
    "objectID": "content/lab4-sampling.html",
    "href": "content/lab4-sampling.html",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "",
    "text": "library(tidyverse)\nload('data/nhanes.RData')\nload('data/temps.RData')\nThe goal of this lab is to learn to compute a point estimate, standard error, and interval estimate for a population mean “by hand” by performing the arithmetic directly in R. You will have an opportunity to practice interpreting these quantities as you go.\nWe will also use this lab for a short class activity to explore how often the interval estimate we introduced is “correct”."
  },
  {
    "objectID": "content/lab4-sampling.html#your-turn-1",
    "href": "content/lab4-sampling.html#your-turn-1",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "Your turn",
    "text": "Your turn\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error"
  },
  {
    "objectID": "content/lab4-estimation.html",
    "href": "content/lab4-estimation.html",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "",
    "text": "library(tidyverse)\nload('data/nhanes.RData')\nload('data/temps.RData')\n\nThe goal of this lab is to learn to compute a point estimate, standard error, and interval estimate for a population mean “by hand” by performing the arithmetic directly in R. You will have an opportunity to practice interpreting these quantities as you go.\nWe will also use this lab for a short class activity to explore how often the interval estimate we introduced is “correct”.\n\nPoint estimation\n\nEstimate for the population mean\nSince the point estimate for the population mean of a numeric variable is the sample mean, you already know how to perform the calculation in R. We’ll store this for later use:\n\n# retrieve total cholesterol variable\ntotchol &lt;- nhanes$totchol\n\n# compute and store sample mean\ntotchol.mean &lt;- mean(totchol)\ntotchol.mean\n\n[1] 5.042938\n\n\nThe only novelty here is that we now interpret this as a point estimate of the population mean total cholesterol:\n\nThe mean total cholesterol of U.S. adults is estimated to be 5.043 mmol/L.\n\nThis is in contrast to the interpretation as a descriptive summary:\n\nThe average total cholesterol among the respondents in the NHANES survey was 5.043 mmol/L.\n\nBoth interpretations are valid; just different. By interpreting the sample mean as a point estimate, we are implicitly assuming that the data are a random sample from the U.S. adult population.\n\n\n\n\n\n\nYour turn\n\n\n\nUse the temps data to estimate mean body temperature.\n\n# retrieve variable of interest\n\n# compute and store sample mean\n\nCheck your understanding:\n\ninterpret the result as a descriptive summary\ninterpret the result as a point estimate\n\n\n\n\n\nStandard error for the sample mean\nA standard error is a measure of the sampling variability of a point estimate. Technically, it’s an estimate of the point estimate’s standard deviation across all possible random samples of a fixed size.\nThe standard error for the sample mean is calculated according to the formula: \\[SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\] Where:\n\n\\(s_x\\) is the sample standard deviation\n\\(n\\) is the sample size\n\nTo calculate this in R, we perform the arithmetic by hand (for now):\n\n# store sample sd and sample size\ntotchol.sd &lt;- sd(totchol)\ntotchol.n &lt;- length(totchol)\n\n# compute standard error\ntotchol.se &lt;- totchol.sd/sqrt(totchol.n)\ntotchol.se\n\n[1] 0.01906042\n\n\nThis result is interpreted as follows:\n\nThe root average deviation of the sample mean from the population mean is estimated to be 0.0191 mmol/L.\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error\n\n\n\n\n\n\nInterval estimation\n\nInterval estimate for the mean\nA common interval for the population mean is:\n\\[\\bar{x} \\pm \\underbrace{2\\times SE(\\bar{x})}_{\\text{margin of error}}\\]\nFor now, we’ll calculate this by directly performing the arithmetic. Later, you’ll use commands that return interval estimates by default.\n\n# add/subtract two standard errors from the mean\ntotchol.mean + c(-2, 2)*totchol.se\n\n[1] 5.004817 5.081059\n\n\nWe’ll talk more about the exact interpretation later; for now, you should think of this as a range of plausible values for the population mean.\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate an interval estimate for the mean body temperature using the body temperature data.\n\n# interval estimate for mean body temp\n\n\n\n\n\nHow often is the interval correct?\nAn interval “covers” the population mean if the true value is between the interval endpoints.\nWe can explore how often the interval covers the parameter by having everyone in the class simulate their own sample from a population with a known mean and check whether the interval they obtain from the sample covers the population mean or not.\nThe commands below simulate a sample and then compute an interval.\n\n# function to simulate body temp data from a population with mean 98.6\nsample.bodytemps &lt;- function(n){\n  rnorm(n, mean = 98.6, sd = 1)\n}\n\n# simulate a sample of body temperatures\nbodytemp &lt;- sample.bodytemps(n = 150)\n\n# compute interval 'ingredients'\nbodytemp.mean &lt;- mean(bodytemp)\nbodytemp.sd &lt;- sd(bodytemp)\nbodytemp.n &lt;- length(bodytemp)\nbodytemp.se &lt;- bodytemp.sd/sqrt(bodytemp.n)\n\n# compute interval estimate\nbodytemp.mean + c(-2, 2)*bodytemp.se\n\n[1] 98.41325 98.73135\n\n# margin of error\n2*bodytemp.se\n\n[1] 0.1590505\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nUse the example above to generate a sample of size 20 and compute an interval estimate for the mean body temperature.\nThen:\n\nDetermine whether your interval covers the population mean.\nCompute the margin of error used in your interval (\\(2\\times SE(\\bar{x})\\)).\n\nRepeat with \\(n = 150\\). Then fill out this form.\n\n\n\n\n\nPractice problems\n\nVu and Harrington exercise 4.1. Additionally:\n\nCompute an interval estimate for the mean BGC of nests.\nSupposing a sample of 30 nests returned exactly the same summary statistics, recompute your interval in (e). Is the margin of error smaller or larger?\n\n\n\nThe brfss dataset contains a measurement of body weight, weight, as well as a variable, wtdesire, that is the desired weight reported by respondents.\n\nEstimate the mean difference between actual and desired weight. Report the point estimate and standard error.\nDoes the point estimate suggest that the average U.S. adult would prefer to lose or gain weight?\nCompute an interval estimate for the mean difference between actual and desired weight."
  },
  {
    "objectID": "content/lab4-estimation.html#your-turn-1",
    "href": "content/lab4-estimation.html#your-turn-1",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "Your turn",
    "text": "Your turn\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error"
  },
  {
    "objectID": "content/week4-intervals.html",
    "href": "content/week4-intervals.html",
    "title": "Interval estimation",
    "section": "",
    "text": "HW2 remarks/discussion\n[Lecture] A basic interval estimate for the mean\n[Lecture/lab] Exploring interval coverage\n[Lecture/lab] Comparing normal and \\(t\\) models"
  },
  {
    "objectID": "content/week4-intervals.html#todays-agenda",
    "href": "content/week4-intervals.html#todays-agenda",
    "title": "Confidence intervals",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] \\(t\\) confidence intervals for the mean\n[lab] computing and interpreting confidence intervals"
  },
  {
    "objectID": "content/week4-intervals.html#from-last-time",
    "href": "content/week4-intervals.html#from-last-time",
    "title": "Confidence intervals",
    "section": "From last time",
    "text": "From last time\n\n\nUnder simple random sampling:\n\nthe sample mean \\(\\bar{x}\\) provides a good point estimate of the population mean \\(\\mu\\)\nits estimated sampling variability is given by the standard error \\(SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}} = \\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\nse\n\n\n\n\n5.043\n1.075\n3179\n0.01906\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean total HDL cholesterol among the U.S. adult population is estimated to be 5.043 mmol/L (SE 0.0191)."
  },
  {
    "objectID": "content/week4-intervals.html#interval-estimation",
    "href": "content/week4-intervals.html#interval-estimation",
    "title": "Confidence intervals",
    "section": "Interval estimation",
    "text": "Interval estimation\nA common interval estimate for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\nA range of plausible values for the mean total cholesterol among U.S. adults is 5.005 to 5.081 mmol/L.\n\nTwo related questions:\n\nWhat do we mean by “plausible”?\nWhere did the number 2 come from?"
  },
  {
    "objectID": "content/week4-intervals.html#precision-and-coverage",
    "href": "content/week4-intervals.html#precision-and-coverage",
    "title": "Confidence intervals",
    "section": "Precision and coverage",
    "text": "Precision and coverage\n\n\nIntervals have two main and attributes:\n\nprecision refers to how wide or narrow the interval is\ncoverage refers to how often, under random sampling, the interval captures the parameter of interest\n\n\nThese properties are inversely related:\n\nif I say mean cholesterol is between 0 and 50 I’m almost certainly right, but the estimate is useless\nif I say mean cholesterol is between 5.0299 and 5.0301, I’ve made a very precise guess, but I’m likely wrong (think about sampling variability)\n\n\n\n\nAn accurate interval should maintain high coverage while achieving practically useful precision. This isn’t always possible!"
  },
  {
    "objectID": "content/week4-intervals.html#an-interval-for-the-mean",
    "href": "content/week4-intervals.html#an-interval-for-the-mean",
    "title": "Confidence intervals",
    "section": "An interval for the mean",
    "text": "An interval for the mean\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.031 \\pm 2\\times 0.1396 = (4.75, 5.31)\\]\n\nIn R:\n\nc(lwr = mean(cholesterol) - 2*sd(cholesterol)/sqrt(50), \n  upr = mean(cholesterol) + 2*sd(cholesterol)/sqrt(50))\n\n     lwr      upr \n4.738974 5.346902 \n\n\n\n\nThe precision is evident from the interval width (0.5611). But what about coverage?"
  },
  {
    "objectID": "content/week4-intervals.html#exploring-interval-coverage",
    "href": "content/week4-intervals.html#exploring-interval-coverage",
    "title": "Confidence intervals",
    "section": "Exploring interval coverage",
    "text": "Exploring interval coverage\nLet’s carry on pretending that the NHANES data comprise a population.\nThe first section of lab5-intervals contains some simple commands to draw a sample and calculate an interval estimate.\n\nEach of you will generate an interval based on a different sample\nWe’ll tally how many of you obtained intervals capturing the population mean\n\nOur tally will give an approximate idea of the coverage."
  },
  {
    "objectID": "content/week4-intervals.html#more-simulation",
    "href": "content/week4-intervals.html#more-simulation",
    "title": "Confidence intervals",
    "section": "More simulation",
    "text": "More simulation\n\n\nArtificially simulating a larger number of intervals provides a slightly better approximation of coverage.\n\nat right, 100 intervals\n97% cover the population mean (vertical dashed line)\n\nWhat do you expect would happen to coverage if, for the same samples…\n\na wider margin of error (say, \\(3\\times SE\\)) were used?\na narrower margin of error (say, \\(1\\times SE\\)) were used?"
  },
  {
    "objectID": "content/week4-intervals.html#so-why-2-standard-errors",
    "href": "content/week4-intervals.html#so-why-2-standard-errors",
    "title": "Confidence intervals",
    "section": "So why 2 standard errors?",
    "text": "So why 2 standard errors?\n\n\nThe margin of error of \\(2\\times SE\\) comes from the so-called “empirical rule”.\n\nunder the normal model, 95% of values are within 2SD of center\nso for 95% of samples, the sample mean is within 2SD of the population mean\n\nSo in theory, according to the normal model, \\(\\bar{x} \\pm 2\\times SD\\) achieves 95% coverage.\n\n\n\n\nBut we are using standard error (SE), not standard deviation (SD). Do we still get the same coverage using the normal model?"
  },
  {
    "objectID": "content/week4-intervals.html#normal-model-coverage",
    "href": "content/week4-intervals.html#normal-model-coverage",
    "title": "Confidence intervals",
    "section": "Normal model coverage",
    "text": "Normal model coverage\n\n\nAt right, the misses are compared between intervals calculated with SD (left) and SE (right) using the multiplier from the normal model on the same 10,000 simulated datasets with sample size \\(n = 15\\).\n\nSE misses more often\nso the normal model produces under-coverage\n\n\n\n\n\n\n\n\n\n\ntype\ncoverage\n\n\n\n\nsd\n0.954\n\n\nse\n0.9294\n\n\n\n\n\nWhat do you think: the multiplier should be [smaller/larger] to ensure 95% coverage."
  },
  {
    "objectID": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "href": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "title": "Confidence intervals",
    "section": "A closer look at the normal model",
    "text": "A closer look at the normal model\nAn alternate but equivalent way to understand the normal model for the sampling distribution of \\(\\bar{x}\\) is in terms of deviations. The following are equivalent:\n\nThe expression \\(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\\) measures the number of standard deviations from center."
  },
  {
    "objectID": "content/week4-intervals.html#simulating-deviations",
    "href": "content/week4-intervals.html#simulating-deviations",
    "title": "Confidence intervals",
    "section": "Simulating deviations",
    "text": "Simulating deviations\nAnother way to check normal model coverage is to use deviations:\n\nSimulate many samples\nCompute scaled deviations\nTally how many scaled deviations are between -2 and 2\n\nThe proportion of samples for which the scaled deviation is between -2 and 2 approximates the coverage.\nWe’ll try it in the next part of the lab5-intervals. Hypotheses:\n\ndeviations scaled by SD should be between -2 and 2 95% of the time\ndeviations scaled by SE should be between -2 and 2 [more/less] than 95% of the time"
  },
  {
    "objectID": "content/week4-intervals.html#the-t-model",
    "href": "content/week4-intervals.html#the-t-model",
    "title": "Confidence intervals",
    "section": "The \\(t\\) model",
    "text": "The \\(t\\) model\n\n\nConsider the statistic:\n\\[\nT = \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\]\nThe sampling distribution of \\(T\\) is well-approximated by a \\(t_{n - 1}\\) model whenever either:\n\nthe population model is symmetric and unimodal\n\nOR\n\nthe sample size is not too small"
  },
  {
    "objectID": "content/week4-intervals.html#model-specification",
    "href": "content/week4-intervals.html#model-specification",
    "title": "Confidence intervals",
    "section": "Model specification",
    "text": "Model specification\nThe \\(t\\) model is characterized by its degrees of freedom.\n\nfor interval estimates for the mean, \\(n - 1\\) is used\ndepending on the degrees of freedom (i.e., sample size), a different multiplier is applied to the standard error to obtain the margin of error\n\nThe multiplier is called a critical value, and can be found in R via:\n\n# pseudo code -- replace coverage with desired level, e.g., 0.95\nqt((1 - coverage)/2, df = (n - 1), lower.tail = F)\n\n\nchosen to ensure a specified nominal coverage level (usually 95%)\nhigher nominal coverage levels utilize larger critical values, producing wider intervals"
  },
  {
    "objectID": "content/week4-intervals.html#model-validation",
    "href": "content/week4-intervals.html#model-validation",
    "title": "Confidence intervals",
    "section": "Model validation",
    "text": "Model validation\nUsing the \\(t\\) model should produce coverage closer to the nominal level compared with the normal model. Let’s check through simulation.\n\n\nAt right, misses are compared between intervals using SE and critical values from the normal model (left) and \\(t\\) model (right) constructed on the same 10,000 simulated datasets with sample size \\(n = 10\\).\n\n\n\n\n\n\n\n\n\nmodel\ncoverage\n\n\n\n\nnormal\n0.9219\n\n\nt\n0.9461\n\n\n\n\n\nThe \\(t\\) model produces coverage much closer to the nominal level."
  },
  {
    "objectID": "content/week4-intervals.html#calculations",
    "href": "content/week4-intervals.html#calculations",
    "title": "Confidence intervals",
    "section": "Calculations",
    "text": "Calculations\nSo, to sum up, the general formula for an interval for a population mean is: \\[\\bar{x} \\pm c \\times SE(\\bar{x}) \\quad\\text{where}\\quad SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\]\n\n\nRules of thumb:\n\nfor moderate to large samples, use the normal model\n\n\\(c = 1\\) for 68% coverage\n\\(c = 2\\) for 95% coverage\n\\(c = 3\\) for 99.7% coverage\n\nfor small sample sizes, use the \\(t\\) model\nwhen in doubt, use the \\(t\\) model\n\n\nExact critical values in R:\n\n# normal critical value\nc &lt;- qnorm((1 - coverage)/2, lower.tail = F)\n\n# t critical value\nc &lt;- qt((1 - coverage)/2, df = n - 1, lower.tail = F)\n\nInterval calculation:\n\n# pseudo code\nmean(data_vec) + c(-1, 1)*c*sd(data_vec)/sqrt(n)"
  },
  {
    "objectID": "content/week4-intervals.html#interpretation",
    "href": "content/week4-intervals.html#interpretation",
    "title": "Confidence intervals",
    "section": "Interpretation",
    "text": "Interpretation\nAs we’ve seen, coverage pertains to how often an interval of a particular form captures the population parameter of interest across samples of a fixed size. Loosely speaking, this represents how often you’d be right if you were to fully replicate your study ad infinitum.\nThis leads to the following interpretation:\n\nWith [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units].\n\nFor this reason, statisticians call interval estimates confidence intervals."
  },
  {
    "objectID": "content/week4-intervals.html#revisiting-initial-example",
    "href": "content/week4-intervals.html#revisiting-initial-example",
    "title": "Confidence intervals",
    "section": "Revisiting initial example",
    "text": "Revisiting initial example\n\n\nSo in the example we began with:\n\n# calculate 95% interval\nmean(cholesterol) + c(-1, 1)*2*sd(cholesterol)/sqrt(50)\n\n[1] 4.738974 5.346902\n\n\nWith 95% confidence, the mean total HDL cholesterol is estimated to be between 4.739 and 5.347 mmol/L.\nRemember, “95% confidence” refers to coverage under sampling variation.\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.043\n0.01906\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-inference.html#todays-agenda",
    "href": "content/week4-inference.html#todays-agenda",
    "title": "Introduction to inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nTest 1 discussion\n[lecture] Inference vs. description, point estimation, interval estimation\n[lab] Exploring interval coverage"
  },
  {
    "objectID": "content/week4-inference.html#description-vs.-inference",
    "href": "content/week4-inference.html#description-vs.-inference",
    "title": "Introduction to inference",
    "section": "Description vs. inference",
    "text": "Description vs. inference\n\nStatistical inferences are statements about population statistics based on samples.\n\nTo appreciate the meaning of this, consider the contrast with descriptive findings, which are statements about sample statistics:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nA descriptive finding is:\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nThe corresponding inference would be:\nThe median percent change in strength is highest among adults with genotype TT."
  },
  {
    "objectID": "content/week4-inference.html#inference-or-description",
    "href": "content/week4-inference.html#inference-or-description",
    "title": "Introduction to inference",
    "section": "Inference or description?",
    "text": "Inference or description?\nSee if you can tell the difference:\n\nThe proportion of children who developed a peanut allergy was 0.133 in the avoidance group.\nThe proportion of children who develop peanut allergies is estimated to be 0.133 when peanut protein is avoided in infancy.\nThe average lifetime of mice on normal 85kCal diets is estimated to be 32.7 months.\nThe 57 mice on the normal 85kCal diet lived 32.7 months on average.\nThe relative risk of a CHD event in the high trait anger group compared with the low trait anger group was 2.5.\nThe relative risk of a CHD event among adults with high trait anger compared with adults with low trait anger is estimated to be 2.5."
  },
  {
    "objectID": "content/week4-inference.html#random-sampling",
    "href": "content/week4-inference.html#random-sampling",
    "title": "Introduction to inference",
    "section": "Random sampling",
    "text": "Random sampling\n\nSampling establishes the link (or lack thereof) between a sample and a population.\n\n\n\n\n\nIn a simple random sample, units are chosen in such a way that every individual in the population has an equal probability of inclusion. For the SRS:\n\nsample statistics mirror population statistics (sample is representative)\nsampling variability depends only on population variability and sample size"
  },
  {
    "objectID": "content/week4-inference.html#population-models",
    "href": "content/week4-inference.html#population-models",
    "title": "Introduction to inference",
    "section": "Population models",
    "text": "Population models\n\nInference consists in using statistics of a random sample to ascertain population statistics under a population model.\n\nA population model represents the distribution of values you’d see if you measured every individual in the study population. We think of the sample values as a random draw.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Density is an alternative scale to frequency that is independent of population size.)"
  },
  {
    "objectID": "content/week4-inference.html#point-estimates",
    "href": "content/week4-inference.html#point-estimates",
    "title": "Introduction to inference",
    "section": "Point estimates",
    "text": "Point estimates\n\nSample statistics, viewed as guesses for the values of population statistics, are called ‘point estimates’.\n\nWe’ll focus on inferences involving the following:\n\n\n\nPopulation statistic\nParameter\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)"
  },
  {
    "objectID": "content/week4-inference.html#a-difficulty",
    "href": "content/week4-inference.html#a-difficulty",
    "title": "Introduction to inference",
    "section": "A difficulty",
    "text": "A difficulty\n\nDifferent samples yield different estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample means:\n\n\n\n\n\n\n\n\n\nsample.1\nsample.2\n\n\n\n\n5.093\n5.136\n\n\n\n\n\n\nestimates are close but not identical\nthe population mean can’t be both 5.093 and 5.136\nprobably neither estimate is exactly correct\n\nEstimation error and sample-to-sample variability are inherent to point estimation."
  },
  {
    "objectID": "content/week4-inference.html#simulating-sampling-variability",
    "href": "content/week4-inference.html#simulating-sampling-variability",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\n\n\n\n\n\n\n\n\n\n\n\n\nThese are 20 random samples with the sample mean indicated by the dashed line and the population distribution and mean overlaid in red.\n\nsample size \\(n = 20\\)\nfrequency distributions differ a lot\nsample means differ some\n\nWe can actually measure this variability!"
  },
  {
    "objectID": "content/week4-inference.html#simulating-sampling-variability-1",
    "href": "content/week4-inference.html#simulating-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\nIf we had means calculated from a much larger number of samples, we could make a frequency distribution for the values of the sample mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample\n1\n2\n\\(\\cdots\\)\n10,000\n\n\nmean\n4.957\n5.039\n\\(\\cdots\\)\n5.24\n\n\n\n\nWe could then use the usual measures of center and spread to characterize the distribution of sample means.\n\nmean of \\(\\bar{x}\\): 5.0373228\nstandard deviation of \\(\\bar{x}\\): 0.2387869\n\n\nAcross 10,000 random samples of size 20, the typical sample mean was 5.04 and the root average squared distance of the sample mean from its typical value was 0.239."
  },
  {
    "objectID": "content/week4-inference.html#sampling-distributions",
    "href": "content/week4-inference.html#sampling-distributions",
    "title": "Introduction to inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nWhat we are simulating is known as a sampling distribution: the frequency of values of a statistic across all possible random samples.\n\n\n\n\n\n\n\n\n\n\n\n\nProvided data are from a random sample, the sample mean \\(\\bar{x}\\) has a sampling distribution with\n\nmean \\(\\color{red}{\\mu}\\) (population mean)\nstandard deviation \\(\\color{red}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\nregardless of its exact form.\n\n\nIn other words, across all random samples of a fixed size…\n\nThe average value of the sample mean is the population mean.\nThe average squared error (sample mean - population mean)\\(^2\\) is \\(\\frac{\\sigma^2}{n}\\)"
  },
  {
    "objectID": "content/week4-inference.html#effect-of-sample-size",
    "href": "content/week4-inference.html#effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\) is inversely proportional to sample size.\n\n\n\n\n\n\n\n\n\n\n\n\nAs sample size increases…\n\naccuracy remains the same\nestimates get more precise\nskewness vanishes"
  },
  {
    "objectID": "content/week4-inference.html#measuring-sampling-variability",
    "href": "content/week4-inference.html#measuring-sampling-variability",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nIn practice \\(\\sigma\\) is not known so we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared error of the sample mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-inference.html#reporting-point-estimates",
    "href": "content/week4-inference.html#reporting-point-estimates",
    "title": "Introduction to inference",
    "section": "Reporting point estimates",
    "text": "Reporting point estimates\nIt is common style to report the value of a point estimate with a standard error given parenthetically.\n\n\nStatistics from full NHANES sample:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\n\n\n\n\n5.043\n1.075\n3179\n\n\n\n\n\n\n\nThe mean total cholesterol among the population is estimated to be 5.043 mmol/L (SE 0.019)\n\n\n\nThis style of report communicates:\n\nparameter of interest\nvalue of point estimate\nerror/variability of point estimate"
  },
  {
    "objectID": "content/week4-inference.html#interval-estimation",
    "href": "content/week4-inference.html#interval-estimation",
    "title": "Introduction to inference",
    "section": "Interval estimation",
    "text": "Interval estimation\n\nAn interval estimate is a range of plausible values for a population parameter.\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.043 \\pm 2\\times 0.0191 = (5.005, 5.081)\\]\n\nIn R:\n\navg.totchol &lt;- mean(totchol)\nse.totchol &lt;- sd(totchol)/sqrt(length(totchol))\navg.totchol + c(-2, 2)*se.totchol\n\n[1] 5.004817 5.081059\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-intervals.html#the-t-model-1",
    "href": "content/week4-intervals.html#the-t-model-1",
    "title": "Confidence intervals",
    "section": "The \\(t\\) model",
    "text": "The \\(t\\) model\nWe’re actually using \\(\\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\\) to construct intervals, because we don’t know \\(\\sigma\\).\nThese deviations are better approximated by a \\(t\\) model, which adjusts the normal model for the extra uncertainty that comes from estimating the standard deviation.\n\n\nThe difference between models depends mainly on sample size:\n\nbehaves almost exactly the same for moderate to large samples\nlarger deviations from center for small samples\nleads to larger multipliers for computing margin of error\n\n\n\n\n\nComparison of \\(t\\) model with normal model for various degrees of freedom."
  },
  {
    "objectID": "content/week4-intervals.html#model-interpretation",
    "href": "content/week4-intervals.html#model-interpretation",
    "title": "Confidence intervals",
    "section": "Model interpretation",
    "text": "Model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\n50% of samples give negative values of \\(T\\)\n\n\npt(0, df = 20 - 1) # area less than 0\n\n[1] 0.5"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation",
    "href": "content/week4-intervals.html#t-model-interpretation",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 50% of samples, \\(T &lt; 0\\)\n\n\n# area less than 0\npt(0, df = 20 - 1) \n\n[1] 0.5\n\n\n\nwritten as \\(P(T &lt; 0) = 0.5\\)"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-1",
    "href": "content/week4-intervals.html#t-model-interpretation-1",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 83.5% of samples, \\(T &lt; 1\\)\n\n\n# area less than 1\npt(1, df = 20 - 1) \n\n[1] 0.8350616\n\n\n\nwritten as \\(P(T &lt; 1) = 0.835\\)"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-2",
    "href": "content/week4-intervals.html#t-model-interpretation-2",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 97% of samples, \\(T &lt; 2\\)\n\n\n# area less than 2\npt(2, df = 20 - 1) \n\n[1] 0.969999\n\n\n\nwritten as \\(P(T &lt; 2) = 0.97\\)"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-3",
    "href": "content/week4-intervals.html#t-model-interpretation-3",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 3% of samples, \\(T &gt; 2\\)\n\n\n# area greater than 2\npt(2, df = 20 - 1, lower.tail = F) \n\n[1] 0.03000102\n\n\n\nnotice: \\[\n\\begin{align*}\nP(T &gt; 2) &= 1 - P(T &lt; 2) \\\\\n(0.03) &= 1 - (0.97)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-4",
    "href": "content/week4-intervals.html#t-model-interpretation-4",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 13.5% of samples, \\(1 &lt; T &lt; 2\\)\n\n\n# area between 1 and 2\npt(2, df = 20 - 1) - pt(1, df = 20 - 1) \n\n[1] 0.1349374\n\n\n\nnotice: \\[\n\\begin{align*}\nP(1 &lt; T &lt; 2) &= P(T &lt; 2) - P(T &lt; 1) \\\\\n(0.135) &= (0.97) - (0.835)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-5",
    "href": "content/week4-intervals.html#t-model-interpretation-5",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 94% of samples, \\(-2 &lt; T &lt; 2\\)\n\n\n# area between 1 and 2\npt(2, df = 20 - 1) - pt(-2, df = 20 - 1) \n\n[1] 0.939998\n\n\n\nwritten \\(P(-2 &lt; T &lt; 2) = 0.94\\)"
  },
  {
    "objectID": "content/week4-intervals.html#revisiting-intervals",
    "href": "content/week4-intervals.html#revisiting-intervals",
    "title": "Confidence intervals",
    "section": "Revisiting intervals",
    "text": "Revisiting intervals\nSo where did that 2 come from in the margin of error for our interval estimate?\n\\[\n\\bar{x} \\pm \\color{blue}{2}\\times SE(\\bar{x})\n\\]\nWell:\n\\[\n\\begin{align*}\n0.94 &= P(-\\color{blue}{2} &lt; T &lt; \\color{blue}{2}) \\\\\n&= P\\left(-\\color{blue}{2} &lt; \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}} &lt; \\color{blue}{2}\\right) \\\\\n&= P(\\underbrace{\\bar{x} - \\color{blue}{2}\\times SE(\\bar{x}) &lt; \\mu &lt; \\bar{x} + \\color{blue}{2}\\times SE(\\bar{x})}_{\\text{interval covers population mean}})\n\\end{align*}\n\\] So for 94% of all random samples, the interval covers the population mean."
  },
  {
    "objectID": "content/week4-intervals.html#a-closer-look-at-interval-construction",
    "href": "content/week4-intervals.html#a-closer-look-at-interval-construction",
    "title": "Confidence intervals",
    "section": "A closer look at interval construction",
    "text": "A closer look at interval construction\nSo where did that 2 come from in the margin of error for our interval estimate?\n\\[\n\\bar{x} \\pm \\color{blue}{2}\\times SE(\\bar{x})\n\\]\nWell:\n\n\n\\[\n\\begin{align*}\n0.94 &= P(-\\color{blue}{2} &lt; T &lt; \\color{blue}{2}) \\\\\n&= P\\left(-\\color{blue}{2} &lt; \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}} &lt; \\color{blue}{2}\\right) \\\\\n&= P(\\underbrace{\\bar{x} - \\color{blue}{2}\\times SE(\\bar{x}) &lt; \\mu &lt; \\bar{x} + \\color{blue}{2}\\times SE(\\bar{x})}_{\\text{interval covers population mean}})\n\\end{align*}\n\\]\n\n\nFor 94% of all random samples, the interval covers the population mean.\n\n\n\nSo the number 2 determines the proportion of samples for which the interval covers the mean, known as its coverage."
  },
  {
    "objectID": "content/week4-intervals.html#coverage",
    "href": "content/week4-intervals.html#coverage",
    "title": "Confidence intervals",
    "section": "Coverage",
    "text": "Coverage\nConsider a slightly more general expression for an interval for the mean:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nThe value of \\(c\\) determines the coverage rate, or simply “coverage”.\nA common empirical rule:\n\n\\(c = 1 \\longrightarrow\\) approximately 68% coverage\n\\(c = 2 \\longrightarrow\\) approximately 95% coverage\n\\(c = 3 \\longrightarrow\\) approximately 99.7% coverage"
  },
  {
    "objectID": "content/week4-intervals.html#effect-of-sample-size",
    "href": "content/week4-intervals.html#effect-of-sample-size",
    "title": "Confidence intervals",
    "section": "Effect of sample size",
    "text": "Effect of sample size\n\nThe sample size determines the exact shape of the \\(t\\) model through its ‘degrees of freedom’ \\(n - 1\\). This changes the areas slightly.\n\nThe exact coverage quickly converges to just over 95% as the sample size increases.\n\n\n\n\n\n\n\n\n\n\n\nn\ncoverage\n\n\n\n\n4\n0.8607\n\n\n8\n0.9144\n\n\n16\n0.9361\n\n\n32\n0.9457\n\n\n64\n0.9502\n\n\n128\n0.9524\n\n\n256\n0.9534"
  },
  {
    "objectID": "content/week4-intervals.html#empirical-rule",
    "href": "content/week4-intervals.html#empirical-rule",
    "title": "Confidence intervals",
    "section": "Empirical rule",
    "text": "Empirical rule\nConsider a slightly more general expression for an interval for the mean:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nThe value of \\(c\\) determines the coverage.\n\nlarger \\(c\\) \\(\\longrightarrow\\) higher coverage\nsmaller \\(c\\) \\(\\longrightarrow\\) lower coverage\n\nThe so-called “empirical rule” is that:\n\n\\(c = 1 \\longrightarrow\\) approximately 68% coverage\n\\(c = 2 \\longrightarrow\\) approximately 95% coverage\n\\(c = 3 \\longrightarrow\\) approximately 99.7% coverage"
  },
  {
    "objectID": "content/week4-intervals.html#effect-of-sample-size-on-coverage",
    "href": "content/week4-intervals.html#effect-of-sample-size-on-coverage",
    "title": "Confidence intervals",
    "section": "Effect of sample size on coverage",
    "text": "Effect of sample size on coverage\n\nThe sample size determines the exact shape of the \\(t\\) model through its ‘degrees of freedom’ \\(n - 1\\). This changes the areas slightly, but not by much.\n\nThe exact coverage rate, or simply “coverage”, quickly converges to just over 95%.\n\n\n\n\n\n\n\n\n\n\n\nn\ncoverage\n\n\n\n\n4\n0.8607\n\n\n8\n0.9144\n\n\n16\n0.9361\n\n\n32\n0.9457\n\n\n64\n0.9502\n\n\n128\n0.9524\n\n\n256\n0.9534"
  },
  {
    "objectID": "content/week4-intervals.html#changing-the-coverage",
    "href": "content/week4-intervals.html#changing-the-coverage",
    "title": "Confidence intervals",
    "section": "Changing the coverage",
    "text": "Changing the coverage\nConsider a slightly more general expression for an interval for the mean:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nThe number \\(c\\) is called a critical value. It determines the coverage.\n\nlarger \\(c\\) \\(\\longrightarrow\\) higher coverage\nsmaller \\(c\\) \\(\\longrightarrow\\) lower coverage\n\nThe so-called “empirical rule” is that:\n\n\\(c = 1 \\longrightarrow\\) approximately 68% coverage\n\\(c = 2 \\longrightarrow\\) approximately 95% coverage\n\\(c = 3 \\longrightarrow\\) approximately 99.7% coverage"
  },
  {
    "objectID": "content/week4-intervals.html#contrasting-coverage-with-precision",
    "href": "content/week4-intervals.html#contrasting-coverage-with-precision",
    "title": "Confidence intervals",
    "section": "Contrasting coverage with precision",
    "text": "Contrasting coverage with precision\n\nPrecision refers to how wide or narrow the interval is.\n\nPrecision depends on every component of the margin of error:\n\ncritical value used\nsample size\nvariability of values\n\nBy contrast, coverage depends only on the critical value used."
  },
  {
    "objectID": "content/week4-intervals.html#exact-coverage-using-t-quantiles",
    "href": "content/week4-intervals.html#exact-coverage-using-t-quantiles",
    "title": "Confidence intervals",
    "section": "Exact coverage using \\(t\\) quantiles",
    "text": "Exact coverage using \\(t\\) quantiles\nTo engineer an interval with a specific coverage, use the \\(p\\)th quantile where:\n\\[p = \\left[1 - \\left(\\frac{1 - \\text{coverage}}{2}\\right)\\right]\\] In R:\n\n# coverage 95% using t quantile\ncoverage &lt;- 0.95\nq.val &lt;- 1 - (1 - coverage)/2\ncrit.val &lt;- qt(q.val, df = 20 - 1)\ncrit.val\n\n[1] 2.093024\n\n\nThe effect of increasing/decreasing coverage on the quantile is:\n\nincrease coverage \\(\\longrightarrow\\) larger quantile \\(\\longrightarrow\\) wider interval\ndecrease coverage \\(\\longrightarrow\\) smaller quantile \\(\\longrightarrow\\) narrower interval"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-quantiles",
    "href": "content/week4-intervals.html#t-model-quantiles",
    "title": "Confidence intervals",
    "section": "\\(t\\) model quantiles",
    "text": "\\(t\\) model quantiles\n\n\n\\[\nP(\\color{#FF6459}{-2 &lt; T &lt; 2}) = 1 - 2\\times P(\\color{blue}{T &gt; 2})\n\\]\nLook at how the areas add up so that: \\[\nP(\\color{blue}{T &gt; 2}) = 0.03\n\\] Moreover: \\[\nP(T &lt; 2) = 1 - 0.03 = 0.97\n\\]"
  },
  {
    "objectID": "content/week4-intervals.html#interpreting-critical-values",
    "href": "content/week4-intervals.html#interpreting-critical-values",
    "title": "Confidence intervals",
    "section": "Interpreting critical values",
    "text": "Interpreting critical values\n\n\n\\[\nP(\\color{#FF6459}{-2 &lt; T &lt; 2}) = 1 - 2\\times P(\\color{blue}{T &gt; 2})\n\\]\nLook at how the areas add up so that: \\[\nP(\\color{blue}{T &gt; 2}) = 0.03\n\\] Moreover: \\[\nP(T &lt; 2) = 1 - 0.03 = 0.97\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSo the critical value 2 is actually the 97th percentile of the sampling distribution of \\(T\\).\n\nalso called the 0.97 “quantile”\n(percentiles expressed in proportions are called quantiles)"
  },
  {
    "objectID": "content/week4-intervals.html#simulation-of-coverage",
    "href": "content/week4-intervals.html#simulation-of-coverage",
    "title": "Confidence intervals",
    "section": "Simulation of coverage",
    "text": "Simulation of coverage\n\n\nArtificially simulating a large number of intervals provides an empirical approximation of coverage.\n\nat right, 200 intervals\n94% cover the population mean (vertical dashed line)\npretty close to nominal coverage level 95%\n\nThis is also a handy way to remember the proper interpretation:\n\nIf I made a lot of intervals from independent samples, 95% of them would ‘get it right’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-intervals.html#confidence-intervals",
    "href": "content/week4-intervals.html#confidence-intervals",
    "title": "Confidence intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nInterval estimates constructed to achieve a specified coverage are called “confidence intervals”; the coverage is interpreted and reported as a “confidence level”.\n\n\n\n# ingredients\ncholesterol.mean &lt;- mean(cholesterol)\ncholesterol.sd &lt;- sd(cholesterol)\ncholesterol.n &lt;- length(cholesterol)\ncholesterol.se &lt;- cholesterol.sd/sqrt(cholesterol.n)\ncrit.val &lt;- qt(1 - (1 - 0.95)/2, df = cholesterol.n - 1)\n\n# interval\ncholesterol.mean + c(-1, 1)*crit.val*cholesterol.se\n\n[1] 5.005566 5.080310\n\n\n\n\nWith 95% confidence, the mean total cholesterol among U.S. adults is estimated to be between 5.0056 and 5.0803 mmol/L.\n\n\n\nThe general formula for a confidence interval for the population mean is\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nwhere \\(c\\) is a critical value, obtained as a quantile of the \\(t_{n - 1}\\) model and chosen to ensure a specific coverage."
  },
  {
    "objectID": "content/week4-intervals.html#recap",
    "href": "content/week4-intervals.html#recap",
    "title": "Confidence intervals",
    "section": "Recap",
    "text": "Recap\nThe “common” interval estimate for the mean is actually an approximate 95% confidence interval:\n\\[\n\\bar{x} \\pm 2 \\times SE(\\bar{x})\n\\]\n\ncaptures the population mean \\(\\mu\\) for roughly 95% of random samples\nreplacing 2 with a \\(t_{n - 1}\\) quantile allows the analyst to adjust coverage\nthe \\(t_{n - 1}\\) model is an approximation for the sampling distribution of \\(\\frac{\\bar{x} - \\mu}{SE(\\bar{x})}\\)\n\napproximation improves with increasing sample size or symmetry\nusually good quality except in “extreme” situations\n\n\nInterval interpretation:\n\nWith [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units]."
  },
  {
    "objectID": "content/lab5-intervals.html",
    "href": "content/lab5-intervals.html",
    "title": "Lab 5: Confidence intervals",
    "section": "",
    "text": "The objective of this lab is to learn to compute confidence intervals for a population mean, and more specifically, to learn to adjust interval coverage by calculating appropriate critical values. Since you already learned to calculate an interval in the last lab, the basic mechanics of the arithmetic are familiar.\nWe’ll use data from a sample of 100 births in North Carolina in 2004. To change things up a little, the data is stored as a .csv file (not an .RData file). If you were to download and open this file on your computer, it would likely appear as a spreadsheet (try if you’re curious). Read in the data using the command below.\n\n# read in data and preview\nncbirths &lt;- read_csv('data/ncbirths.csv')\nhead(ncbirths)\n\n# A tibble: 6 × 4\n  mother.age weeks birth.weight sex   \n       &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt; \n1         36    39         7.69 male  \n2         35    40         8.88 male  \n3         40    40         9    female\n4         37    40         7.94 male  \n5         35    28         1.63 female\n6         25    40         8.75 female\n\n\nRecall that the general formula for an interval is:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\] Throughout this lab, you’ll manipulate the coverage by obtaining different values of the critical value \\(c\\). We’ll start the back-of-the-envelope approach following the empirical rule.\n\nIntervals using the empirical rule\nThe empirical rule allows us to construct intervals using whole number multiples of the standard error and obtain the following approximate coverages:\n\n\\(c = 1\\) gives 68% coverage\n\\(c = 2\\) gives 95% coverage\n\\(c = 3\\) gives 99.7% coverage\n\nAn approximate 95% confidence interval for the mean birth weight (lbs) in NC in 2004 is:\n\n# retrieve variable of interest\nbweight &lt;- ncbirths$birth.weight\n\n# interval ingredients\nbweight.mean &lt;- mean(bweight)\nbweight.sd &lt;- sd(bweight)\nbweight.n &lt;- length(bweight)\n\n# standard error\nbweight.se &lt;- bweight.sd/sqrt(bweight.n)\n\n# 95% interval using empirical rule\nbweight.mean + c(-2, 2)*bweight.se\n\n[1] 6.89267 7.46633\n\n\nFollowing class discussion, we’d interpret this as follows:\n\nWith 95% confidence, the mean birth weight of babies born in North Carolina in 2004 is estimated to be between 6.893 and 7.466 lbs.\n\nTo compute a 68% interval, we need only change the critical value. All of the above remains the same except the last command, which we change to:\n\n# 68% interval using empirical rule\nbweight.mean + c(-1, 1)*bweight.se\n\n[1] 7.036085 7.322915\n\n\nNotice that the interval got narrower: a more precise estimate can be given at a reduced coverage rate (which of course means the estimate is wrong more often).\n\nWith 68% confidence, the mean birth weight of babies born in North Carolina in 2004 is estimated to be between 7.036 and 7.323 lbs.\n\n\n\n\n\n\n\nYour turn 1\n\n\n\nCalculate and interpret a 99.7% confidence interval for the mean number of weeks at birth.\n\n# retrieve variable of interest (no. weeks at birth)\n\n# interval ingredients\n\n# standard error\n\n# 99.7% interval for mean number of weeks at birth using empirical rule\n\n\n\nBecause we’re only changing the critical value here, let’s save some work and write a simple function to calculate an interval from a vector of values and a critical value. (You don’t need to understand the syntax or be able to write functions in R, as this is a programming technique, but it may interest you to see how it can be done.)\n\n# run this before continuing, but ignore unless interested\nmake_ci &lt;- function(vec, cval){\n  vec.mean &lt;- mean(vec)\n  vec.mean.se &lt;- sd(vec)/sqrt(length(vec) - 1)\n  interval &lt;- vec.mean + c(-1, 1)*cval*vec.mean.se \n  names(interval) &lt;- c('lwr', 'upr')\n  return(interval)\n}\n\nWe can use this function to compute an interval a bit more efficiently. Check that the following give the same intervals as obtained above using fully manual calculations.\n\n# 95% interval\nmake_ci(bweight, cval = 2)\n\n     lwr      upr \n6.891225 7.467775 \n\n# 68% interval\nmake_ci(bweight, cval = 1)\n\n     lwr      upr \n7.035362 7.323638 \n\n\nTry it yourself to get the hang of using this function.\n\n\n\n\n\n\nYour turn 2\n\n\n\nUse make_ci(...) to compute 95% and 99.7% confidence intervals for the mean number of weeks at birth.\n\n# 99.7% interval for mean number of weeks at birth, using make_ci(...)\n\n# 95% interval for mean number of weeks at birth, using make_ci(...)\n\n\n\n\n\nIntervals using \\(t\\) critical values\nIf you want to construct an interval with a coverage other than 68%, 95%, or 99.7%, you’ll need to use a different critical value. Instead of a whole number, you’ll need the \\(p\\)th quantile from the \\(t_{n-1}\\) model where:\n\\[p = \\left[1 - \\left(\\frac{1 - \\text{coverage}}{2}\\right)\\right]\\] This is perhaps a little more complex than it looks. You could probably determine which quantile to use in your head – the quantile you want is just the midpoint between your coverage level and 1. Consider the following examples:\n\nfor a 95% interval, use \\(p = 0.975\\)\nfor an 80% interval, use \\(p = 0.9\\)\nfor a 99% interval, use \\(p = 0.995\\)\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nWhich quantile would you use for…\n\nA 96% confidence interval?\nAn 85% confidence interval?\nA 98% confidence interval?\n\n\n\n\nCalculating quantiles\nThe qt(...) function in R will calculate quantiles for you. It takes two arguments: which quantile you want (\\(p\\)) and the degrees of freedom for the \\(t_{n - 1}\\) model. The degrees of freedom is one less than the sample size in this case (\\(n - 1\\)). The following commands illustrate the calculation.\n\n# for 80% interval from n = 15 observations, use this quantile\nqt(p = 0.9, df = 14)\n\n[1] 1.34503\n\n# for a 92% interval from n = 30 observations, use this quantile\nqt(p = 0.96, df = 29)\n\n[1] 1.814238\n\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nCalculate \\(t\\) quantiles for the following scenarios:\n\n90% interval from 27 observations\n95% interval from 18 observations\n99% interval from 51 observations\n\n\n# quantile for a 90% interval from 27 observations\n\n# quantile for a 95% interval from 18 observations\n\n# quantile for a 99% interval from 51 observations\n\n\n\nThese quantiles are the critical values you’d use to construct an interval with the specified coverage and number of observations.\n\n\nConstructing intervals\nIf we want a confidence interval for the mean with a specific coverage, first determine which quantile is needed as above and compute it, and then construct the interval as usual using that quantile as the critical value.\nFor example, if we want confidence intervals for the mean birth weight:\n\n# 98% ci for mean birth weight\ncrit.val &lt;- qt(p = 0.99, df = 99)\nmake_ci(bweight, cval = crit.val)\n\n     lwr      upr \n6.838671 7.520329 \n\n# 95% ci for mean birth weight\ncrit.val &lt;- qt(p = 0.975, df = 99)\nmake_ci(bweight, cval = crit.val)\n\n     lwr      upr \n6.893499 7.465501 \n\n# 90% ci for mean birth weight\ncrit.val &lt;- qt(p = 0.95, df = 99)\nmake_ci(bweight, cval = crit.val)\n\n     lwr      upr \n6.940175 7.418825 \n\n\nAs a matter of interest, note that the critical value for the 95% interval is 1.9842. Technically, this is the value that provides an interval with 95% coverage; the approximation of 2 provided by the empirical rule is just that – an approximation.\n\n\n\n\n\n\nYour turn 5\n\n\n\nConstruct and interpret a 99% confidence interval for the mean number of weeks at birth.\n\n# 99% ci for mean number of weeks at birth\n\n\n\nNow that you have a sense of the critical value calculation, it’s helpful to remind yourself how to make the interval fully from scratch.\n\n\n\n\n\n\nYour turn 6\n\n\n\nRepeat the previous calculation, but without using the make_ci(...) function. You should obtain exactly the same numerical result.\n\n## repeat last calculation but fully 'by hand'\n\n# point estimate and standard error\n\n# critical value\n\n# interval\n\n\n\n\n\n\nWorking backwards to determine coverage\nNow let’s try doing the above backwards. If you’re given a confidence interval and you know the summary statistics, you can figure out the interval coverage by solving for the critical value and using the pt(...) function. Really, you only need to know the standard error (or sample size and standard deviation) to do this.\nFirst, find the margin of error by taking half the interval width.\n\\[\n\\text{margin of error} = \\frac{\\text{upr} - \\text{lwr}}{2}\n\\]\nThen, divide by the standard error to solve for the critical value:\n\\[\nc = \\frac{\\text{margin of error}}{SE(\\bar{x})}\n\\] Lastly, find the coverage as the area of the sampling distribution below the critical value: \\[\n\\text{coverage} = P(T &lt; c)\n\\]\nIn R:\n\n# example interval for mean birth weight\nbweight.ci &lt;- c(7.030077, 7.328923)\n\n# margin of error (half the width)\nbweight.ci.me &lt;- diff(bweight.ci)/2\n\n# divide out standard error to get critical value\ncrit.val &lt;- bweight.ci.me/bweight.se\n\n# coverage\npt(q = crit.val, df = bweight.n - 1)\n\n[1] 0.85\n\n\nTake a moment to align the R commands with the calculations shown above and check to make sure you see how this is consistent with the way we formed the interval in the first place. Then try it on your own.\n\n\n\n\n\n\nYour turn 7\n\n\n\nDetermine the coverage for the interval below for the mean number of weeks at birth.\n\n# interval for mean number of weeks at birth\nbweeks.ci &lt;- c(37.79485, 39.30515)\n\n# margin of error (half the width)\n\n# divide out standard error to get critical value\n\n# coverage"
  },
  {
    "objectID": "content/week5-hypothesis.html#todays-agenda",
    "href": "content/week5-hypothesis.html#todays-agenda",
    "title": "Introduction to hypothesis testing",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nLoose end: working backwards to determine interval coverage\n[lecture] the \\(t\\)-test for a population mean\n[lab] computing test statistics, critical values, and \\(p\\)-values"
  },
  {
    "objectID": "content/week5-hypothesis.html#ddt-data",
    "href": "content/week5-hypothesis.html#ddt-data",
    "title": "Hypothesis testing",
    "section": "DDT data",
    "text": "DDT data\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm). Each measurement was taken by a different laboratory.\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu \\leq 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]"
  },
  {
    "objectID": "content/week5-hypothesis.html#hypothesis-testing",
    "href": "content/week5-hypothesis.html#hypothesis-testing",
    "title": "Hypothesis testing",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nThis is an example of a hypothesis testing problem: we want to test the hypothesis that mean DDT in kale is within safe limits. Hypothesis testing is another form of statistical inference.\nThe general pattern for performing a hypothesis test is:\n\nFormulate the hypothesis to test in terms of the values of a population parameter.\nAssess the likelihood of the data under the hypothesis through use of a “test statistic”.\nConclude whether the data provide evidence favoring an alternative.\n\nToday we’ll cover each step in turn in the context of tests for a population mean."
  },
  {
    "objectID": "content/week5-hypothesis.html#formulating-hypotheses",
    "href": "content/week5-hypothesis.html#formulating-hypotheses",
    "title": "Hypothesis testing",
    "section": "1. Formulating hypotheses",
    "text": "1. Formulating hypotheses\nHypotheses cannot be tested in isolation, but must be considered relative to a specified alternative.\nTo articulate the hypotheses for a test, we need:\n\npopulation parameter of interest\nnull hypothesis \\(H_0\\): possible value(s) under the claim to be tested\nalternative hypothesis \\(H_A\\): possible value(s) if the claim is found to be false\n\nIn the context of the DDT example…\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\]"
  },
  {
    "objectID": "content/week5-hypothesis.html#test-statistic",
    "href": "content/week5-hypothesis.html#test-statistic",
    "title": "Hypothesis testing",
    "section": "2. Test statistic",
    "text": "2. Test statistic\nTest statistics are data summaries that:\n\ndepend on the null value of the population parameter\nhave a known sampling distribution\n\nFor a population mean, we use: \\[T = \\frac{\\bar{x} - \\mu_0}{s_x/\\sqrt{n}}\\]\nThis is well-described by a \\(t_{n - 1}\\) model when \\(\\mu = \\mu_0\\). It is useful for the test because:\n\nlarge (absolute) values of \\(T\\) are unlikely if \\(\\mu = \\mu_0\\)\nsmall (absolute) values of \\(T\\) are expected if \\(\\mu = \\mu_0\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#drawing-a-conclusion",
    "href": "content/week5-hypothesis.html#drawing-a-conclusion",
    "title": "Hypothesis testing",
    "section": "3. Drawing a conclusion",
    "text": "3. Drawing a conclusion\n\n\nIn the DDT example, \\(T\\) = 2.906. This favors \\(H_A\\), but by how much?\nAccording to the \\(t\\) model, less than 1% of samples would produce a result more favorable to \\(H_A\\).\n\n\n\n\n\n\n\n\n\n\nPoint estimate:\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n3.328\n0.1129\n\n\n\n\n\nTest statistic:\n\\[T = \\frac{\\bar{x} - \\mu_0}{SE(\\bar{x})} = \\]\n\nThis is strong evidence against the claim that the DDT level is 3ppm or less and in favor of the claim that the DDT level exceeds 3ppm."
  },
  {
    "objectID": "content/week5-hypothesis.html#recap-of-ddt-example",
    "href": "content/week5-hypothesis.html#recap-of-ddt-example",
    "title": "Hypothesis testing",
    "section": "Recap of DDT example",
    "text": "Recap of DDT example\n\nIs the mean DDT level in kale 3ppm or less?\n\n\n\nData are measurements of DDT levels in ppm from 15 labs.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n3.328\n0.4372\n0.1129\n\n\n\n\n\n\\(t_{14}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#another-example-sleep",
    "href": "content/week5-hypothesis.html#another-example-sleep",
    "title": "Hypothesis testing",
    "section": "Another example: sleep",
    "text": "Another example: sleep\n\nDoes the average U.S. adult sleep at least 7 hours per night?\n\n\n\nData are reported average hours of sleep per night from 135 NHANES respondents.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n6.896\n1.394\n0.12\n\n\n\n\n\n\\(t_{134}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#your-turn-body-temperatures",
    "href": "content/week5-hypothesis.html#your-turn-body-temperatures",
    "title": "Hypothesis testing",
    "section": "Your turn: body temperatures",
    "text": "Your turn: body temperatures\n\nIs mean body temperature actually 98.6 °F, or is it lower?\n\n\n\nData are 130 observations of body temperature (°F) derived from a JAMA study.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n98.25\n0.7332\n0.0643\n\n\n\n\n\n\\(t_{129}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#strength-of-evidence",
    "href": "content/week5-hypothesis.html#strength-of-evidence",
    "title": "Hypothesis testing",
    "section": "Strength of evidence",
    "text": "Strength of evidence\nThe result that 0.575% of samples would produce a test statistic more strongly favoring the alternative hypothesis is an example of a p-value:\n\nthe probability under \\(H_0\\) of obtaining a sample for which the test statistic is at least as favorable to \\(H_A\\) as the value actually observed\n\nIn other words, \\(p\\)-values assume the null hypothesis is true, and then ask, “what is the chance I’d obtain data at least as suggestive as what I have that the alternative is more likely than the null?”\n\nsmaller \\(p\\)-values: if \\(H_0\\) is true, equally or more favorable results are not expected often by chance\nlarger \\(p\\)-values: if \\(H_0\\) is true, equally or more favorable results are expected often by chance"
  },
  {
    "objectID": "content/week5-hypothesis.html#evidence-thresholds",
    "href": "content/week5-hypothesis.html#evidence-thresholds",
    "title": "Hypothesis testing",
    "section": "Evidence thresholds",
    "text": "Evidence thresholds\nIt remains to define an evidence threshold above which we decide to reject \\(H_0\\).\nA heuristic is to fix a significance level \\(\\alpha\\) and reject \\(H_0\\) whenever \\(p &lt; \\alpha\\).\n\nrepresents an evidence threshold\nconventionally, \\(\\alpha = 0.05\\)\ncontrols error rates\n\nImagine that indeed mean DDT in kale is 3ppm. Then 0.575% of samples produce test statistics at least as favorable to the alternative as what we saw in the study.\n\nso if we set the evidence threshold for rejecting \\(H_0\\) exactly here (\\(\\alpha = 0.00575\\)) we’ll be wrong 0.575% of the time\nif we set the evidence threshold lower (say \\(\\alpha = 0.01\\)) we’ll be wrong more than 0.575% of the time (in fact 1% of the time)"
  },
  {
    "objectID": "content/week5-hypothesis.html#interpreting-results",
    "href": "content/week5-hypothesis.html#interpreting-results",
    "title": "Introduction to hypothesis testing",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\nCalculations in R:\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# compute critical value for a 5% error tolerance\ncrit.val &lt;- qt(p = 0.975, df = 38)\ncrit.val\n\n[1] 2.024394\n\n# test decision\nabs(tstat) &gt; crit.val\n\n[1] FALSE\n\n# p-value\n2*pt(abs(tstat), df = 38, lower.tail = F)\n\n[1] 0.1920133\n\n\n\nConventional narrative summary style:\n\nThe data do not provide evidence that the mean body temperature differs from 98.6°F (T = -1.328 on 38 degrees of freedom, p = 0.192).\n\nConveys a lot of info succinctly:\n\ntest conclusion\nhypotheses tested\nnumber of standard errors from hypothesized value (\\(T\\))\nsample size (degrees of freedom + 1)\nstrength of evidence (\\(p\\)-value)"
  },
  {
    "objectID": "content/week5-hypothesis.html#composite-hypotheses",
    "href": "content/week5-hypothesis.html#composite-hypotheses",
    "title": "Hypothesis testing",
    "section": "Composite hypotheses",
    "text": "Composite hypotheses\nThe null hypothesis is a composite of values, so why did we choose just one (\\(\\mu_0 = 3\\)) to perform the test?\nAny null value farther from the alternative will produce stronger results.\n\n\n\n\n\n\n\n\nNull value\nTest statistic numerator\nProportion of samples more favorable to \\(H_A\\)\n\n\n\n\n\\(\\mu_0 = 3\\)\n0.328\n0.00575\n\n\n\\(\\mu_0 = 2.99\\)\n0.338\n0.00483\n\n\n\\(\\mu_0 = 2.95\\)\n0.378\n0.00239\n\n\n\nSo by using \\(\\mu_0 = 3\\), we are choosing the most conservative null value for the test."
  },
  {
    "objectID": "content/week5-hypothesis.html#components-of-a-test",
    "href": "content/week5-hypothesis.html#components-of-a-test",
    "title": "Introduction to hypothesis testing",
    "section": "Components of a test",
    "text": "Components of a test\n\n\n\n\n\n\n\n\nComponent\nExplanation\nExample\n\n\n\n\nPopulation parameter\nThe quantity of interest\nMean body temp \\(\\mu\\)\n\n\nNull hypothesis\nThe claim to be tested\n\\(\\mu = 98.6\\)\n\n\nAlternative hypothesis\nThe alternative claim\n\\(\\mu \\neq 98.6\\)\n\n\nTest statistic\nA function of the sample data and the hypothetical parameter value\n\\(T = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}} = -1.328\\)\n\n\nModel\nSampling distribution of the test statistic under \\(H_0\\)\n\\(t_{38}\\) model\n\n\n\\(p\\)-value\nProbability under \\(H_0\\) of obtaining a result at least as favorable to \\(H_A\\)\n19.2% of samples produce a test statistic at least as large\n\n\nDecision\nReject or fail to reject \\(H_0\\) in favor of \\(H_A\\)\nFail to reject\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-hypothesis.html#performing-tests-in-r",
    "href": "content/week5-hypothesis.html#performing-tests-in-r",
    "title": "Hypothesis testing",
    "section": "Performing tests in R",
    "text": "Performing tests in R\n\n\nInputs:\n\ndata vector\nnull value of parameter\nalternative hypothesis\n\nOutputs:\n\ntest statistic\ndegrees of freedom for \\(t\\) model\n\\(p\\)-value\nconfidence interval\npoint estimate\n\n\nt.test performs all calculations. Locate each input (1-3) and output (4-7) below:\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#directional-hypotheses",
    "href": "content/week5-hypothesis.html#directional-hypotheses",
    "title": "Hypothesis testing",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm). Each measurement was taken by a different laboratory.\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu \\leq 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]"
  },
  {
    "objectID": "content/week5-hypothesis.html#lab-t-tests-in-r",
    "href": "content/week5-hypothesis.html#lab-t-tests-in-r",
    "title": "Hypothesis testing",
    "section": "Lab: \\(t\\)-tests in R",
    "text": "Lab: \\(t\\)-tests in R\nOpen up lab6-hypotesting in the class workspace. The goals for this lab are:\n\nLearn how to implement \\(t\\) tests in R and interpret output\nPractice formulating and testing hypotheses from simple research questions\n(If time) Explore decision errors\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-hypothesis.html#from-last-time",
    "href": "content/week5-hypothesis.html#from-last-time",
    "title": "Hypothesis testing",
    "section": "From last time",
    "text": "From last time\n\n\nWe developed interval estimates based on the \\(t_{n - 1}\\) model for the sampling distribution of the statistic: \\[\nT = \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\]\n\n\\(T = 2\\) means we over-estimated the population mean by 2 standard errors\narea under the curve \\(=\\) proportion of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#body-temperatures",
    "href": "content/week5-hypothesis.html#body-temperatures",
    "title": "Introduction to hypothesis testing",
    "section": "Body temperatures",
    "text": "Body temperatures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\nse\n\n\n\n\n98.41\n0.9162\n39\n0.1467\n\n\n\n\n\n\nIs the true mean body temperature actually 98.6°F?\nSeems plausible given our data.\nBut what if the sample mean were instead…\n\n\n\n\\(\\bar{x}\\)\nconsistent with \\(\\mu = 98.6\\)?\n\n\n\n\n98.30\nprobably still yes\n\n\n98.15\nmaybe\n\n\n98.00\nhesitating\n\n\n97.85\nskeptical\n\n\n97.40\nunlikely\n\n\n\n\n\n\nIf the estimation error is “big enough” the hypothesis seems implausible."
  },
  {
    "objectID": "content/week5-hypothesis.html#an-intution",
    "href": "content/week5-hypothesis.html#an-intution",
    "title": "Hypothesis testing",
    "section": "An intution",
    "text": "An intution\n\nIf the estimation error from a hypothesized value is “big enough”, the hypothesis seems implausible."
  },
  {
    "objectID": "content/week5-hypothesis.html#statistical-hypotheses",
    "href": "content/week5-hypothesis.html#statistical-hypotheses",
    "title": "Hypothesis testing",
    "section": "Statistical hypotheses",
    "text": "Statistical hypotheses\n“The mean body temperature of adults 98.6°F” is an example of a statistical hypothesis: a definitive statement about the value of a population parameter.\n\\[H_0: \\mu = 98.6 \\qquad(\\text{population mean is 98.6°F})\\]\nAny hypothesis has an opposing or “alternative” hypothesis.\n\\[H_A: \\mu \\neq 98.6 \\qquad(\\text{population mean is not 98.6°F})\\]\n\nHypothesis testing means making a decision between a hypothesis and its alternative on the basis of a sample."
  },
  {
    "objectID": "content/week5-hypothesis.html#how-much-error-is-too-much",
    "href": "content/week5-hypothesis.html#how-much-error-is-too-much",
    "title": "Introduction to hypothesis testing",
    "section": "How much error is too much?",
    "text": "How much error is too much?\nConsider how many standard errors away from the hypothesized value we’d be:\n\n\n\n\\(\\bar{x}\\)\nestimation error\nno. SE’s\ninterpretation\n\n\n\n\n98.30\n-0.3\n2\ndouble the average error\n\n\n98.15\n-0.45\n3\ntriple the average error\n\n\n98.00\n-0.6\n4\nquadruple\n\n\n97.85\n-0.75\n5\nquintuple\n\n\n97.40\n-1.2\n8\noctuple!\n\n\n\nWe know from discussing confidence intervals that we’d estimate the mean temperature to be within about 2SE of the sample mean, and from interval coverage that:\n\nan error less than 2SE occurs for about 95% of samples\nan error greater than 2SE occurs for only about 5% of samples\n\nExactly how often would we see the error we did if the population mean is in fact 98.6°F?"
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model."
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-1",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-1",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-2",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-2",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate by more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-3",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-3",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate by more in 9.6% of samples\noverestimate by more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-4",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-4",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate by more in 9.6% of samples\noverestimate by more in 9.6% of samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’d see at least as much estimation error 19.2% of the time, assuming the hypothesis is true."
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model",
    "href": "content/week5-hypothesis.html#applying-the-t-model",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model."
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-1",
    "href": "content/week5-hypothesis.html#applying-the-t-model-1",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-2",
    "href": "content/week5-hypothesis.html#applying-the-t-model-2",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-3",
    "href": "content/week5-hypothesis.html#applying-the-t-model-3",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate more in 9.6% of samples\noverestimate more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-4",
    "href": "content/week5-hypothesis.html#applying-the-t-model-4",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate more in 9.6% of samples\noverestimate more in 9.6% of samples\n\n\n\n\n\n\n\n\n\n\n\n\\[P(|T| &gt; 1.328) = 0.192\\]\n\n\n\nWe’d see at least as much (absolute) estimation error 19.2% of the time, assuming the hypothesis is true. So this amount of error isn’t surprising."
  },
  {
    "objectID": "content/week5-hypothesis.html#a-hypothetical-scenario",
    "href": "content/week5-hypothesis.html#a-hypothetical-scenario",
    "title": "Hypothesis testing",
    "section": "A hypothetical scenario",
    "text": "A hypothetical scenario\n\n\nNow if instead we had a sample with \\(\\bar{x} = 98.2\\) but all other summary statistics were the same, then if the population mean is in fact 98.6°F\n\n\\(T\\) = -2.726\nunderestimate by more in 0.48% of samples\noverestimate by more in 0.48% of samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’d see at least as much estimation error 0.96% of the time, assuming the hypothesis is true. So the hypothesis seems plausible given our data."
  },
  {
    "objectID": "content/week5-hypothesis.html#a-more-extreme-scenario",
    "href": "content/week5-hypothesis.html#a-more-extreme-scenario",
    "title": "Introduction to hypothesis testing",
    "section": "A more extreme scenario",
    "text": "A more extreme scenario\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nsuppose instead \\(\\bar{x} = 98.2\\) so \\(T\\) = -2.726\nunderestimate more in 0.48% of samples\noverestimate more in 0.48% of samples\n\n\n\n\n\n\n\n\n\n\n\n\\[P(|T| &gt; 2.726) = 0.0096\\]\n\n\n\nWe’d see at least as much estimation error only 0.96% of the time. So if the hypothesis were true, this sample would be really unusual."
  },
  {
    "objectID": "content/week5-hypothesis.html#decisions-decisions",
    "href": "content/week5-hypothesis.html#decisions-decisions",
    "title": "Introduction to hypothesis testing",
    "section": "Decisions, decisions",
    "text": "Decisions, decisions\n\nWhat would happen if we decided that \\(T = -1.328\\) was unusual?\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\n\n\n98.2\n0.1467\n-2.726\n0.009632\n\n\n\n\n\nSuppose we drew a line at 20%. Then if in fact \\(\\mu = 98.6\\):\n\nerrors in the ‘reject’ regime occur by chance 20% of the time\nso we’ll reach the wrong conclusion for 1 in 5 samples\n\nThis error rate is too high."
  },
  {
    "objectID": "content/week5-hypothesis.html#formalizing-a-test-for-the-mean",
    "href": "content/week5-hypothesis.html#formalizing-a-test-for-the-mean",
    "title": "Introduction to hypothesis testing",
    "section": "Formalizing a test for the mean",
    "text": "Formalizing a test for the mean\nA statistical hypothesis is a statement about a population parameter. For every hypothesis there is an opposing or “alternative” hypothesis.\nA hypothesis test is a procedure for deciding between a hypothesis and its alternative.\n\n\nWe just tested the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = 98.6 \\quad(\\text{\"null\" hypothesis}) \\\\\nH_A: &\\mu \\neq 98.6 \\quad(\\text{\"alternative\" hypothesis})\n\\end{cases}\n\\]\nOur decision was based on the “test statistic”:\n\\[\nT = \\frac{\\bar{x} - 98.6}{SE(\\bar{x})}\n\\]\nIf \\(H_0\\) is true, the sampling distribution of \\(T\\) is well-approximated by a \\(t_{n-1}\\) model.\n\nWe reject \\(H_0\\) if it entails that the estimation error is unusually large relative to the standard error.\n\n‘unusual’ determined by considering error rate\ntwo equivalent approaches:\n\ncritical values\n\\(p\\)-values"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-critical-value-approach",
    "href": "content/week5-hypothesis.html#the-critical-value-approach",
    "title": "Introduction to hypothesis testing",
    "section": "The critical value approach",
    "text": "The critical value approach\n\nReject \\(H_0\\) if \\(|T|\\) exceeds the \\(1 - \\frac{\\alpha}{2}\\) quantile of the \\(t_{n - 1}\\) model\n\n\n\nSteps:\n\nDecide on an error tolerance \\(\\alpha\\).\nFind the \\(1 - \\frac{\\alpha}{2}\\) quantile \\(q\\).\nReject if \\(|T| &gt; q\\).\n\n\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# compute critical value for a 5% error tolerance\ncrit.val &lt;- qt(p = 0.975, df = 38)\ncrit.val\n\n[1] 2.024394\n\n# compare\nabs(tstat) &gt; crit.val \n\n[1] FALSE\n\n\n\n\nRationale: if \\(H_0\\) is true…\n\n\\(|T|\\) will be smaller than the quantile for \\((1 - \\alpha)\\times 100\\)% of samples\nso using this rule you’ll only make a mistake \\(\\alpha\\times 100\\)% of the time"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-unusualness-approach",
    "href": "content/week5-hypothesis.html#the-unusualness-approach",
    "title": "Hypothesis testing",
    "section": "The ‘unusualness’ approach",
    "text": "The ‘unusualness’ approach\n\nReject \\(H_0\\) if the observed value of \\(T\\) occurs for less than \\(100\\times\\alpha\\)% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#directional-test",
    "href": "content/week5-hypothesis.html#directional-test",
    "title": "Hypothesis testing",
    "section": "Directional test",
    "text": "Directional test\n\n\nIn the DDT example, \\(T = 2.905\\); if the mean is 3 we overestimated, but how unusual is the error?\nAccording to the \\(t\\) model, less than 1% of samples would produce an error of this magnitude or more.\n\n\n\n\n\n\n\n\n\n\nPoint estimate:\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n3.328\n0.1129\n\n\n\n\n\nTest statistic:\n\\[T = \\frac{\\bar{x} - 3}{SE(\\bar{x})} = \\frac{0.328}{0.1129} = 2.905\\]\n\nThis is strong evidence against the claim that the DDT level is 3ppm or less and in favor of the claim that the DDT level exceeds 3ppm."
  },
  {
    "objectID": "content/week5-hypothesis.html#directional-hypotheses-1",
    "href": "content/week5-hypothesis.html#directional-hypotheses-1",
    "title": "Hypothesis testing",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nTests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.\n\n\n\n\n\n\n\n\n\nTest type\nNull\nAlternative\nFavors alternative\n\n\n\n\nUpper-sided\n\\(\\mu \\leq \\mu_0\\)\n\\(\\mu &gt; \\mu_0\\)\npositive \\(T\\)\n\n\nLower-sided\n\\(\\mu \\geq \\mu_0\\)\n\\(\\mu &lt; \\mu_0\\)\nnegative \\(T\\)\n\n\nTwo-sided\n\\(\\mu = \\mu_0\\)\n\\(\\mu \\neq \\mu_0\\)\nlarge \\(|T|\\)\n\n\n\nThe direction of the test affects the \\(p\\)-value calculation (and thus decision), but won’t change the test statistic.\n\n# upper-sided\nt.test(ddt, mu = mu_0, alternative = 'greater')\n\n# lower-sided\nt.test(ddt, mu = mu_0, alternative = 'less')\n\n# two-sided (default)\nt.test(ddt, mu = mu_0, alternative = 'two.sided')\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-hypothesis.html#significance-conventions",
    "href": "content/week5-hypothesis.html#significance-conventions",
    "title": "Hypothesis testing",
    "section": "Significance conventions",
    "text": "Significance conventions\n\n\nConvention 1: statistical significance\n\n\\(p &lt; 0.05\\): reject \\(H_0\\)\n\\(p \\geq 0.05\\): fail to reject \\(H_0\\)\n\n\n“The data provide significant evidence at level \\(\\alpha\\) = 0.05 against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (T = -5.4548 on 129 degrees of freedom, p = .0000002411).”\n\n\nConvention 2: weight of evidence against \\(H_0\\)\n\n\\(p &lt; 0.01\\): strong evidence\n\\(0.01 \\leq p &lt; 0.05\\): moderate evidence\n\\(0.05 \\leq p &lt; 0.1\\): weak evidence\n\\(0.1 \\leq p\\): no evidence\n\n\n“The data provide strong evidence against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (T = -5.4548 on 129 degrees of freedom, p = .0000002411).”\n\n\n\nYou may use either convention to interpret test results."
  },
  {
    "objectID": "content/week5-hypothesis.html#interpreting-results-1",
    "href": "content/week5-hypothesis.html#interpreting-results-1",
    "title": "Introduction to hypothesis testing",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\nCalculations in R:\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# compute critical value for a 5% error tolerance\ncrit.val &lt;- qt(p = 0.975, df = 38)\ncrit.val\n\n[1] 2.024394\n\n# test decision\nabs(tstat) &gt; crit.val\n\n[1] FALSE\n\n# p-value\n2*pt(abs(tstat), df = 38, lower.tail = F)\n\n[1] 0.1920133\n\n\n\nConventional narrative summary style:\n\nThe data do not provide evidence that the mean body temperature differs from 98.6°F (T = -1.328 on 38 degrees of freedom, p = 0.192).\n\nConveys:\n\nconclusion\nhypotheses tested\nnumber of standard errors from hypothesized value (\\(T\\))\nsample size (degrees of freedom + 1)\nstrength of evidence (\\(p\\)-value)"
  },
  {
    "objectID": "content/week5-hypothesis.html#narrative-summary",
    "href": "content/week5-hypothesis.html#narrative-summary",
    "title": "Hypothesis testing",
    "section": "Narrative summary",
    "text": "Narrative summary\n\n\n\n# retrieve body temp variable\nbody.temp &lt;- temps$body.temp\n\n# perform t test\nt.test(body.temp, mu = 98.6)\n\n\n    One Sample t-test\n\ndata:  body.temp\nt = -1.3283, df = 38, p-value = 0.192\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.10813 98.70213\nsample estimates:\nmean of x \n 98.40513 \n\n\nLocate above in the output:\n\nvalue of test statistic \\(T\\)\ndegrees of freedom for \\(t_{n - 1}\\) model\n\\(p\\)-value\npoint and interval estimates for \\(\\mu\\)\n\n\n\nThe data do not provide evidence that the mean body temperature differs from 98.6°F (T = -1.328 on 38 degrees of freedom, p = 0.192). With 95% confidence, the mean body temperature is estimated to be between 98.11°F and 98.70°F, with a point estimate of 98.41°F (SE 0.1467)."
  },
  {
    "objectID": "content/week5-hypothesis.html#the-logic-of-a-hypothesis-test",
    "href": "content/week5-hypothesis.html#the-logic-of-a-hypothesis-test",
    "title": "Hypothesis testing",
    "section": "The logic of a hypothesis test",
    "text": "The logic of a hypothesis test\nWhat if we want to decide whether mean body temp is 98.6°F?\nA decision can be based on whether the estimation error would be unusually large (i.e., infrequent) if the hypothesis were true.\n\nunusually large error \\(\\longrightarrow\\) hypothesis is implausible \\(\\longrightarrow\\) can be rejected\nnot unusually large error \\(\\longrightarrow\\) hypothesis is plausible \\(\\longrightarrow\\) can’t be rejected\n\nEach scenario we considered supports a different decision"
  },
  {
    "objectID": "content/week5-hypothesis.html#evaluating-a-hypothesis",
    "href": "content/week5-hypothesis.html#evaluating-a-hypothesis",
    "title": "Hypothesis testing",
    "section": "Evaluating a hypothesis",
    "text": "Evaluating a hypothesis\nWhat if we want to decide whether mean body temp is 98.6°F?\nTo evaluate the hypothesis that \\(\\mu = 98.6\\), we assume it is true and then consider whether the estimation error would be unusually large according to the \\(t\\) model:\n\nunusually large error \\(\\longrightarrow\\) hypothesis is implausible \\(\\longrightarrow\\) can be rejected\nnot unusually large error \\(\\longrightarrow\\) hypothesis is plausible \\(\\longrightarrow\\) can’t be rejected\n\nEach scenario we considered supports a different decision\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\nevaluation\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\nnot unusual\n\n\n98.2\n0.1467\n-2.726\n0.009632\nunusual"
  },
  {
    "objectID": "content/week5-hypothesis.html#evaluating-the-hypothesis",
    "href": "content/week5-hypothesis.html#evaluating-the-hypothesis",
    "title": "Introduction to hypothesis testing",
    "section": "Evaluating the hypothesis",
    "text": "Evaluating the hypothesis\nTo evaluate the hypothesis that \\(\\mu = 98.6\\), we assume it is true and then consider whether the estimation error would be unusually large purely by chance according to the \\(t\\) model:\n\nunusually large error \\(\\longrightarrow\\) hypothesis is implausible \\(\\longrightarrow\\) can be rejected\nnot unusually large error \\(\\longrightarrow\\) hypothesis is plausible \\(\\longrightarrow\\) can’t be rejected\n\nWe just made these assessments:\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\nevaluation\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\nnot unusual\n\n\n98.2\n0.1467\n-2.726\n0.009632\nunusual\n\n\n\n\n\nSeems reasonable, but why exactly isn’t 19.2% of the time ‘unusual’?"
  },
  {
    "objectID": "content/week5-hypothesis.html#decisions-decisions-1",
    "href": "content/week5-hypothesis.html#decisions-decisions-1",
    "title": "Hypothesis testing",
    "section": "Decisions, decisions",
    "text": "Decisions, decisions\n\nWhat would happen if we decided that \\(T = -2.726\\) was not unusual?\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\n\n\n98.2\n0.1467\n-2.726\n0.009632\n\n\n\n\n\nSuppose we drew a line at 0.5%. Then if in fact \\(\\mu = 98.6\\):\n\nerrors in the ‘reject’ regime occur by chance 0.5% of the time\nso we’ll reach the wrong conclusion for 1 in 200 samples\n\nThis error rate is good,"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-p-value-approach",
    "href": "content/week5-hypothesis.html#the-p-value-approach",
    "title": "Introduction to hypothesis testing",
    "section": "The \\(p\\)-value approach",
    "text": "The \\(p\\)-value approach\n\nReject \\(H_0\\) if \\(T\\) exceeds the observed value for less than \\(\\alpha\\times 100\\)% of samples: \\[2\\times P(T &gt; |T_\\text{obs}|) &lt; \\alpha\\]\n\n\n\nSteps:\n\nDecide on an error tolerance \\(\\alpha\\).\nCompute the proportion \\(p\\) of samples for which \\(T\\) exceeds observed value.\nReject if \\(p &lt; \\alpha\\).\n\n\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# proportion of samples where T exceeds observed value\np.val &lt;- 2*pt(abs(tstat), df = 38, lower.tail = F)\np.val\n\n[1] 0.1920133\n\n# decision with error rate controlled at 5%\np.val &lt; 0.05\n\n[1] FALSE\n\n\n\n\nRationale:\n\n\\(p\\)-value conveys exactly how unusual the test statistic is\n\\(p &lt; \\alpha\\) exactly when \\(|T| &gt; q\\), so this rule controls the error rate at \\(\\alpha\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#test-outcomes",
    "href": "content/week5-hypothesis.html#test-outcomes",
    "title": "Introduction to hypothesis testing",
    "section": "Test outcomes",
    "text": "Test outcomes\nThere are two possible findings for a test:\n\n[crosses decision threshold] reject \\(H_0\\) in favor of \\(H_A\\)\n[doesn’t cross decision threshold] fail to reject \\(H_0\\) in favor of \\(H_A\\)\n\nA reject decision is interpreted as:\n\nThe data provide evidence that… [against \\(H_0\\)/favoring \\(H_A\\)]\n\nA fail to reject decision is interpreted as:\n\nThe data do not provide evidence that… [against \\(H_0\\)/favoring \\(H_A\\)]"
  },
  {
    "objectID": "content/lab6-hypothesis.html",
    "href": "content/lab6-hypothesis.html",
    "title": "Lab 6: Hypothesis testing basics",
    "section": "",
    "text": "In class we discussed the \\(t\\) test for the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = \\mu_0 \\\\\nH_A: &\\mu \\neq \\mu_0\n\\end{cases}\n\\]\nThe objective of this lab is to learn to perform the basic calculations involved in this \\(t\\) test “by hand” (without the use of the function that we’ll apply later):\nWe’ll use the temps dataset to illustrate.\nlibrary(tidyverse)\nload('data/temps.RData')\nhead(temps)\n\n  body.temp    sex heart.rate\n1      98.8 female         69\n2      98.6 female         85\n3      98.4   male         68\n4      97.2 female         66\n5      99.5   male         75\n6      97.1   male         82"
  },
  {
    "objectID": "content/lab6-hypothesis.html#footnotes",
    "href": "content/lab6-hypothesis.html#footnotes",
    "title": "Lab 6: Hypothesis testing basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI usually think in the following terms for the \\(t\\)-test:\n\nsmall: \\(n \\leq 20\\)\nmodest: \\(20 &lt; n \\leq 50\\)\nlarge: \\(50 &lt; n\\)\n\nUnless I’m in the ‘small’ regime, I’m not too worried about skew or outliers. In the ‘modest’ regime, I’m not concerned unless I spot very pronounced skew or outliers. In the ‘large’ regime, I’m really only concerned about (strong) multimodality. Interestingly, in the latter case, the \\(t\\) test still works for multimodal populations, but the population mean isn’t meaningful.↩︎"
  },
  {
    "objectID": "content/week5-hypothesis.html",
    "href": "content/week5-hypothesis.html",
    "title": "Introduction to hypothesis testing",
    "section": "",
    "text": "Loose end: working backwards to determine interval coverage\n[lecture] the \\(t\\)-test for a population mean\n[lab] computing test statistics, critical values, and \\(p\\)-values"
  },
  {
    "objectID": "content/week5-directional.html#todays-agenda",
    "href": "content/week5-directional.html#todays-agenda",
    "title": "Tests for directional hypotheses",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nQuick review of decision criteria for the \\(t\\) test\n[lecture] test-interval relationship; directional \\(t\\) tests\n[lab] upper-sided, lower-sided, and two-sided tests for the population mean"
  },
  {
    "objectID": "content/week5-directional.html#from-last-time",
    "href": "content/week5-directional.html#from-last-time",
    "title": "Tests for directional hypotheses",
    "section": "From last time",
    "text": "From last time\nPractice problem: test the hypothesis that the average U.S. adult sleeps 8 hours.\n\n\n\n\n\n\n\n\n\n\n\n\n# calculations\nsleep.mean &lt;- mean(sleep) \nsleep.mean.se &lt;- sd(sleep)/sqrt(length(sleep))\ntstat &lt;- (sleep.mean - 8)/sleep.mean.se \ncrit.val &lt;- qt(0.975, df = 3178) \np.val &lt;- 2*pt(abs(tstat), df = 3178, lower.tail = F)\nci &lt;- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstd.err\ntstat\ncval\npval\n\n\n\n\n6.959\n0.02447\n-42.53\n1.961\n2.622e-313\n\n\n\n\n\n95% confidence interval: (6.91, 7.01)\n\nA complete narrative summary:\n\nThe data provide evidence that the average U.S. adult does not sleep 8 hours per night (T = -42.53 on 3178 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean nightly hours of sleep among U.S. adults is estimated to be between 6.91 and 7.01 hours, with a point estimate of 6.59 hours (SE: 0.0245)."
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals",
    "href": "content/week5-directional.html#tests-and-intervals",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\n\nTests and intervals are usually reported together\n\nConsider how the test and interval provide complementary information:\n\nthe test tells you U.S. adults don’t sleep 8 hours\nthe interval tells you how much they do sleep\n\nIt is reasonable that they should be consistent, and in fact they are.\n\n\n\n# calculations\nsleep.mean &lt;- mean(sleep) \nsleep.mean.se &lt;- sd(sleep)/sqrt(length(sleep))\ntstat &lt;- (sleep.mean - 8)/sleep.mean.se \ncrit.val &lt;- qt(0.975, df = 3178) \np.val &lt;- 2*pt(abs(tstat), df = 3178, lower.tail = F)\nci &lt;- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se\n\n\n\nNotice that the critical value in the 5% significance level test is exactly the same as that used in the 95% confidence interval."
  },
  {
    "objectID": "content/week5-directional.html#the-t.test...-function",
    "href": "content/week5-directional.html#the-t.test...-function",
    "title": "Tests for directional hypotheses",
    "section": "The t.test(...) function",
    "text": "The t.test(...) function\nSince tests and intervals go together, there is a single R function that computes both.\n\n\n\nt.test(sleep, mu = 8, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -42.533, df = 3178, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 8\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nNo critical value is reported, so you have to make the decision using the \\(p\\) value:\n\n\\(p &lt; \\alpha\\): reject\n\\(p &gt; \\alpha\\): fail to reject\n\n\n\nTake a moment to locate each component of the test and estimates from the output."
  },
  {
    "objectID": "content/week5-directional.html#decision-errors",
    "href": "content/week5-directional.html#decision-errors",
    "title": "Tests for directional hypotheses",
    "section": "Decision errors",
    "text": "Decision errors\n\n\nThere are two ways to make a mistake in a hypothesis test:\n\nreject a true \\(H_0\\)\nfail to reject a false \\(H_0\\)\n\nThese are known as type I and type II errors.\nBecause rejecting \\(H_0\\) is a stronger conclusion, type I errors are considered more severe.\n\n\n\nThe significance level of a test is a cap on the type I error rate."
  },
  {
    "objectID": "content/week5-directional.html#directional-hypotheses",
    "href": "content/week5-directional.html#directional-hypotheses",
    "title": "Tests for directional hypotheses",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nTests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.\n\n\n\nTest type\nAlternative\nDirection favoring alternative\n\n\n\n\nUpper-sided\n\\(\\mu &gt; \\mu_0\\)\nlarger \\(T\\)\n\n\nLower-sided\n\\(\\mu &lt; \\mu_0\\)\nsmaller \\(T\\)\n\n\nTwo-sided\n\\(\\mu \\neq \\mu_0\\)\nlarger \\(|T|\\)\n\n\n\nThe direction of the test affects the \\(p\\)-value calculation (and thus decision), but won’t change the test statistic.\nConceptually tricky, but easy in R:\n\n# upper-sided\nt.test(ddt, mu = mu_0, alternative = 'greater')\n\n# lower-sided\nt.test(ddt, mu = mu_0, alternative = 'less')\n\n# two-sided (default)\nt.test(ddt, mu = mu_0, alternative = 'two.sided')"
  },
  {
    "objectID": "content/week5-directional.html#directional-test",
    "href": "content/week5-directional.html#directional-test",
    "title": "Tests for directional hypotheses",
    "section": "Directional test",
    "text": "Directional test\n\n\nAccording to the \\(t\\) model, 0.58% of samples would produce an error of this magnitude or more in the direction of the alternative if in fact \\(\\mu = 3\\):\n\n\n\n\n\n\n\n\n\n\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328 \n\n\n\nThe data provide strong evidence that mean DDT in kale exceeds 3ppm (T = 2.9059 on 14 degrees of freedom, p = 0.0058)."
  },
  {
    "objectID": "content/week5-directional.html#composite-hypotheses",
    "href": "content/week5-directional.html#composite-hypotheses",
    "title": "Tests for directional hypotheses",
    "section": "Composite hypotheses",
    "text": "Composite hypotheses\nThe null hypothesis is a composite of values, so why did we choose just one (\\(\\mu_0 = 3\\)) to perform the test?\nAny null value farther from the alternative will produce stronger results.\n\n\n\n\n\n\n\n\nNull value\nTest statistic numerator\nProportion of samples more favorable to \\(H_A\\)\n\n\n\n\n\\(\\mu_0 = 3\\)\n0.328\n0.00575\n\n\n\\(\\mu_0 = 2.99\\)\n0.338\n0.00483\n\n\n\\(\\mu_0 = 2.95\\)\n0.378\n0.00239\n\n\n\nSo by using \\(\\mu_0 = 3\\), we are choosing the most conservative null value for the test."
  },
  {
    "objectID": "content/week5-directional.html#directional-hypotheses-1",
    "href": "content/week5-directional.html#directional-hypotheses-1",
    "title": "Tests for directional hypotheses",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm). Each measurement was taken by a different laboratory.\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu \\leq 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]"
  },
  {
    "objectID": "content/week5-directional.html#choosing-alternatives",
    "href": "content/week5-directional.html#choosing-alternatives",
    "title": "Tests for directional hypotheses",
    "section": "Choosing alternatives",
    "text": "Choosing alternatives\n\n\n\nIs the mean body temp less than 98.6?\n\nWhich test should you use? Consider the interpretations:\n\n[lower] evidence favoring lower temp\n[upper] no evidence against lower temp\n\nThe conclusions are consistent but not equivalent – (a) is a better answer.\nYour alternative should be the claim you hope to support with evidence, and your null the claim you hope to refute.\n\n\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -1.3283, df = 38, p-value = 0.09601\nalternative hypothesis: true mean is less than 98.6\n95 percent confidence interval:\n     -Inf 98.65248\nsample estimates:\nmean of x \n 98.40513 \n\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -1.3283, df = 38, p-value = 0.904\nalternative hypothesis: true mean is greater than 98.6\n95 percent confidence interval:\n 98.15778      Inf\nsample estimates:\nmean of x \n 98.40513"
  },
  {
    "objectID": "content/week5-directional.html#reporting-test-results",
    "href": "content/week5-directional.html#reporting-test-results",
    "title": "Tests for directional hypotheses",
    "section": "Reporting test results",
    "text": "Reporting test results\n\n\nTo report the result of a hypothesis test, you should:\n\nLead with your conclusion\nState the hypotheses tested\nInterpret the conclusion of the test in context\nProvide the \\(T\\) statistic, degrees of freedom, and \\(p\\)-value\nState and interpret the point estimate and interval for the mean\n\n\n\nOur results suggest mean body temperature is less than 98.6 °F. We tested the null hypothesis that mean body temperature is 98.6 °F or greater against the alternative that mean body temperature is less than 98.6 °F. The data provide sufficiently strong evidence against the hypothesis that mean body temperature is 98.6 °F or greater in favor of the alternative that mean body temperature is less than 98.6 °F (T = -5.4548 on 129 degrees of freedom, p-value = .0000001205). With 95% confidence, the mean nightly hours of sleep is estimated to be at most 98.36 °F, with a point estimate of 98.23 (SE = 0.0643).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-directional.html#your-turn-sleep-data",
    "href": "content/week5-directional.html#your-turn-sleep-data",
    "title": "Tests for directional hypotheses",
    "section": "Your turn: sleep data",
    "text": "Your turn: sleep data\nOpen lab7-moretests in the class workspace.\n\n\nWork with your group on just one of the questions below.\n\nDo US adults sleep 7.5 hours per night on average?\nDo US adults sleep less than 7.5 hours per night on average?\nDo US adults sleep more than 7.5 hours per night on average?\nDo US adults sleep more than 6.5 hours per night on average?\n\n\nYour task is to determine and carry out an appropriate test, and then write a complete report of the test outcome:\n\nanswer the question\nhypotheses tested\ntest conclusion, interpreted in context\ntest statistic, degrees of freedom, \\(p\\)-value\nconfidence interval and point estimate, interpreted in context\n\nThese elements should be summarized together in complete sentences."
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals-1",
    "href": "content/week5-directional.html#tests-and-intervals-1",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\nThe critical value used in a \\(\\alpha\\) significance level test is identical to the critical value used in a \\((1 - \\alpha)\\) interval. Consequently:\n\\[\n\\underbrace{\\bar{x} - c\\times SE(\\bar{x}) &lt; \\mu_0 &lt; \\bar{x} + c\\times SE(\\bar{x})}_\\text{hypothesized value is in the interval}\n\\quad\\Longleftrightarrow\\quad\n\\underbrace{-c &lt; \\frac{\\bar{x} - \\mu_{0}}{SE(\\bar{x})} &lt; c}_{|T| &lt; c}\n\\]\nMeaning: the interval includes exactly those values that the test fails to reject.\n\nSensible considering both use the same information: the distance between the point estimate and population mean, relative to the variability of the estimate"
  },
  {
    "objectID": "content/week5-directional.html#decision-errors-1",
    "href": "content/week5-directional.html#decision-errors-1",
    "title": "Tests for directional hypotheses",
    "section": "Decision errors",
    "text": "Decision errors\n\n\nThere are two ways to make a mistake in a hypothesis test:\n\nreject a true \\(H_0\\)\nfail to reject a false \\(H_0\\)\n\nThese are known as type I and type II errors.\nBecause rejecting \\(H_0\\) is a stronger conclusion, type I errors are considered more severe.\n\n\n\nThe significance level of a test is a cap on the type I error rate."
  },
  {
    "objectID": "content/week5-directional.html#exploring-type-i-error-rates",
    "href": "content/week5-directional.html#exploring-type-i-error-rates",
    "title": "Tests for directional hypotheses",
    "section": "Exploring type I error rates",
    "text": "Exploring type I error rates\nIn lab7-moretests, there are codes to draw a sample from a mock population and test a true null hypothesis. Run these and take note of your \\(p\\)-value.\n\n\nIn this situation, a type I error is rejecting \\(H_0\\).\n\nrule: reject if \\(p &lt; \\alpha\\).\nwe will tally rejections for various \\(\\alpha\\) values\n\n\n\n\n\nSignificance level\nError frequency\n\n\n\n\n\\(\\alpha = 0.2\\)\n\n\n\n\\(\\alpha = 0.1\\)\n\n\n\n\\(\\alpha = 0.05\\)\n\n\n\n\\(\\alpha = 0.02\\)\n\n\n\n\n\n\n\nTakeaway: using larger significance thresholds leads to [more/less] type 1 errors"
  },
  {
    "objectID": "content/week5-directional.html#exploring-type-ii-errors",
    "href": "content/week5-directional.html#exploring-type-ii-errors",
    "title": "Tests for directional hypotheses",
    "section": "Exploring type II errors",
    "text": "Exploring type II errors\nNow let’s test a false null hypothesis.\n\n\nA type II error is failing to reject \\(H_0\\).\n\nrule: \\(p &lt; 0.05\\)\nuse example commands to test each hypothesis at right\nuse a two-sided test\n\n\n\n\n\nNull value \\(\\mu_0\\)\nError frequency\n\n\n\n\n4.2\n\n\n\n4.6\n\n\n\n4.9\n\n\n\n5.1\n\n\n\n5.4\n\n\n\n5.7\n\n\n\n\n\n\n\nNotice that the type II error is quite high for null values near the true mean; this indicates the test has little power to detect such alternatives."
  },
  {
    "objectID": "content/week5-directional.html#swimsuit-data",
    "href": "content/week5-directional.html#swimsuit-data",
    "title": "Tests for directional hypotheses",
    "section": "Swimsuit data",
    "text": "Swimsuit data\nLet’s consider our first two-sample problem.\n\nAre swimmers faster in bodysuits than in regular swimsuits?\n\nBelow are the first few observations of the average velocity of competitive swimmers in a 1500m; one measurement was taken in a swimsuit, the other in a bodysuit.\n\n\n\n\n\n\n\n\n\n\nswimmer.number\nwet.suit.velocity\nswim.suit.velocity\n\n\n\n\n1\n1.57\n1.49\n\n\n2\n1.47\n1.37\n\n\n3\n1.42\n1.35\n\n\n\n\n\nCan you formulate a pair of hypotheses to test to answer this question using methods from last time?"
  },
  {
    "objectID": "content/week5-directional.html#comparing-two-means",
    "href": "content/week5-directional.html#comparing-two-means",
    "title": "Tests for directional hypotheses",
    "section": "Comparing two means",
    "text": "Comparing two means\nWe can formulate the question as a comparison of two means:\n\\[H_0: \\mu_\\text{bodysuit} \\leq \\mu_\\text{swimsuit}\\] \\[H_A: \\mu_\\text{bodysuit} &gt; \\mu_\\text{swimsuit}\\] It is common to express hypotheses of this form in terms of a difference in means: \\[H_0: \\underbrace{\\mu_\\text{bodysuit} - \\mu_\\text{swimsuit}}_\\delta \\leq 0\\] \\[H_A: \\underbrace{\\mu_\\text{bodysuit} - \\mu_\\text{swimsuit}}_\\delta &gt; 0\\]"
  },
  {
    "objectID": "content/week5-directional.html#pairing",
    "href": "content/week5-directional.html#pairing",
    "title": "Tests for directional hypotheses",
    "section": "Pairing",
    "text": "Pairing\nThe velocities in the swimsuit dataset are paired because every swimmer is measured in both suits.\n\nData are paired just in case the measurements in each group are taken on exactly the same study units or the study units can be placed in one-to-one correspondence.\n\nThis is the easiest situation to handle, because it reduces to a one-sample problem with the observed differences."
  },
  {
    "objectID": "content/week5-directional.html#inference-for-paired-data",
    "href": "content/week5-directional.html#inference-for-paired-data",
    "title": "Tests for directional hypotheses",
    "section": "Inference for paired data",
    "text": "Inference for paired data\n\nCalculate paired differences.\n\n\n\n\n\n\n\n\n\n\n\n\nswimmer.number\nwet.suit.velocity\nswim.suit.velocity\nvelocity.diff\n\n\n\n\n1\n1.57\n1.49\n0.08\n\n\n2\n1.47\n1.37\n0.1\n\n\n3\n1.42\n1.35\n0.07\n\n\n\n\n\n\n\n\nPerform test as before.\n\n\n# test for paired difference\nt.test(diffs, mu = 0, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  diffs\nt = 12.318, df = 11, p-value = 4.443e-08\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 0.06620114        Inf\nsample estimates:\nmean of x \n   0.0775 \n\n\n\n\nReport the result of the test. You try.\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals-2",
    "href": "content/week5-directional.html#tests-and-intervals-2",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\n\nThe level-\\(\\alpha\\) test rejects \\(H_0: \\mu = \\mu_0\\) exactly when \\(\\mu_0\\) is outside the \\((1 - \\alpha)\\times 100\\)% confidence interval for \\(\\mu\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft, \\(p\\)-values for a sequence of tests:\n\n\\(p &gt; 0.05\\) precisely for \\(\\mu_0\\) in 95% CI\n\\(p &gt; 0.01\\) precisely for \\(\\mu_0\\) in 99% CI\n\nIn other words:\n\\[\\text{level $\\alpha$ test rejects} \\Longleftrightarrow \\text{$1 - \\alpha$ CI excludes}\\]"
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals-3",
    "href": "content/week5-directional.html#tests-and-intervals-3",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\nNotice that t.test produces a confidence interval.\n\nfor the two-sided test, the interval is exactly the one we calculated before\nfor the one-sided tests, the interval is a lower/upper confidence bound: \\[\\begin{align*}\n\\bar{x} - c \\times SE(\\bar{x}) &\\qquad \\text{lower confidence bound} \\quad\\Longleftrightarrow\\quad \\text{upper-sided test}\\\\\n\\bar{x} + c \\times SE(\\bar{x}) &\\qquad \\text{upper confidence bound} \\quad\\Longleftrightarrow\\quad \\text{lower-sided test}\n\\end{align*}\\] where \\(c\\) is chosen to ensure a specified coverage level\n\nThe tests and intervals correspond in the following sense:\n\nThe level \\(\\alpha\\) test rejects just in case the \\((1 - \\alpha)\\) confidence interval excludes \\(\\mu_0\\)\n\n\n# changing confidence level for interval\nt.test(body_temps, mu = 98.6, alternative = 'less', conf.level = 0.99)"
  },
  {
    "objectID": "content/week5-directional.html#interpreting-p-values",
    "href": "content/week5-directional.html#interpreting-p-values",
    "title": "Tests for directional hypotheses",
    "section": "Interpreting \\(p\\)-values",
    "text": "Interpreting \\(p\\)-values\n\n\\(p\\)-values measure the strength of evidence against \\(H_0\\) and favoring \\(H_A\\): smaller \\(p\\)-values indicate stronger evidence; larger \\(p\\)-values indicate weaker evidence.\n\n\n\nThe mathematical definition is: \\[p = P(|T| &gt; |T_\\text{observed}|)\\]\n\ntechnically, the probability under \\(H_0\\) that \\(T\\) exceeds the observed value in magnitude\ninformally, how unusual/rare your data are\n\n\nAs a measure of the strength of evidence favoring the alternative:\n\n\n\nvalue\nstrength of evidence\n\n\n\n\n\\(p &lt; 0.001\\)\nvery strong\n\n\n\\(0.001 &lt; p &lt; 0.01\\)\nstrong\n\n\n\\(0.01 &lt; p &lt; 0.05\\)\nmoderate\n\n\n\\(0.05 &lt; p &lt; 0.1\\)\nsuggestive\n\n\n\\(0.1 &lt; p\\)\nno evidence"
  },
  {
    "objectID": "content/week5-directional.html#a-different-test",
    "href": "content/week5-directional.html#a-different-test",
    "title": "Tests for directional hypotheses",
    "section": "A different test",
    "text": "A different test\n\nDoes the average U.S. adult sleep less than 7 hours?\n\n\n\nThis example leads to a directional test:\n\\[\n\\begin{cases}\nH_0: &\\mu = 8 \\\\\nH_0: &\\mu &lt; 8\n\\end{cases}\n\\]\nThe test statistic is the same as before:\n\\[\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n\\]\n\nThe upper-sided \\(p\\)-value is 0.9526:\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(p\\)-value, we look at how often \\(T\\) is larger in the direction of the alternative."
  },
  {
    "objectID": "content/week5-directional.html#ddt-data",
    "href": "content/week5-directional.html#ddt-data",
    "title": "Tests for directional hypotheses",
    "section": "DDT data",
    "text": "DDT data\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm).\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu = 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]\nWe choose this direction because we’re concerned with evidence that mean DDT exceeds the threshold."
  },
  {
    "objectID": "content/week5-directional.html#your-turn-interpret-these-p-values",
    "href": "content/week5-directional.html#your-turn-interpret-these-p-values",
    "title": "Tests for directional hypotheses",
    "section": "Your turn: interpret these \\(p\\)-values",
    "text": "Your turn: interpret these \\(p\\)-values\n\nDon’t just match the value to the table; add context, and state the test outcome.\n\n\n\n\n# example 1\nt.test(sleep, mu = 6.8)$p.value\n\n[1] 9.205865e-11\n\n# example 2\nt.test(sleep, mu = 6.9)$p.value\n\n[1] 0.01578191\n\n# example 3\nt.test(sleep, mu = 7)$p.value\n\n[1] 0.09482291\n\n# example 4\nt.test(sleep, mu = 7.1)$p.value\n\n[1] 9.366935e-09\n\n# example 5\nt.test(sleep, mu = 7.2)$p.value\n\n[1] 1.532371e-22\n\n\n\n\n\n\nvalue\nstrength of evidence\n\n\n\n\n\\(p &lt; 0.001\\)\nvery strong\n\n\n\\(0.001 &lt; p &lt; 0.01\\)\nstrong\n\n\n\\(0.01 &lt; p &lt; 0.05\\)\nmoderate\n\n\n\\(0.05 &lt; p &lt; 0.1\\)\nsuggestive\n\n\n\\(0.1 &lt; p\\)\nno evidence\n\n\n\n\n\nExample: “the data [DO/DO NOT] provide [STRENGTH] evidence that [ALTERNATIVE]”"
  },
  {
    "objectID": "content/week5-directional.html#a-directional-test",
    "href": "content/week5-directional.html#a-directional-test",
    "title": "Tests for directional hypotheses",
    "section": "A directional test",
    "text": "A directional test\n\nDoes the average U.S. adult sleep less than 7 hours?\n\n\n\nThis example leads to a directional test:\n\\[\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu &lt; 7\n\\end{cases}\n\\]\nThe test statistic is the same as before:\n\\[\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n\\]\n\nThe lower-sided \\(p\\)-value is 0.0474:\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(p\\)-value, we look at how often \\(T\\) is larger in the direction of the alternative.\n\nin this case, how often \\(T\\) is smaller (underestimate by more)"
  },
  {
    "objectID": "content/week5-directional.html#the-other-direction",
    "href": "content/week5-directional.html#the-other-direction",
    "title": "Tests for directional hypotheses",
    "section": "The other direction",
    "text": "The other direction\n\nDoes the average U.S. adult sleep more than 7 hours?\n\n\n\nNow the alternative is the opposite direction:\n\\[\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu &gt; 7\n\\end{cases}\n\\]\nThe test statistic is the same as before:\n\\[\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n\\]\n\nThe upper-sided \\(p\\)-value is 0.9526:\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(p\\)-value, we look at how often \\(T\\) is larger in the direction of the alternative.\n\nin this case, how often \\(T\\) is larger (overestimate by more)"
  },
  {
    "objectID": "content/week5-directional.html#another-example-ddt-data",
    "href": "content/week5-directional.html#another-example-ddt-data",
    "title": "Tests for directional hypotheses",
    "section": "Another example: DDT data",
    "text": "Another example: DDT data\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm).\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu = 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]\nWe choose this direction because we’re concerned with evidence that mean DDT exceeds the threshold."
  },
  {
    "objectID": "content/week5-directional.html#another-example-ddt-data-1",
    "href": "content/week5-directional.html#another-example-ddt-data-1",
    "title": "Tests for directional hypotheses",
    "section": "Another example: DDT data",
    "text": "Another example: DDT data\n\n\nIf in fact \\(\\mu = 3\\), then according to the \\(t\\) model 0.58% of samples would produce an error of this magnitude or more in the direction of the alternative:\n\n\n\n\n\n\n\n\n\n\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328 \n\n\n\nThe data provide strong evidence that mean DDT in kale exceeds 3ppm (T = 2.9059 on 14 degrees of freedom, p = 0.0058). With 95% confidence, the mean DDT is estimated to be at least 3.129, with a point estimate of 3.32 (SE: 0.1168).\n\n\n\nNotice the one-sided interval! (Inf = \\(\\infty\\).) This is called a “lower confidence bound”."
  },
  {
    "objectID": "content/week5-directional.html#your-turn-which-alternative",
    "href": "content/week5-directional.html#your-turn-which-alternative",
    "title": "Tests for directional hypotheses",
    "section": "Your turn: which alternative?",
    "text": "Your turn: which alternative?\n\nWrite the hypotheses in notation and identify which test (upper/lower/two sided) should be used.\n\nUsing the temperature/heartrate data:\n\nIs mean body temperature less than 98.6°F?\nIs mean heart rate greater than 60 bpm?\nIs mean heart rate 65 bpm?\n\nUsing the NC births data:\n\nIs the mean number of weeks at birth 40?\nIs the mean birth weight at least 7 lbs?\nIs the mean birth weight under 8 lbs?\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-directional.html#three-t-tests",
    "href": "content/week5-directional.html#three-t-tests",
    "title": "Tests for directional hypotheses",
    "section": "Three \\(t\\)-tests",
    "text": "Three \\(t\\)-tests\n\n\nDo U.S. adults sleep 7 hours per night?\n\n# two sided test\nt.test(sleep, \n       mu = 7)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nSuggestive but insufficient evidence that U.S. adults don’t sleep 7 hours\n\n\nDo U.S. adults sleep less than 7 hours per night?\n\n# lower-sided test\nt.test(sleep, \n       mu = 7, \n       alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.04741\nalternative hypothesis: true mean is less than 7\n95 percent confidence interval:\n     -Inf 6.999372\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nModerate evidence that U.S. adults sleep less than 7 hours\n\n\nDo U.S. adults sleep more than 7 hours per night?\n\n# upper-sided test\nt.test(sleep, \n       mu = 7, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.9526\nalternative hypothesis: true mean is greater than 7\n95 percent confidence interval:\n 6.918841      Inf\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nNo evidence that U.S. adults sleep more than 7 hours"
  },
  {
    "objectID": "content/week5-directional.html#recap-decision-criteria",
    "href": "content/week5-directional.html#recap-decision-criteria",
    "title": "Tests for directional hypotheses",
    "section": "Recap: decision criteria",
    "text": "Recap: decision criteria\n\nA hypothesis test boils down to deciding whether your estimate is too far from a hypothetical value for that hypothesis to be plausible.\n\n\n\nTo test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = \\mu_0 \\\\\nH_A: &\\mu \\neq \\mu_0\n\\end{cases}\n\\]\nWe use the test statistic:\n\\[\nT = \\frac{\\bar{x} - \\mu_0}{SE(\\bar{x})}\n\\quad\\left(\\frac{\\text{estimation error under } H_0}{\\text{standard error}}\\right)\n\\]\n\nWe say \\(H_0\\) is implausible at level \\(\\alpha\\) if either:\n\n\\(|T| &gt; q\\) for the \\(\\alpha\\)-critical value \\(q\\)\n\n\\(q\\) is the \\(1 - \\frac{\\alpha}{2}\\) quantile of the \\(t_{n - 1}\\) model\n\n\\(\\underbrace{P(|T| &gt; |T_\\text{observed}|)}_\\text{p-value} &lt; \\alpha\\)\n\n\n\nThis procedure controls the error rate \\(\\alpha\\): the proportion of samples for which we’d make a false rejection."
  },
  {
    "objectID": "content/lab7-directional.html",
    "href": "content/lab7-directional.html",
    "title": "Lab 7: Directional \\(t\\)-tests",
    "section": "",
    "text": "This lab has two objectives:\n\nLearn to use the t.test(...) function\nLearn to discern the appropriate direction for a \\(t\\) test\n\nWe’ll use two familiar datasets: body temperature and heart rate measurements for 39 individuals; and data on birth weights and weeks at birth for a sample of 100 births in North Carolina in 2004.\n\nlibrary(tidyverse)\nload('data/temps.RData')\nload('data/nhanes.RData')\nncbirths &lt;- read_csv('data/ncbirths.csv')\n\n\nThe t.test(...) function\nThe t.test(...) function produces both a hypothesis test and an confidence interval, and can be used to obtain either or both in practice.\nLet’s demonstrate with the practice problem you completed most recently: inference on the mean nightly hours of sleep among U.S. adults based on NHANES data.\nThe default behavior of t.test(...) if given no arguments besides a vector of data is to test \\(H_0: \\mu = 0\\) against a two-sided alternative (\\(H_A: \\mu \\neq 0\\)) and provide a 95% confidence interval. There are three key arguments that allow you to adjust this behavior:\n\nmu = ... adjusts the value for the mean in the null hypothesis \\(H_0\\)\n\ndefault mu = 0\n\nalternative = ... adjusts the direction of the alternative, with options\n\n'less' for a lower-sided alternative\n'greater' for an upper-sided alternative\n'two.sided' (default) for a two-sided alternative\n\nconf.level = ... adjusts the confidence level for the interval estimate\n\ndefault conf.level = 0.95\n\n\nThe examples below illustrate this usage. Run each command and look at the output closely to determine what changes.\n\n# extract sleep variable\nsleep &lt;- nhanes$sleephrsnight\n\n# default behavior (these are equivalent)\nt.test(sleep)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = 284.36, df = 3178, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\nt.test(sleep, mu = 0, alternative = 'two.sided', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = 284.36, df = 3178, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n# change null value to 7 hours of sleep\nt.test(sleep, mu = 7, alternative = 'two.sided', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n# change the confidence level\nt.test(sleep, mu = 7, alternative = 'two.sided', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n99 percent confidence interval:\n 6.896032 7.022182\nsample estimates:\nmean of x \n 6.959107 \n\n# change the direction of the alternative\nt.test(sleep, mu = 7, alternative = 'less', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.04741\nalternative hypothesis: true mean is less than 7\n99 percent confidence interval:\n     -Inf 7.016067\nsample estimates:\nmean of x \n 6.959107 \n\n\nFocus for a moment on the last example. In detail, this tests, at the 1% significance level, the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu &lt; 7\n\\end{cases}\n\\]\nWhile the conf.level argument doesn’t affect the \\(p\\)-value, it does imply a significance level – in this case, \\(\\alpha = 0.01\\). So, even though the \\(p\\)-value is less than the conventional level (\\(p &lt; 0.05\\)), it is not less than the implied significance level (here \\(p &gt; 0.01\\)), so the test output implies we’d fail to reject the hypothesis that adults sleep less than 7 hours.\nIn general, it’s important to set the confidence level to correspond to the significance level of the test you wish to perform, so that the test interpretation and interval provided match.\n\n\n\n\n\n\nYour turn 1\n\n\n\nAdjust the arguments of the t.test(...) function to achieve the following:\n\nfind a 90% CI for the mean\ntest whether mean sleep is 6.9 at the 5% level\ntest whether mean sleep is 6.9 at the 1% level\ntest whether mean sleep exceeds 6.9 at the 5% level\ntest whether mean sleep exceeds 6.9 at the 1% level\n\n\n# obtain a 90% confidence interval for the mean hours of sleep\n\n# test whether mean sleep is 6.9 at the 5% level\n\n# test whether mean sleep is 6.9 at the 1% level\n\n# test whether mean sleep exceeds 6.9 at the 5% level\n\n# test whether mean sleep exceeds 6.9 at the 1% level\n\nFor extra practice, write a short interpretation of the results of each test following the style introduced in class.\n\n\n\n\nDistinguishing directional alternatives\nHere we’ll use the temperature/heartrate data to illustrate a variety of directional tests based on questions of interest.\nAs you’re looking over the examples, focus on the correspondence between the questions and the direction of the alternative.\n\n# extract body temperature variable\nbodytemps &lt;- temps$body.temp\n\n# is mean temperature different from 98.6 at the 5% significance level?\nt.test(bodytemps, mu = 98.6, alternative = 'two.sided', conf.level = 0.95)\n\n# is mean temperature less than 98.6 at the 5% significance level?\nt.test(bodytemps, mu = 98.6, alternative = 'less', conf.level = 0.95)\n\n# is mean temperature greater than 98.1 at the 5% significance level?\nt.test(bodytemps, mu = 98.1, alternative = 'greater', conf.level = 0.95)\n\n# is mean temperature greater than 98.1 at the 1% significance level?\nt.test(bodytemps, mu = 98.1, alternative = 'greater', conf.level = 0.99)\n\n# is mean temperature less than 98.9 at the 5% significance level?\nt.test(bodytemps, mu = 98.9, alternative = 'less', conf.level = 0.95)\n\nAs an aside (but an important one!), performing all of these tests together is only meant to illustrate how the function works, not how to perform an analysis. Trying out many tests until you obtain significant results is known as “\\(p\\) hacking”, and is not an acceptable practice.\n\n\n\n\n\n\nYour turn 2\n\n\n\nUsing the heart.rate variable, test the following hypotheses:\n\nIs mean heart rate 65bpm at the 5% level?\nIs mean heart rate 70bpm at the 1% level?\nIs mean heart rate greater than 70bpm at the 1% level?\nIs mean heart rate less than 75bpm at the 10% level?\nIs mean heart rate greater than 75bpm at the 10% level?\n\n\n# extract heart rate variable\n\n# is mean heart rate 65bpm at the 5% level?\n\n# is mean heart rate 70bpm at the 1% level?\n\n# is mean heart rate greater than 70bpm at the 1% level?\n\n# is mean heart rate less than 75bpm at the 10% level?\n\n# is mean heart rate greater than 75bpm at the 10% level?\n\n\n\n\n\nA brief analysis\nNow that you’re familiar with using the t.test(...) function, let’s do something a bit more realistic. Suppose that, using the ncbirths data, you want to perform inference on the number of weeks at birth. We’re told that 40 weeks is typical.\n\nAdvance decisions\nIn advance of looking at the data (or perhaps even having data) we should determine:\n\nthe hypotheses to test\nthe level at which we’ll perform the test\n\nTo make these choices, first note that there’s no obvious directional question to ask here. So, we’ll test whether the mean number of weeks at birth is 40. A 5% significance level is conventional, so we’ll stick with that.\n\n\nAssessing assumptions\nBefore going ahead, let’s inspect the data.\n\n# extract variable of interest\nweeks &lt;- ncbirths$weeks\n\n# inspect distribution\nhist(weeks, breaks = 15)\n\n\n\n\n\n\n\n\nThis is an interesting case, because we do have a left-skewed distribution and there are a few outliers below 30 weeks. However, the sample size is large (\\(n = 100\\)), so the test should still work well regardless.\n\n\nPerforming the test\nFor inference we’ll want to report a test result and interval estimate. Both of these are obtained using t.test(...) as above, but of course we only perform one test/interval calculation.\n\n# inference\nt.test(weeks, mu = 40, alternative = 'two.sided', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  weeks\nt = -5.0421, df = 99, p-value = 2.084e-06\nalternative hypothesis: true mean is not equal to 40\n95 percent confidence interval:\n 37.97938 39.12062\nsample estimates:\nmean of x \n    38.55 \n\n\nTake a moment to inspect the results.\n\n\nInterpreting results\nFollowing the format in class, a report of the results should interpret the test and interval in context, providing supporting statistics parenthetically:\n\nData from North Carolina in 2004 provide strong evidence that the mean number of weeks at birth differs from 40 (T = -5.0421 on 99 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean number of weeks at birth is estimated to be between 37.98 and 39.12 weeks, with a point estimate of 38.55 weeks (SE 0.289).\n\n\n\n\nPractice problems\n\nPerform and interpret the results of inference on the mean birth weight to investigate the claim that the typical birth weight is at least 7 lbs. Carry out inference at the 5% significance level.\n\nDetermine your hypotheses.\nCheck test assumptions.\nPerform the calculations.\nWrite a short report of the results.\n\n\n\n[REVISED] Using the BRFSS data, test whether actual body weight exceeds desired body weight and estimate the difference. Perform the test at the 1% level."
  },
  {
    "objectID": "content/test2.html",
    "href": "content/test2.html",
    "title": "Test 2",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/10. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test2.html#instructions",
    "href": "content/test2.html#instructions",
    "title": "Test 2",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/10. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test2.html#question-prompts",
    "href": "content/test2.html#question-prompts",
    "title": "Test 2",
    "section": "Question prompts",
    "text": "Question prompts\n\n[L3, L4, L5] The dataset lizards contains running speeds measured in a laboratory race track for two species of lizards, Western Fence (WF) and Sagebrush (S) lizards. Speeds are recorded in meters per second.\n\n[L3] Construct side-by-side boxplots of top speed by species. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Compute point estimates and standard errors for the mean top speed for each species.\n[L4] Compute and interpret 99.5% confidence intervals for the mean top speed for each species.\n[L5] Test for a difference in mean top speed between species at significance level 0.01. Interpret the test result following the style introduced in class.\n[L4] Construct and interpret a 99% confidence interval for the difference in mean top speed.\n\n[L3, L4] The tuition dataset contains in-state and out-of-state tuition at a random sample of 25 public universities from 2011-2012.\n\n[L3] Visualize the distribution of differences between in-state and out-of-state tuition. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Calculate and interpret a 95% confidence interval for the mean difference between in-state and out-of-state tuition.\n[L4] Interpret your interval in context following the style introduced in class.\n\n[L1, L2, L3, L4, L5] The creativity dataset contains data from an experiment on the effect of intrinsic vs. extrinsic motivation on creativity. A random sample of 47 creative writing students at an unnamed university were randomly assigned to one of two groups, extrinsic and intrinsic; each subject was instructed to write two short poems, but those in the extrinsic motivation group were primed on the task in a way that oriented them to external motivations for writing, and those in the intrinsic group were primed on the task in a way that oriented them to internal motivations for writing. Poems were scored by judges for creativity on a 40-point scale, and each subject received an average score.\n\n[L1] What is the study population? Based on the study description, is the sample representative, and if so, why?\n[L2] What type of study is this? Based on the study description, can the data support causal inferences about motivation and creativity, and if so, why?\n[L3] Construct an appropriate graphical summary comparing the distributions of average scores by treatment group.\n[L3] Provide appropriate summary statistics indicating the center, spread, and number of observations of average scores in each group.\n[L5] Test the hypothesis that motivational framing has no effect on creativity at the 1% significance level. Use your results from (c)-(d) to check assumptions.\n[L4] Compute an interval estimate for the difference in mean scores at the level corresponding to your test.\n[LX] Write a short narrative summary of your results in (e)-(f) following the style introduced in class. (Don’t forget to include a point estimate and standard error.)"
  },
  {
    "objectID": "content/week6-power.html",
    "href": "content/week6-power.html",
    "title": "Power analyses",
    "section": "",
    "text": "Reading quiz [2pm section] [4pm section]\nType II errors: review and further exploration\n[lecture/lab] Power analysis\n\nSample size calculations\nPost-hoc power analyses"
  },
  {
    "objectID": "content/week6-power.html#todays-agenda",
    "href": "content/week6-power.html#todays-agenda",
    "title": "Power analyses",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] Statistical power; post-hoc and sample size power analyses.\n[review/lab] Test 2 practice problems."
  },
  {
    "objectID": "content/week6-power.html#a-thought-experiment",
    "href": "content/week6-power.html#a-thought-experiment",
    "title": "Power analyses",
    "section": "A thought experiment",
    "text": "A thought experiment\n\n\nSuppose that for the cloud data you’d performed a two-sided test: \\[H_0: \\mu_\\text{seeded} = \\mu_\\text{unseeded}\\] \\[H_A: \\mu_\\text{seeded} \\neq \\mu_\\text{unseeded}\\]\n\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.05377\nalternative hypothesis: true difference in means between group seeded and group unseeded is not equal to 0\n95 percent confidence interval:\n  -4.764295 559.556603\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\nAlmost below the significance threshold but not quite.\n\n\nThe data do not provide sufficient evidence to reject the null hypothesis that seeding has no effect relative to the alternative of an increase or decrease in mean rainfall due to seeding (T = 1.998 on 33.86 degrees of freedom, p = 0.05377).\n\nThe point estimate for the difference is 277.4 acre-feet.\n\nThe test says this observed difference could plausibly be due to sampling variation\nBut is it also plausible that our test result is wrong if the difference is real?"
  },
  {
    "objectID": "content/week6-power.html#type-ii-error-rates",
    "href": "content/week6-power.html#type-ii-error-rates",
    "title": "Power analyses",
    "section": "Type II error rates",
    "text": "Type II error rates\n\nRecall: a type II error is failing to reject a false null hypothesis.\n\nIn the context of two-sample inference a type II error occurs when:\n\nthe true difference is \\(\\delta \\neq 0\\)\nwe test and fail to reject \\(H_0: \\delta \\neq 0\\)\n\nThe type II error rate depends on both known and unknown factors:\n\n[unknown] magnitude of \\(\\delta\\)\n[unknown] population variability \\(\\sigma\\)\n[known] significance level\n[known] sample sizes\n\nWhat was the type II error rate for the cloud seeding test?"
  },
  {
    "objectID": "content/week6-power.html#simulating-type-ii-errors",
    "href": "content/week6-power.html#simulating-type-ii-errors",
    "title": "Power analyses",
    "section": "Simulating type II errors",
    "text": "Simulating type II errors\n\n\nSummary stats for cloud data:\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nmean\nsd\nn\n\n\n\n\nseeded\n442\n650.8\n26\n\n\nunseeded\n164.6\n278.4\n26\n\n\n\n\n\nWe can approximate the type II error rate by:\n\nsimulating datasets with matching statistics\nperforming two-sided tests of no difference\ncomputing the proportion of fail-to-reject decisions\n\n\n\ntype2sim(delta = 277, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf in fact the effect size is exactly 277, a level 5% test with similar data will fail to reject ~70% of the time!"
  },
  {
    "objectID": "content/week6-power.html#simulating-type-ii-errors-1",
    "href": "content/week6-power.html#simulating-type-ii-errors-1",
    "title": "Power analyses",
    "section": "Simulating type II errors",
    "text": "Simulating type II errors\n\n\nOpen the lab and use the simulation function type2sim to fill in the table by changing arguments accordingly.\n\ntry a few magnitudes of difference for each scenario\nrepeat runs for each setting once or twice to confirm effect\n\n\n\n\n\nFactor\nChange\nEffect on error rate\n\n\n\n\ntrue difference in means\nlarger\n\n\n\ntrue difference in means\nsmaller\n\n\n\npopulation variability\nlarger\n\n\n\npopulation variability\nsmaller\n\n\n\nsample size\nlarger\n\n\n\nsample size\nsmaller\n\n\n\nsignificance level\nlarger\n\n\n\nsignificance level\nsmaller\n\n\n\n\n\n\n\nBased on your explorations, do you think our original test decision was erroneous?"
  },
  {
    "objectID": "content/week6-power.html#statistical-power",
    "href": "content/week6-power.html#statistical-power",
    "title": "Power analyses",
    "section": "Statistical power",
    "text": "Statistical power\nThe power of a test refers to its true rejection rate across alternatives and is defined as: \\[\\beta = \\underbrace{(1 - \\text{type II error rate})}_\\text{correct decision rate when null is false}\\]\nPower is often interpreted as a detection rate:\n\nhigh type II error \\(\\longrightarrow\\) low power \\(\\longrightarrow\\) low detection rate\nlow type II error \\(\\longrightarrow\\) high power \\(\\longrightarrow\\) high detection rate\n\n\nIn general tests have low power for alternatives close to the null value (where “close” is relative to sampling variability)."
  },
  {
    "objectID": "content/week6-power.html#power-curves",
    "href": "content/week6-power.html#power-curves",
    "title": "Power analyses",
    "section": "Power curves",
    "text": "Power curves\n\nPower is usually construed as a curve depending on the true difference.\n\n\n\nPower curve for the test exactly as performed with the cloud seeding data:\n\n\n\n\n\n\n\n\n\n\nAll other attributes of the test are fixed to approximate the test performed:\n\nsample size \\(n = 26\\)\nsignificance level \\(\\alpha = 0.05\\)\npopulation standard deviation \\(\\sigma = 650\\) (larger of two group estimates)"
  },
  {
    "objectID": "content/week6-power.html#factors-affecting-power",
    "href": "content/week6-power.html#factors-affecting-power",
    "title": "Power analyses",
    "section": "Factors affecting power",
    "text": "Factors affecting power\n\nPower depends on all the same factors as type II error rates\n\n\n\n\n\n\n\n\n\n\nFactor\nChange\nEffect on error rate\nEffect on power\n\n\n\n\ntrue difference in means\nlarger\n\n\n\n\ntrue difference in means\nsmaller\n\n\n\n\npopulation variability\nlarger\n\n\n\n\npopulation variability\nsmaller\n\n\n\n\nsample size\nlarger\n\n\n\n\nsample size\nsmaller\n\n\n\n\nsignificance level\nlarger\n\n\n\n\nsignificance level\nsmaller"
  },
  {
    "objectID": "content/week6-power.html#two-common-power-analyses",
    "href": "content/week6-power.html#two-common-power-analyses",
    "title": "Power analyses",
    "section": "Two common power analyses",
    "text": "Two common power analyses\n\n\nPost hoc analysis: how much power does the test I conducted have if the true difference is exactly equal to my estimate?\nHelps to interpret negative results:\n\nlow power \\(\\rightarrow\\) failure to reject was likely\nhigh power \\(\\rightarrow\\) failure to reject was not likely\n\n\n\n\n\n\n\nDon’t over-interpret post-hoc analyses\n\n\nFailure to reject using a well-powered test does not confirm the null hypothesis.\n\n\n\n\nSample size determination: how much data do I need to collect to detect a difference of \\(\\delta\\) using a particular test?\nHelps avoid two potential issues:\n\ntoo little data \\(\\rightarrow\\) study not likely to yield significant results\ntoo much data \\(\\rightarrow\\) study is too likely to yield significant results"
  },
  {
    "objectID": "content/week6-power.html#post-hoc-analysis",
    "href": "content/week6-power.html#post-hoc-analysis",
    "title": "Power analyses",
    "section": "Post-hoc analysis",
    "text": "Post-hoc analysis\n\nCan we estimate the power of a test we already performed?\n\n\n\nFeasible if we assume (a) a population standard deviation and (b) test conditions are met.\nFor the cloud seeding test:\n\npower.t.test(delta = 250, # magnitude of difference\n             sd = 650, # largest population SD\n             n = 26, # smallest sample size\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n\n\n     Two-sample t test power calculation \n\n              n = 26\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.2743235\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nFor a conservative estimate, use:\n\nsmallest of the two sample sizes\nlargest of the two standard deviations\nsmaller difference than observed\n\n\n\\(\\Longrightarrow\\) our test would only reject in favor of a difference of the observed magnitude about 27% of the time\n\nFailure to reject doesn’t strongly rule out the alternative."
  },
  {
    "objectID": "content/week6-power.html#your-turn-post-hoc-analysis",
    "href": "content/week6-power.html#your-turn-post-hoc-analysis",
    "title": "Power analyses",
    "section": "Your turn: post-hoc analysis",
    "text": "Your turn: post-hoc analysis\n\n\nConsider testing whether body temperature differs by sex.\nSummary stats and test result:\n\n\n\n\n\n\n\n\n\n\n\nsex\nmean\nsd\nn\n\n\n\n\nfemale\n98.66\n0.9929\n19\n\n\nmale\n98.17\n0.7876\n20\n\n\n\n\n\n\nt.test(body.temp ~ sex, data = temps)\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 1.7118, df = 34.329, p-value = 0.09595\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.09204497  1.07783444\nsample estimates:\nmean in group female   mean in group male \n            98.65789             98.16500 \n\n\n\nAssume the true difference is actually 0.5 °F. Determine the power of the test above when:\n\nPopulation SD is the smaller of the two groups\nPopulation SD is the larger of the two groups\nA one-sided test is used instead\n\n\nBased on your answers, do you think the negative test result rules out the alternative?"
  },
  {
    "objectID": "content/week6-power.html#power-curve-for-body-temps",
    "href": "content/week6-power.html#power-curve-for-body-temps",
    "title": "Power analyses",
    "section": "Power curve for body temps",
    "text": "Power curve for body temps\n\n\nAssuming we underestimated the population standard deviation a bit, the power curve for a one-sided test would look like this:\n\n\n\n\n\n\n\n\n\n\nAssumptions:\n\nn = 19 per group\n\\(\\sigma = 1.2\\) per group\nsignificance level \\(\\alpha = 0.05\\)\none-sided test\n\n\nFairly low power for alternatives near the estimated difference (dashed line), so failure to reject doesn’t strongly rule out the alternative."
  },
  {
    "objectID": "content/week6-power.html#the-equal-variance-t-test",
    "href": "content/week6-power.html#the-equal-variance-t-test",
    "title": "Power analyses",
    "section": "The equal-variance \\(t\\)-test",
    "text": "The equal-variance \\(t\\)-test\nIf it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power by using a different standard error:\n\\[SE_\\text{pooled}(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{\\color{red}{s_p^2}}{n_x} + \\frac{\\color{red}{s_p^2}}{n_y}}\n\\quad\\text{where}\\quad \\color{red}{s_p} = \\underbrace{\\sqrt{\\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\\text{weighted average of } s_x^2 \\;\\&\\; s_y^2}\\]\nIn the case of the body temperature data, \\(s_p\\) = 0.8934. Check:\n\nHow much power do we gain if we assume a common SD of 0.89?\nDoes it change the outcome of the test (add var.equal = T)?\n\n\nProduces minimal gains and inflates type I error if not warranted, so better avoided unless you have a small sample size"
  },
  {
    "objectID": "content/week6-power.html#sample-size-calculation",
    "href": "content/week6-power.html#sample-size-calculation",
    "title": "Power analyses",
    "section": "Sample size calculation",
    "text": "Sample size calculation\n\nIf you were (re)designing the study, how much data should you collect to detect a specified effect size?\n\n\n\nTo detect a difference of 250 or more due to cloud seeding with power 0.9:\n\npower.t.test(power = 0.9, # target power level\n             delta = 250, # smallest difference\n             sd = 650, # largest population SD\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n\n\n     Two-sample t test power calculation \n\n              n = 143.0276\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nFor a conservative estimate, use:\n\noverestimate of the larger of the two standard deviations\nminimum difference of interest\n\n\n\\(\\Longrightarrow\\) we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time"
  },
  {
    "objectID": "content/week6-power.html#your-turn-sample-size-calculation",
    "href": "content/week6-power.html#your-turn-sample-size-calculation",
    "title": "Power analyses",
    "section": "Your turn: sample size calculation",
    "text": "Your turn: sample size calculation\nSuppose you are designing a follow-up study and wish to detect a difference of 0.4 °F at least 70% of the time. You know women have slightly higher body temperatures than men on average.\n\n\n\n\n\n\n\n\nKnown direction?\nPopulation SD\nMinimum \\(n\\)\n\n\n\n\nNo\nlarger of prior estimates\n\n\n\nNo\n1.2 times larger than larger of prior estimates\n\n\n\nYes\nlarger of prior estimates\n\n\n\nYes\n1.2 times larger than larger of prior estimates\n\n\n\n\n\nIf it costs $10 per participant to run the study, what’s the best power achievable within a $2K budget for the target detection magnitude?"
  },
  {
    "objectID": "content/week6-power.html#power-vs.-sample-size-curves",
    "href": "content/week6-power.html#power-vs.-sample-size-curves",
    "title": "Power analyses",
    "section": "Power vs. sample size curves",
    "text": "Power vs. sample size curves\n\n\nMinimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:\n\n\n\n\n\n\n\n\n\nAssumes \\(\\sigma = 650\\) for a conservative estimate.\n\nIt may not be affordable to obtain data for 144 days per treatment group (pilots and planes are expensive). What is achievable within constraints?\n\npower of 0.8 will require n = 59 per group\n\n138 days total\n\ndecreasing to 0.7 will require n = 45 per group\n\n90 days total\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week6-twosample.html",
    "href": "content/week6-twosample.html",
    "title": "Two sample inference",
    "section": "",
    "text": "Reading quiz [2pm section] [4pm section]\n[lecture/lab] Two-sample inference for population means\n\nPaired data\nIndependent data\n\n[if time] Introduction to power analysis"
  },
  {
    "objectID": "content/week6-twosample.html#todays-agenda",
    "href": "content/week6-twosample.html#todays-agenda",
    "title": "Two sample inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] two sample inference for means\n[lab] two-sample \\(t\\) tests in R\n[test prep] practice problems"
  },
  {
    "objectID": "content/week6-twosample.html#from-last-time",
    "href": "content/week6-twosample.html#from-last-time",
    "title": "Two sample inference",
    "section": "From last time",
    "text": "From last time\nPractice problem: test whether actual body weight exceeds desired body weight.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubject\nactual\ndesired\ndifference\n\n\n\n\n1\n265\n225\n40\n\n\n2\n150\n150\n0\n\n\n3\n137\n150\n-13\n\n\n4\n159\n125\n34\n\n\n5\n145\n125\n20\n\n\n\n\n\n\n\nweight.diffs &lt;- brfss$weight - brfss$wtdesire\nt.test(weight.diffs, \n       mu = 0, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  weight.diffs\nt = 4.2172, df = 59, p-value = 4.311e-05\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 10.99824      Inf\nsample estimates:\nmean of x \n 18.21667 \n\n\n\n\n\nThe data provide very strong evidence that the average U.S. adult’s actual weight exceeds their desired weight (T = 4.2172 on 59 degrees of freedom, p &lt; 0.0001).\n\nInference is on the mean difference: \\(H_0: \\delta = 0\\) vs. \\(H_A: \\delta &gt; 0\\).\nCan we also do inference on a difference in means?"
  },
  {
    "objectID": "content/week6-twosample.html#inference-for-paired-data",
    "href": "content/week6-twosample.html#inference-for-paired-data",
    "title": "Two sample inference",
    "section": "Inference for paired data",
    "text": "Inference for paired data\n\nCalculate paired differences.\n\n\n\n\n\n\n\n\n\n\n\n\nswimmer\nbody.suit.velocity\nswim.suit.velocity\nvelocity.diff\n\n\n\n\n1\n1.57\n1.49\n0.08\n\n\n2\n1.47\n1.37\n0.1\n\n\n3\n1.42\n1.35\n0.07\n\n\n\n\n\n\n\n\nPerform test as before.\n\n\n# test for paired difference\nt.test(diffs, mu = 0, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  diffs\nt = 12.318, df = 11, p-value = 4.443e-08\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 0.06620114        Inf\nsample estimates:\nmean of x \n   0.0775 \n\n\n\n\nReport the result of the test. You try."
  },
  {
    "objectID": "content/week6-twosample.html#formulating-a-two-sample-problem",
    "href": "content/week6-twosample.html#formulating-a-two-sample-problem",
    "title": "Two sample inference",
    "section": "Formulating a two-sample problem",
    "text": "Formulating a two-sample problem\n\n\nTwo-sample problems are characterized by:\n\none variable of interest\ntwo groups of observations\nobjective to compare group means\n\nInference concerns the difference in means\n\\[\\delta = \\mu_1 - \\mu_2\\]\nWe just tested:\n\\[H_0: \\mu_\\text{body} \\leq \\mu_\\text{swim}\\] \\[H_A: \\mu_\\text{body} &gt; \\mu_\\text{swim}\\]\n\nRearranging the data to emphasize two-sample problem structure:\n\n\n\n\n\n\n\n\n\n\nswimmer\nsuit\nvelocity\n\n\n\n\n1\nbody\n1.57\n\n\n1\nswim\n1.49\n\n\n2\nbody\n1.47\n\n\n2\nswim\n1.37\n\n\n3\nbody\n1.42\n\n\n\n\n\n\nvariable of interest: velocity\ngrouping: suit\npairing: swimmer"
  },
  {
    "objectID": "content/week6-twosample.html#hypotheses-for-two-sample-tests",
    "href": "content/week6-twosample.html#hypotheses-for-two-sample-tests",
    "title": "Two sample inference",
    "section": "Hypotheses for two-sample tests",
    "text": "Hypotheses for two-sample tests\nWe can articulate two-sided and directional tests for the difference in means \\(\\delta = \\mu_1 - \\mu_2\\) and the corresponding interpretation in terms of the group means.\n\n\nDifference in means\n\\[\\text{two-sided}\\begin{cases} H_0: \\delta \\qquad 0 \\\\ H_A: \\delta \\qquad 0 \\end{cases}\\]\n\\[\\text{lower-sided}\\begin{cases} H_0: \\delta \\qquad 0 \\\\ H_A: \\delta \\qquad 0 \\end{cases}\\]\n\\[\\text{upper-sided}\\begin{cases} H_0: \\delta \\qquad 0 \\\\ H_A: \\delta \\qquad 0 \\end{cases}\\]\n\nGroup interpretation\n\\[\\begin{cases} H_0: \\mu_1 \\qquad \\mu_2 \\\\ H_A: \\mu_1 \\qquad \\mu_2 \\end{cases}\\]\n\\[\\begin{cases} H_0: \\mu_1 \\qquad \\mu_2 \\\\ H_A: \\mu_1 \\qquad \\mu_2 \\end{cases}\\]\n\\[\\begin{cases} H_0: \\mu_1 \\qquad \\mu_2 \\\\ H_A: \\mu_1 \\qquad \\mu_2 \\end{cases}\\]"
  },
  {
    "objectID": "content/week6-twosample.html#your-turn-famuss",
    "href": "content/week6-twosample.html#your-turn-famuss",
    "title": "Two sample inference",
    "section": "Your turn: FAMuSS",
    "text": "Your turn: FAMuSS\n\nDoes resistance training lead to greater strength gains on the nondominant arm?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n\n\n\nArticulate and test an appropriate hypothesis for \\(\\delta = \\mu_\\text{ndrm} - \\mu_\\text{drm}\\)\n\n\n\nHypotheses: \\[H_0: \\hspace{15cm}\\] \\[H_A: \\hspace{15cm}\\]\n\n\n\nResult:"
  },
  {
    "objectID": "content/week6-twosample.html#evolution-of-darwins-finches",
    "href": "content/week6-twosample.html#evolution-of-darwins-finches",
    "title": "Two sample inference",
    "section": "Evolution of Darwin’s finches",
    "text": "Evolution of Darwin’s finches\n\n\nPeter and Rosemary Grant caught and measured birds from more than 20 generations of finches on Daphne Major.\n\nsevere drought in 1977 limited food to large tough seeds\nselection pressure favoring larger and stronger beaks\nhypothesis: beak depth increased in 1978 relative to 1976\n\n\n\n\n\n\n\n\n\n\n\nyear\ndepth\n\n\n\n\n1976\n10.8\n\n\n1976\n7.4\n\n\n1978\n11.4\n\n\n1978\n10.6\n\n\n\n\n\n\n\nTo answer this, we need to test a hypothesis involving two means:\n\\[\n\\begin{cases}\nH_0: &\\mu_{1976} = \\mu_{1978} \\\\\nH_A: &\\mu_{1976} &lt; \\mu_{1978}\n\\end{cases}\n\\]\n\ncan’t do inference on a mean difference here (no pairing of observations)\ntreat each year as an independent sample"
  },
  {
    "objectID": "content/week6-twosample.html#inference-for-independent-data",
    "href": "content/week6-twosample.html#inference-for-independent-data",
    "title": "Two sample inference",
    "section": "Inference for independent data",
    "text": "Inference for independent data\n\nBeak depths exemplify independent data: the groups of observations are unrelated.\n\n\n\nInference is based on the difference in group means:\n\\[\nT = \\frac{\\bar{x} - \\bar{y}}{SE(\\bar{x} - \\bar{y})}\n\\]\n\n\\(\\bar{x}, \\bar{y}\\) are groupwise sample means\n\\(SE(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{s_x^2}{n_x} + \\frac{s_y^2}{n_y}}\\)\ndegrees of freedom for \\(t\\) model are approximated\n\n\n\\[H_0: \\mu_{1976} \\geq \\mu_{1978}\\] \\[H_A: \\mu_{1976} &lt; \\mu_{1978}\\]\n\nt.test(Depth ~ Year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  Depth by Year\nt = -4.5833, df = 172.98, p-value = 4.37e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n      -Inf -0.427321\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.469663          10.138202"
  },
  {
    "objectID": "content/week6-twosample.html#two-input-formats",
    "href": "content/week6-twosample.html#two-input-formats",
    "title": "Two sample inference",
    "section": "Two input formats",
    "text": "Two input formats\n\n\nThe formula format takes inputs:\n\nan R formula\na data frame\n\n\n# two-sample test (formula inputs)\nt.test(depth ~ year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n\n\nDepth ~ Year: “depth depends on year”\n\nThe vector format takes inputs:\n\nvector of observations for one group\nvector of observations for the other group\n\n\n# two-sample test (vector inputs)\nt.test(depth76, depth78,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth76 and depth78\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean of x mean of y \n 9.453448 10.190769"
  },
  {
    "objectID": "content/week6-twosample.html#interpreting-results",
    "href": "content/week6-twosample.html#interpreting-results",
    "title": "Two sample inference",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\n\nt.test(depth ~ year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-paired-or-independent",
    "href": "content/week6-twosample.html#cloud-data-paired-or-independent",
    "title": "Two sample inference",
    "section": "Cloud data: paired or independent?",
    "text": "Cloud data: paired or independent?\n\nDoes dropping silver iodide onto clouds increase rainfall?\n\n\n\nData are rainfall measurements in a target area from 26 days when clouds were seeded and 26 days when clouds were not seeded.\n\nrainfall gives volume of rainfall in acre-feet\ntreatment indicates whether clouds were seeded\n\nHypotheses to test: \\[H_0: \\mu_\\text{seeded} \\quad \\mu_\\text{unseeded}\\] \\[H_A: \\mu_\\text{seeded} \\quad \\mu_\\text{unseeded}\\]\n\n\n\n\n\n\n\n\n\n\nrainfall\ntreatment\n\n\n\n\n703.4\nSeeded\n\n\n17.5\nSeeded\n\n\n242.5\nSeeded\n\n\n1698\nSeeded\n\n\n830.1\nUnseeded\n\n\n11.5\nUnseeded\n\n\n4.9\nUnseeded\n\n\n26.3\nUnseeded"
  },
  {
    "objectID": "content/week6-twosample.html#sleep-drugs-paired-or-independent",
    "href": "content/week6-twosample.html#sleep-drugs-paired-or-independent",
    "title": "Two sample inference",
    "section": "Sleep drugs: paired or independent?",
    "text": "Sleep drugs: paired or independent?\n\nWhich (if either) of two soporific drugs is more effective?\n\n\n\nData are extra hours of sleep for 10 study participants when taking each of two drugs.\n\nextra.sleep gives hours of additional sleep relative to control\ndrug indicates which sleep drug was taken\nsubject indicates study participant id\n\nHypotheses to test: \\[H_0: \\mu_1 \\quad \\mu_2\\] \\[H_A: \\mu_1 \\quad \\mu_2\\]\n\n\n\n\n\n\n\n\n\n\n\nextra.sleep\ndrug\nsubject\n\n\n\n\n0.7\n1\n1\n\n\n1.9\n2\n1\n\n\n-1.6\n1\n2\n\n\n0.8\n2\n2\n\n\n-0.2\n1\n3\n\n\n1.1\n2\n3"
  },
  {
    "objectID": "content/week6-twosample.html#test-assumptions",
    "href": "content/week6-twosample.html#test-assumptions",
    "title": "Two sample inference",
    "section": "Test assumptions",
    "text": "Test assumptions\n\n\nInference relies on a \\(t\\) model providing a good approximation to the sampling distribution. This requires three assumptions:\n\nvariable of interest is numeric and not too discrete\nobservations are independent (besides pairing)\neither:\n\nsample sizes are not too small\nor distribution(s) are symmetric and unimodal\n\n\n\nCommon issues:\n\n\n\n\n\n\n\nIssue\nConsequence\n\n\n\n\nHighly discrete data\n\\(t\\) model not appropriate\n\n\nDependent observations\n\\(SE\\) is a biased estimate: nominal error rates and coverage are inaccurate\n\n\nSmall samples with heavy skew or extreme outliers\n\\(SE\\) too small: inflated type I error and under-coverage\n\n\n\n\nIn each of these scenarios, different inference procedures should be used."
  },
  {
    "objectID": "content/week6-twosample.html#power-calculations",
    "href": "content/week6-twosample.html#power-calculations",
    "title": "Two sample inference",
    "section": "Power calculations",
    "text": "Power calculations\n\nHow much data do you need to collect in order to detect a difference of \\(\\delta\\)?\n\n\n\nThe statistical power of a test captures how often it detects a specified alternative.\n\nmeasures how often the test correctly rejects (proportion of samples)\nvalue depends on…\n\nmagnitude of difference between null value and true value of parameter\nsignificance level\nsample size\n\n\n\n\npower.t.test(power = 0.95, \n             delta = 0.5, \n             sig.level = 0.05, \n             type = 'two.sample',\n             alternative = 'two.sided')\n\n\n     Two-sample t test power calculation \n\n              n = 104.928\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\\(\\Rightarrow\\) need 105 observations in each group to detect a difference of 0.5 standard deviations for 95% of samples with a 5% significance level test"
  },
  {
    "objectID": "content/week6-twosample.html#the-equal-variance-t-test",
    "href": "content/week6-twosample.html#the-equal-variance-t-test",
    "title": "Two sample inference",
    "section": "The equal-variance \\(t\\)-test",
    "text": "The equal-variance \\(t\\)-test\nIf it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power by using a different standard error:\n\\[SE_\\text{pooled}(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{\\color{red}{s_p^2}}{n_x} + \\frac{\\color{red}{s_p^2}}{n_y}}\n\\quad\\text{where}\\quad \\color{red}{s_p} = \\underbrace{\\sqrt{\\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\\text{weighted average of } s_x^2 \\;\\&\\; s_y^2}\\]\nImplement by adding var.equal = T as an argument to t.test().\n\nlarger df is used, hence more frequent rejections\navoid unless you have a small sample\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/test2.html#problems",
    "href": "content/test2.html#problems",
    "title": "Test 2",
    "section": "Problems",
    "text": "Problems\n\nProblem 1: gifted children\n\n[L3, L4, L5] The gifted dataset contains data on 36 children identified as gifted in a large city. Assume for the purpose of the problem that the data are from a random sample of gifted children in the metropolitan region where the data were collected.\n\n[L3] Is there a relationship between the mother’s IQ and the child’s test score? Construct a scatterplot and compute and interpret the correlation.\n[L3] Repeat but with the father’s IQ.\n[L5] Given your results in (a)-(b), formulate a hypothesis about which parent’s IQ is higher. Explain your reasoning.\n[L5] Construct a histogram of the pairwise differences between the mother’s IQ and father’s IQ for each child in the dataset and check the assumptions for inference using the \\(t\\) model.\n[L4, L5] Test the hypothesis you proposed in (c) at the 1% level and provide a corresponding interval estimate. Report your results in the narrative style introduced in class.\n[L4, L5] It’s thought that the mean age by which infants can count to 10 is around two years old. Test the hypothesis at the 5% level that gifted children do this sooner and provide a corresponding interval estimate. Interpret the test results and interval estimate in context following the narrative style introduced in class.\n\n\n\n\nProblem 2: lizard running speeds\n\n[L3, L4, L5] The dataset lizards contains running speeds measured in a laboratory race track for two species of lizards, Western Fence (WF) and Sagebrush (S) lizards. Speeds are recorded in meters per second.\n\n[L3] Construct side-by-side boxplots of top speed by species. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Compute point estimates and standard errors for the mean top speed for each species.\n[L4] Compute and interpret 99.5% confidence intervals for the mean top speed for each species.\n[L4, L5] Test for a difference in mean top speed between species at the 1% significance level and provide an interval estimate at the appropriate confidence level. Interpret the test and estimate following the narrative style introduced in class.\n\n\n\n\nProblem 3: self- and cross-fertilization and plant vigor\n\n[L3, L4, L5] Does self-fertilization produce less vigorous plants than cross-fertilization? The dataset plants contains measurements of plant heights in inches for 15 pairs of plants of the same age; one plant in each pair was grown from a seed from a cross-fertilized flower, and the other was grown from a seed from a self-fertilized flower.\n\n[L3] Visualize the distribution of differences in plant heights between the cross-fertilized and self-fertilized individuals. Does the plot alone suggest an answer to the question of interest?\n[L4, L5] Test, at the 2% level, whether mean height of plants grown from cross-fertilized seeds exceeds that of plants grown from self-fertilized seeds and provide a confidence bound for the difference at the level corresponding to your test. Report the results of your analysis in context following the narrative style introduced in class.\n\n\n\n\nProblem 4: creativity and motivation\n\n[L1, L2, L3, L4, L5] The creativity dataset contains data from an experiment on the effect of intrinsic vs. extrinsic motivation on creativity. A random sample of 47 creative writing students at an unnamed university were randomly assigned to one of two groups, extrinsic and intrinsic; each subject was instructed to write two short poems, but those in the extrinsic motivation group were primed on the task in a way that oriented them to external motivations for writing, and those in the intrinsic group were primed on the task in a way that oriented them to internal motivations for writing. Poems were scored by judges for creativity on a 40-point scale, and each subject received an average score.\n\n[L1] What is the study population? Based on the study description, is the sample representative, and if so, why?\n[L2] What type of study is this? Based on the study description, can the data support causal inferences about motivation and creativity, and if so, why?\n[L3] Construct an appropriate graphical summary comparing the distributions of average scores by treatment group.\n[L4] Provide point estimates and standard errors for the mean creativity score in each group.\n[L5] Test the hypothesis that motivational framing has no effect on creativity at the 1% significance level. Compute an interval estimate for the difference in mean scores at the level corresponding to your test. Use your results from (c)-(d) to check assumptions. Write a short narrative summary of your results following the style introduced in class. (Don’t forget to include a point estimate and standard error.)"
  },
  {
    "objectID": "content/test2-practice.html",
    "href": "content/test2-practice.html",
    "title": "Test 2 practice problems",
    "section": "",
    "text": "Test 2 information\nThe test will comprise four problems focused on point estimation, interval estimation, and hypothesis tests for means (learning outcomes L4-L5). Each problem will have multiple parts, some of which may require skills from earlier (especially summary statistics and statistical graphics).\nYou will have 48 hours to complete the test; a Posit cloud project will be provided with comment outlines to help you organize your calculations. You’ll submit your work via an online form, and will be expected to also upload your R script from your Posit cloud project.\nThe problems below are intended to help you practice the skills and concepts that will be assessed in the test. An expandable “solution” is provided below each prompt that shows the calculations needed to answer the prompts; of course, resolving the problems satisfactorily also requires interpreting results accurately. You’re encouraged to ask about interpretations in class.\n\n\nPractice problems\n\n[L3, L4] The tuition dataset contains in-state and out-of-state tuition at a random sample of 25 public universities from 2011-2012.\n\n[L3] Visualize the distribution of differences between in-state and out-of-state tuition. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Calculate and interpret a 95% confidence interval for the mean difference between in-state and out-of-state tuition.\n[L4] Interpret your interval in context following the style introduced in class.\n\n\n\n\nSolution\n# load data\nload('data/tuition.RData')\n\n# part a: visualize distribution of differences; are assumptions for use of t model met?\ntuition.diffs &lt;- tuition$out.of.state - tuition$in.state\nhist(tuition.diffs, breaks = 10)\n\n# part b: 95% interval estimate for differences\nt.test(tuition.diffs)$conf.int\n\n\n\n[L3, L4, L5] The dataset cancer contains skin cancer rates per 100,000 people in Connecticut each year from 1938 to 1972. Each year is also classified as following a period of higher than average or lower than average sunspot activity. The delta variable is the change in cancer rate relative to the previous year. In this problem, you’ll perform inference on the mean delta by sunspot activity level to determine whether higher than average sunspot activity is associated with an increase in mean skin cancer rates from the prior year.\n\n[L4] Estimate the mean delta (irrespective of sunspot activity level). Provide both a point estimate and standard error, and interpret the estimate in context. Does the estimate suggest that the cancer rate is increasing or decreasing? Explain.\n[L4, L5] Perform a test for mean delta to determine whether the mean cancer rate is increasing. Use a 5% significance level, and report your test result together with an interval estimate following the narrative style introduced in class.\n[L3] Plot the ‘raw’ cancer rate (i.e., not the delta) against year. (Add the argument type = 'b' to draw a path connecting the observations.) Is your answer in (b) consistent with any trend(s) you see?\n[L3] Make a side-by-side boxplot of the delta variable for each level of sunspot activity. Comment on the plot: does there seem to be a difference?\n[L5] Test whether the mean change in cancer rate is higher in years with higher than average sunspot activity. Carry out inference at the 5% significance level.\n\n\n\n\nSolution\n# load data\ncancer &lt;- read_csv('data/cancer.csv')\n\n# part a: point estimate of mean delta and standard error\nmean(cancer$delta)\nsd(cancer$delta)/sqrt(length(cancer))\n\n# part b: is delta increasing? test at 5% level\nt.test(cancer$delta, mu = 0, alternative = 'greater', conf.level = 0.95)\n\n# part c: plot \nplot(cancer$year, cancer$rate, type = 'b')\n\n# part d: boxplots by activity level; different?\nboxplot(delta ~ sunspot, data = cancer, horizontal = T)\n\n# part e: test for a difference in mean delta by sunspot activity at the 5% level\nt.test(delta ~ sunspot, alternative = 'greater', data = cancer, conf.level = 0.95)\n\n\n\n[L4, L5] Studies have provided evidence that the hippocampus is smaller in schizophrenic patients on average. The dataset hippocampus contains data on volumes of the left hippocampus in cubic centimeters for 15 pairs of monozygotic twins; one twin in each pair was affected by schizophrenia and the other was not.\n\n[L5] Compute the pairwise differences in hippocampal volume by twin pair and inspect the distribution. Do assumptions for inference using the \\(t\\) model seem plausible?\n[L5] Formulate a hypothesis to test whether hippocampal volume is smaller among the affected twin on average. Write the hypotheses in notation.\n[L4, L5] Carry out the test in (b) at the 1% significance level. Report your test result along with a corresponding interval estimate following the narrative style introduced in class.\n\n\n\n\nSolution\n# load and inspect data\ntwins &lt;- read_csv('data/hippocampus.csv')\nhead(twins)\n\n# part a: compute differences; check distribution for t inference assumptions\nhvolume.diff &lt;- twins$affected - twins$unaffected\nhist(hvolume.diff, breaks = 5)\n\n# part c: carry out test at 1% significance level\nt.test(hvolume.diff, mu = 0, alternative = 'less', conf.level = 0.99)"
  },
  {
    "objectID": "content/week6-twosample.html#paired-differences",
    "href": "content/week6-twosample.html#paired-differences",
    "title": "Two sample inference",
    "section": "Paired differences",
    "text": "Paired differences\nPractice problem from last time: test whether actual exceeds desired body weight.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubject\nactual\ndesired\ndifference\n\n\n\n\n1\n265\n225\n40\n\n\n2\n150\n150\n0\n\n\n3\n137\n150\n-13\n\n\n4\n159\n125\n34\n\n\n5\n145\n125\n20\n\n\n\n\n\n\n\nweight.diffs &lt;- brfss$weight - brfss$wtdesire\nt.test(weight.diffs, \n       mu = 0, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  weight.diffs\nt = 4.2172, df = 59, p-value = 4.311e-05\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 10.99824      Inf\nsample estimates:\nmean of x \n 18.21667 \n\n\n\n\n\nThe data provide very strong evidence that the average U.S. adult’s actual weight exceeds their desired weight.\n\nInference is on the mean difference: \\(H_0: \\delta = 0\\) vs. \\(H_A: \\delta &gt; 0\\).\nCan we also do inference on a difference in means based on two independent samples?"
  },
  {
    "objectID": "content/week6-twosample.html#two-sample-inference",
    "href": "content/week6-twosample.html#two-sample-inference",
    "title": "Two sample inference",
    "section": "Two-sample inference",
    "text": "Two-sample inference\nIf \\(x_1, \\dots, x_{58}\\) are the 1976 observations and \\(y_1, \\dots, y_{65}\\) are the 1978 observations:\n\n\\(\\bar{x}\\) is a point estimate for \\(\\mu_{1976}\\) with standard error \\(SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\)\n\\(\\bar{y}\\) is a point estimate for \\(\\mu_{1978}\\) with standard error \\(SE(\\bar{y}) = \\frac{s_y}{\\sqrt{n}}\\)\n\n\n\nInference uses a new \\(T\\) statistic:\n\\[\nT = \\frac{\\bar{x} - \\bar{y} - \\delta_0}{SE(\\bar{x} - \\bar{y})}\n\\]\n\n\\(\\delta_0\\) is the hypothesized difference in means\n\\(SE(\\bar{x} - \\bar{y}) = \\sqrt{SE(\\bar{x})^2 + SE(\\bar{y})^2}\\)\n\\(t_\\nu\\) model approximates the sampling distribution when each sample meets assumptions for one-sample inference"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data",
    "href": "content/week6-twosample.html#cloud-data",
    "title": "Two sample inference",
    "section": "Cloud data",
    "text": "Cloud data\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\nData are rainfall measurements in a target area from 26 days when clouds were seeded and 26 days when clouds were not seeded.\n\nrainfall gives volume of rainfall in acre-feet\ntreatment indicates whether clouds were seeded\n\nHypotheses to test: \\[\n\\begin{cases}\nH_0: &\\mu_\\text{seeded} = \\mu_\\text{unseeded} \\\\\nH_A: &\\mu_\\text{seeded} &gt; \\mu_\\text{unseeded}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\nrainfall\ntreatment\n\n\n\n\n334.1\nseeded\n\n\n489.1\nseeded\n\n\n200.7\nseeded\n\n\n40.6\nseeded\n\n\n21.7\nunseeded\n\n\n17.3\nunseeded\n\n\n68.5\nunseeded\n\n\n830.1\nunseeded"
  },
  {
    "objectID": "content/week6-twosample.html#difference-in-means",
    "href": "content/week6-twosample.html#difference-in-means",
    "title": "Two sample inference",
    "section": "Difference in means",
    "text": "Difference in means\nWhat if we want to compare two population means based on independent samples?"
  },
  {
    "objectID": "content/week6-twosample.html#interpreting-outputs-and-results",
    "href": "content/week6-twosample.html#interpreting-outputs-and-results",
    "title": "Two sample inference",
    "section": "Interpreting outputs and results",
    "text": "Interpreting outputs and results\n\n\n\nt.test(depth ~ year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n\n\n\n\nThe data provide very strong evidence that mean beak depth increased following the drought (T = -4.5727 on 111.79 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean increase is estimated to be at least 0.4699 mm, with a point estimate of 0.7373 (SE 0.1612).\n\n\n\nHighly similar, but notice:\n\ninput is a formula depth ~ year (“depth depends on year”) and data frame finch\nmu now indicates hypothesized difference in means\ndecimal degrees of freedom\nalternative is relative to the order in which groups appear"
  },
  {
    "objectID": "content/week6-twosample.html#directional-and-non-directional-hypotheses",
    "href": "content/week6-twosample.html#directional-and-non-directional-hypotheses",
    "title": "Two sample inference",
    "section": "Directional and non-directional hypotheses",
    "text": "Directional and non-directional hypotheses"
  },
  {
    "objectID": "content/week6-twosample.html#directional-and-nondirectional-hypotheses",
    "href": "content/week6-twosample.html#directional-and-nondirectional-hypotheses",
    "title": "Two sample inference",
    "section": "Directional and nondirectional hypotheses",
    "text": "Directional and nondirectional hypotheses"
  },
  {
    "objectID": "content/week6-twosample.html#directional-and-two-sided-tests",
    "href": "content/week6-twosample.html#directional-and-two-sided-tests",
    "title": "Two sample inference",
    "section": "Directional and two-sided tests",
    "text": "Directional and two-sided tests"
  },
  {
    "objectID": "content/week6-twosample.html#directional-and-nondirectional-tests",
    "href": "content/week6-twosample.html#directional-and-nondirectional-tests",
    "title": "Two sample inference",
    "section": "Directional and nondirectional tests",
    "text": "Directional and nondirectional tests"
  },
  {
    "objectID": "content/week6-twosample.html#checking-assumptions",
    "href": "content/week6-twosample.html#checking-assumptions",
    "title": "Two sample inference",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\nThe two-sample test is appropriate whenever two one-sample tests would be.\n\n\n\nIn other words, the test assumes that both samples are either:\n\nsufficiently large; or\nhave little skew and few outliers\n\nTo check, simply inspect each histogram.\n\nboth distributions unimodal\nboth a bit left skewed\nno extreme outliers\nlarge sample sizes (58, 65)"
  },
  {
    "objectID": "content/week6-twosample.html#checking-assumptions-alternative",
    "href": "content/week6-twosample.html#checking-assumptions-alternative",
    "title": "Two sample inference",
    "section": "Checking assumptions (alternative)",
    "text": "Checking assumptions (alternative)\n\nThe two-sample test is appropriate whenever two one-sample tests would be.\n\n\n\nIn other words, the test assumes that both samples are either:\n\nsufficiently large; or\nhave little skew and few outliers\n\nCould also check side-by-side boxplots for:\n\napproximate symmetry of boxes\noutliers far from whiskers\n\nThis is also a nice visualization of differences between samples."
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-which-test",
    "href": "content/week6-twosample.html#cloud-data-which-test",
    "title": "Two sample inference",
    "section": "Cloud data: which test?",
    "text": "Cloud data: which test?\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.9731\nalternative hypothesis: true difference in means between group Seeded and group Unseeded is less than 0\n95 percent confidence interval:\n     -Inf 512.1582\nsample estimates:\n  mean in group Seeded mean in group Unseeded \n              441.9846               164.5885 \n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group Seeded and group Unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group Seeded mean in group Unseeded \n              441.9846               164.5885"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-which-alternative",
    "href": "content/week6-twosample.html#cloud-data-which-alternative",
    "title": "Two sample inference",
    "section": "Cloud data: which alternative?",
    "text": "Cloud data: which alternative?\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.9731\nalternative hypothesis: true difference in means between group seeded and group unseeded is less than 0\n95 percent confidence interval:\n     -Inf 512.1582\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\n\nYou can tell which group R considers first based on which estimate is printed first.\n\n'greater' is interpreted as [FIRST GROUP] &gt; [SECOND GROUP]\n'less' is interpreted as [FIRST GROUP] &lt; [SECOND GROUP]"
  },
  {
    "objectID": "content/week6-twosample.html#another-input-format",
    "href": "content/week6-twosample.html#another-input-format",
    "title": "Two sample inference",
    "section": "Another input format",
    "text": "Another input format\nIf you have trouble keeping track of directions, another option is to give t.test two vectors; it will always interpret the alternative in the order you specify.\n\n# extract observations in each group\nseeded &lt;- cloud |&gt; filter(treatment == 'seeded')\nunseeded &lt;- cloud |&gt; filter(treatment == 'unseeded') \n\n# perform t test\nt.test(seeded$rainfall, unseeded$rainfall, mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  seeded$rainfall and unseeded$rainfall\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\nmean of x mean of y \n 441.9846  164.5885"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-interpretation",
    "href": "content/week6-twosample.html#cloud-data-interpretation",
    "title": "Two sample inference",
    "section": "Cloud data: interpretation",
    "text": "Cloud data: interpretation\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\n\nThe data provide moderate evidence that cloud seeding increases mean rainfall (T = 1.9982 on 33.855 degrees of freedom, p = 0.02689). With 95% confidence, seeding is estimated to increase mean rainfall by at least 42.63 acre-feet, with a point estimate of 277.4 (SE 138.8199)."
  },
  {
    "objectID": "content/week6-twosample.html#body-temperatures-again",
    "href": "content/week6-twosample.html#body-temperatures-again",
    "title": "Two sample inference",
    "section": "Body temperatures (again)",
    "text": "Body temperatures (again)\n\nDoes mean body temperature differ between men and women?\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest \\(H_0: \\mu_F = \\mu_M\\) against \\(H_A: \\mu_F \\neq \\mu_M\\)\n\nt.test(body.temp ~ sex, data = temps, \n       mu = 0, alternative = 'two.sided')\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 1.7118, df = 34.329, p-value = 0.09595\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.09204497  1.07783444\nsample estimates:\nmean in group female   mean in group male \n            98.65789             98.16500 \n\n\n\n\nSuggestive but insufficient evidence that mean body temperature differs by sex.\nNotice: estimated difference (F - M) is 0.493 °F (SE 0.2879)"
  },
  {
    "objectID": "content/week6-twosample.html#what-if-we-had-more-data",
    "href": "content/week6-twosample.html#what-if-we-had-more-data",
    "title": "Two sample inference",
    "section": "What if we had more data?",
    "text": "What if we had more data?\nHere are estimates from two larger samples of 65 individuals each (compared with 19, 20):\n\n\n\n\n\n\n\n\n\n\n\nsex\nmean.temp\nse\nn\n\n\n\n\nfemale\n98.39\n0.09222\n65\n\n\nmale\n98.1\n0.08667\n65\n\n\n\n\n\n\nestimated difference (F - M) is smaller 0.2892 °F\nbut so is the standard error SE 0.1266 (recall more data \\(\\longleftrightarrow\\) better precision)\n\n\n\n\nt.test(body.temp ~ sex, data = temps.aug, \n       mu = 0, alternative = 'two.sided')\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 2.2854, df = 127.51, p-value = 0.02394\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n 0.03881298 0.53964856\nsample estimates:\nmean in group female   mean in group male \n            98.39385             98.10462 \n\n\n\n\nThe data provide moderate evidence that mean body temperature differs by sex (T = 2.29 on 127.51 degrees of freedom, p = 0.02394)."
  },
  {
    "objectID": "content/week6-twosample.html#a-paradox",
    "href": "content/week6-twosample.html#a-paradox",
    "title": "Two sample inference",
    "section": "A paradox?",
    "text": "A paradox?\n\nIf you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo keep in mind:\n\nstatistical significance \\(\\neq\\) practical significance\nalways check your point estimates"
  },
  {
    "objectID": "content/week6-twosample.html#a-surprising-result",
    "href": "content/week6-twosample.html#a-surprising-result",
    "title": "Two sample inference",
    "section": "A surprising result",
    "text": "A surprising result\n\nIf you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo keep in mind:\n\nstatistical significance \\(\\neq\\) practical significance\nalways check your point estimates"
  },
  {
    "objectID": "content/week6-twosample.html#a-seeming-paradox",
    "href": "content/week6-twosample.html#a-seeming-paradox",
    "title": "Two sample inference",
    "section": "A seeming paradox",
    "text": "A seeming paradox\n\nIf you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo keep in mind:\n\nstatistical significance \\(\\neq\\) practical significance\nalways check your point estimates"
  },
  {
    "objectID": "content/week6-twosample.html#a-statistical-trap",
    "href": "content/week6-twosample.html#a-statistical-trap",
    "title": "Two sample inference",
    "section": "A statistical trap",
    "text": "A statistical trap\n\nIf you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo keep in mind:\n\nstatistical significance \\(\\neq\\) practical significance\nalways check your point estimates"
  },
  {
    "objectID": "content/lab8-twosample.html",
    "href": "content/lab8-twosample.html",
    "title": "Lab 8: Two-sample inference",
    "section": "",
    "text": "This lab focuses on two-sample inference for differences in population means. The main objectives are:\n\nLearn to implement two-sample \\(t\\) tests in R\nPractice distinguishing directional and nondirectional tests and providing appropriate specifications to the t.test(...) function in R\n\nThe lab uses several datasets for which we will consider two-sample comparisons:\n\nfinch: mean finch beak depths in generations before and after a drought on Daphne Major\ntemps: body temperatures and heart rates for men and women\n\n\nlibrary(tidyverse)\nload('data/finch.RData')\nload('data/temps2.RData')\n\nExamples will utilize the finch data; you’ll practice using the temps data. Here are the summary statistics for the finch data for reference:\n\nfinch |&gt;\n  group_by(year) |&gt;\n  summarize(depth.mean = mean(depth),\n            depth.sd = sd(depth),\n            n = n())\n\n# A tibble: 2 × 4\n   year depth.mean depth.sd     n\n  &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1  1976       9.45    0.962    58\n2  1978      10.2     0.807    65\n\n\n\nChecking assumptions for two-sample tests\nA two-sample \\(t\\) test can be used whenever two one-sample tests are appropriate. So, to check assumptions, we need to inspect the frequency distributions of the variable of interest in both samples.\n\nOption A: two histograms\nOne way to do this is to make two separate histograms. To do that, we’ll need to separate the samples. This can be done by ‘filtering’ observations according to whether year is 1978 or 1976.\n\n# separate samples\nfinch.1978 &lt;- finch |&gt; filter(year == 1978)\nfinch.1976 &lt;- finch |&gt; filter(year == 1976)\n\n# extract depths\ndepth.1978 &lt;- finch.1978$depth\ndepth.1976 &lt;- finch.1976$depth\n\n# make histograms\nhist(depth.1978)\n\n\n\n\n\n\n\nhist(depth.1976)\n\n\n\n\n\n\n\n\nBoth distributions are a bit left-skewed, but each sample is large enough that this isn’t a problem for performing the test.\n\n\n\n\n\n\nYour turn 1\n\n\n\nFilter the temps data by sex to separate the samples, and make histograms of the heart rates. Comment on whether assumptions seem to be met.\n\n# separate samples\n\n# extract heart rates\n\n# make histograms\n\n\n\n\n\nOption B: side-by-side boxplots\nA slightly more efficient alternative is to make side-by-side boxplots. This doesn’t involve filtering the data, and will produce just a single graphic.\nHowever, some details of the distribution (such as multiple modes) may not be evident from the boxplots, so it’s not a perfect substitute for checking histograms.\n\n# side-by-side boxplots\nboxplot(depth ~ year, data = finch, horizontal = T)\n\n\n\n\n\n\n\n\nHere we want to see two things:\n\napproximate symmetry of boxes\nfew to no large outliers\n\nWhile there is a bit of left skewness, the sample sizes are large enough that it’s not a concern.\n\n\n\n\n\n\nYour turn 2\n\n\n\nMake side-by-side boxplots for heart rate and reassess test assumptions.\n\n# side-by-side boxplots for heart rate\n\n\n\n\n\n\nTwo-sample \\(t\\)-tests\nGiven that assumptions seem plausible (both samples show little skew and few outliers, and are sufficiently large), we can go ahead with the test. To test whether the drought imposed selection pressure on the finch population, we want to know whether finch beak depth increased after the drought.\nObserve, first, which sample appears first in the dataset: 1976. R will treat this as the first sample; to keep track of directions, we’ll want to formulate the hypotheses as a comparison between 1976 (first sample) and 1978 (second sample).\nWe want to test whether the mean of the first sample is less than the mean of the second:\n\\[\n\\begin{cases}\nH_0: &\\mu_{1976} = \\mu_{1978} \\\\\nH_A: &\\mu_{1976} &lt; \\mu_{1978}\n\\end{cases}\n\\] Let’s carry out the test at the 5% significance level. The inputs to t.test(...) that implement this test are:\n\na formula &lt;VARIABLE&gt; ~ &lt;SAMPLE&gt; as the first argument: formula = depth ~ year\na data frame containing the variable names mentioned in the formula: data = finch\na null value for the difference: mu = 0\nan alternative: alternative = 'less'\na confidence level to complement the significance level of the test: conf.level = 0.95\n\n\n# perform t test (notice which group comes first)\nt.test(formula = depth ~ year, data = finch, mu = 0, alternative = 'less', conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n\n\nTake a moment to inspect the output and identify each number appearing. We’d report the test result as follows:\n\nThe data provide very strong evidence that mean beak depth increased in the generation of finches following the drought (T = -4.5727 on 111.79 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean beak depth is estimated to have increased by at least 0.4699 mm, with a point estiamte of 0.7373 mm (SE 0.1612).\n\nThe point estimate and standard error can be retrieved by storing the output of t.test(...).\n\n# store t test result\ntt.rslt &lt;- t.test(formula = depth ~ year, data = finch, mu = 0, alternative = 'less', conf.level = 0.95)\n\n# estimates\ntt.rslt$estimate\n\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n\n# estimate for difference in means\ntt.rslt$estimate |&gt; diff()\n\nmean in group 1978 \n          0.737321 \n\n# standard error for estimate of difference in means\ntt.rslt$stderr\n\n[1] 0.1612445\n\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nTest whether mean heart rate differs between men and women at the 1% significance level. Report the test result, confidence interval, and point estimate and standard error for the difference in means.\n\n# perform t test\n\n# store t test result\n\n# estimate for difference in means\n\n# standard error\n\n\n\n\n\nPractice problems\n\nUsing the temps2 dataset, test whether mean body temperature is lower for men.\n\nCheck the assumptions for the test by making both a pair of histograms and side-by-side boxplots.\nPerform the test at the 1% significance level.\nReport the test result, confidence interval, and point estimate and standard error for the difference in means.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 2.2854, df = 127.51, p-value = 0.01197\nalternative hypothesis: true difference in means between group female and group male is greater than 0\n99 percent confidence interval:\n -0.008923783          Inf\nsample estimates:\nmean in group female   mean in group male \n            98.39385             98.10462 \n\n\nmean in group female   mean in group male \n            98.39385             98.10462 \n\n\nmean in group male \n        -0.2892308 \n\n\n[1] 0.126554\n\n\n\nUsing the brfss2 data, test whether actual body weight exceeds desired body weight by more for women than for men.\n\nCheck the assumptions for the test by making both a pair of histograms and side-by-side boxplots.\nPerform the test at the 1% significance level.\nReport the test result, confidence interval, and point estimate and standard error for the difference in means.\n\n\n\n\n# A tibble: 6 × 4\n  sex   weight wtdesire weight.diff\n  &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 m        265      225          40\n2 m        150      150           0\n3 m        137      150         -13\n4 f        159      125          34\n5 f        145      125          20\n6 f        125      120           5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  weight.diff by sex\nt = 2.048, df = 38.875, p-value = 0.02368\nalternative hypothesis: true difference in means between group f and group m is greater than 0\n99 percent confidence interval:\n -3.108832       Inf\nsample estimates:\nmean in group f mean in group m \n      26.354839        9.517241 \n\n\nmean in group m \n       -16.8376 \n\n\n[1] 8.22135"
  },
  {
    "objectID": "content/week6-power.html#decision-errors",
    "href": "content/week6-power.html#decision-errors",
    "title": "Power analyses",
    "section": "Decision errors",
    "text": "Decision errors\n\nThere are two ways to make a mistake in a hypothesis test – two “error types”.\n\n\n\n\n\n\n\n\n\n\nReject \\(H_0\\)\nFail to reject \\(H_0\\)\n\n\n\n\nTrue \\(H_0\\)\ntype I error\ncorrect decision\n\n\nFalse \\(H_0\\)\ncorrect decision\ntype II error\n\n\n\n\n\nAny statistical test will have certain error rates:\n\ntype I error rate is denoted \\(\\alpha\\)\ntype II error rate is denoted \\(1 - \\beta\\)\n\n\nThe significance level of a test is its type I error rate.\n\nreject when \\(p &lt; \\alpha\\) \\(\\Longleftrightarrow\\) mistakenly reject \\(\\alpha\\times 100\\)% of the time\n\nBut we don’t know the type II error rate!\n\ndepends on which alternative parameter value is true"
  },
  {
    "objectID": "content/week6-power.html#p-values-and-errors",
    "href": "content/week6-power.html#p-values-and-errors",
    "title": "Power analyses",
    "section": "\\(p\\)-values and errors",
    "text": "\\(p\\)-values and errors\nA \\(p\\)-value measures how often you’d see a more extreme test statistic if \\(H_0\\) were true.\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\nIf there is no effect of cloud seeding, then we would see \\(T &gt; 1.9982\\) for 2.689% of samples.\n\n\nBut just because the data are unusual doesn’t strictly entail \\(H_0\\) is false.\n\nwhile unlikely, our sample could have been one of the 26 in 1000 where \\(T\\) exceeds 1.9982 despite no effect\nby rejecting here (when \\(T = 1.9982\\)) we are willing to be wrong 2.689% of the time\n\nMore generally, by rejecting when \\(p &lt; \\alpha\\) we are willing to be wrong \\(\\alpha\\times 100\\)% of the time."
  },
  {
    "objectID": "content/week6-power.html#a-different-kind-of-error",
    "href": "content/week6-power.html#a-different-kind-of-error",
    "title": "Power analyses",
    "section": "A different kind of error?",
    "text": "A different kind of error?\n\nBut you can also make a mistake when \\(H_0\\) is false!\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'two.sided')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.05377\nalternative hypothesis: true difference in means between group seeded and group unseeded is not equal to 0\n95 percent confidence interval:\n  -4.764295 559.556603\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\nWe’d see \\(|T| &gt; 1.9982\\) for 5.377% of samples if there’s no effect. But what if there is an effect?\n\nThe two-sided test fails to reject at the 5% significance level (\\(p &gt; 0.05\\)), but that doesn’t completely rule out \\(H_A\\).\n\nthe estimated effect – increase of 277.4 acre-feet – could be too small relative to the variability in rainfall\nhard to say how often we’d make this kind of mistake without knowing the real difference\n\nThe rate of fail-to-reject errors depends on the (unknown) true parameter value."
  },
  {
    "objectID": "content/week6-power.html#p-values-and-false-rejections",
    "href": "content/week6-power.html#p-values-and-false-rejections",
    "title": "Power analyses",
    "section": "\\(p\\)-values and false rejections",
    "text": "\\(p\\)-values and false rejections\n\nA \\(p\\)-value captures how often you’d make a mistake if \\(H_0\\) were true.\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\nIf there is no effect of cloud seeding, then we would see \\(T &gt; 1.9982\\) for 2.689% of samples.\n\nThe test rejects at the 5% significance level (\\(p &lt; 0.05\\)), but that doesn’t completely rule out \\(H_0\\).\n\nwhile unlikely, our sample could have been one of the 26 in 1000 where \\(T\\) exceeds 1.9982 despite no effect\nby rejecting here (when \\(T = 1.9982\\)) we are willing to be wrong 2.689% of the time\n\nBy rejecting when \\(p &lt; \\alpha\\) we are willing to be wrong \\(\\alpha\\times 100\\)% of the time."
  },
  {
    "objectID": "content/week6-power.html#larger-effect-size",
    "href": "content/week6-power.html#larger-effect-size",
    "title": "Power analyses",
    "section": "Larger effect size",
    "text": "Larger effect size\n\n\nSummary stats for cloud data:\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nmean\nsd\nn\n\n\n\n\nseeded\n442\n650.8\n26\n\n\nunseeded\n164.6\n278.4\n26\n\n\n\n\n\nWe can approximate the type II error rate by:\n\nsimulating datasets with matching statistics\nperforming two-sided tests of no difference\ncomputing the proportion of fail-to-reject decisions\n\n\n\ntype2sim(delta = 350, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf in fact the effect size is exactly 400, a level 5% test with similar data will fail to reject ~40% of the time."
  },
  {
    "objectID": "content/week6-power.html#smaller-effect-size",
    "href": "content/week6-power.html#smaller-effect-size",
    "title": "Power analyses",
    "section": "Smaller effect size",
    "text": "Smaller effect size\n\n\nSummary stats for cloud data:\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nmean\nsd\nn\n\n\n\n\nseeded\n442\n650.8\n26\n\n\nunseeded\n164.6\n278.4\n26\n\n\n\n\n\nWe can approximate the type II error rate by:\n\nsimulating datasets with matching statistics\nperforming two-sided tests of no difference\ncomputing the proportion of fail-to-reject decisions\n\n\n\ntype2sim(delta = 100, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf in fact the effect size is exactly 100, a level 5% test with similar data will fail to reject ~90% of the time."
  },
  {
    "objectID": "content/week6-power.html#practical-constraints",
    "href": "content/week6-power.html#practical-constraints",
    "title": "Power analyses",
    "section": "Practical constraints",
    "text": "Practical constraints\n\n\nMinimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:\n\n\n\n\n\n\n\n\n\nAssumes \\(\\sigma = 650\\) for a conservative estimate.\n\nIt may not be affordable to obtain data for 144 days per treatment group (pilots and planes are expensive). What is achievable within constraints?\n\npower of 0.8 will require n = 59 per group\n\n138 days total\n\ndecreasing to 0.7 will require n = 45 per group\n\n90 days total\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#todays-agenda",
    "href": "content/week7-anova.html#todays-agenda",
    "title": "Analysis of Variance",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] inference comparing several population means\n[lab] fitting ANOVA models in R"
  },
  {
    "objectID": "content/week7-anova.html#chicks",
    "href": "content/week7-anova.html#chicks",
    "title": "Analysis of Variance",
    "section": "Chicks",
    "text": "Chicks\n\n\nYou previously considered this data on chick weights by diet:\n\n\n\n\n\n\n\n\n\nAre observed differences sufficiently large to conclude diets cause growth differences?\n\nHere we have four means to compare rather than just two.\nPoint estimates and standard errors are:\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nse\n\n\n\n\n1\n170.4\n13.45\n\n\n2\n205.6\n22.22\n\n\n3\n258.9\n20.63\n\n\n4\n233.9\n12.52\n\n\n\n\n\n\nDo the population means differ by diet? How do you test this?"
  },
  {
    "objectID": "content/week7-anova.html#an-ad-hoc-approach",
    "href": "content/week7-anova.html#an-ad-hoc-approach",
    "title": "Analysis of Variance",
    "section": "An ad hoc approach",
    "text": "An ad hoc approach\n\n\nOne option is to make confidence intervals and check for overlap:\n\n\n\n\n\n\n\n\n\nIdea: if any of the intervals don’t overlap, then there’s a difference.\n\ne.g., diet 1 differs from diets 3 and 4\n\n\nNot satisfactory, because interval coverage is not simultaneous:\n\nintervals won’t always cover or not cover all four means at the same time\nso the “joint” coverage is less than 95%\n\n\nThere is extra uncertainty due to the multiplicity; inferences must account for this.\n\nStrategy:\n\nfirst test for any group differences\nthen determine which groups differ"
  },
  {
    "objectID": "content/week7-anova.html#a-hypothesis-to-test",
    "href": "content/week7-anova.html#a-hypothesis-to-test",
    "title": "Analysis of Variance",
    "section": "A hypothesis to test",
    "text": "A hypothesis to test\nLet \\(\\mu_i = \\text{mean weight on diet } i\\). Consider testing the hypotheses:\n\\[\\begin{align*}\n&H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\quad &(\\text{all means are the same}) \\\\\n&H_A: \\mu_i \\neq \\mu_j \\quad &(\\text{at least one pair differs})\n\\end{align*}\\]\nConsider this hypothetical scenario:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe means differ more at right, but is it “enough”?\n\n\n\nHow variable are the means? How variable are the groups (i.e., error bars)?"
  },
  {
    "objectID": "content/week7-anova.html#partitioning-variation",
    "href": "content/week7-anova.html#partitioning-variation",
    "title": "Analysis of Variance",
    "section": "Partitioning variation",
    "text": "Partitioning variation\n\nPartitioning variation into two or more components is called “analysis of variance”\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the chick data, two sources of variability:\n\ngroup variability between diets\nerror variability among chicks\n\nThe analysis of variance (ANOVA) model:\n\\[\\color{grey}{\\text{total variation}} = \\color{red}{\\text{group variation}} + \\color{blue}{\\text{error variation}}\\]\n\n\nWe’ll base the test on the ratio \\(F = \\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\)."
  },
  {
    "objectID": "content/week7-anova.html#analysis-of-variance",
    "href": "content/week7-anova.html#analysis-of-variance",
    "title": "Analysis of Variance",
    "section": "Analysis of variance",
    "text": "Analysis of variance\n\nThe analysis of variance is based on a type of summary called an \\(F\\) statistic.\n\nOur test statistic is the ratio:\n\\[F = \\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\]\n\na large \\(F\\) indicates that more variability is attributed to groups than individuals\na small \\(F\\) indicates that more variability is attributed to individuals than groups\n\nTo implement this idea, we need measures of group and error variation"
  },
  {
    "objectID": "content/week7-anova.html#the-f-statistic",
    "href": "content/week7-anova.html#the-f-statistic",
    "title": "Analysis of Variance",
    "section": "The \\(F\\) statistic",
    "text": "The \\(F\\) statistic\n\nThe \\(F\\) statistic measures the variability attributable to group differences relative to variability attributable to individual differences.\n\n\n\nNotation:\n\n\\(\\bar{x}\\): “grand” mean of all observations\n\\(\\bar{x}_i\\): mean of observations in group \\(i\\)\n\\(s_i\\): SD of observations in group \\(i\\)\n\\(k\\) groups\n\\(n\\) total observations\n\\(n_i\\) observations per group\n\n\nMeasures of variability:\n\\[\\color{red}{MSG} = \\frac{1}{k - 1}\\sum_i n_i(\\bar{x}_i - \\bar{x})^2 \\quad(\\color{red}{\\text{group}})\\] \\[\\color{blue}{MSE} = \\frac{1}{n - k}\\sum_i (n_i - 1)s_i^2 \\quad(\\color{blue}{\\text{error}})\\] Ratio:\n\\[F = \\frac{\\color{red}{MSG}}{\\color{blue}{MSE}} \\quad\\left(\\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\right)\\]"
  },
  {
    "objectID": "content/week7-anova.html#analysis-of-variance-table",
    "href": "content/week7-anova.html#analysis-of-variance-table",
    "title": "Analysis of Variance",
    "section": "Analysis of variance table",
    "text": "Analysis of variance table\nThe results of an analysis of variance are traditionally displayed in a table.\n\n\n\n\n\n\n\n\n\n\n\nSource\ndegrees of freedom\nSum of squares\nMean square\nF statistic\np-value\n\n\n\n\nGroup\n\\(k - 1\\)\nSSG\n\\(MSG = \\frac{SSG}{k - 1}\\)\n\\(\\frac{MSG}{MSE}\\)\n\\(P(F &gt; F_\\text{obs})\\)\n\n\nError\n\\(n - k\\)\nSSE\n\\(MSE = \\frac{SSE}{n - k}\\)\n\n\n\n\n\n\nthe sum of square terms are ‘raw’ measures of variability\nthe mean square terms are averages adjusted for the amount of data available to estimate variability due to each source\n\nFormally, the ANOVA model says \\((n - 1)s^2 = SSG + SSE\\)."
  },
  {
    "objectID": "content/week7-anova.html#anova-in-r",
    "href": "content/week7-anova.html#anova-in-r",
    "title": "Analysis of Variance",
    "section": "ANOVA in R",
    "text": "ANOVA in R\nThe aov(...) function fits ANOVA models using a formula/dataframe specification:\n\n# fit anova model\nfit &lt;- aov(weight ~ diet, data = chicks)\n\n# generate table\nsummary(fit)\n\n\n\n\nAnalysis of Variance Model\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ndiet\n3\n55881\n18627\n5.464\n0.002909\n\n\nResiduals\n42\n143190\n3409\nNA\nNA\n\n\n\n\n\nThe typical style for interpretation closely follows that of previous inferences for the mean:\n\nThe data provide strong evidence of an effect of diet on mean weight (F = 5.464 on 3 and 42 df, p = 0.0029)."
  },
  {
    "objectID": "content/week7-anova.html#interpreting-the-f-statistic",
    "href": "content/week7-anova.html#interpreting-the-f-statistic",
    "title": "Analysis of Variance",
    "section": "Interpreting the \\(F\\) statistic",
    "text": "Interpreting the \\(F\\) statistic\n\nF = 5.4636 means the proportion of variation in weight attributable to diets is 5.46 times greater than the proportion of variation attributable to chicks.\n\nThe significance of this result is measured by a \\(p\\) value:\n\nif there is in fact no difference in means, then 0.291% of samples (i.e., 2 in 1000) would produce more diet-to-diet variability than what we observed.\n\nThe \\(p\\)-value is based on an \\(F\\) model for the sampling distribution. This is a parametric model.\n\nparameters are numerator and denominator degrees of freedom\nassumes underlying population distributions for each group are well-approximated by a normal model"
  },
  {
    "objectID": "content/week7-anova.html#summing-up",
    "href": "content/week7-anova.html#summing-up",
    "title": "Analysis of Variance",
    "section": "Summing up",
    "text": "Summing up\n\nThe ANOVA setup (for us) is comparing \\(k\\) population means.\n\nHypotheses:\n\\[\\begin{align*}\n&H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\quad &(\\text{all means are the same}) \\\\\n&H_A: \\mu_i \\neq \\mu_j \\quad &(\\text{at least one pair differs})\n\\end{align*}\\]\nAssumptions:\n\npopulation standard deviations are the same for every group\ngroupwise population distributions follow a normal model\n\nModel and approach:\n\npartition total variation into group and error components \\((SST = SSG + SSE)\\)\nreject \\(H_0\\) if group variation is sufficiently large relative to error variation"
  },
  {
    "objectID": "content/week7-anova.html#another-example-treating-anorexia",
    "href": "content/week7-anova.html#another-example-treating-anorexia",
    "title": "Analysis of Variance",
    "section": "Another example: treating anorexia",
    "text": "Another example: treating anorexia\n\n\nWeight change was measured for 72 young female anorexia patients randomly allocated to three treatment groups:\n\ncognitive behavioral therapy (CBT)\nfamily treatment (FT)\na control (Cont)\n\nGrouped summary statistics:\n\n\n\n\n\n\n\n\n\n\n\ntreat\npost - pre\nsd\nn\n\n\n\n\nCBT\n3.007\n7.309\n29\n\n\nCont\n-0.45\n7.989\n26\n\n\nFT\n7.265\n7.157\n17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWere any of the treatments more effective than others?"
  },
  {
    "objectID": "content/week7-anova.html#another-example-treating-anorexia-1",
    "href": "content/week7-anova.html#another-example-treating-anorexia-1",
    "title": "Analysis of Variance",
    "section": "Another example: treating anorexia",
    "text": "Another example: treating anorexia\n\n# fit anova model\nfit &lt;- aov(change ~ treat, data = anorexia)\n\n# generate table\nsummary(fit)\n\n\n\n\nAnalysis of Variance Model\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ntreat\n2\n614.6\n307.3\n5.422\n0.006499\n\n\nResiduals\n69\n3911\n56.68\nNA\nNA\n\n\n\n\n\n\nThe data provide strong evidence of an effect of therapeutic treatment on mean weight change among young women with anorexia (F = 5.422 on 2 and 69 degrees of freedom, p = 0.0065).\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#checking-assumptions",
    "href": "content/week7-anova.html#checking-assumptions",
    "title": "Analysis of Variance",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\n\nThe ANOVA test assumes:\n\nthe distribution of values is symmetric and unimodal within each group\nthe variability (standard deviation) is roughly the same across groups\n\nTo check these assumptions:\n\ncompare group standard deviations for similarity\nvisually inspect distributions within each group for approximate symmetry\n\nSimilar to the \\(t\\) test, greater departures from these assumptions are allowable for larger sample sizes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nse\nsd\nn\n\n\n\n\n1\n170.4\n13.45\n55.44\n17\n\n\n2\n205.6\n22.22\n70.25\n10\n\n\n3\n258.9\n20.63\n65.24\n10\n\n\n4\n233.9\n12.52\n37.57\n9"
  },
  {
    "objectID": "content/week7-anova.html#lab-anova-in-r",
    "href": "content/week7-anova.html#lab-anova-in-r",
    "title": "Analysis of Variance",
    "section": "Lab: ANOVA in R",
    "text": "Lab: ANOVA in R\n\n\nLab 11 demonstrates the basic process of performing an analysis of variance:\n\nPrepare a graphical display of the data\nCalculate grouped summaries\nFit an ANOVA model and construct an ANOVA table\n\n\nHere is a basic template for these steps\n\n# graphical display: boxplot\nboxplot([VARIABLE] ~ [GROUPS], data = [DATASET])\n\n# grouped summaries\n[DATASET] |&gt;\n  group_by([GROUPS]) |&gt;\n  summarize([SUMMARY1 NAME] = fn1([VARIABLE]),\n            [SUMMARY2 NAME] = fn2([VARIABLE]))\n\n# fit ANOVA model\nfit &lt;- aov([VARIABLE] ~ [GROUPS], data = [DATASET])\n\n# generate ANOVA table\nsummary(fit)\n\n\n\nYour task is simply to recreate examples from lecture by executing provided commands."
  },
  {
    "objectID": "content/week7-anova.html#practice-problem",
    "href": "content/week7-anova.html#practice-problem",
    "title": "Analysis of Variance",
    "section": "Practice problem",
    "text": "Practice problem\n\n\nFemale mice were randomly assigned to six treatment groups to investigate whether restricting dietary intake increases life expectancy:\n\n[NP] mice ate unlimited amount of nonpurified, standard diet\n[N/N85] normal diet before weaning and normal diet after weaning (85 kcal/wk)\n[N/R50] normal diet before weaning and reduced calorie diet after weaning (50 kcal/wk)\n[N/R40] normal diet before weaning and reduced diet after weaning (40 Kcal/wk)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiet\nlife.mean\nlife.sd\nn\n\n\n\n\nNP\n27.4\n6.134\n49\n\n\nN/N85\n32.69\n5.125\n57\n\n\nN/R50\n42.3\n7.768\n71\n\n\nN/R40\n45.12\n6.703\n60\n\n\n\n\n\n\nTest whether diet restriction has an effect on longevity. Make a boxplot of the data, write the hypotheses you are testing, produce an ANOVA table, and write a narrative summary of the test result.\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-anova.html#from-before",
    "href": "content/week7-anova.html#from-before",
    "title": "Analysis of Variance",
    "section": "From before",
    "text": "From before"
  },
  {
    "objectID": "content/week7-anova.html#hypotheses",
    "href": "content/week7-anova.html#hypotheses",
    "title": "Analysis of Variance",
    "section": "Hypotheses",
    "text": "Hypotheses\nLet \\(\\mu_i = \\text{mean weight on diet } i = 1, 2, 3, 4\\).\nThe hypothesis that there are no differences in means by diet is:\n\\[\nH_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\quad (\\text{no difference in means})\n\\]\nThe alternative, if this is false, is that there is at least one difference:\n\\[\nH_A: \\mu_i \\neq \\mu_j \\quad (\\text{at least one difference})\n\\]"
  },
  {
    "objectID": "content/week7-anova.html#approach",
    "href": "content/week7-anova.html#approach",
    "title": "Analysis of Variance",
    "section": "Approach",
    "text": "Approach\nConsider this hypothetical scenario:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe means differ more at right, but is it “enough”?\n\n\n\nHow variable are the means? How variable are the groups (i.e., error bars)?"
  },
  {
    "objectID": "content/week7-anova.html#how-much-difference-is-enough",
    "href": "content/week7-anova.html#how-much-difference-is-enough",
    "title": "Analysis of Variance",
    "section": "How much difference is enough?",
    "text": "How much difference is enough?\nHere are two made-up examples of four sample means.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy does it look like there’s a difference at right but not at left?\n\n\n\nThink about the \\(t\\)-test: we say there’s a difference if \\(T = \\frac{\\text{estimate} - \\text{hypothesis}}{\\text{variability}}\\) is large.\nSame idea here: we see differences if they are big relative to the variability in estimates."
  },
  {
    "objectID": "content/week7-anova.html#approach-partitioning-variation",
    "href": "content/week7-anova.html#approach-partitioning-variation",
    "title": "Analysis of Variance",
    "section": "Approach: partitioning variation",
    "text": "Approach: partitioning variation\n\nPartitioning variation into two or more components is called “analysis of variance”\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the chick data, two sources of variability:\n\ngroup variability between diets\nerror variability among chicks\n\nA model:\n\\[\\color{grey}{\\text{total variation}} = \\color{red}{\\text{group variation}} + \\color{blue}{\\text{error variation}}\\]\n\n\nWe’ll use the ratio \\(\\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\) to measure how much means differ relative to chicks."
  },
  {
    "objectID": "content/week7-anova.html#example-calculation",
    "href": "content/week7-anova.html#example-calculation",
    "title": "Analysis of Variance",
    "section": "Example calculation",
    "text": "Example calculation\n\n\nGrouped summaries:\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nsd\nn\n\n\n\n\n1\n170.4\n55.44\n17\n\n\n2\n205.6\n70.25\n10\n\n\n3\n258.9\n65.24\n10\n\n\n4\n233.9\n37.57\n9\n\n\n\n\n\nUngrouped summaries:\n\n\n\n\n\n\n\n\n\nmean\nn\n\n\n\n\n209.7\n46\n\n\n\n\n\n\n\\[\\begin{align*}\nMSG &= \\frac{1}{k - 1}\\sum_i n_i(\\bar{x}_i - \\bar{x})^2 \\\\\n&= \\hspace{10cm}\\\\\\\\\n&= 18627 \\\\\\\\\nMSE &= \\frac{1}{n - k}\\sum_i (n_i - 1)s_i^2 \\\\\n&= \\hspace{10cm} \\\\\\\\\n&= 3409.3\\\\\\\\\nF &= \\frac{MSG}{MSE} = \\hspace{5cm} = 5.4636\n\\end{align*}\\]"
  },
  {
    "objectID": "content/week7-anova.html#sampling-distribution-for-the-f-statistic",
    "href": "content/week7-anova.html#sampling-distribution-for-the-f-statistic",
    "title": "Analysis of Variance",
    "section": "Sampling distribution for the \\(F\\) statistic",
    "text": "Sampling distribution for the \\(F\\) statistic"
  },
  {
    "objectID": "content/week7-anova.html#sampling-distribution-for-f",
    "href": "content/week7-anova.html#sampling-distribution-for-f",
    "title": "Analysis of Variance",
    "section": "Sampling distribution for \\(F\\)",
    "text": "Sampling distribution for \\(F\\)\n\n\nIf the data satisfy these conditions:\n\nthe distribution of values is symmetric and unimodal within each group\nthe variability (standard deviation) is roughly the same across groups\n\nThen the \\(F\\) statistic has a sampling distribution well-approximated by an \\(F_{k - 1, n - k}\\) model.\n\nnumerator degrees of freedom \\(k - 1\\)\ndenominator degrees of freedom \\(n - k\\)\n\n\n\n\n\n\n\n\\(F\\) models for several different numerator degrees of freedom \\(k - 1\\) with fixed \\(n = 30\\)."
  },
  {
    "objectID": "content/week7-anova.html#variance-ratio",
    "href": "content/week7-anova.html#variance-ratio",
    "title": "Analysis of Variance",
    "section": "Variance ratio",
    "text": "Variance ratio\n\nThe analysis of variance is based on a type of summary called an \\(F\\) statistic.\n\nOur test statistic is the ratio:\n\\[F = \\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\]\n\na large \\(F\\) indicates that more variability is attributed to groups than individuals\na small \\(F\\) indicates that more variability is attributed to individuals than groups\n\nTo implement this idea, we need measures of group and error variation"
  },
  {
    "objectID": "content/week7-anova.html#the-f-statistic-a-variance-ratio",
    "href": "content/week7-anova.html#the-f-statistic-a-variance-ratio",
    "title": "Analysis of Variance",
    "section": "The \\(F\\) statistic: a variance ratio",
    "text": "The \\(F\\) statistic: a variance ratio\n\nThe \\(F\\) statistic measures variability attributable to group differences relative to variability attributable to individual differences.\n\n\n\nNotation:\n\n\\(\\bar{x}\\): “grand” mean of all observations\n\\(\\bar{x}_i\\): mean of observations in group \\(i\\)\n\\(s_i\\): SD of observations in group \\(i\\)\n\\(k\\) groups\n\\(n\\) total observations\n\\(n_i\\) observations per group\n\n\nMeasures of variability:\n\\[\\color{red}{MSG} = \\frac{1}{k - 1}\\sum_i n_i(\\bar{x}_i - \\bar{x})^2 \\quad(\\color{red}{\\text{group}})\\] \\[\\color{blue}{MSE} = \\frac{1}{n - k}\\sum_i (n_i - 1)s_i^2 \\quad(\\color{blue}{\\text{error}})\\] Ratio:\n\\[F = \\frac{\\color{red}{MSG}}{\\color{blue}{MSE}} \\quad\\left(\\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\right)\\]"
  },
  {
    "objectID": "content/week7-anova.html#more-than-two-means",
    "href": "content/week7-anova.html#more-than-two-means",
    "title": "Analysis of Variance",
    "section": "More than two means?",
    "text": "More than two means?\n\n\nYou previously considered this data on chick weights at 20 days of age by diet:\n\n\n\n\n\n\n\n\n\n\nHere we have four means to compare rather than just two.\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nse\nsd\nn\n\n\n\n\n1\n170.4\n13.45\n55.44\n17\n\n\n2\n205.6\n22.22\n70.25\n10\n\n\n3\n258.9\n20.63\n65.24\n10\n\n\n4\n233.9\n12.52\n37.57\n9\n\n\n\n\n\n\n\n\nDoes mean weight at 20 days differ by diet? How do you test this?"
  },
  {
    "objectID": "content/week7-anova.html#hypotheses-for-a-difference-in-means",
    "href": "content/week7-anova.html#hypotheses-for-a-difference-in-means",
    "title": "Analysis of Variance",
    "section": "Hypotheses for a difference in means",
    "text": "Hypotheses for a difference in means\nLet \\(\\mu_i = \\text{mean weight on diet } i = 1, 2, 3, 4\\).\nThe hypothesis that there are no differences in means by diet is:\n\\[\nH_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\quad (\\text{no difference in means})\n\\]\nThe alternative, if this is false, is that there is at least one difference:\n\\[\nH_A: \\mu_i \\neq \\mu_j \\quad (\\text{at least one difference})\n\\]"
  },
  {
    "objectID": "content/week7-anova.html#anova-f-test-by-hand",
    "href": "content/week7-anova.html#anova-f-test-by-hand",
    "title": "Analysis of Variance",
    "section": "ANOVA \\(F\\) test “by hand”",
    "text": "ANOVA \\(F\\) test “by hand”"
  },
  {
    "objectID": "content/week7-anova.html#p-values-for-the-f-test",
    "href": "content/week7-anova.html#p-values-for-the-f-test",
    "title": "Analysis of Variance",
    "section": "\\(p\\)-values for the \\(F\\) test",
    "text": "\\(p\\)-values for the \\(F\\) test\n\n\nTo test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\\\\nH_0: &\\mu_i \\neq \\mu_j \\quad\\text{for some}\\quad i \\neq j\n\\end{cases}\n\\] Calculate the \\(F\\) statistic:\n\n# ingredients of mean squares\nk &lt;- nrow(chicks.summary)\nn &lt;- nrow(chicks)\nn.i &lt;- chicks.summary$n\nxbar.i &lt;- chicks.summary$mean\ns.i &lt;- chicks.summary$sd\nxbar &lt;- mean(chicks$weight)\n\n# mean squares\nmsg &lt;- sum(n.i*(xbar.i - xbar)^2)/(k - 1)\nmse &lt;- sum((n.i - 1)*s.i^2)/(n - k)\n\n# f statistic\nfstat &lt;- msg/mse\nfstat\n\n[1] 5.463598\n\n\nAnd reject \\(H_0\\) when \\(F\\) is large.\n\nFor a significance level \\(\\alpha\\) test, reject \\(H_0\\) when \\(\\underbrace{P(F &gt; F_\\text{obs})}_\\text{p-value} &lt; \\alpha\\).\n\n\n\n\n\n\n\n\n\n\npf(fstat, 4 - 1, 46 - 4, lower.tail = F)\n\n[1] 0.002909054"
  },
  {
    "objectID": "content/week7-anova.html#interpreting-the-f-statistic-and-p-value",
    "href": "content/week7-anova.html#interpreting-the-f-statistic-and-p-value",
    "title": "Analysis of Variance",
    "section": "Interpreting the \\(F\\) statistic and \\(p\\)-value",
    "text": "Interpreting the \\(F\\) statistic and \\(p\\)-value\n\nF = 5.4636 means the proportion of variation in weight attributable to diets is 5.46 times greater than the proportion of variation attributable to chicks.\n\nThe statistical significance of this result is measured by the \\(p\\)-value:\n\nif there is in fact no difference in means, then 0.29% of samples (i.e., 2 in 1000) would produce more diet-to-diet variability than what we observed."
  },
  {
    "objectID": "content/week7-anova.html#interpreting-f-statistics-and-p-values",
    "href": "content/week7-anova.html#interpreting-f-statistics-and-p-values",
    "title": "Analysis of Variance",
    "section": "Interpreting \\(F\\) statistics and \\(p\\)-values",
    "text": "Interpreting \\(F\\) statistics and \\(p\\)-values\n\n\n\\[\n\\begin{cases}\nH_0: &\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\\\\nH_0: &\\mu_i \\neq \\mu_j \\quad\\text{for some}\\quad i \\neq j\n\\end{cases}\n\\]\n\\(F = \\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}} = \\frac{MSG}{MSE} = 5.4636\\).\n\n\n\n\n\n\n\n\n\n\npf(fstat, 4 - 1, 46 - 4, lower.tail = F)\n\n[1] 0.002909054\n\n\n\n\nF = 5.4636 means the proportion of variation in weight attributable to diets is 5.46 times greater than the proportion of variation attributable to chicks.\n\nThe statistical significance of this result is measured by the \\(p\\)-value:\n\nif there is in fact no difference in means, then only 0.29% of samples (i.e., 2 in 1000) would produce at least as much diet-to-diet variability as we observed.\nso in this case we reject \\(H_0\\) at the 1% level"
  },
  {
    "objectID": "content/lab9-anova.html",
    "href": "content/lab9-anova.html",
    "title": "Lab 9: Analysis of variance",
    "section": "",
    "text": "The goal of this lab is to learn how to implement analysis of variance (ANOVA) in R. In its most basic form, this comprises three steps: a visual check of the data, fitting a model and generating the ANOVA table, and interpreting results.\nRemember throughout that the first goal of an analysis of variance is to test the hypothesis of no difference in means (interpreted as no effect in the case of an experiment with random treatment allocation). In notation:\n\\[\n\\begin{cases}\nH_0: &\\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_A: &\\mu_i \\neq \\mu_j \\text{ for some } i \\neq j\n\\end{cases}\n\\]\nThis is essentially an extension of the two-sample \\(t\\) test to arbitrarily many means. We’ll use the chicks dataset to illustrate these steps and you’ll reproduce using the anorexia dataset.\nlibrary(tidyverse)\nload('data/chicks-20d.RData')\nanorexia &lt;- read_csv('data/anorexia.csv')"
  },
  {
    "objectID": "content/lab9-anova.html#footnotes",
    "href": "content/lab9-anova.html#footnotes",
    "title": "Lab 9: Analysis of variance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWeindruch, R., Walford, R.L., Fligiel, S. and Guthrie D. (1986). The Retardation of Aging in Mice by Dietary Restriction: Longevity, Cancer, Immunity and Lifetime Energy Intake, Journal of Nutrition 116(4):641–54.↩︎"
  },
  {
    "objectID": "content/week9-activity-nonparametric.html",
    "href": "content/week9-activity-nonparametric.html",
    "title": "Week 7 activity: Nonparametric inference",
    "section": "",
    "text": "library(tidyverse)\nload('data/brfss.RData')\nload('data/temps.RData')\nddt &lt;- MASS::DDT\n\nIn this activity you’ll learn about nonparametric alternatives to the \\(t\\) test for one and two means. The activity is organized much like a lab, but with extra narrative. You should read through the narrative at your own pace and try the exercises provided as you go. At the end there are two practice problems that you’ll be expected to complete before next class.\n\nBackground: parametric and nonparametric inference\nConsider the basis for the inferences developed so far: under certain conditions (typically regularity of the underlying population distribution, assessed by checking histograms for unimodality and approximate symmetry) and with sufficient sample sizes, a model is specified for the sampling distribution of a test statistic.\n\ninference for one mean: \\(t_{n - 1}\\) model for the sampling distribution of \\(T = \\frac{\\bar{x} - \\mu}{SE(\\bar{x})}\\)\ninference comparing two means: \\(t_{\\nu}\\) model for the sampling distribution of \\(T = \\frac{\\bar{x} - \\bar{y} - \\delta}{SE(\\bar{x} - \\bar{y})}\\)\ninference comparing several means: \\(F_{k - 1, n - k}\\) model for the sampling distribution of \\(F = \\frac{MSG}{MSE}\\)\n\nThese are all what are known as parametric models, because they are specified through one or two parameters that determine their exact shape. The parameters in this case are the degrees of freedom terms – \\(n - 1\\) for the one-sample \\(t\\) test, \\(\\nu\\) (usually estimated) for the two-sample \\(t\\) test, and \\(k - 1\\) and \\(n - k\\) for the \\(F\\) test.\nAs such, these procedures are examples of parametric inference – inferences that utilize a parametric model for the data and/or test statistic.\nWhen assumptions for parametric inference aren’t tenable, or when a parametric model is not available, there are so-called nonparametric methods of inference: methods that don’t depend on a parametric model such as the \\(t\\) or \\(F\\) models we’ve learned about in class.\nWe will consider specifically nonparametric procedures based on ranks, i.e., ordering observations from smallest to largest.\n\n\nMotivating examples\nIn practice, the situation that most often leads an analyst to consider rank-based nonparametric methods is that the assumptions for the \\(t\\) test either don’t hold or are difficult to assess.\nBelow are two such situations you’ve already encountered in this class.\n\nSmall sample sizes\nWhen sample sizes are small, it’s awkward to assess assumptions for parametric inference, because with few observations histograms can lack any discernible shape. For example, the most we can say about the following data on heart rates for 19 women and 20 men is that there are no evident outliers.\n\nheart.m &lt;- temps |&gt; filter(sex == 'male') |&gt; pull(heart.rate)\nheart.f &lt;- temps |&gt; filter(sex == 'female') |&gt; pull(heart.rate)\n\npar(mfrow = c(1, 2))\nhist(heart.m)\nhist(heart.f)\n\n\n\n\n\n\n\n\nIn practical terms, the \\(t\\) test is likely still fine under these circumstances; however, some may wish to consider an inference for comparing heart rates between groups that doesn’t depend on distributional assumptions.\n\n\nAssumptions don’t hold\nOn occasion you may go to check assumptions and find that they’re clearly violated. For example, the pairwise differences between actual and desired weights from BRFSS respondents showed clear skewness and several large outliers. In that case, the sample size was big enough that we overlooked the issue, but it’s not hard to imagine a similar situation cropping up with fewer observations.\nSuppose you only had 12 observations that showed the same skew and had one big outlier:\n\nset.seed(51424)\nweight.diff &lt;- sample(brfss$weight - brfss$wtdesire, 12)\nhist(weight.diff, breaks = 10)\n\n\n\n\n\n\n\n\nHere the \\(t\\) test isn’t appropriate, and using it anyway would likely result in a true significance level, coverage, and power quite different from the nominal levels specified in the test, so it would be hard to trust the result. This is a great situation to use a rank-based nonparametric alternative.\n\n\n\nInference on “location” (not mean)\nThe usual parametric inferences pertain to the population mean; not so with rank-based nonparametrics. Instead, these inferences pertain simply to “location”.\nOften “location” is characterized in terms of the “center” of a distribution so that inferences can be interpreted in a manner similar to parametric tests and intervals.\nWe will follow this convention and consider the hypotheses to be about the center(s) of the population model(s), denoted \\(c\\). For example, the two-sided test of center would test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &c = c_0 \\\\\nH_A: &c \\neq c_0\n\\end{cases}\n\\]\nFor the two-sided test of difference in centers, we will test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &c_1 = c_2 \\\\\nH_A: &c_1 \\neq c_2\n\\end{cases}\n\\]\nHowever, you should keep in mind that “location” is a more general notion that encompasses all measures of location of a distribution.\n\n\nOne-sample inference: signed rank test\nThe basic premise of this test is that, if the population model is symmetric, its center should evenly divide the data.\n\n\n\n\n\n\nWarm up\n\n\n\nConsider the following 15 measurements of DDT in kale in ppm in order from smallest to largest:\n\nsort(ddt)\n\n [1] 2.79 2.93 3.06 3.07 3.08 3.18 3.22 3.22 3.33 3.34 3.34 3.38 3.56 3.78 4.64\n\n\nSuppose you wish to test whether the center \\(c\\) of the population model is \\(c_0 = 3\\) ppm and assume a symmetric population model.\n\nHow many observations would you expect to be smaller than \\(c_0 = 3\\) if 3ppm is in fact the center?\nHow many observations are actually smaller than 3 ppm?\nBased on your answers to 1-2, do you think it is likely that in fact \\(c = 3\\)?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the population is symmetric about \\(c_0\\), you’d expect roughly half of observations (7.5) to be smaller than 3ppm.\nThe actual number of observations smaller than 3ppm is:\n\n# number of observations below 3\nsum(ddt &lt; 3)\n\n[1] 2\n\n\nThis is much less than half, so it seems unlikely that the center is actually 3ppm.\n\n\n\nThe signed rank test is a nonparametric alternative to the one-sample \\(t\\) test applicable to any symmetric population model. The particular form of symmetry and presence of outliers do not affect the test.\n\nHypotheses\nThe test can be directional or two-sided, just like the \\(t\\) test. Thus, the possible hypotheses are:\n\\[\nH_0: c = c_0 \\quad\\text{vs.}\\quad H_A: c \\mathrel{\\substack{&lt;\\\\\\neq\\\\ &gt;}} c_0\n\\]\n\n\nTest procedure\nWhile the intuition of the test is that half of observations should be smaller than the true center under population symmetry, the test statistic is not quite as direct as a tally of how many observations are below the hypothetical value. Instead, the procedure is as follows:\n\n[center] Calculate deviations \\(d_i = x_i - c_0\\)\n[rank] Sort and rank the absolute deviations \\(|d_i|\\)\n\naverage ranks in case of ties\ndrop zeros\n\n[sum] Add up the positive ‘signed ranks’ \\(\\sum_{\\text{sign}(d_i) &gt; 0} r_i\\)\n\nThis produces the test statistic:\n\\[V = \\sum_{i = 1}^n \\text{sign}(d_i) \\times R_i\\]\n\n\n\n\n\n\nCheck your understanding\n\n\n\nTry working out the rank sum procedure manually using the DDT data – it’s small enough that you could jot down the steps on some scratch paper. See if you can calculate \\(V\\).\nTo help, here are the deviations sorted smallest to largest:\n\n# calculate deviations\ndi &lt;- ddt - 3\nsort(di)\n\n [1] -0.21 -0.07  0.06  0.07  0.08  0.18  0.22  0.22  0.33  0.34  0.34  0.38\n[13]  0.56  0.78  1.64\n\n\n\nStart by writing the deviations in order of absolute value in a column.\nThen rank them 1-15 in an adjacent column. If there is a tie – e.g., two absolute deviations of 0.05 – then assign them both the average of the ranks. For example, if 0.05 occurs twice in positions 2 and 3, then give them both rank 2.5.\nWrite down the sign of the deviation in a new column.\nThen write down the “signed rank”, or product of the sign and the rank, in a fourth column.\nAdd up the positive signed ranks. This is the signed rank statistic.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis shows the steps in R, but don’t worry about the codes; the output is meant to illustrate what you would do on paper to find the rank sum statistic.\n\n# this shows the steps, column by column; focus on output\nddt.srank &lt;- tibble(di = di) |&gt;\n  mutate(abs.di = abs(di), \n         rank = rank(abs.di),\n         sign = sign(di),\n         signed.rank = sign*rank) |&gt;\n  arrange(abs.di)\nddt.srank\n\n# A tibble: 15 × 5\n        di abs.di  rank  sign signed.rank\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1  0.0600 0.0600   1       1         1  \n 2 -0.0700 0.0700   2.5    -1        -2.5\n 3  0.0700 0.0700   2.5     1         2.5\n 4  0.0800 0.0800   4       1         4  \n 5  0.180  0.180    5       1         5  \n 6 -0.21   0.21     6      -1        -6  \n 7  0.220  0.220    7.5     1         7.5\n 8  0.220  0.220    7.5     1         7.5\n 9  0.33   0.33     9       1         9  \n10  0.34   0.34    10.5     1        10.5\n11  0.34   0.34    10.5     1        10.5\n12  0.38   0.38    12       1        12  \n13  0.56   0.56    13       1        13  \n14  0.78   0.78    14       1        14  \n15  1.64   1.64    15       1        15  \n\n# signed rank statistic\nvstat &lt;- ddt.srank |&gt; filter(sign &gt; 0) |&gt; pull(signed.rank) |&gt; sum()\nvstat\n\n[1] 111.5\n\n\n\n\n\n\n\n\\(p\\)-values for the test\nJust like other hypothesis tests, the signed rank test rejects if \\(V\\) is sufficiently large in the direction of the alternative.\nA sampling distribution for \\(V\\) can be found exactly using combinatorics, or approximated using probability theory. In this caseWe won’t go into details about either approach, except to indicate that there is a sampling distribution for \\(V\\) that we can use to obtain \\(p\\)-values in the same fashion that the \\(t_{n - 1}\\) model was used to obtain \\(p\\)-values for the \\(t\\) test.\nIn this case, there are 3.2768^{4} possible sign combinations; of these, only about 0.375% give a larger value of \\(V\\). That provides a \\(p\\)-value for the test, and since \\(p = 0.0018 &lt; 0.05\\) we would reject \\(H_0\\) at the 5% significance level. This result is interpreted as:\n\nThe data provide strong evidence that the typical DDT concentration in kale is not 3ppm (signed rank statistic V = 111.5, p = 0.00375).\n\n\n\nImplementation with wilcox.test(...)\nThe implementation in R looks and functions much like t.test:\n\n# signed rank test at 1% level\nwilcox.test(ddt, mu = 3, alternative = 'two.sided', \n            exact = F, conf.int = T, conf.level = 0.99)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  ddt\nV = 111.5, p-value = 0.003751\nalternative hypothesis: true location is not equal to 3\n99 percent confidence interval:\n 3.060070 3.780032\nsample estimates:\n(pseudo)median \n       3.26001 \n\n\nSome remarks: - exact = F produces approximate \\(p\\)-values and confidence intervals; you may see warning messages if this is excluded - pseudo-median is a measure of center, but not the same as a median or mean (check!)\n\n\n\n\n\n\nYour turn 1\n\n\n\nPerform the analogous inference using the \\(t\\) test and compare the results. Do the tests agree at the 1% significance level?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe tests do not agree: the signed rank test rejects at the 1% level, but the \\(t\\) test does not.\n\n# perform t test at 1% level to compare\nt.test(ddt, mu = 3, alternative = 'two.sided', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.01151\nalternative hypothesis: true mean is not equal to 3\n99 percent confidence interval:\n 2.991996 3.664004\nsample estimates:\nmean of x \n    3.328 \n\n\n\n\n\n\n\n\n\n\n\nYour turn 2\n\n\n\nPerform the signed rank test to determine whether actual weight exceeds desired weight by more than 10lbs at the 5% significance level. Report the result in the usual narrative format. Compare your result with the inference obtained from a \\(t\\) test.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# use signed rank test to determine whether actual exceeds desired by at least 10lbs\nwilcox.test(weight.diff, mu = 10, alternative = 'greater', \n            exact = F, conf.int = T, conf.level = 0.95)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  weight.diff\nV = 61, p-value = 0.04559\nalternative hypothesis: true location is greater than 10\n95 percent confidence interval:\n 10.00003      Inf\nsample estimates:\n(pseudo)median \n       22.8617 \n\n# check t test result to compare\nt.test(weight.diff, mu = 10, alternative = 'greater', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  weight.diff\nt = 1.6127, df = 11, p-value = 0.06755\nalternative hypothesis: true mean is greater than 10\n95 percent confidence interval:\n 7.019188      Inf\nsample estimates:\nmean of x \n    36.25 \n\n\nInterpretation of the signed rank test test:\n\nThe data provide evidence that actual weight exceeds desired weight by more than 10lbs (signed rank statistic V = 61, p = 0.0456).\n\nThe signed rank test finds evidence that actual weight exceeds desired weight by more than 10lbs, where the \\(t\\) test does not.\n\n\n\n\n\n\nTwo-sample inference: rank sum test\nThe rank-sum test is a nonparametric alternative to the two-sample \\(t\\) test based on ranks. The key idea for the test is that if observations in both groups come from the same population distribution then they should be exchangeable (i.e., groupings don’t matter).\nThus, the only assumption for this test is that data are independent. The test can be directional, and thus it is possible to test the following hypotheses comparing centers:\n\\[\nH_0: c_1 = c_2\\quad\\text{vs}\\quad H_A: c_1 \\mathrel{\\substack{&lt;\\\\\\neq\\\\&gt;}} c_2\n\\]\n\nThe alternative hypothesis\nThis test is a bit funny in that the null hypothesis is really that the distributions are exactly the same. The alternative to this possibility can come about in a number of ways. The alternative is usually interpreted as a difference in location, primarily because this is the situation that the test has power to detect.\nSo, it is often said that the test also assumes that the samples differ only in location. In other words, the test is most appropriate when the histograms look “shifted”, but not fundamentally different, as illustrated below.\n\n\n\n\n\n\n\n\n\nIn all of these cases, there is a (true) difference in location, but if shape differs too much, the rank sum test should not be used. That said, it can be hard to tell with small samples, so it may not really be practical to to check this assumption.\nExample 1. Out-of-state tuition costs from 26 public and 26 private universities. These data differ primarily in spread, not location; so the rank-sum test might not work well here.\n\n\n\n\n\n\n\n\n\nExample 2. Deviations from expected cancer rates in CT in years with high and low sunspot activity. The shape is a bit hard to discern here, but it seems plausible that the distribution for high sunspot years is shifted to the right of that for low sunspot years. So, the rank sum test would be appropriate here.\n\n\n\n\n\n\n\n\n\n\n\nRank sum test procedure\nThe rank sum procedure, though a bit opaque, is rather simple:\n\n[pool] Combine observations from both groups\n[rank] Sort and rank pooled observations\n[sum] Add up ranks in the first group\n[adjust] Subtract \\(\\frac{n_1(n_1 + 1)}{2}\\), where \\(n_1\\) is the sample size of the first group\n\nThe test rejects if the sum is larger than expected in the direction of the alternative. As in the signed rank test, combinatorics are used to determine a \\(p\\)-value for the test.\nThe rationale for this procedure is that if the distributions are the same, then the ranks should be evenly distributed among the two groups; this induces a particular sampling distribution on the sum of the ranks in each group. The adjustment facilitates computation of \\(p\\)-values.\nLet’s illustrate using data from an experiment in which participants were randomly assigned to receive a fish oil supplement or a regular oil supplement. For each subject, the reduction in blood pressure was measured after a period of time on the treatments.\n\n# load dataset\nfish.oil &lt;- Sleuth3::ex0112 |&gt; rename_with(tolower)\n\n# make boxplot\nboxplot(bp ~ diet, data = fish.oil, horizontal = T)\n\n\n\n\n\n\n\n\nThere’s a bit of difference in spread, but enough of a shift in location that the rank sum test is reasonable to apply.\n\n\n\n\n\n\nCheck your understanding\n\n\n\nCarry out the rank sum procedure outlined above and compute the rank sum statistic by summing up the ranks in the fish oil group.\nTo facilitate calculations, here are the data in order of increasing blood pressure:\n\nfish.oil |&gt; arrange(bp)\n\n   bp       diet\n1  -6 RegularOil\n2  -4 RegularOil\n3  -3 RegularOil\n4   0    FishOil\n5   0    FishOil\n6   0 RegularOil\n7   1 RegularOil\n8   2    FishOil\n9   2 RegularOil\n10  2 RegularOil\n11  8    FishOil\n12 10    FishOil\n13 12    FishOil\n14 14    FishOil\n\n\nAdd a column of ranks by hand (or in R if you can figure out how!), averaging ranks for any ties. Then add up the ranks in the FishOil group, and subtract \\(\\frac{n_1(n_1 + 1)}{2} = \\frac{7\\times 8}{2} = 28\\) to obtain the rank sum statistic.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe output below illustrates the rankings and selection of which ones to add up. Don’t worry about the codes\n\n# again, ignore the codes; look at output\nfish.oil.ranksum &lt;- fish.oil |&gt; \n  mutate(rank = rank(bp),\n         ranks.fish = rank*(diet == 'FishOil')) |&gt;\n  arrange(bp)\nfish.oil.ranksum\n\n   bp       diet rank ranks.fish\n1  -6 RegularOil    1          0\n2  -4 RegularOil    2          0\n3  -3 RegularOil    3          0\n4   0    FishOil    5          5\n5   0    FishOil    5          5\n6   0 RegularOil    5          0\n7   1 RegularOil    7          0\n8   2    FishOil    9          9\n9   2 RegularOil    9          0\n10  2 RegularOil    9          0\n11  8    FishOil   11         11\n12 10    FishOil   12         12\n13 12    FishOil   13         13\n14 14    FishOil   14         14\n\n# rank sum statistic\nn1 &lt;- count(fish.oil, diet)$n[1]\nsum(fish.oil.ranksum$ranks.fish) - n1*(n1 + 1)/2\n\n[1] 41\n\n\n\n\n\n\n\nImplementation using wilcox.test(...)\nThe wilcox.test function also implements the rank sum test, using the same syntax as t.test(...). For example, using the fish oil data, we might test at the 1% significance level whether the fish oil supplement caused a greater reduction in blood pressure:\n\nwilcox.test(bp ~ diet, data = fish.oil, \n            mu = 0, alternative = 'greater', \n            exact = F, conf.int = T, conf.level = 0.99)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bp by diet\nW = 41, p-value = 0.01957\nalternative hypothesis: true location shift is greater than 0\n99 percent confidence interval:\n -0.9999444        Inf\nsample estimates:\ndifference in location \n              7.999962 \n\n\nFor this test, the \\(p\\)-value gives the percentage of possible rank allocations among the groups for which the rank sum is at least as favorable to \\(H_A\\); again this is computed using combinatorics or approximation methods.\nIn this instance, the \\(p\\)-value indicates that only about 1.96% of all possible rank allocations among the two groups would produce a rank sum statistic at least as large. Thus, testing at the 1% significance level:\n\nThe data provide do not provide sufficient evidence at the 1% significance level that the fish oil supplement caused a greater reduction in blood pressure than the regular oil supplement (rank sum statistic W = 41, p = 0.01957).\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nPerform the corresponding \\(t\\) test for comparison at the 1% significance level. Do the tests agree?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe tests do not agree; the \\(t\\) test supports evidence of an effect at the 1% level where the rank sum test does not.\n\n# t test for effect of treatment at 1% level\nt.test(bp ~ diet, data = fish.oil, mu = 0, \n       alternative = 'greater', conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  bp by diet\nt = 3.0621, df = 9.2643, p-value = 0.006542\nalternative hypothesis: true difference in means between group FishOil and group RegularOil is greater than 0\n95 percent confidence interval:\n 3.111056      Inf\nsample estimates:\n   mean in group FishOil mean in group RegularOil \n                6.571429                -1.142857 \n\n\n\n\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nUse the cancer dataset (specifically the delta variable) to test whether the change in cancer rate is higher in years with high sunspot activity. Carry out the test at the 5% level, and report the result in the usual narrative style.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# test whether change in cancer rate is higher in years with high sunspot activity at the 5% level\nwilcox.test(delta ~ sunspot, data = cancer, mu = 0,\n            exact = F, alternative = 'greater', \n            conf.int = T, conf.level = 0.95)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  delta by sunspot\nW = 157.5, p-value = 0.3072\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n -0.1000484        Inf\nsample estimates:\ndifference in location \n            0.09999013 \n\n\nInterpretation:\n\nThe data provide no evidence that the change in cancer rate is higher in years with high sunspot activity (rank sum statistic W = 157.5, p = 0.3072).\n\n\n\n\n\n\n\nPractice problem\n\nIs there a difference in cholesterol associated with consuming oat bran compared with consuming corn flakes? Use the cholesterol dataset to test for a difference at the 5% level using a nonparametric test.\n\nConstruct boxplots to compare the distributions for location shift. Does the nonparametric test seem appropriate?\nCarry out the test.\nReport the test result in the usual narrative style."
  },
  {
    "objectID": "content/week8-pairwise.html#todays-agenda",
    "href": "content/week8-pairwise.html#todays-agenda",
    "title": "Post-hoc inference in ANOVA",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] inference on group means and contrasts after performing ANOVA\n[lab] estimates, tests, and intervals using emmeans"
  },
  {
    "objectID": "content/week8-pairwise.html#from-last-time-diet-restriction",
    "href": "content/week8-pairwise.html#from-last-time-diet-restriction",
    "title": "Post-hoc inference",
    "section": "From last time: diet restriction",
    "text": "From last time: diet restriction\n\n\nData summaries:\n\n\n\n\n\n\n\n\n\nQuitting from lines at lines 40-51 [unnamed-chunk-3] (week8-pairwise.rmarkdown) Error in group_by(): ! Must group by variables found in .data. ✖ Column Diet is not found. Backtrace: 1. pander::pander(…) 4. dplyr:::group_by.data.frame(longevity, Diet)\n\n\n\nInference:\n\\[\\begin{align*}\n&H_0: \\mu_\\text{NP} = \\mu_\\text{N/N85} = \\mu_\\text{N/R50} = \\mu_\\text{N/R40} \\\\\n&H_A: \\text{at least two means differ}\n\\end{align*}\\]\n\nfit &lt;- aov(lifetime ~ diet, data = longevity)\nsummary(fit)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndiet          3  11426    3809   87.41 &lt;2e-16 ***\nResiduals   233  10152      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe data provide strong evidence against the null hypothesis that diet restriction has no effect on mean lifetime among mice (F = 87.41 on 3 and 233 degrees of freedom, p &lt; 0.0001)."
  },
  {
    "objectID": "content/week8-pairwise.html#unresolved-questions",
    "href": "content/week8-pairwise.html#unresolved-questions",
    "title": "Post-hoc inference in ANOVA",
    "section": "Unresolved questions",
    "text": "Unresolved questions\n\nWhich dietary restriction levels produce significant gains in lifespan?\n\nWhat do you think? Which levels of diet restriction do you think differ “enough”?\n\n\nClass vote tallies:\n\n\n\nComparison\nDifferent\nNo different\n\n\n\n\nNP-N/N85\n\n\n\n\nNP-N/R50\n\n\n\n\nNP-N/R40\n\n\n\n\nN/N85-N/R50\n\n\n\n\nN/N85-N/R40\n\n\n\n\nN/R50-N/R40"
  },
  {
    "objectID": "content/week8-pairwise.html#model-based-estimates",
    "href": "content/week8-pairwise.html#model-based-estimates",
    "title": "Post-hoc inference in ANOVA",
    "section": "Model-based estimates",
    "text": "Model-based estimates\n\nInterval estimates for group means in ANOVA use a model-based standard error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nestimate\nSE\n95% CI\n\n\n\n\nNP\n27.4\n0.943\n(25.54, 29.26)\n\n\nN/N85\n32.69\n0.8743\n(30.97, 34.41)\n\n\nN/R50\n42.3\n0.7834\n(40.75, 43.84)\n\n\nN/R40\n45.12\n0.8522\n(43.44, 46.8)\n\n\n\n\n\n\nInterval estimates use a “pooled” standard deviation:\n\\[SE_i = \\frac{s_\\text{pooled}}{\\sqrt{n_i}} = \\frac{\\sqrt{MSE}}{\\sqrt{n_i}}\\]\n\n\nRationale:\n\nthe ANOVA model assumes equal variability (standard deviations) across groups\nbetter precision (for variability estimates, not means) when assumption holds"
  },
  {
    "objectID": "content/week8-pairwise.html#adjusting-pointwise-intervals",
    "href": "content/week8-pairwise.html#adjusting-pointwise-intervals",
    "title": "Post-hoc inference in ANOVA",
    "section": "Adjusting pointwise intervals",
    "text": "Adjusting pointwise intervals\n\nA simple way to correct for undercoverage is to increase the confidence level.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bonferroni correction for \\(k\\) intervals consists in changing the individual coverage level to \\(\\left(1 - \\frac{\\alpha}{k}\\right)\\%\\).\n\nEffectively a width increase\nGuarantees joint coverage \\((1 - \\alpha)\\%\\)\nTends to be over conservative with many means (large \\(k\\))"
  },
  {
    "objectID": "content/week8-pairwise.html#model-estimates-in-r",
    "href": "content/week8-pairwise.html#model-estimates-in-r",
    "title": "Post-hoc inference in ANOVA",
    "section": "Model estimates in R",
    "text": "Model estimates in R\nThe emmeans package has useful functions for model-based estimates from ANOVA.\n\n\n\n# table\nemmeans(fit, ~ diet, level = 1 - 0.05/4)\n\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.0     29.8\n N/N85   32.7 0.874 233     30.5     34.9\n N/R50   42.3 0.783 233     40.3     44.3\n N/R40   45.1 0.852 233     43.0     47.3\n\nConfidence level used: 0.9875 \n\n\n\nnote Bonferroni adjustment\nthese are model-based estimates, not descriptive summaries\n\n\nWhy do you think it might be misleading to conclude which means differ based on these intervals?\n\n\n\n# visualization\nemmip(fit, ~ diet, CIs = T, level = 1 - 0.05/4)"
  },
  {
    "objectID": "content/week8-pairwise.html#pairwise-comparisons",
    "href": "content/week8-pairwise.html#pairwise-comparisons",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons",
    "text": "Pairwise comparisons\n\nPairwise comparisons are inferences made on “contrasts” between pairs of means.\n\nThe difference \\(\\mu_{NP} - \\mu_{N/N85}\\) is an example of a contrast. It is common to perform inference on all pairwise contrasts to determine which means differ and by how much.\n\n\n\nemmeans(fit, ~ diet) |&gt; pairs(adjust = 'bonferroni')\n\n\n\n\n\n\n\n\n\n\n\ndifference\nestimate\nSE\n\n\n\n\nNP - (N/N85)\n-5.289\n1.286\n\n\nNP - (N/R50)\n-14.9\n1.226\n\n\nNP - (N/R40)\n-17.71\n1.271\n\n\n(N/N85) - (N/R50)\n-9.606\n1.174\n\n\n(N/N85) - (N/R40)\n-12.43\n1.221\n\n\n(N/R50) - (N/R40)\n-2.819\n1.158\n\n\n\n\n\n\n\nestimates are \\(\\bar{x}_i - \\bar{x}_j\\)\n\\(SE_{ij} = SE(\\bar{x}_i - \\bar{x}_j) = s_\\text{pooled}\\sqrt{\\frac{1}{n_i} + \\frac{1}{n_j}}\\)\ninference based on a \\(t_{n - k}\\) model\n\ndegrees of freedom: \\(n - k\\)\ntests: \\(T_{ij} = \\frac{\\bar{x}_i - \\bar{x}_j}{SE_{ij}}\\)\nintervals: \\(\\bar{x}_i - \\bar{x}_j \\pm c\\times SE_{ij}\\)\n\n\n\n\nMultiplicity corrections must adjust for the number of contrasts (6), not means (4)."
  },
  {
    "objectID": "content/week8-pairwise.html#pairwise-comparisons-tests",
    "href": "content/week8-pairwise.html#pairwise-comparisons-tests",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: tests",
    "text": "Pairwise comparisons: tests\n\nWhich means differ significantly? All except R50 and R40.\n\n\n\nemmeans() then pairs() then test():\n\n# tests\nemmeans(fit, ~ diet) |&gt; \n  pairs(adjust = 'bonferroni') |&gt;\n  test()\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n\n\n\np-values are adjusted for multiplicity\nreject if adjusted p-value is below the significance threshold\n\n\nHypotheses for pairwise tests:\n\\[\\begin{cases} H_0: \\mu_i - \\mu_j = 0 \\\\ H_A: \\mu_i - \\mu_j \\neq 0 \\end{cases}\\] Test statistic:\n\\[T_{ij} = \\frac{\\bar{x}_i - \\bar{x}_j}{SE_{ij}}\\]\n\\(p\\)-values are obtained from a \\(t_{n - k}\\) model for the sampling distribution of \\(T_{ij}\\)."
  },
  {
    "objectID": "content/week8-pairwise.html#pairwise-comparisons-intervals",
    "href": "content/week8-pairwise.html#pairwise-comparisons-intervals",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: intervals",
    "text": "Pairwise comparisons: intervals\n\nHow much do means differ? Anywhere from 2 - 21 months, depending.\n\n\n\nemmeans() then pairs() then confint():\n\n# tests\nemmeans(fit, ~ diet) |&gt; \n  pairs(adjust = 'bonferroni') |&gt;\n  confint(level = 0.95)\n\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n\n\n\nlevel specifies joint coverage after adjustment\n\n\nIntervals are for the parameter \\(\\mu_i - \\mu_j\\):\n\\[\\bar{x}_i - \\bar{x}_j \\pm c\\times SE_{ij}\\]\nThe critical value \\(c\\) is obtained from the \\(t_{n - k}\\) model.\nFor a \\((1 - \\alpha)\\times 100\\%\\) confidence interval with Bonferroni correction:\n\\[c = \\left(1 - \\frac{\\alpha}{2k}\\right) \\;\\text{quantile}\\]"
  },
  {
    "objectID": "content/week8-pairwise.html#pairwise-comparisons-visualizations",
    "href": "content/week8-pairwise.html#pairwise-comparisons-visualizations",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: visualizations",
    "text": "Pairwise comparisons: visualizations\n\n\nAnother option is to visualize the pairwise comparison inferences by displaying simultaneous 95% intervals.\n\nlevel = ... specifies simultaneous coverage level\nintervals exclude 0 \\(\\Leftrightarrow\\) tests reject\n\n\nemmeans() then pairs() then plot():\n\n# visualization\nemmeans(fit, ~ diet) |&gt;\n  pairs(adjust = 'bonferroni') |&gt; \n  plot(level = 0.95)"
  },
  {
    "objectID": "content/week8-pairwise.html#multiple-correction-methods",
    "href": "content/week8-pairwise.html#multiple-correction-methods",
    "title": "Post-hoc inference in ANOVA",
    "section": "Multiple correction methods",
    "text": "Multiple correction methods\n\nThere are many multiple inference corrections that could be applied in pairwise comparisons. Not all of them will agree.\n\nAmong the most common:\n\n\n\n\n\n\n\n\nMethod(s)\nApproach\nWhen to use\n\n\n\n\nBonferroni correction\nUniformly increase confidence level\nSmall \\(k\\)\n\n\nTukey’s method\nAdjust based on largest difference\nGeneral purpose\n\n\nScheffe’s method\nCorrect for all possible comparisons\nLarge \\(k\\)\n\n\nHolm/Hochberg\nAdaptive corrections based on significance level\nConcerned about power"
  },
  {
    "objectID": "content/week8-pairwise.html#comparisons-with-a-control",
    "href": "content/week8-pairwise.html#comparisons-with-a-control",
    "title": "Post-hoc inference in ANOVA",
    "section": "Comparisons with a control",
    "text": "Comparisons with a control\n\nDoes diet restriction increase mean lifespan, and if so by how much?\n\n\n\n\nemmeans(fit, trt.vs.ctrl ~ diet) |&gt; confint(level = 0.95)\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\n95% CI\n\n\n\n\n(N/N85) - NP\n5.289\n1.286\n(2.23, 8.34)\n\n\n(N/R50) - NP\n14.9\n1.226\n(11.98, 17.81)\n\n\n(N/R40) - NP\n17.71\n1.271\n(14.7, 20.73)\n\n\n\n\n\n\n\nMultiple inference adjustment uses Dunnett’s method (specialized correction for comparisons with a control)\nComparisons will be relative to first group in R\n\n\n\nWith 95% confidence, relative to an unrestricted diet…\n\na 85kcal/day diet increases lifespan by an estimated 2.23 to 8.34 months\na 50kcal/day diet increases lifespan by an estimated 11.98 to 17.81 months\na 40kcal/day diet increases lifespan by an estimated 14.70 to 20.73 months"
  },
  {
    "objectID": "content/week8-pairwise.html#lab-post-hoc-inferences",
    "href": "content/week8-pairwise.html#lab-post-hoc-inferences",
    "title": "Post-hoc inference in ANOVA",
    "section": "Lab: post-hoc inferences",
    "text": "Lab: post-hoc inferences\nOpen lab12-pairwise in the class workspace. The goal of the activity is to learn to perform post-hoc inference in R:\n\nreview of basic ANOVA\nintervals and visualizations for group means\npost-hoc tests for group means\npairwise comparisons\ncontrasts with a control\n\nYou’ll largely replicate the analysis presented so far in class."
  },
  {
    "objectID": "content/week8-pairwise.html#practice-problem",
    "href": "content/week8-pairwise.html#practice-problem",
    "title": "Post-hoc inference in ANOVA",
    "section": "Practice problem",
    "text": "Practice problem\n\n\n\n\n\n\n\n\n\n\n\n\nData are standardized lengths of the anterior adductor muscle (AAM) of Mytilus trossulus mussels from five populations.\n\nEstimate mean AAM lengths for each population and test for differences between populations. If differences are determined to be significant, determine which populations differ significantly and provide interval estimates for the differences."
  },
  {
    "objectID": "content/week8-pairwise.html#power-calculations-for-anova",
    "href": "content/week8-pairwise.html#power-calculations-for-anova",
    "title": "Post-hoc inference in ANOVA",
    "section": "Power calculations for ANOVA",
    "text": "Power calculations for ANOVA\n\nHow much data should we collect to detect a difference in mean lifespan of 1 month?\n\n\n\nTo perform sample size power calculations, one needs:\n\nnumber of groups\ntarget significance level (\\(\\alpha\\))\ntarget power level\nguess or prior estimate of variance ratio \\(\\frac{\\text{group variation}}{\\text{error variation}}\\)\n\nFrom the existing study: \\[\n1 \\text{ month } = 0.151 \\times \\sqrt{MSE}\n\\]\n\nSo to detect effects on the order of one month at 90% power:\n\npower.anova.test(groups = 4, \n                 sig.level = 0.05, \n                 power = 0.9, \n                 within.var = 1, \n                 between.var = 0.151)\n\n\n     Balanced one-way analysis of variance power calculation \n\n         groups = 4\n              n = 32.27901\n    between.var = 0.151\n     within.var = 1\n      sig.level = 0.05\n          power = 0.9\n\nNOTE: n is number in each group\n\n\n… we need 33 mice per treatment group.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week8-pairwise.html#practice-problem-diet-restriction",
    "href": "content/week8-pairwise.html#practice-problem-diet-restriction",
    "title": "Post-hoc inference in ANOVA",
    "section": "Practice problem: diet restriction",
    "text": "Practice problem: diet restriction\n\n\nData summaries:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nsd\nn\n\n\n\n\nNP\n27.4\n6.134\n49\n\n\nN/N85\n32.69\n5.125\n57\n\n\nN/R50\n42.3\n7.768\n71\n\n\nN/R40\n45.12\n6.703\n60\n\n\n\n\n\n\nInference using ANOVA:\n\\[\\begin{align*}\n&H_0: \\mu_\\text{NP} = \\mu_\\text{N/N85} = \\mu_\\text{N/R50} = \\mu_\\text{N/R40} \\\\\n&H_A: \\text{at least two means differ}\n\\end{align*}\\]\n\nfit &lt;- aov(lifetime ~ diet, data = longevity)\nsummary(fit)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndiet          3  11426    3809   87.41 &lt;2e-16 ***\nResiduals   233  10152      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe data provide strong evidence that diet restriction has an effect on mean lifetime among mice (F = 87.41 on 3 and 233 degrees of freedom, p &lt; 0.0001)."
  },
  {
    "objectID": "content/week8-pairwise.html#from-before-diet-restriction",
    "href": "content/week8-pairwise.html#from-before-diet-restriction",
    "title": "Post-hoc inference in ANOVA",
    "section": "From before: diet restriction",
    "text": "From before: diet restriction\n\n\nData summaries:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nsd\nn\n\n\n\n\nNP\n27.4\n6.134\n49\n\n\nN/N85\n32.69\n5.125\n57\n\n\nN/R50\n42.3\n7.768\n71\n\n\nN/R40\n45.12\n6.703\n60\n\n\n\n\n\n\nInference using ANOVA:\n\\[\n\\begin{align*}\n&H_0: \\mu_\\text{NP} = \\mu_\\text{N/N85} = \\mu_\\text{N/R50} = \\mu_\\text{N/R40} \\\\\n&H_A: \\text{at least two means differ}\n\\end{align*}\n\\]\n\nfit &lt;- aov(lifetime ~ diet, data = longevity)\nsummary(fit)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndiet          3  11426    3809   87.41 &lt;2e-16 ***\nResiduals   233  10152      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe data provide strong evidence that diet restriction has an effect on mean lifetime among mice (F = 87.41 on 3 and 233 degrees of freedom, p &lt; 0.0001)."
  },
  {
    "objectID": "content/week8-pairwise.html#follow-up-questions",
    "href": "content/week8-pairwise.html#follow-up-questions",
    "title": "Post-hoc inference in ANOVA",
    "section": "Follow-up questions",
    "text": "Follow-up questions\nThe ANOVA tells us there’s evidence of an effect of diet restriction on lifespan.\nSo now we’d want to know:\n\nWhat are the mean lifespans for each level of restriction?\nFor which levels of dietary restriction do mean lifespans differ?\nWhat are the gains in mean lifespan for each level of restriction relative to an unrestricted diet?\nFor which levels of restriction are gains in mean lifespan significant?\n\nAnswers require post-hoc inferences (done after-the-fact) on:\n\ngroup means \\(\\mu_i\\) (question 1)\n“contrasts” \\(\\mu_i - \\mu_j\\) (questions 2-4)"
  },
  {
    "objectID": "content/week8-pairwise.html#estimates-for-group-means",
    "href": "content/week8-pairwise.html#estimates-for-group-means",
    "title": "Post-hoc inference in ANOVA",
    "section": "Estimates for group means",
    "text": "Estimates for group means\n\nInterval estimates for group means in ANOVA use a model-based standard error.\n\n\n\n\nemmeans(fit, ~diet) |&gt; confint(level = 0.95)\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nestimate\nSE\n95% CI\n\n\n\n\nNP\n27.4\n0.943\n(25.54, 29.26)\n\n\nN/N85\n32.69\n0.8743\n(30.97, 34.41)\n\n\nN/R50\n42.3\n0.7834\n(40.75, 43.84)\n\n\nN/R40\n45.12\n0.8522\n(43.44, 46.8)\n\n\n\n\n\n\nInterval estimates use a “pooled” standard deviation:\n\\[SE_i = \\frac{s_\\text{pooled}}{\\sqrt{n_i}} = \\frac{\\sqrt{MSE}}{\\sqrt{n_i}}\\]\nOtherwise identical to \\(t_{n - k}\\) confidence intervals.\n\n\nRationale:\n\nthe ANOVA model assumes equal variability (standard deviations) across groups\nbetter precision (for variability estimates, not means) when assumption holds"
  },
  {
    "objectID": "content/week8-pairwise.html#simultaneous-vs.",
    "href": "content/week8-pairwise.html#simultaneous-vs.",
    "title": "Post-hoc inference in ANOVA",
    "section": "Simultaneous vs. ",
    "text": "Simultaneous vs. \n\nA simple way to correct for undercoverage is to increase the confidence level.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bonferroni correction for \\(k\\) intervals consists in changing the individual coverage level to \\(\\left(1 - \\frac{\\alpha}{k}\\right)\\%\\).\n\nEffectively a width increase\nGuarantees joint coverage \\((1 - \\alpha)\\%\\)\nTends to be over conservative with many means (large \\(k\\))"
  },
  {
    "objectID": "content/week8-pairwise.html#bonferroni-adjustment",
    "href": "content/week8-pairwise.html#bonferroni-adjustment",
    "title": "Post-hoc inference in ANOVA",
    "section": "Bonferroni adjustment",
    "text": "Bonferroni adjustment\n\nProblem: several 95% intervals don’t have simultaneous 95% coverage.\n\n\nindividual coverage: how often one interval covers the population mean\nsimultaneous coverage: how often all intervals cover population means at the same time\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bonferroni correction for \\(k\\) intervals consists in changing the individual coverage level to \\(\\left(1 - \\frac{\\alpha}{k}\\right)\\%\\).\n\nEffectively a width increase\nGuarantees joint coverage \\((1 - \\alpha)\\%\\)\nTends to be over conservative with many means (large \\(k\\))"
  },
  {
    "objectID": "content/week8-pairwise.html#implementation-using-emmeans...",
    "href": "content/week8-pairwise.html#implementation-using-emmeans...",
    "title": "Post-hoc inference in ANOVA",
    "section": "Implementation using emmeans(...)",
    "text": "Implementation using emmeans(...)\n\n\n\n# table\nemmeans(fit, ~ diet, level = 0.95, adjust = 'bonferroni')\n\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.0     29.8\n N/N85   32.7 0.874 233     30.5     34.9\n N/R50   42.3 0.783 233     40.3     44.3\n N/R40   45.1 0.852 233     43.0     47.3\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \n\n\n\nnote Bonferroni adjustment\nthese are model-based estimates, not descriptive summaries\n\n\nWhy do you think it might be misleading to conclude which means differ based on these intervals?\n\n\n\n# visualization\nemmip(fit, ~ diet, CIs = T, level = 0.95, adjust = 'bonferroni')"
  },
  {
    "objectID": "content/week8-pairwise.html#implementation-in-r",
    "href": "content/week8-pairwise.html#implementation-in-r",
    "title": "Post-hoc inference in ANOVA",
    "section": "Implementation in R",
    "text": "Implementation in R\n\n\n\n# table\nemmeans(fit, ~ diet, level = 0.95, \n        adjust = 'bonferroni')\n\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.0     29.8\n N/N85   32.7 0.874 233     30.5     34.9\n N/R50   42.3 0.783 233     40.3     44.3\n N/R40   45.1 0.852 233     43.0     47.3\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \n\n\n\nnote Bonferroni adjustment\nthese are model-based estimates that depend on the fitted ANOVA model\n\n\n\n# visualization\nemmip(fit, ~ diet, CIs = T, level = 0.95, \n      adjust = 'bonferroni')\n\n\n\n\n\n\n\n\n\n\n\n\nCaution: these intervals do NOT indicate which means differ significantly."
  },
  {
    "objectID": "content/week8-pairwise.html#pairwise-comparisons-tests-1",
    "href": "content/week8-pairwise.html#pairwise-comparisons-tests-1",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: tests",
    "text": "Pairwise comparisons: tests\n\nWhich means differ significantly? All except R50 and R40.\n\n\n\nemmeans() then pairs() then test():\n\n# tests\nemmeans(fit, ~ diet) |&gt; \n  pairs(adjust = 'bonferroni') |&gt;\n  test()\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n\n\n\nInterpretation:\n\nThe data provide evidence at the 5% significance level that mean lifespan differs among all levels of diet restriction except the N/R40 and N/R50 groups (p = 0.0937), for which the evidence is suggestive but inconclusive."
  },
  {
    "objectID": "content/week8-pairwise.html#pairwise-comparisons-tests-2",
    "href": "content/week8-pairwise.html#pairwise-comparisons-tests-2",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: tests",
    "text": "Pairwise comparisons: tests\n\nWhich means differ significantly? All except R50 and R40.\n\n\n\nemmeans() then pairs() then test():\n\n# tests\nemmeans(fit, ~ diet) |&gt; \n  pairs(adjust = 'none') |&gt;\n  test()\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0001\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0156\n\n\n\nFailure to adjust for multiple inferences leads to a different conclusion:\n\nThe data provide evidence at the 5% significance levelthat mean lifespan differs among all levels of diet restriction.\n\n\n\nThis is incorrect, because the joint significance level is not 5%.\nWithout adjustment, type I error could be as high as \\(k\\times\\alpha = 6\\times 0.05 = 0.3\\)."
  },
  {
    "objectID": "content/week8-pairwise.html#pairwise-comparisons-intervals-1",
    "href": "content/week8-pairwise.html#pairwise-comparisons-intervals-1",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: intervals",
    "text": "Pairwise comparisons: intervals\n\nHow much do means differ? Anywhere from 2 - 21 months, depending.\n\n\n\nemmeans() then pairs() then confint():\n\n# tests\nemmeans(fit, ~ diet) |&gt; \n  pairs(adjust = 'bonferroni') |&gt;\n  confint(level = 0.95)\n\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n\n\n\nInterpretations are the same as usual:\n\nWith 95% confidence, mean lifespan on a normal diet is estimated to exceed mean lifespan on an unrestricted diet by between 1.87 and 8.71 months, with a point estimate of 5.29 months difference (SE 1.29)."
  },
  {
    "objectID": "content/week8-pairwise.html#multiple-testing-matters",
    "href": "content/week8-pairwise.html#multiple-testing-matters",
    "title": "Post-hoc inference in ANOVA",
    "section": "Multiple testing matters",
    "text": "Multiple testing matters\n\nWhich means differ significantly? All except R50 and R40.\n\n\n\nemmeans() then pairs() then test():\n\n# tests\nemmeans(fit, ~ diet) |&gt; \n  pairs(adjust = 'none') |&gt;\n  test()\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0001\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0156\n\n\n\nFailure to adjust for multiple inferences leads to a different conclusion:\n\nThe data provide evidence at the 5% significance levelthat mean lifespan differs among all levels of diet restriction.\n\n\n\nThis is incorrect, because the joint significance level is not 5%.\nWithout adjustment, type I error could be as high as \\(k\\times\\alpha = 6\\times 0.05 = 0.3\\)."
  },
  {
    "objectID": "content/week8-pairwise.html#multiple-testing-correction-matters",
    "href": "content/week8-pairwise.html#multiple-testing-correction-matters",
    "title": "Post-hoc inference in ANOVA",
    "section": "Multiple testing correction matters",
    "text": "Multiple testing correction matters\n\nUsing unadjusted \\(p\\)-values will inflate type I error rates.\n\n\n\nemmeans() then pairs() then test():\n\n# tests\nemmeans(fit, ~ diet) |&gt; \n  pairs(adjust = 'none') |&gt;\n  test()\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0001\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0156\n\n\n\nFailure to adjust for multiple inferences leads to a different conclusion:\n\nThe data provide evidence at the 5% significance levelthat mean lifespan differs among all levels of diet restriction.\n\n\n\nThis is incorrect, because the joint significance level is not 5%.\nWithout adjustment, type I error could be as high as \\(k\\times\\alpha = 6\\times 0.05 = 0.3\\)."
  },
  {
    "objectID": "content/week8-pairwise.html#transforming-intervals",
    "href": "content/week8-pairwise.html#transforming-intervals",
    "title": "Post-hoc inference in ANOVA",
    "section": "Transforming intervals",
    "text": "Transforming intervals\n\n\n\n\n\n\n\n\n\ncontrast\n95% CI\n\n\n\n\n(N/N85) - NP\n(0.08, 0.3)\n\n\n(N/R50) - NP\n(0.44, 0.65)\n\n\n(N/R40) - NP\n(0.54, 0.76)"
  },
  {
    "objectID": "content/week8-pairwise.html#data-transformations-for-interpretability",
    "href": "content/week8-pairwise.html#data-transformations-for-interpretability",
    "title": "Post-hoc inference in ANOVA",
    "section": "Data transformations for interpretability",
    "text": "Data transformations for interpretability\n\nWhat if we wanted instead to estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would do the trick:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity) \nemmeans(fit.log, trt.vs.ctrl ~ diet)\n\n\n\n\n\n\n\n\n\n\ncontrast\n95% CI\n\n\n\n\n(N/N85)/NP\n(1.1, 1.35)\n\n\n(N/R50)/NP\n(1.42, 1.73)\n\n\n(N/R40)/NP\n(1.52, 1.87)"
  },
  {
    "objectID": "content/week8-pairwise.html#data-transformation",
    "href": "content/week8-pairwise.html#data-transformation",
    "title": "Post-hoc inference in ANOVA",
    "section": "Data transformation",
    "text": "Data transformation\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity) \nemmeans(fit.log, trt.vs.ctrl ~ diet)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/N85)/NP\n1.222\n(1.1, 1.35)\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%."
  },
  {
    "objectID": "content/week8-pairwise.html#a-clever-data-transformation",
    "href": "content/week8-pairwise.html#a-clever-data-transformation",
    "title": "Post-hoc inference in ANOVA",
    "section": "A clever data transformation",
    "text": "A clever data transformation\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity) \nemmeans(fit.log, trt.vs.ctrl ~ diet)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/N85)/NP\n1.222\n(1.1, 1.35)\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%."
  },
  {
    "objectID": "content/week8-pairwise.html#log-contrasts",
    "href": "content/week8-pairwise.html#log-contrasts",
    "title": "Post-hoc inference in ANOVA",
    "section": "Log contrasts",
    "text": "Log contrasts\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity) \nemmeans(fit.log, trt.vs.ctrl ~ diet)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/N85)/NP\n1.222\n(1.1, 1.35)\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%."
  },
  {
    "objectID": "content/week8-pairwise.html#logcontrasts",
    "href": "content/week8-pairwise.html#logcontrasts",
    "title": "Post-hoc inference in ANOVA",
    "section": "Logcontrasts",
    "text": "Logcontrasts\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity) \nemmeans(fit.log, trt.vs.ctrl ~ diet)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/N85)/NP\n1.222\n(1.1, 1.35)\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%."
  },
  {
    "objectID": "content/week8-pairwise.html#log-contrasts-for-estimating-relative-change",
    "href": "content/week8-pairwise.html#log-contrasts-for-estimating-relative-change",
    "title": "Post-hoc inference in ANOVA",
    "section": "Log contrasts for estimating relative change",
    "text": "Log contrasts for estimating relative change\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity) \nemmeans(fit.log, trt.vs.ctrl ~ diet)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/N85)/NP\n1.222\n(1.1, 1.35)\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%."
  },
  {
    "objectID": "content/week8-pairwise.html#log-contrasts-relative-change-in-medians",
    "href": "content/week8-pairwise.html#log-contrasts-relative-change-in-medians",
    "title": "Post-hoc inference in ANOVA",
    "section": "Log contrasts: relative change in medians",
    "text": "Log contrasts: relative change in medians\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity) \nemmeans(fit.log, trt.vs.ctrl ~ diet)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/N85)/NP\n1.222\n(1.1, 1.35)\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%."
  },
  {
    "objectID": "content/week8-pairwise.html#log-contrasts-capture-relative-change",
    "href": "content/week8-pairwise.html#log-contrasts-capture-relative-change",
    "title": "Post-hoc inference in ANOVA",
    "section": "Log contrasts capture relative change",
    "text": "Log contrasts capture relative change\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity) \nemmeans(fit.log, trt.vs.ctrl ~ diet)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/N85)/NP\n1.222\n(1.1, 1.35)\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%."
  },
  {
    "objectID": "content/week8-pairwise.html#log-contrasts-relative-change",
    "href": "content/week8-pairwise.html#log-contrasts-relative-change",
    "title": "Post-hoc inference in ANOVA",
    "section": "Log contrasts: relative change",
    "text": "Log contrasts: relative change\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity) \nemmeans(fit.log, trt.vs.ctrl ~ diet)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/N85)/NP\n1.222\n(1.1, 1.35)\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%."
  },
  {
    "objectID": "content/week8-posthoc.html#todays-agenda",
    "href": "content/week8-posthoc.html#todays-agenda",
    "title": "Post-hoc inference in ANOVA",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] inference on group means and contrasts after performing ANOVA\n[lab] estimates, tests, and intervals using emmeans"
  },
  {
    "objectID": "content/week8-posthoc.html#from-before-diet-restriction",
    "href": "content/week8-posthoc.html#from-before-diet-restriction",
    "title": "Post-hoc inference in ANOVA",
    "section": "From before: diet restriction",
    "text": "From before: diet restriction\n\n\nData summaries:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nmean\nsd\nn\n\n\n\n\nNP\n27.4\n6.134\n49\n\n\nN/N85\n32.69\n5.125\n57\n\n\nN/R50\n42.3\n7.768\n71\n\n\nN/R40\n45.12\n6.703\n60\n\n\n\n\n\n\nInference using ANOVA:\n\\[\n\\begin{align*}\n&H_0: \\mu_\\text{NP} = \\mu_\\text{N/N85} = \\mu_\\text{N/R50} = \\mu_\\text{N/R40} \\\\\n&H_A: \\text{at least two means differ}\n\\end{align*}\n\\]\n\nfit &lt;- aov(lifetime ~ diet, data = longevity)\nsummary(fit)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndiet          3  11426    3809   87.41 &lt;2e-16 ***\nResiduals   233  10152      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe data provide strong evidence that diet restriction has an effect on mean lifetime among mice (F = 87.41 on 3 and 233 degrees of freedom, p &lt; 0.0001)."
  },
  {
    "objectID": "content/week8-posthoc.html#follow-up-questions",
    "href": "content/week8-posthoc.html#follow-up-questions",
    "title": "Post-hoc inference in ANOVA",
    "section": "Follow-up questions",
    "text": "Follow-up questions\nThe ANOVA tells us there’s evidence of an effect of diet restriction on lifespan.\nSo now we’d want to know:\n\nWhat are the mean lifespans for each level of restriction?\nFor which levels of dietary restriction do mean lifespans differ?\nWhat are the gains in mean lifespan for each level of restriction relative to an unrestricted diet?\nFor which levels of restriction are gains in mean lifespan significant?\n\nAnswers require post-hoc inferences (done after-the-fact) on:\n\ngroup means \\(\\mu_i\\) (question 1)\n“contrasts” \\(\\mu_i - \\mu_j\\) (questions 2-4)"
  },
  {
    "objectID": "content/week8-posthoc.html#estimates-for-group-means",
    "href": "content/week8-posthoc.html#estimates-for-group-means",
    "title": "Post-hoc inference in ANOVA",
    "section": "Estimates for group means",
    "text": "Estimates for group means\n\nInterval estimates for group means in ANOVA use a model-based standard error.\n\n\n\n\nemmeans(object = fit, spec = ~ diet) |&gt; \n  confint(level = 0.95)\n\n\n\n\n\n\n\n\n\n\n\n\ndiet\nestimate\nSE\n95% CI\n\n\n\n\nNP\n27.4\n0.943\n(25.54, 29.26)\n\n\nN/N85\n32.69\n0.8743\n(30.97, 34.41)\n\n\nN/R50\n42.3\n0.7834\n(40.75, 43.84)\n\n\nN/R40\n45.12\n0.8522\n(43.44, 46.8)\n\n\n\n\n\n\nInterval estimates use a “pooled” standard deviation:\n\\[SE_i = \\frac{s_\\text{pooled}}{\\sqrt{n_i}} = \\frac{\\sqrt{MSE}}{\\sqrt{n_i}}\\]\nOtherwise identical to \\(t_{n - k}\\) confidence intervals.\n\n\nRationale:\n\nthe ANOVA model assumes equal variability (standard deviations) across groups\nbetter precision (for variability estimates, not means) when assumption holds"
  },
  {
    "objectID": "content/week8-posthoc.html#bonferroni-adjustment",
    "href": "content/week8-posthoc.html#bonferroni-adjustment",
    "title": "Post-hoc inference in ANOVA",
    "section": "Bonferroni adjustment",
    "text": "Bonferroni adjustment\n\nProblem: several 95% intervals don’t have simultaneous 95% coverage.\n\n\nindividual coverage: how often one interval covers the population mean\nsimultaneous coverage: how often all intervals cover population means at the same time\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bonferroni correction for \\(k\\) intervals consists in changing the individual coverage level to \\(\\left(1 - \\frac{\\alpha}{k}\\right)\\%\\).\n\nEffectively a width increase\nGuarantees joint coverage \\((1 - \\alpha)\\%\\)\nTends to be over conservative with many means (large \\(k\\))"
  },
  {
    "objectID": "content/week8-posthoc.html#implementation-in-r",
    "href": "content/week8-posthoc.html#implementation-in-r",
    "title": "Post-hoc inference in ANOVA",
    "section": "Implementation in R",
    "text": "Implementation in R\n\n\n\n# table\nemmeans(object = fit, spec = ~ diet) |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.0     29.8\n N/N85   32.7 0.874 233     30.5     34.9\n N/R50   42.3 0.783 233     40.3     44.3\n N/R40   45.1 0.852 233     43.0     47.3\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \n\n\n\nnote Bonferroni adjustment\nthese are model-based estimates that depend on the fitted ANOVA model\n\n\n\n# visualization\nemmeans(object = fit, spec = ~ diet) |&gt;\n  confint(level = 0.95, adjust = 'bonferroni') |&gt;\n  plot(xlab = 'mean lifespan (months)', \n       ylab = 'diet')\n\n\n\n\n\n\n\n\n\n\n\n\nCaution: these intervals do NOT indicate which means differ significantly."
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons",
    "href": "content/week8-posthoc.html#pairwise-comparisons",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons",
    "text": "Pairwise comparisons\n\nPairwise comparisons are inferences made on “contrasts” between pairs of means.\n\nThe difference \\(\\mu_{NP} - \\mu_{N/N85}\\) is an example of a contrast. It is common to perform inference on all pairwise contrasts to determine which means differ and by how much.\n\n\n\nemmeans(object = fit, specs = ~ diet) |&gt; \n  contrast('pairwise')\n\n\n\n\n\n\n\n\n\n\n\ndifference\nestimate\nSE\n\n\n\n\nNP - (N/N85)\n-5.289\n1.286\n\n\nNP - (N/R50)\n-14.9\n1.226\n\n\nNP - (N/R40)\n-17.71\n1.271\n\n\n(N/N85) - (N/R50)\n-9.606\n1.174\n\n\n(N/N85) - (N/R40)\n-12.43\n1.221\n\n\n(N/R50) - (N/R40)\n-2.819\n1.158\n\n\n\n\n\n\n\nestimates are \\(\\bar{x}_i - \\bar{x}_j\\)\n\\(SE_{ij} = SE(\\bar{x}_i - \\bar{x}_j) = s_\\text{pooled}\\sqrt{\\frac{1}{n_i} + \\frac{1}{n_j}}\\)\ninference based on a \\(t_{n - k}\\) model\n\ndegrees of freedom: \\(n - k\\)\ntests: \\(T_{ij} = \\frac{\\bar{x}_i - \\bar{x}_j}{SE_{ij}}\\)\nintervals: \\(\\bar{x}_i - \\bar{x}_j \\pm c\\times SE_{ij}\\)\n\n\n\n\nMultiplicity corrections must adjust for the number of contrasts (6), not means (4)."
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-tests",
    "href": "content/week8-posthoc.html#pairwise-comparisons-tests",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: tests",
    "text": "Pairwise comparisons: tests\n\nWhich means differ significantly? All except R50 and R40.\n\n\n\nemmeans then contrast then test:\n\nemmeans(object = fit, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  test(adjust = 'bonferroni')\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n\n\n\np-values are adjusted for multiplicity\nreject if adjusted p-value is below the significance threshold\n\n\nHypotheses for pairwise tests:\n\\[\\begin{cases} H_0: \\mu_i - \\mu_j = 0 \\\\ H_A: \\mu_i - \\mu_j \\neq 0 \\end{cases}\\] Test statistic:\n\\[T_{ij} = \\frac{\\bar{x}_i - \\bar{x}_j}{SE_{ij}}\\]\n\\(p\\)-values are obtained from a \\(t_{n - k}\\) model for the sampling distribution of \\(T_{ij}\\)."
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-tests-1",
    "href": "content/week8-posthoc.html#pairwise-comparisons-tests-1",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: tests",
    "text": "Pairwise comparisons: tests\n\nWhich means differ significantly? All except R50 and R40.\n\n\n\nemmeans then contrast then test:\n\nemmeans(object = fit, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  test(adjust = 'bonferroni')\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n\n\n\nInterpretation:\n\nThe data provide evidence at the 5% significance level that mean lifespan differs among all levels of diet restriction except the N/R40 and N/R50 groups (p = 0.0937), for which the evidence is suggestive but inconclusive."
  },
  {
    "objectID": "content/week8-posthoc.html#multiple-testing-correction-matters",
    "href": "content/week8-posthoc.html#multiple-testing-correction-matters",
    "title": "Post-hoc inference in ANOVA",
    "section": "Multiple testing correction matters",
    "text": "Multiple testing correction matters\n\nUsing unadjusted \\(p\\)-values will inflate type I error rates.\n\n\n\nsetting adjust = 'none':\n\nemmeans(object = fit, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  test(adjust = 'none')\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0001\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0156\n\n\n\nFailure to adjust for multiple inferences leads to a different conclusion:\n\nThe data provide evidence at the 5% significance levelthat mean lifespan differs among all levels of diet restriction.\n\n\n\nThis is incorrect, because the joint significance level is not 5%.\nWithout adjustment, type I error could be as high as \\(k\\times\\alpha = 6\\times 0.05 = 0.3\\)!"
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-intervals",
    "href": "content/week8-posthoc.html#pairwise-comparisons-intervals",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: intervals",
    "text": "Pairwise comparisons: intervals\n\nHow much do means differ? Anywhere from 2 - 21 months, depending.\n\n\n\nemmeans then contrast then confint:\n\nemmeans(object = fit, specs = ~ diet) |&gt; \n  contrast('pairwise') |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n\n\n\nlevel specifies joint coverage after adjustment\n\n\nIntervals are for the parameter \\(\\mu_i - \\mu_j\\):\n\\[\\bar{x}_i - \\bar{x}_j \\pm c\\times SE_{ij}\\]\nThe critical value \\(c\\) is obtained from the \\(t_{n - k}\\) model.\nFor a \\((1 - \\alpha)\\times 100\\%\\) confidence interval with Bonferroni correction:\n\\[c = \\left(1 - \\frac{\\alpha}{2k}\\right) \\;\\text{quantile}\\]"
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-intervals-1",
    "href": "content/week8-posthoc.html#pairwise-comparisons-intervals-1",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: intervals",
    "text": "Pairwise comparisons: intervals\n\nHow much do means differ? Anywhere from 2 - 21 months, depending.\n\n\n\nemmeans then contrast then confint:\n\nemmeans(object = fit, specs = ~ diet) |&gt; \n  contrast('pairwise') |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n\n\n\nInterpretations are the same as usual:\n\nWith 95% confidence, mean lifespan on a normal diet is estimated to exceed mean lifespan on an unrestricted diet by between 1.87 and 8.71 months, with a point estimate of 5.29 months difference (SE 1.29)."
  },
  {
    "objectID": "content/week8-posthoc.html#pairwise-comparisons-visualizations",
    "href": "content/week8-posthoc.html#pairwise-comparisons-visualizations",
    "title": "Post-hoc inference in ANOVA",
    "section": "Pairwise comparisons: visualizations",
    "text": "Pairwise comparisons: visualizations\n\n\nAnother option is to visualize the pairwise comparison inferences by displaying simultaneous 95% intervals.\nEasy to spot significant contrasts:\n\nintervals exclude 0 \\(\\Leftrightarrow\\) tests reject\n\n\nemmeans then contrast then confint then plot:\n\nemmeans(object = fit, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  confint(level = 0.95, adjust = 'bonferroni') |&gt;\n  plot(xlab = 'difference in mean lifetime (months)', \n       ylab = 'contrast')"
  },
  {
    "objectID": "content/week8-posthoc.html#comparisons-with-a-control",
    "href": "content/week8-posthoc.html#comparisons-with-a-control",
    "title": "Post-hoc inference in ANOVA",
    "section": "Comparisons with a control",
    "text": "Comparisons with a control\n\nDoes diet restriction increase mean lifespan, and if so by how much?\n\n\n\nSpecify contrast('trt.vs.ctrl'):\n\nemmeans(object = fit, specs = ~ diet) |&gt; \n  contrast('trt.vs.ctrl') |&gt;\n  confint(level = 0.95, adjust = 'dunnett')\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\n95% CI\n\n\n\n\n(N/N85) - NP\n5.289\n1.286\n(2.23, 8.34)\n\n\n(N/R50) - NP\n14.9\n1.226\n(11.98, 17.81)\n\n\n(N/R40) - NP\n17.71\n1.271\n(14.7, 20.73)\n\n\n\n\n\n\n\nMultiple inference adjustment uses Dunnett’s method\n\nspecialized correction for comparisons with a control\nadjust = 'dunnett'\n\nComparisons will be relative to first group in R\n\n\n\nWith 95% confidence, relative to an unrestricted diet…\n\na 85kcal/day diet increases lifespan by an estimated 2.23 to 8.34 months\na 50kcal/day diet increases lifespan by an estimated 11.98 to 17.81 months\na 40kcal/day diet increases lifespan by an estimated 14.70 to 20.73 months"
  },
  {
    "objectID": "content/week8-posthoc.html#log-contrasts-relative-change",
    "href": "content/week8-posthoc.html#log-contrasts-relative-change",
    "title": "Post-hoc inference in ANOVA",
    "section": "Log contrasts: relative change",
    "text": "Log contrasts: relative change\n\nCan we instead estimate a percent change in lifespan relative to the control?\n\n\n\nThe contrast in log-lifetimes would be:\n\\[\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n\\] So to answer the question:\n\nrefit the ANOVA model with log lifetimes\ncompute contrasts with control group using Dunnett’s method\nexponentiate estimates to obtain ratios (and hence percentages)\n\n\n\nfit.log &lt;- aov(log(lifetime) ~ diet, data = longevity)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\n95% CI\n\n\n\n\n(N/R50)/NP\n1.572\n(1.42, 1.73)\n\n\n(N/R40)/NP\n1.688\n(1.52, 1.87)\n\n\n\n\n\nFact: mean log lifetime = log median lifetime.\n\nWith 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%.\n\nExtra credit: work out and interpret the interval estimate for the contrast not shown above."
  },
  {
    "objectID": "content/week8-posthoc.html#power-calculations-for-anova",
    "href": "content/week8-posthoc.html#power-calculations-for-anova",
    "title": "Post-hoc inference in ANOVA",
    "section": "Power calculations for ANOVA",
    "text": "Power calculations for ANOVA\n\nHow much data should we collect to detect a difference in mean lifespan of 1 month?\n\n\n\nTo perform sample size power calculations, one needs:\n\nnumber of groups\ntarget significance level (\\(\\alpha\\))\ntarget power level\nguess or prior estimate of variance ratio \\(\\frac{\\text{group variation}}{\\text{error variation}}\\)\n\nFrom the existing study, \\(\\sqrt{MSE} = 6.6\\)\n\nSo to detect effects on the order of one month at 90% power:\n\npower.anova.test(groups = 4, \n                 sig.level = 0.05, \n                 power = 0.9, \n                 within.var = 6.633, \n                 between.var = 1)\n\n\n     Balanced one-way analysis of variance power calculation \n\n         groups = 4\n              n = 32.32851\n    between.var = 1\n     within.var = 6.633\n      sig.level = 0.05\n          power = 0.9\n\nNOTE: n is number in each group\n\n\n… we need 33 mice per treatment group.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week7-activity-nonparametric.html",
    "href": "content/week7-activity-nonparametric.html",
    "title": "Week 7 activity: Nonparametric inference",
    "section": "",
    "text": "library(tidyverse)\nload('data/brfss.RData')\nload('data/temps.RData')\nddt &lt;- MASS::DDT\n\nIn this activity you’ll learn about nonparametric alternatives to the \\(t\\) test for one and two means. The activity is organized much like a lab, but with extra narrative. You should read through the narrative at your own pace and try the exercises provided as you go. At the end there are two practice problems that you’ll be expected to complete before next class.\n\nBackground: parametric and nonparametric inference\nConsider the basis for the inferences developed so far: under certain conditions (typically regularity of the underlying population distribution, assessed by checking histograms for unimodality and approximate symmetry) and with sufficient sample sizes, a model is specified for the sampling distribution of a test statistic.\n\ninference for one mean: \\(t_{n - 1}\\) model for the sampling distribution of \\(T = \\frac{\\bar{x} - \\mu}{SE(\\bar{x})}\\)\ninference comparing two means: \\(t_{\\nu}\\) model for the sampling distribution of \\(T = \\frac{\\bar{x} - \\bar{y} - \\delta}{SE(\\bar{x} - \\bar{y})}\\)\ninference comparing several means: \\(F_{k - 1, n - k}\\) model for the sampling distribution of \\(F = \\frac{MSG}{MSE}\\)\n\nThese are all what are known as parametric models, because they are specified through one or two parameters that determine their exact shape. The parameters in this case are the degrees of freedom terms – \\(n - 1\\) for the one-sample \\(t\\) test, \\(\\nu\\) (usually estimated) for the two-sample \\(t\\) test, and \\(k - 1\\) and \\(n - k\\) for the \\(F\\) test.\nAs such, these procedures are examples of parametric inference – inferences that utilize a parametric model for the data and/or test statistic.\nWhen assumptions for parametric inference aren’t tenable, or when a parametric model is not available, there are so-called nonparametric methods of inference: methods that don’t depend on a parametric model such as the \\(t\\) or \\(F\\) models we’ve learned about in class.\nWe will consider specifically nonparametric procedures based on ranks, i.e., ordering observations from smallest to largest.\n\n\nMotivating examples\nIn practice, the situation that most often leads an analyst to consider rank-based nonparametric methods is that the assumptions for the \\(t\\) test either don’t hold or are difficult to assess.\nBelow are two such situations you’ve already encountered in this class.\n\nSmall sample sizes\nWhen sample sizes are small, it’s awkward to assess assumptions for parametric inference, because with few observations histograms can lack any discernible shape. For example, the most we can say about the following data on heart rates for 19 women and 20 men is that there are no evident outliers.\n\nheart.m &lt;- temps |&gt; filter(sex == 'male') |&gt; pull(heart.rate)\nheart.f &lt;- temps |&gt; filter(sex == 'female') |&gt; pull(heart.rate)\n\npar(mfrow = c(1, 2))\nhist(heart.m)\nhist(heart.f)\n\n\n\n\n\n\n\n\nIn practical terms, the \\(t\\) test is likely still fine under these circumstances; however, some may wish to consider an inference for comparing heart rates between groups that doesn’t depend on distributional assumptions.\n\n\nAssumptions don’t hold\nOn occasion you may go to check assumptions and find that they’re clearly violated. For example, the pairwise differences between actual and desired weights from BRFSS respondents showed clear skewness and several large outliers. In that case, the sample size was big enough that we overlooked the issue, but it’s not hard to imagine a similar situation cropping up with fewer observations.\nSuppose you only had 12 observations that showed the same skew and had one big outlier:\n\nset.seed(51424)\nweight.diff &lt;- sample(brfss$weight - brfss$wtdesire, 12)\nhist(weight.diff, breaks = 10)\n\n\n\n\n\n\n\n\nHere the \\(t\\) test isn’t appropriate, and using it anyway would likely result in a true significance level, coverage, and power quite different from the nominal levels specified in the test, so it would be hard to trust the result. This is a great situation to use a rank-based nonparametric alternative.\n\n\n\nInference on “location” (not mean)\nThe usual parametric inferences pertain to the population mean; not so with rank-based nonparametrics. Instead, these inferences pertain simply to “location”.\nOften “location” is characterized in terms of the “center” of a distribution so that inferences can be interpreted in a manner similar to parametric tests and intervals.\nWe will follow this convention and consider the hypotheses to be about the center(s) of the population model(s), denoted \\(c\\). For example, the two-sided test of center would test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &c = c_0 \\\\\nH_A: &c \\neq c_0\n\\end{cases}\n\\]\nFor the two-sided test of difference in centers, we will test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &c_1 = c_2 \\\\\nH_A: &c_1 \\neq c_2\n\\end{cases}\n\\]\nHowever, you should keep in mind that “location” is a more general notion that encompasses all measures of location of a distribution.\n\n\nOne-sample inference: signed rank test\nThe basic premise of this test is that, if the population model is symmetric, its center should evenly divide the data.\n\n\n\n\n\n\nWarm up\n\n\n\nConsider the following 15 measurements of DDT in kale in ppm in order from smallest to largest:\n\nsort(ddt)\n\n [1] 2.79 2.93 3.06 3.07 3.08 3.18 3.22 3.22 3.33 3.34 3.34 3.38 3.56 3.78 4.64\n\n\nSuppose you wish to test whether the center \\(c\\) of the population model is \\(c_0 = 3\\) ppm and assume a symmetric population model.\n\nHow many observations would you expect to be smaller than \\(c_0 = 3\\) if 3ppm is in fact the center?\nHow many observations are actually smaller than 3 ppm?\nBased on your answers to 1-2, do you think it is likely that in fact \\(c = 3\\)?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the population is symmetric about \\(c_0\\), you’d expect roughly half of observations (7.5) to be smaller than 3ppm.\nThe actual number of observations smaller than 3ppm is:\n\n# number of observations below 3\nsum(ddt &lt; 3)\n\n[1] 2\n\n\nThis is much less than half, so it seems unlikely that the center is actually 3ppm.\n\n\n\nThe signed rank test is a nonparametric alternative to the one-sample \\(t\\) test applicable to any symmetric population model. The particular form of symmetry and presence of outliers do not affect the test.\n\nHypotheses\nThe test can be directional or two-sided, just like the \\(t\\) test. Thus, the possible hypotheses are:\n\\[\nH_0: c = c_0 \\quad\\text{vs.}\\quad H_A: c \\mathrel{\\substack{&lt;\\\\\\neq\\\\ &gt;}} c_0\n\\]\n\n\nTest procedure\nWhile the intuition of the test is that half of observations should be smaller than the true center under population symmetry, the test statistic is not quite as direct as a tally of how many observations are below the hypothetical value. Instead, the procedure is as follows:\n\n[center] Calculate deviations \\(d_i = x_i - c_0\\)\n[rank] Sort and rank the absolute deviations \\(|d_i|\\)\n\naverage ranks in case of ties\ndrop zeros\n\n[sum] Add up the positive ‘signed ranks’ \\(\\sum_{\\text{sign}(d_i) &gt; 0} r_i\\)\n\nThis produces the test statistic:\n\\[V = \\sum_{i = 1}^n \\text{sign}(d_i) \\times R_i\\]\n\n\n\n\n\n\nCheck your understanding\n\n\n\nTry working out the rank sum procedure manually using the DDT data – it’s small enough that you could jot down the steps on some scratch paper. See if you can calculate \\(V\\).\nTo help, here are the deviations sorted smallest to largest:\n\n# calculate deviations\ndi &lt;- ddt - 3\nsort(di)\n\n [1] -0.21 -0.07  0.06  0.07  0.08  0.18  0.22  0.22  0.33  0.34  0.34  0.38\n[13]  0.56  0.78  1.64\n\n\n\nStart by writing the deviations in order of absolute value in a column.\nThen rank them 1-15 in an adjacent column. If there is a tie – e.g., two absolute deviations of 0.05 – then assign them both the average of the ranks. For example, if 0.05 occurs twice in positions 2 and 3, then give them both rank 2.5.\nWrite down the sign of the deviation in a new column.\nThen write down the “signed rank”, or product of the sign and the rank, in a fourth column.\nAdd up the positive signed ranks. This is the signed rank statistic.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis shows the steps in R, but don’t worry about the codes; the output is meant to illustrate what you would do on paper to find the rank sum statistic.\n\n# this shows the steps, column by column; focus on output\nddt.srank &lt;- tibble(di = di) |&gt;\n  mutate(abs.di = abs(di), \n         rank = rank(abs.di),\n         sign = sign(di),\n         signed.rank = sign*rank) |&gt;\n  arrange(abs.di)\nddt.srank\n\n# A tibble: 15 × 5\n        di abs.di  rank  sign signed.rank\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1  0.0600 0.0600   1       1         1  \n 2 -0.0700 0.0700   2.5    -1        -2.5\n 3  0.0700 0.0700   2.5     1         2.5\n 4  0.0800 0.0800   4       1         4  \n 5  0.180  0.180    5       1         5  \n 6 -0.21   0.21     6      -1        -6  \n 7  0.220  0.220    7.5     1         7.5\n 8  0.220  0.220    7.5     1         7.5\n 9  0.33   0.33     9       1         9  \n10  0.34   0.34    10.5     1        10.5\n11  0.34   0.34    10.5     1        10.5\n12  0.38   0.38    12       1        12  \n13  0.56   0.56    13       1        13  \n14  0.78   0.78    14       1        14  \n15  1.64   1.64    15       1        15  \n\n# signed rank statistic\nvstat &lt;- ddt.srank |&gt; filter(sign &gt; 0) |&gt; pull(signed.rank) |&gt; sum()\nvstat\n\n[1] 111.5\n\n\n\n\n\n\n\n\\(p\\)-values for the test\nJust like other hypothesis tests, the signed rank test rejects if \\(V\\) is sufficiently large in the direction of the alternative.\nA sampling distribution for \\(V\\) can be found exactly using combinatorics, or approximated using probability theory. In this caseWe won’t go into details about either approach, except to indicate that there is a sampling distribution for \\(V\\) that we can use to obtain \\(p\\)-values in the same fashion that the \\(t_{n - 1}\\) model was used to obtain \\(p\\)-values for the \\(t\\) test.\nIn this case, there are 3.2768^{4} possible sign combinations; of these, only about 0.375% give a larger value of \\(V\\). That provides a \\(p\\)-value for the test, and since \\(p = 0.0018 &lt; 0.05\\) we would reject \\(H_0\\) at the 5% significance level. This result is interpreted as:\n\nThe data provide strong evidence that the typical DDT concentration in kale is not 3ppm (signed rank statistic V = 111.5, p = 0.00375).\n\n\n\nImplementation with wilcox.test(...)\nThe implementation in R looks and functions much like t.test:\n\n# signed rank test at 1% level\nwilcox.test(ddt, mu = 3, alternative = 'two.sided', \n            exact = F, conf.int = T, conf.level = 0.99)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  ddt\nV = 111.5, p-value = 0.003751\nalternative hypothesis: true location is not equal to 3\n99 percent confidence interval:\n 3.060070 3.780032\nsample estimates:\n(pseudo)median \n       3.26001 \n\n\nSome remarks: - exact = F produces approximate \\(p\\)-values and confidence intervals; you may see warning messages if this is excluded - pseudo-median is a measure of center, but not the same as a median or mean (check!)\n\n\n\n\n\n\nYour turn 1\n\n\n\nPerform the analogous inference using the \\(t\\) test and compare the results. Do the tests agree at the 1% significance level?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe tests do not agree: the signed rank test rejects at the 1% level, but the \\(t\\) test does not.\n\n# perform t test at 1% level to compare\nt.test(ddt, mu = 3, alternative = 'two.sided', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.01151\nalternative hypothesis: true mean is not equal to 3\n99 percent confidence interval:\n 2.991996 3.664004\nsample estimates:\nmean of x \n    3.328 \n\n\n\n\n\n\n\n\n\n\n\nYour turn 2\n\n\n\nPerform the signed rank test to determine whether actual weight exceeds desired weight by more than 10lbs at the 5% significance level. Report the result in the usual narrative format. Compare your result with the inference obtained from a \\(t\\) test.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# use signed rank test to determine whether actual exceeds desired by at least 10lbs\nwilcox.test(weight.diff, mu = 10, alternative = 'greater', \n            exact = F, conf.int = T, conf.level = 0.95)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  weight.diff\nV = 61, p-value = 0.04559\nalternative hypothesis: true location is greater than 10\n95 percent confidence interval:\n 10.00003      Inf\nsample estimates:\n(pseudo)median \n       22.8617 \n\n# check t test result to compare\nt.test(weight.diff, mu = 10, alternative = 'greater', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  weight.diff\nt = 1.6127, df = 11, p-value = 0.06755\nalternative hypothesis: true mean is greater than 10\n95 percent confidence interval:\n 7.019188      Inf\nsample estimates:\nmean of x \n    36.25 \n\n\nInterpretation of the signed rank test test:\n\nThe data provide evidence that actual weight exceeds desired weight by more than 10lbs (signed rank statistic V = 61, p = 0.0456).\n\nThe signed rank test finds evidence that actual weight exceeds desired weight by more than 10lbs, where the \\(t\\) test does not.\n\n\n\n\n\n\nTwo-sample inference: rank sum test\nThe rank-sum test is a nonparametric alternative to the two-sample \\(t\\) test based on ranks. The key idea for the test is that if observations in both groups come from the same population distribution then they should be exchangeable (i.e., groupings don’t matter).\nThus, the only assumption for this test is that data are independent. The test can be directional, and thus it is possible to test the following hypotheses comparing centers:\n\\[\nH_0: c_1 = c_2\\quad\\text{vs}\\quad H_A: c_1 \\mathrel{\\substack{&lt;\\\\\\neq\\\\&gt;}} c_2\n\\]\n\nThe alternative hypothesis\nThis test is a bit funny in that the null hypothesis is really that the distributions are exactly the same. The alternative to this possibility can come about in a number of ways. The alternative is usually interpreted as a difference in location, primarily because this is the situation that the test has power to detect.\nSo, it is often said that the test also assumes that the samples differ only in location. In other words, the test is most appropriate when the histograms look “shifted”, but not fundamentally different, as illustrated below.\n\n\n\n\n\n\n\n\n\nIn all of these cases, there is a (true) difference in location, but if shape differs too much, the rank sum test should not be used. That said, it can be hard to tell with small samples, so it may not really be practical to to check this assumption.\nExample 1. Out-of-state tuition costs from 26 public and 26 private universities. These data differ primarily in spread, not location; so the rank-sum test might not work well here.\n\n\n\n\n\n\n\n\n\nExample 2. Deviations from expected cancer rates in CT in years with high and low sunspot activity. The shape is a bit hard to discern here, but it seems plausible that the distribution for high sunspot years is shifted to the right of that for low sunspot years. So, the rank sum test would be appropriate here.\n\n\n\n\n\n\n\n\n\n\n\nRank sum test procedure\nThe rank sum procedure, though a bit opaque, is rather simple:\n\n[pool] Combine observations from both groups\n[rank] Sort and rank pooled observations\n[sum] Add up ranks in the first group\n[adjust] Subtract \\(\\frac{n_1(n_1 + 1)}{2}\\), where \\(n_1\\) is the sample size of the first group\n\nThe test rejects if the sum is larger than expected in the direction of the alternative. As in the signed rank test, combinatorics are used to determine a \\(p\\)-value for the test.\nThe rationale for this procedure is that if the distributions are the same, then the ranks should be evenly distributed among the two groups; this induces a particular sampling distribution on the sum of the ranks in each group. The adjustment facilitates computation of \\(p\\)-values.\nLet’s illustrate using data from an experiment in which participants were randomly assigned to receive a fish oil supplement or a regular oil supplement. For each subject, the reduction in blood pressure was measured after a period of time on the treatments.\n\n# load dataset\nfish.oil &lt;- Sleuth3::ex0112 |&gt; rename_with(tolower)\n\n# make boxplot\nboxplot(bp ~ diet, data = fish.oil, horizontal = T)\n\n\n\n\n\n\n\n\nThere’s a bit of difference in spread, but enough of a shift in location that the rank sum test is reasonable to apply.\n\n\n\n\n\n\nCheck your understanding\n\n\n\nCarry out the rank sum procedure outlined above and compute the rank sum statistic by summing up the ranks in the fish oil group.\nTo facilitate calculations, here are the data in order of increasing blood pressure:\n\nfish.oil |&gt; arrange(bp)\n\n   bp       diet\n1  -6 RegularOil\n2  -4 RegularOil\n3  -3 RegularOil\n4   0    FishOil\n5   0    FishOil\n6   0 RegularOil\n7   1 RegularOil\n8   2    FishOil\n9   2 RegularOil\n10  2 RegularOil\n11  8    FishOil\n12 10    FishOil\n13 12    FishOil\n14 14    FishOil\n\n\nAdd a column of ranks by hand (or in R if you can figure out how!), averaging ranks for any ties. Then add up the ranks in the FishOil group, and subtract \\(\\frac{n_1(n_1 + 1)}{2} = \\frac{7\\times 8}{2} = 28\\) to obtain the rank sum statistic.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe output below illustrates the rankings and selection of which ones to add up. Don’t worry about the codes\n\n# again, ignore the codes; look at output\nfish.oil.ranksum &lt;- fish.oil |&gt; \n  mutate(rank = rank(bp),\n         ranks.fish = rank*(diet == 'FishOil')) |&gt;\n  arrange(bp)\nfish.oil.ranksum\n\n   bp       diet rank ranks.fish\n1  -6 RegularOil    1          0\n2  -4 RegularOil    2          0\n3  -3 RegularOil    3          0\n4   0    FishOil    5          5\n5   0    FishOil    5          5\n6   0 RegularOil    5          0\n7   1 RegularOil    7          0\n8   2    FishOil    9          9\n9   2 RegularOil    9          0\n10  2 RegularOil    9          0\n11  8    FishOil   11         11\n12 10    FishOil   12         12\n13 12    FishOil   13         13\n14 14    FishOil   14         14\n\n# rank sum statistic\nn1 &lt;- count(fish.oil, diet)$n[1]\nsum(fish.oil.ranksum$ranks.fish) - n1*(n1 + 1)/2\n\n[1] 41\n\n\n\n\n\n\n\nImplementation using wilcox.test(...)\nThe wilcox.test function also implements the rank sum test, using the same syntax as t.test(...). For example, using the fish oil data, we might test at the 1% significance level whether the fish oil supplement caused a greater reduction in blood pressure:\n\nwilcox.test(bp ~ diet, data = fish.oil, \n            mu = 0, alternative = 'greater', \n            exact = F, conf.int = T, conf.level = 0.99)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bp by diet\nW = 41, p-value = 0.01957\nalternative hypothesis: true location shift is greater than 0\n99 percent confidence interval:\n -0.9999444        Inf\nsample estimates:\ndifference in location \n              7.999962 \n\n\nFor this test, the \\(p\\)-value gives the percentage of possible rank allocations among the groups for which the rank sum is at least as favorable to \\(H_A\\); again this is computed using combinatorics or approximation methods.\nIn this instance, the \\(p\\)-value indicates that only about 1.96% of all possible rank allocations among the two groups would produce a rank sum statistic at least as large. Thus, testing at the 1% significance level:\n\nThe data provide do not provide sufficient evidence at the 1% significance level that the fish oil supplement caused a greater reduction in blood pressure than the regular oil supplement (rank sum statistic W = 41, p = 0.01957).\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nPerform the corresponding \\(t\\) test for comparison at the 1% significance level. Do the tests agree?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe tests do not agree; the \\(t\\) test supports evidence of an effect at the 1% level where the rank sum test does not.\n\n# t test for effect of treatment at 1% level\nt.test(bp ~ diet, data = fish.oil, mu = 0, \n       alternative = 'greater', conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  bp by diet\nt = 3.0621, df = 9.2643, p-value = 0.006542\nalternative hypothesis: true difference in means between group FishOil and group RegularOil is greater than 0\n95 percent confidence interval:\n 3.111056      Inf\nsample estimates:\n   mean in group FishOil mean in group RegularOil \n                6.571429                -1.142857 \n\n\n\n\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nUse the cancer dataset (specifically the delta variable) to test whether the change in cancer rate is higher in years with high sunspot activity. Carry out the test at the 5% level, and report the result in the usual narrative style.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# test whether change in cancer rate is higher in years with high sunspot activity at the 5% level\nwilcox.test(delta ~ sunspot, data = cancer, mu = 0,\n            exact = F, alternative = 'greater', \n            conf.int = T, conf.level = 0.95)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  delta by sunspot\nW = 157.5, p-value = 0.3072\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n -0.1000484        Inf\nsample estimates:\ndifference in location \n            0.09999013 \n\n\nInterpretation:\n\nThe data provide no evidence that the change in cancer rate is higher in years with high sunspot activity (rank sum statistic W = 157.5, p = 0.3072).\n\n\n\n\n\n\n\nPractice problem\n\nIs there a difference in cholesterol associated with consuming oat bran compared with consuming corn flakes? Use the cholesterol dataset to test for a difference at the 5% level using a nonparametric test.\n\nConstruct boxplots to compare the distributions for location shift. Does the nonparametric test seem appropriate?\nCarry out the test.\nReport the test result in the usual narrative style."
  },
  {
    "objectID": "content/lab10-posthoc.html",
    "href": "content/lab10-posthoc.html",
    "title": "Lab 10: Post-hoc inference in ANVOA",
    "section": "",
    "text": "The objective of this lab is to learn how to perform post-hoc inference for group means and contrasts in R. These procedures are called post-hoc because they are typically performed after detecting a treatment effect or difference in means using the ANOVA \\(F\\) test.\nIn other words, suppose you tested the hypotheses: \\[\n\\begin{cases}\nH_0: &\\mu_i = \\mu_j \\quad\\text{for all}\\quad &i, j \\\\\nH_A: &\\mu_i \\neq \\mu_j \\quad\\text{for some}\\quad &i\\neq j \\\\\n\\end{cases}\n\\] And found evidence favoring \\(H_A\\). This raises the question, “which means differ and by how much?”\nWe will cover:\n\nSimultaneous interval estimates for \\(\\mu_i\\)\nIntervals for pairwise contrasts \\(\\mu_i - \\mu_j\\)\nSignificance tests for pairwise contrasts \\(\\mu_i - \\mu_j\\)\nInference for contrasts with a control group \\(\\mu_i - \\mu_\\text{ctrl}\\)\n\nWe will use the \\(\\texttt{emmeans}\\) package in R. However, you should be advised that there are other common implementations of these procedures that you might encounter if you search on your own.\nExamples will use the longevity dataset, which contain observations of lifetimes of mice randomly allocated to four different diet restriction groups. You’ll practice using the anorexia dataset, which contains observations of percent change in body weight after a treatment period for young women randomly allocated to two treatment groups and a control group.\n\nlibrary(tidyverse)\nlibrary(emmeans)\nload('data/longevity.RData')\nload('data/anorexia.RData')\n\n\nRefresher: fitting ANOVA models\nPost-hoc inferences all utilize fitted ANOVA models. We will skip the step of making a graphical check on model assumptions, and proceed directly with fitting models. However, if you’re not sure what that step consists of, you should take a moment to look at the previous lab to remind yourself.\nFor the longevity data, inference compares the mean lifetime in months for four levels of dietary restriction.\n\n# fit the model\nfit.longevity &lt;- aov(lifetime ~ diet, data = longevity)\n\n# generate the ANOVA table\nsummary(fit.longevity)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \ndiet          3  11426    3809   87.41 &lt;2e-16 ***\nResiduals   233  10152      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA \\(F\\) test is significant, indicating an effect of diet restriction on mean lifetime.\n\nThe data provide evidence of an effect of diet restriction on mean lifetime among mice (F = 87.41 on 3 & 233 degrees of freedom, p &lt; 0.0001).\n\n\n\n\n\n\n\nYour turn 1\n\n\n\nFit an ANOVA model to the anorexia data. Here, inference is on the mean percent change in body weight. Interpret the result of the test in context.\n\n\ntibble [72 × 2] (S3: tbl_df/tbl/data.frame)\n $ pct.change: num [1:72] 0.994 0.896 0.941 1.166 0.974 ...\n $ treatment : Factor w/ 3 levels \"Cont\",\"CBT\",\"FT\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nThe emmeans(...) function (and other related functions) work directly with fitted ANOVA models to produce inferences for group means and contrasts. emmeans(...) is short for “estimated marginal means”. The function takes as its arguments a fitted ANOVA model, and a “specification” in the form of a formula that determines its precise behavior:\nemmeans(object = &lt;FITTED MODEL&gt;, spec = &lt;SPECIFICATION&gt;)\nFor us, the specification is always a one-sided formula simply reiterating the grouping variable.\n\n# default behavior is to produce unadjusted 95% confidence intervals for group means\nemmeans(object = fit.longevity, specs = ~ diet)\n\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.5     29.3\n N/N85   32.7 0.874 233     31.0     34.4\n N/R50   42.3 0.783 233     40.8     43.8\n N/R40   45.1 0.852 233     43.4     46.8\n\nConfidence level used: 0.95 \n\n\nThe result can be piped to helper functions to obtain estimates of group means and contrasts.\n\n\nEstimating group means\nInitially we might like to estimate the group means. All we need to do is implement the adjustment for multiple inference and specify the confidence level. This is done using the confint(...) helper function:\n\n# simultaneous 95% confidence intervals for group means with bonferroni adjustment (correct)\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.0     29.8\n N/N85   32.7 0.874 233     30.5     34.9\n N/R50   42.3 0.783 233     40.3     44.3\n N/R40   45.1 0.852 233     43.0     47.3\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \n\n\nThe results can be interpreted in the usual way, for instance:\n\nWith 95% confidence, the mean lifetime for mice on a normal 85kcal diet is estimated to be between 30.5 and 34.9 months, with a point estimate of 32.7 months (SE 0.874).\n\nBecause of the Bonferroni adjustment, the confidence level is simultaneous for all intervals, meaning that all four intervals cover the mean at the same time 95% of the time.\n\n\n\n\n\n\nYour turn 2\n\n\n\nUsing the anorexia data, compute simultaneous 99% confidence intervals for the mean percent change in body weight in each treatment group and the control group. Interpret the interval for the family therapy (FT) group.\n\n\nSometimes a plot is preferable to a table of estimates. This is accomplished by adding one more pipe to plot(...) and specifying labels:\n\n# plot via: emmeans(...) |&gt; confint(...) |&gt; plot(...)\nemmeans(object = fit.longevity, specs = ~ diet) |&gt; \n  confint(level = 0.95, adjust = 'bonferroni') |&gt; \n  plot(xlab = 'mean lifetime (months)', ylab = 'diet')\n\n\n\n\n\n\n\n\nIf you’re curious, remove the label arguments and see what the default looks like.\n\n\n\n\n\n\nYour turn 3\n\n\n\nMake a plot showing the simultaneous 99% interval estimates for the mean percent change in body weight that you computed in the previous “your turn”.\n\n\n\n\nEstimating contrasts\nA difference in means is an example of a “contrast”. Inferences for contrasts allow us to determine which groups differ and by how much. There are several types of contrasts, but the most common are pairwise differences in means and differences between treatments and a control group.\n\nInference for pairwise contrasts\nFirst we’ll consider computing intervals and tests for all pairwise contrasts. This is accomplished by simply passing the result of emmeans(...) to contrast(...) and specifying the type of contrast you wish to obtain. The result can be passed to test() to obtain tests and confint() to obtain intervals. In the context of the longevity example:\n\n# test for pairwise differences at the 5% significance level\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  test(adjust = 'bonferroni')\n\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  &lt;.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  &lt;.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  &lt;.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  &lt;.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n\n# simultaneous 95% intervals for all pairwise contrasts\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('pairwise') |&gt;\n  confint(level = 0.95, adjust = 'bonferroni')\n\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n\n\nThe tests indicate which means differ significantly; the intervals indicate by how much. For example:\n\nThe data provide strong evidence that mean lifespan differs significantly between mice on a normal 85kcal compared with mice on an unrestricted diet (p = 0.0003). With 95% confidence, the difference in mean lifespan (normal - unrestricted) is estimated to be between 1.87 and 8.71 months, with a point estimate of 5.29 (SE 1.29).\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nUsing the data from the anorexia study…\n\nCompute simultaneous 90% confidence intervals for all pairwise contrasts.\nCompute adjusted \\(p\\)-values for all pairwise contrasts and determine which groups differ significantly at the 10% level.\nInterpret the test and interval for the contrast between family therapy and the control group.\n\n\n\n\n\nContrasts with a control\nMany studies involve a control group; naturally, it is of interest to compare treatments to controls. This is a special category of contrasts because all contrasts involve the same group; as such, there is a special adjustment method for multiple inference that achieves better power for this particular setting.\nTo perform inference for contrasts with a control, change the contrast type from 'pairwise' to 'trt.vs.ctrl'. R will assume that your control group is the first level of the grouping variable. The datasets for this class are organized in just this way, so you don’t have to worry about this detail for now.\n\n# tests for contrasts with a control group\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('trt.vs.ctrl') |&gt;\n  test(adjust = 'dunnett')\n\n contrast     estimate   SE  df t.ratio p.value\n (N/N85) - NP     5.29 1.29 233   4.113  0.0002\n (N/R50) - NP    14.90 1.23 233  12.150  &lt;.0001\n (N/R40) - NP    17.71 1.27 233  13.938  &lt;.0001\n\nP value adjustment: dunnettx method for 3 tests \n\n# simultaneous 95% intervals for contrasts with a control group\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('trt.vs.ctrl') |&gt;\n  confint(level = 0.95, adjust = 'dunnett')\n\n contrast     estimate   SE  df lower.CL upper.CL\n (N/N85) - NP     5.29 1.29 233     2.23     8.34\n (N/R50) - NP    14.90 1.23 233    11.98    17.81\n (N/R40) - NP    17.71 1.27 233    14.70    20.73\n\nConfidence level used: 0.95 \nConf-level adjustment: dunnettx method for 3 estimates \n\n\nThe results indicate that all levels of diet restriction have an effect on mean lifetime that differs from the control group. Moreover, the intervals indicate that mean lifetimes are longer in every treatment group than in the control group; so diet restriction at every level causes an increase in lifetime. In particular, for instance:\n\nThe data provide very strong evidence that mean lifespan differs significantly between mice on a restricted 40kcal diet compared with mice on an unrestricted diet (p &lt; 0.0001). With 95% confidence, the difference (restricted - unrestricted) in mean lifetime is estimated to be between 11.98 and 17.81 months, with a point estimate of 14.90 months (SE 1.23).\n\n\n\n\n\n\n\nYour turn 5\n\n\n\nUsing the data from the anorexia study…\n\nTest, at the 1% significance level, for significant differences in mean percent change in body weight for each treatment compared with the control group. Are significant differences improvements relative to the control?\nEstimate the efficacy (difference relative to control) of each treatment at the confidence level appropriate for the test you performed. Interpret the result in context.\n\n\n# test for differences relative to control at 1% level\n\n# estimate differences at the appropriate confidence level\n\n\n\n\n\nExtra: testing a minimum difference\nWhile not especially common, sometimes you might wish to test whether group means differ by at least a certain amount. The usual hypothesis tests for pairwise differences are: \\[\n\\begin{cases}\nH_0: &\\mu_i - \\mu_j = 0 \\\\\nH_A: &\\mu_i - \\mu_j \\neq 0 \\\\\n\\end{cases}\n\\] We could instead test for a minimum difference of \\(c\\) by testing: \\[\n\\begin{cases}\nH_0: &|\\mu_i - \\mu_j| = c \\\\\nH_A: &|\\mu_i - \\mu_j| &gt; c \\\\\n\\end{cases}\n\\]\nThis looks tricky on face value because of the absolute value. However, if the groups are ordered in R monotonically by means (i.e., in increasing/decreasing order of group mean), the signs for pairwise contrasts will all match, as they do in the example provided. In this case, the hypothesis above reduce, for \\(i &gt; j\\), to: \\[\n\\begin{cases}\nH_0: &\\mu_i - \\mu_j = c \\\\\nH_A: &\\mu_i - \\mu_j &gt; c \\\\\n\\end{cases}\n\\]\nSo, to test for a minimum difference, we simply do a directional test with a nonzero null value: add null = ... and side = ... arguments to test(...). In the context of comparisons with the control:\n\n# test whether mean lifetime exceeds control by more than 1 year\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('trt.vs.ctrl') |&gt;\n  test(null = 12, side = '&gt;')\n\n contrast     estimate   SE  df null t.ratio p.value\n (N/N85) - NP     5.29 1.29 233   12  -5.219  1.0000\n (N/R50) - NP    14.90 1.23 233   12   2.362  0.0283\n (N/R40) - NP    17.71 1.27 233   12   4.496  &lt;.0001\n\nP value adjustment: sidak method for 3 tests \nP values are right-tailed \n\n# interval estimate\nemmeans(object = fit.longevity, specs = ~ diet) |&gt;\n  contrast('trt.vs.ctrl') |&gt;\n  confint(level = 0.95, side = '&gt;')\n\n contrast     estimate   SE  df lower.CL upper.CL\n (N/N85) - NP     5.29 1.29 233     2.55      Inf\n (N/R50) - NP    14.90 1.23 233    12.28      Inf\n (N/R40) - NP    17.71 1.27 233    15.00      Inf\n\nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 3 estimates \n\n\nNotice that a different adjustment method is used. This has a slightly more precise interpretation:\n\nThe data provide moderate evidence that mean lifespan increases by at least one year when intake is restricted to 50kcal/day compared with an unrestricted diet (T = 2.362 on 233 degrees of freedom, p = 0.0283). With 95% confidence, the mean increase is estimated to be at least 12.28 months, with a point estimate of 14.9 months (SE 1.23).\n\n\n\n\nPractice problems\n\n[L9] The mussels dataset includes observations of a shell measurement, anterior adductor muscle (AAM) scar length, for mytilus trossulus mussels from five populations. A plot of the data is provided below.\n\nFit an ANOVA model and test for significant differences among the populations at the 1% significance level.\nEstimate mean AAM length for each population.\nTest for pairwise differences to determine which populations differ at the 1% significance level.\nProvide simultaneous interval estimates at an appropriate confidence level for each significant difference.\n\n\n\n\n\n\n\n\n\n\n\n\n[L9] The plantgrowth dataset includes measurements of dry weight of plants grown using one of two fertilizer treatments or no fertilizer (control); treatments were randomly allocated to plants. A plot of the data is provided below.\n\nAssess assumptions for ANOVA based on the plot.\nFit an ANOVA model and test for a difference in mean dry weight at the 5% significance level.\nTest for significant differences in mean dry weight between each treatment compared with the control at the 5% level.\nInterpret your results. Do treatments cause a change in growth, as measured by dry weight, compared with the control?\n\n\n\n\n\n\n\n\n\n\n\n\n[Extra credit] Using the longevity data from lecture, compute interval estimates for log-contrasts and back-transform to obtain estimates for the percent change in median lifespan relative to the control group. Report the comparison between the normal (N/N85) diet and the unrestricted (NP) diet."
  },
  {
    "objectID": "content/week8-proportions.html#todays-agenda",
    "href": "content/week8-proportions.html#todays-agenda",
    "title": "Inference for population proportions",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] Inference for binomial proportions\n[lab] Tests for proportions in R"
  },
  {
    "objectID": "content/week8-proportions.html#review-categorical-data",
    "href": "content/week8-proportions.html#review-categorical-data",
    "title": "Inference for population proportions",
    "section": "Review: categorical data",
    "text": "Review: categorical data\n\nA variable is categorical if its values are one of several categories.\n\n\n\nInference for categorical data has a different flavor:\n\nnon-numeric values \\(\\Rightarrow\\) can’t compute usual statistics (mean, variance, etc.)\nfocus on proportions instead\n\n\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n173\n261\n161\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n0.2908\n0.4387\n0.2706"
  },
  {
    "objectID": "content/week8-proportions.html#on-binomial-proportions",
    "href": "content/week8-proportions.html#on-binomial-proportions",
    "title": "Inference for population proportions",
    "section": "On binomial proportions",
    "text": "On binomial proportions\n\nInference for categorical data focuses on population proportions\n\n\n\nConsider the binomial data setting:\n\na single categorical variable with two categories\none category represents an outcome, trait, or property of interest\n\nThe population proportion is the frequency of occurrence of the category of interest.\n\\[p = \\frac{\\# \\text{ total occurrences}}{\\text{population size } N}\\]\nIf an individual is selected at random, \\(p\\) also gives the probability of an occurrence.\n\nExample binomial data:\n\n\n\n\n\n\n\n\n\nID\nDiabetes\n\n\n\n\n60312\nNo\n\n\n63545\nNo\n\n\n52054\nYes\n\n\n65850\nYes"
  },
  {
    "objectID": "content/week8-proportions.html#estimating-proportions",
    "href": "content/week8-proportions.html#estimating-proportions",
    "title": "Inference for population proportions",
    "section": "Estimating proportions",
    "text": "Estimating proportions\n\n\n\n\n\nDiabetes data summary\n\n\n\n\n\n\n\n\n \nYes\nNo\ntotal\n\n\n\n\ncount\n57\n443\n500\n\n\nproportion\n0.114\n0.886\n1\n\n\n\n\n\nEstimated diabetes prevalence: 11.4%.\n\nNHANES data are a random sample of the U.S. adult population\nsample statistics should approximate population statistics\n\n\nWe’ll formalize this as estimating the population proportion \\[p = \\frac{\\# \\text{ individuals with diabetes}}{\\text{total population size } N}\\] Using the sample proportion \\[\\hat{p} = \\frac{\\# \\text{ respondents with diabetes}}{\\text{sample size } n}\\]\n\n\nThe first step towards inference is a measure of precision for \\(\\hat{p}\\). What is \\(SE(\\hat{p})\\)?"
  },
  {
    "objectID": "content/week8-proportions.html#two-interpretations",
    "href": "content/week8-proportions.html#two-interpretations",
    "title": "Inference for population proportions",
    "section": "Two interpretations",
    "text": "Two interpretations\nThe point estimate\n\\[\\hat{p} = \\frac{\\# \\text{ respondents with diabetes}}{\\text{sample size } n} = \\frac{57}{500} = 0.114\\]\nhas two (equivalent) interpretations:\n\nthe prevalence of diagnosed diabetes among U.S. adults is estimated to be 11.4%\nthe probability that a randomly chosen U.S. adult has diagnosed diabetes is 0.114"
  },
  {
    "objectID": "content/week8-proportions.html#variability-of-binomial-data",
    "href": "content/week8-proportions.html#variability-of-binomial-data",
    "title": "Inference for population proportions",
    "section": "Variability of binomial data",
    "text": "Variability of binomial data\n\nBinomial data are most variable when \\(p = 0.5\\) and least variable when \\(p \\approx 0\\) or \\(1\\)\n\n\n\nMeasure of spread for binomial data: \\[\\sqrt{\\hat{p}(1 - \\hat{p})}\\]\n\nhighest when \\(\\hat{p} \\approx 0.5\\)\nlowest when \\(\\hat{p} \\approx 0 \\text{ or } 1\\)\n\nAnalogous to estimating a mean: \\[\nSE\\left(\\hat{p}\\right)\n= \\frac{\\text{spread}}{\\sqrt{\\text{sample size}}}\n= \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]"
  },
  {
    "objectID": "content/week8-proportions.html#sampling-distribution-of-hatp",
    "href": "content/week8-proportions.html#sampling-distribution-of-hatp",
    "title": "Inference for population proportions",
    "section": "Sampling distribution of \\(\\hat{p}\\)",
    "text": "Sampling distribution of \\(\\hat{p}\\)\n\n\nThe sample proportion \\(\\hat{p}\\) has a sampling distribution that can be approximated by a normal model, provided:\n\n\\(\\hat{p}\\) isn’t too close to 0 or 1\n\\(n\\) is sufficiently large\n\nA common condition to check:\n\\[n\\hat{p} \\geq 10\\text{ and }n(1 - \\hat{p}) \\geq 10\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis model can be used to construct hypothesis tests and confidence intervals for \\(p\\)."
  },
  {
    "objectID": "content/week8-proportions.html#estimation-uncertainty",
    "href": "content/week8-proportions.html#estimation-uncertainty",
    "title": "Inference for population proportions",
    "section": "Estimation uncertainty",
    "text": "Estimation uncertainty\n\nRecall: a standard error estimates the variability of a sample statistic across samples\n\nThe standard error for the sample proportion is: \\[SE\\left(\\hat{p}\\right) = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\\]\nFor the diabetes data this estimate is \\(SE(\\hat{p})\\) = 0.014213.\n\non average, the sample proportion varies by 0.014 from sample to sample\nusing a normal model:\n\n68% of samples produce a \\(\\hat{p}\\) within 0.014 of \\(p\\)\n95% of samples produce a \\(\\hat{p}\\) within 0.028 of \\(p\\)"
  },
  {
    "objectID": "content/week8-proportions.html#inference-for-a-proportion",
    "href": "content/week8-proportions.html#inference-for-a-proportion",
    "title": "Inference for population proportions",
    "section": "Inference for a proportion",
    "text": "Inference for a proportion\n\n\n\n\n\nPoint estimate for diabetes prevalence\n\n\n\n\n\n\n\np.hat\nse\nn\n\n\n\n\n0.114\n0.01421\n500\n\n\n\n\n\n\n\nIt is estimated that the proportion of the U.S. adult population with diagnosed diabetes is 11.4% (SE = 1.42%).\n\n\n\nCheck assumptions for the normal model:\n\n\\(500\\times 0.114 = 57 \\geq 10\\)\n\\(500\\times 0.886 = 443 \\geq 10\\)"
  },
  {
    "objectID": "content/week8-proportions.html#confidence-intervals",
    "href": "content/week8-proportions.html#confidence-intervals",
    "title": "Inference for population proportions",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nTo calculate a 95% confidence interval “by hand” for the diabetes prevalence:\n\\[\n0.114 \\pm 2\\times 0.01421 = (0.0881, 0.1459)\n\\]\n\n\nWith 95% confidence, it is estimated that between 8.81% and 14.59% of the U.S. adult population has diagosed diabetes.\n\n\n\nThe critical value \\(c\\) comes from the normal model.\n\nexact value depends on desired coverage\nempirical rule:\n\n\\(c = 1\\) gives a 68% interval\n\\(c = 2\\) gives a 95% interval\n\\(c = 3\\) gives a 99.7% interval\n\ncalculation in R: qnorm((1 - coverage)/2, lower.tail = F)"
  },
  {
    "objectID": "content/week8-proportions.html#hypothesis-tests",
    "href": "content/week8-proportions.html#hypothesis-tests",
    "title": "Inference for population proportions",
    "section": "Hypothesis tests",
    "text": "Hypothesis tests\n\n\nConsider testing: \\[\\begin{cases}H_0: p = p_0 \\\\ H_A: p \\mathrel{\\substack{&lt; \\\\\\neq \\\\ &gt;}} p_0\\end{cases}\\]\nHypothesis tests use the test statistic:\n\\[Z = \\frac{\\hat{p} - p_0}{SD_0(\\hat{p})} = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\\]\nAnd reject when \\(Z\\) is large in the direction of the alternative.\nUnder \\(H_0\\), the sampling distribution of \\(Z\\) is approximated by a normal model.\n\nFor instance, to test whether diabetes prevalence exceeds 10%: \\[Z = \\frac{0.114 - 0.1}{\\sqrt{\\frac{0.1\\times 0.9}{500}}} = 1.043\\]"
  },
  {
    "objectID": "content/week8-proportions.html#inference-in-r",
    "href": "content/week8-proportions.html#inference-in-r",
    "title": "Inference for population proportions",
    "section": "Inference in R",
    "text": "Inference in R\n\n\nThree steps:\n\nConstruct a table of the frequency distribution by group\n\noutcomes should be columns\ngroups should be rows\n\nPass to prop.test()\n\nThe alternative reads the same way as in t.test.\n\n\n# variables of interest\ntreatment &lt;- vitamin$treatment\noutcome &lt;- vitamin$outcome\n\n# pass table to prop.test\ntable(treatment, outcome) |&gt;\n  prop.test(alternative = 'greater', \n            correct = F)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  table(treatment, outcome)\nX-squared = 6.3366, df = 1, p-value = 0.005914\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.02548153 1.00000000\nsample estimates:\n   prop 1    prop 2 \n0.8150852 0.7420147 \n\n\n\n\n\nThe data provide strong evidence that vitamin C prevents common cold (Z = 2.517, p = 0.0059). With 95% confidence, the reduction in probability is estimated to be at least 0.0255, with a point estimate of 0.0731 (SE = 0.0289)."
  },
  {
    "objectID": "content/week8-proportions.html#exact-inference",
    "href": "content/week8-proportions.html#exact-inference",
    "title": "Inference for population proportions",
    "section": "Exact inference",
    "text": "Exact inference\n\nThe normal model is an approximation.\n\nThe test can also be performed using the exact sampling distribution obtained from a binomial probability model.\n\nbinom.test(x = 57, n = 500, p = 0.1, alternative = 'greater')\n\n\n    Exact binomial test\n\ndata:  57 and 500\nnumber of successes = 57, number of trials = 500, p-value = 0.1659\nalternative hypothesis: true probability of success is greater than 0.1\n95 percent confidence interval:\n 0.0913675 1.0000000\nsample estimates:\nprobability of success \n                 0.114 \n\n\nInputs:\n\nx gives the number of occurrences of the category of interest\nn gives the sample size"
  },
  {
    "objectID": "content/week8-proportions.html#continuity-correction",
    "href": "content/week8-proportions.html#continuity-correction",
    "title": "Inference for population proportions",
    "section": "Continuity correction",
    "text": "Continuity correction\n\nThe approximation error for the normal model may be adjusted using a “continuity correction”\n\nIdea: adjust the test statistic slightly for a more conservative test.\n\n\n\n\n\n\n\nNotice that the correction (right) results in a test that much more closely matches the exact test."
  },
  {
    "objectID": "content/week8-proportions.html#your-turn-sleep-trouble",
    "href": "content/week8-proportions.html#your-turn-sleep-trouble",
    "title": "Inference for population proportions",
    "section": "Your turn: sleep trouble",
    "text": "Your turn: sleep trouble\n\nUse the NHANES data to estimate the proportion of U.S. adults reporting sleep trouble. Test whether at least 20% of U.S. adults report sleep trouble.\n\n\n\n\nCompute point estimate and standard error\nPerform the hypothesis test\n\nwrite the hypotheses\ncalculate the value of the test statistic\nrecord the \\(p\\)-value\n\nConstruct an interval estimate\n\n\n\n# to get you started\ntrouble &lt;- factor(nhanes$SleepTrouble, \n                  levels = c('Yes', 'No'))\ntrouble.tbl &lt;- table(trouble)\n\n\n\n\nHaving sleep trouble?\n\n\n\n\n\n\nYes\nNo\n\n\n\n\n0\n0"
  },
  {
    "objectID": "content/week8-proportions.html#comparing-two-proportions",
    "href": "content/week8-proportions.html#comparing-two-proportions",
    "title": "Inference for population proportions",
    "section": "Comparing two proportions",
    "text": "Comparing two proportions\n\nInference on the difference between proportions uses substantially similar methods\n\n\n\n\n\n\nVitamin C experiment\n\n\n\n\n\n\n\n\n \nCold\nNoCold\nn\n\n\n\n\nPlacebo\n335\n76\n411\n\n\nVitC\n302\n105\n407\n\n\n\n\n\n\nvitamin C and placebo treatments were randomly allocated to 818 volunteers\nvolunteers took treatments daily for a cold season\nstudy recorded how many volunteers came down with a cold\n\n\nConsider inference on the difference\n\\[p_\\text{placebo} - p_\\text{vitamin C}\\]\n\nInferences based on groupwise estimates:\n\npoint estimate: \\(\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitamin C}\\)\nstandard error: \\(\\sqrt{SE^2(\\hat{p}_\\text{placebo}) + SE^2(\\hat{p}_\\text{vitamin C})}\\)\ninterval: \\(\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitamin C} \\pm c\\times SE\\)"
  },
  {
    "objectID": "content/week8-proportions.html#tests-for-a-difference-in-proportions",
    "href": "content/week8-proportions.html#tests-for-a-difference-in-proportions",
    "title": "Inference for population proportions",
    "section": "Tests for a difference in proportions",
    "text": "Tests for a difference in proportions\n\n\nWe can also test whether vitamin C prevents common cold:\n\\[\n\\begin{cases}\nH_0: &p_\\text{placebo} - p_\\text{vitC} = 0\\\\\nH_A: &p_\\text{placebo} - p_\\text{vitC} &gt; 0\n\\end{cases}\n\\]\nHypothesis tests use the test statistic:\n\\[Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\\]\nWith a slightly different SE where: \\[\\hat{p} = \\frac{n_1\\hat{p}_1 + n_2\\hat{p}_2}{n_1 + n_2}\\]\n\n\n\n\n\n\n\n\n\n\nHere \\(p = P(Z &gt; 2.517) = 0.0059\\), so:\n\nthe data provide strong evidence that vitamin C prevents common cold."
  },
  {
    "objectID": "content/week8-proportions.html#vitamin-c-example-in-detail",
    "href": "content/week8-proportions.html#vitamin-c-example-in-detail",
    "title": "Inference for population proportions",
    "section": "Vitamin C example in detail",
    "text": "Vitamin C example in detail\n\n\nSample proportions:\n\n\n\n\n\n\n\n\n\n\n \nCold\nNoCold\n\n\n\n\nPlacebo\n0.8151\n0.1849\n\n\nVitC\n0.742\n0.258\n\n\n\n\n\nHypotheses \\[\\begin{cases} H_0: p_\\text{placebo} \\leq p_\\text{vitaminC} \\\\ H_A: p_\\text{placebo} &gt; p_\\text{vitaminC} \\end{cases}\\] This corresponds to testing whether vitamin C reduces the probability of getting a cold.\n\n\\[\\begin{align*} &\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitaminC} = \\hspace{15cm} \\\\\\\\\n&\\hat{p} = \\hspace{15cm} \\\\\\\\\n&Z =\n\\end{align*}\\]\n\\(p\\) value from normal model:"
  },
  {
    "objectID": "content/week8-proportions.html#an-interval-for-the-difference",
    "href": "content/week8-proportions.html#an-interval-for-the-difference",
    "title": "Inference for population proportions",
    "section": "An interval for the difference",
    "text": "An interval for the difference\nTo compute a confidence interval for the difference in proportions:\n\\[\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitaminC} \\pm 2\\times \\sqrt{SE^2(\\hat{p}_\\text{placebo}) + SE^2(\\hat{p}_\\text{vitaminC})}\\]\nCarrying out the calculation by hand:\n\\[\\begin{align*}\nSE^2(\\hat{p}_\\text{placebo}) &= \\hspace{8cm}\\\\\\\\\nSE^2(\\hat{p}_\\text{vitaminC}) &= \\hspace{8cm}\\\\\\\\\n\\hspace{3cm} \\pm 2\\times \\hspace{5cm} &= \\Big( \\hspace{2cm}, \\hspace{2cm}\\Big)\n\\end{align*}\\]"
  },
  {
    "objectID": "content/week8-proportions.html#inference-in-r-1",
    "href": "content/week8-proportions.html#inference-in-r-1",
    "title": "Inference for population proportions",
    "section": "Inference in R",
    "text": "Inference in R\n\n\nThree steps:\n\nConstruct a table of the frequency distribution by group\n\noutcome/trait of interest should be first column\ngroups should be rows\n\nPass to prop.test()\n\nIdentify at left:\n\npoint estimates\ntest statistic\n\\(p\\)-value\nconfidence interval\n\n\n\n# 1. construct table\ncold\n\n        Cold NoCold\nPlacebo  335     76\nVitC     302    105\n\n# 2. pass to prop.test\nprop.test(cold, alternative = 'two.sided', \n          correct = F)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  cold\nX-squared = 6.3366, df = 1, p-value = 0.01183\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.01636477 0.12977607\nsample estimates:\n   prop 1    prop 2 \n0.8150852 0.7420147"
  },
  {
    "objectID": "content/week8-proportions.html#your-turn-two-cases",
    "href": "content/week8-proportions.html#your-turn-two-cases",
    "title": "Inference for population proportions",
    "section": "Your turn: two cases",
    "text": "Your turn: two cases\n\n\nResearchers categorized 3,112 individuals in American Samoa according to whether they were obese and recorded whether subjects had cardiovascular disease (CVD).\n\n\n\n\n\n\n\n\n\n\n \nDeaths\nNonDeaths\n\n\n\n\nObese\n16\n2045\n\n\nNotObese\n7\n1044\n\n\n\n\n\nTest for a difference in disease rates between obese and non-obese populations, and produce an interval estimate for the difference in proportions.\n\nResearchers identified 86 lung cancer patients and 86 controls (without lung cancer), and categorized them according to whether they were smokers or non-smokers.\n\n\n\n\n\n\n\n\n\n\n \nSmokers\nNonSmokers\n\n\n\n\nCancer\n83\n3\n\n\nControl\n72\n14\n\n\n\n\n\nTest for a difference in the proportion of smokers among cancer patients compared with controls, and produce an interval estimate for the difference.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week8-proportions.html#categorical-data",
    "href": "content/week8-proportions.html#categorical-data",
    "title": "Inference for population proportions",
    "section": "Categorical data",
    "text": "Categorical data\n\nA variable is categorical if its values are one of several categories.\n\n\n\nInference for categorical data has a different flavor:\n\nnon-numeric values \\(\\Rightarrow\\) can’t compute usual statistics (mean, variance, etc.)\nfocus on counts and proportions instead\n\nBinomial data are observations of a binary variable:\n\na single categorical variable with two categories\none category represents an outcome, trait, or property of interest\n\nWe’ll develop inferences for binomial proportions."
  },
  {
    "objectID": "content/week8-proportions.html#inference-for-binomial-proportions",
    "href": "content/week8-proportions.html#inference-for-binomial-proportions",
    "title": "Inference for population proportions",
    "section": "Inference for binomial proportions",
    "text": "Inference for binomial proportions\n\nA variable is categorical if its values are one of several categories.\n\n\n\nInference for categorical data has a different flavor:\n\nnon-numeric values \\(\\Rightarrow\\) can’t compute usual statistics (mean, variance, etc.)\nfocus on counts and proportions instead\n\nBinomial data are observations of a binary variable:\n\na single categorical variable with two categories\none category represents an outcome, trait, or property of interest\n\nWe’ll develop inferences for binomial proportions."
  },
  {
    "objectID": "content/week8-proportions.html#binomial-proportions",
    "href": "content/week8-proportions.html#binomial-proportions",
    "title": "Inference for population proportions",
    "section": "Binomial proportions",
    "text": "Binomial proportions\n\nA binomial variable is a nominal categorical variable with two unique values.\n\n\n\nUsually, binomial data record the presence/absence of an event, trait, or property of interest.\nInference for binomial data has a different flavor:\n\nnon-numeric values \\(\\Rightarrow\\) can’t compute usual statistics (mean, variance, etc.)\nfocus on proportions instead\n\nExample: prevalence of diabetes among US adults?\n\nestimate and standard error?\nconfidence interval?\nhypothesis test?"
  },
  {
    "objectID": "content/week8-proportions.html#standard-error-for-sample-proportion",
    "href": "content/week8-proportions.html#standard-error-for-sample-proportion",
    "title": "Inference for population proportions",
    "section": "Standard error for sample proportion",
    "text": "Standard error for sample proportion\n\nBinomial data are most variable when \\(p = 0.5\\) and least variable when \\(p \\approx 0\\) or \\(1\\)\n\n\n\nMeasure of spread for binomial data: \\[\\sqrt{\\hat{p}(1 - \\hat{p})}\\]\n\nhighest when \\(\\hat{p} \\approx 0.5\\)\nlowest when \\(\\hat{p} \\approx 0 \\text{ or } 1\\)\n\nAnalogous to estimating a mean: \\[\nSE\\left(\\hat{p}\\right)\n= \\frac{\\text{spread}}{\\sqrt{\\text{sample size}}}\n= \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]"
  },
  {
    "objectID": "content/week8-proportions.html#standard-error-for-a-sample-proportion",
    "href": "content/week8-proportions.html#standard-error-for-a-sample-proportion",
    "title": "Inference for population proportions",
    "section": "Standard error for a sample proportion",
    "text": "Standard error for a sample proportion\n\nBinomial data are most variable when \\(p = 0.5\\) and least variable when \\(p \\approx 0\\) or \\(1\\)\n\n\n\nMeasure of spread for binomial data: \\[\\sqrt{\\hat{p}(1 - \\hat{p})}\\]\n\nhighest when \\(\\hat{p} \\approx 0.5\\)\nlowest when \\(\\hat{p} \\approx 0 \\text{ or } 1\\)\n\nAnalogous to estimating a mean: \\[\nSE\\left(\\hat{p}\\right)\n= \\frac{\\text{spread}}{\\sqrt{\\text{sample size}}}\n= \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]"
  },
  {
    "objectID": "content/week8-proportions.html#se-for-a-sample-proportion",
    "href": "content/week8-proportions.html#se-for-a-sample-proportion",
    "title": "Inference for population proportions",
    "section": "SE for a sample proportion",
    "text": "SE for a sample proportion\n\nBinomial data are most variable when \\(p = 0.5\\) and least variable when \\(p \\approx 0\\) or \\(1\\)\n\n\n\nMeasure of spread for binomial data: \\[\\sqrt{\\hat{p}(1 - \\hat{p})}\\]\n\nhighest when \\(\\hat{p} \\approx 0.5\\)\nlowest when \\(\\hat{p} \\approx 0 \\text{ or } 1\\)\n\nAnalogous to estimating a mean: \\[\nSE\\left(\\hat{p}\\right)\n= \\frac{\\text{spread}}{\\sqrt{\\text{sample size}}}\n= \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]"
  },
  {
    "objectID": "content/week8-proportions.html#confidence-interval-for-p",
    "href": "content/week8-proportions.html#confidence-interval-for-p",
    "title": "Inference for population proportions",
    "section": "Confidence interval for \\(p\\)",
    "text": "Confidence interval for \\(p\\)\nA confidence interval for a binomial proportion \\(p\\) is:\n\\[\\hat{p} \\pm c \\times SE(\\hat{p})\\]\nThe critical value \\(c\\) comes from the normal model.\n\nempirical rule:\n\n\\(c = 1\\) gives a 68% interval\n\\(c = 2\\) gives a 95% interval\n\\(c = 3\\) gives a 99.7% interval\n\nfor a \\((1 - \\alpha)\\times 100 \\%\\) confidence interval use the \\(1 - \\frac{\\alpha}{2}\\) quantile of the normal model\n\n\nqnorm(1 - 0.1/2) # c for 90% interval\nqnorm(1 - 0.05/2) # c for 95% interval\nqnorm(1 - 0.01/2) # c for 99% interval"
  },
  {
    "objectID": "content/week8-proportions.html#computing-confidence-intervals",
    "href": "content/week8-proportions.html#computing-confidence-intervals",
    "title": "Inference for population proportions",
    "section": "Computing confidence intervals",
    "text": "Computing confidence intervals\n\n\n\n\n\nPoint estimate for diabetes prevalence\n\n\n\n\n\n\n\np.hat\nse\nn\n\n\n\n\n0.114\n0.01421\n500\n\n\n\n\n\n\n\nIt is estimated that the proportion of the U.S. adult population with diagnosed diabetes is 11.4% (SE = 1.42%).\n\n\n\nCheck assumptions for the normal model:\n\n\\(500\\times 0.114 = 57 \\geq 10\\)\n\\(500\\times 0.886 = 443 \\geq 10\\)\n\n\n\n95% confidence interval for diabetes prevalence:\n\\[\n0.114 \\pm 2\\times 0.01421 = (0.0881, 0.1459)\n\\]\n\n\nWith 95% confidence, it is estimated that between 8.81% and 14.59% of the U.S. adult population has diagosed diabetes."
  },
  {
    "objectID": "content/week8-proportions.html#example-diabetes-prevalence",
    "href": "content/week8-proportions.html#example-diabetes-prevalence",
    "title": "Inference for population proportions",
    "section": "Example: diabetes prevalence",
    "text": "Example: diabetes prevalence\n\n\n\n\n\nPoint estimate for diabetes prevalence\n\n\n\n\n\n\n\np.hat\nse\nn\n\n\n\n\n0.114\n0.01421\n500\n\n\n\n\n\n\n\nIt is estimated that the proportion of the U.S. adult population with diagnosed diabetes is 11.4% (SE = 1.42%).\n\n\n\nCheck assumptions for the normal model:\n\\[\n500\\times 0.114 = 57 \\geq 10\n\\quad\\text{and}\\quad 500\\times 0.886 = 443 \\geq 10\n\\]\n\n\n95% confidence interval for diabetes prevalence:\n\\[\n0.114 \\pm 2\\times 0.01421 = (0.0881, 0.1459)\n\\]\n\n\nWith 95% confidence, the proportuion of U.S. adults with diagnosed diabetes is estimated to be between 8.81% and 14.59%."
  },
  {
    "objectID": "content/week8-proportions.html#hypothesis-tests-for-p",
    "href": "content/week8-proportions.html#hypothesis-tests-for-p",
    "title": "Inference for population proportions",
    "section": "Hypothesis tests for \\(p\\)",
    "text": "Hypothesis tests for \\(p\\)\n\n\nTo test whether true prevalence is 10%: \\[\n\\begin{cases}\nH_0: &p = 0.1 \\\\\nH_A: &p \\neq 0.1\n\\end{cases}\n\\]\nWe can use the test statistic:\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n= \\frac{\\hat{p} - 0.1}{\\sqrt{\\frac{0.1 (0.9)}{500}}}\n\\] Under \\(H_0\\), the sampling distribution of \\(Z\\) is approximated by a normal model, provided:\n\n\\(np_0 \\geq 10\\)\n\\(n(1 - p_0) \\geq 10\\)\n\n\n\n\n\n\n\n\n\n\n\nHere \\(p = P(|Z| &gt; 1.043) = 0.2967\\), so:\n\nthe data provide no evidence that prevalence differs from 10%."
  },
  {
    "objectID": "content/week8-proportions.html#hypothesis-tests-for-p-1",
    "href": "content/week8-proportions.html#hypothesis-tests-for-p-1",
    "title": "Inference for population proportions",
    "section": "Hypothesis tests for \\(p\\)",
    "text": "Hypothesis tests for \\(p\\)\n\n\nTo test whether true prevalence is 15%: \\[\n\\begin{cases}\nH_0: &p = 0.15 \\\\\nH_A: &p \\neq 0.15\n\\end{cases}\n\\]\nWe can use the test statistic:\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n= \\frac{\\hat{p} - 0.15}{\\sqrt{\\frac{0.15 (0.85)}{500}}}\n\\] Under \\(H_0\\), the sampling distribution of \\(Z\\) is approximated by a normal model, provided:\n\n\\(np_0 \\geq 10\\)\n\\(n(1 - p_0) \\geq 10\\)\n\n\n\n\n\n\n\n\n\n\n\nHere \\(p = P(|Z| &gt; 2.254) = 0.0242\\), so:\n\nthe data provide moderate evidence that prevalence differs from 15%."
  },
  {
    "objectID": "content/week8-proportions.html#hypothesis-tests-for-p-2",
    "href": "content/week8-proportions.html#hypothesis-tests-for-p-2",
    "title": "Inference for population proportions",
    "section": "Hypothesis tests for \\(p\\)",
    "text": "Hypothesis tests for \\(p\\)\n\n\nTo test if prevalence is below 14%: \\[\n\\begin{cases}\nH_0: &p = 0.14 \\\\\nH_A: &p &lt; 0.14\n\\end{cases}\n\\]\nWe can use the test statistic:\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n= \\frac{\\hat{p} - 0.14}{\\sqrt{\\frac{0.14 (0.86)}{500}}}\n\\]\nUnder \\(H_0\\), the sampling distribution of \\(Z\\) is approximated by a normal model, provided:\n\n\\(np_0 \\geq 10\\)\n\\(n(1 - p_0) \\geq 10\\)\n\n\n\n\n\n\n\n\n\n\n\nHere \\(p = P(Z &lt; 1.676) = 0.0469\\), so:\n\nthe data provide moderate evidence that prevalence is less than 14%."
  },
  {
    "objectID": "content/week8-proportions.html#inference-for-a-proportion-in-r",
    "href": "content/week8-proportions.html#inference-for-a-proportion-in-r",
    "title": "Inference for population proportions",
    "section": "Inference for a proportion in R",
    "text": "Inference for a proportion in R\n\n\nInference using the normal model in R:\n\nConstruct a table of the frequency distribution\nPass the table to prop.test()\n\nRemarks about output:\n\nX-squared gives \\(Z^2\\)\ncorrect = F performs the test without continuity correction\n\n\n\n# variable of interest\ndia &lt;- nhanes$diabetes\n\n# pass table to prop.test\ntable(dia) |&gt; \n  prop.test(p = 0.1, alternative = 'two.sided',\n            conf.level = 0.95, correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 1.0889, df = 1, p-value = 0.2967\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.0890369 0.1448491\nsample estimates:\n    p \n0.114 \n\n\n\n\n\nThe data provide no evidence that diabetes prevalence among U.S. adults differs from 10%. With 95% confidence, prevalence is estimated to be between 8.90% and 14.48%, with a point estimate of 11.4% (SE = 1.42%)."
  },
  {
    "objectID": "content/week8-proportions.html#correct-f",
    "href": "content/week8-proportions.html#correct-f",
    "title": "Inference for population proportions",
    "section": "correct = F?",
    "text": "correct = F?\n\nA “continuity correction” reduces approximation error for the normal model.\n\n\n\n\ntable(dia) |&gt; \n  prop.test(p = 0.1, \n            alternative = 'two.sided',\n            conf.level = 0.95, \n            correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 1.0889, df = 1, p-value = 0.2967\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.0890369 0.1448491\nsample estimates:\n    p \n0.114 \n\n\n\n\ntable(dia) |&gt; \n  prop.test(p = 0.1, \n            alternative = 'two.sided',\n            conf.level = 0.95, \n            correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 0.93889, df = 1, p-value = 0.3326\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.08814952 0.14594579\nsample estimates:\n    p \n0.114 \n\n\n\n\nOmitting the correct argument implements the correction by default."
  },
  {
    "objectID": "content/week8-proportions.html#exact-inference-for-a-proportion",
    "href": "content/week8-proportions.html#exact-inference-for-a-proportion",
    "title": "Inference for population proportions",
    "section": "Exact inference for a proportion",
    "text": "Exact inference for a proportion\nThe test can also be performed using the exact sampling distribution obtained from a binomial probability model.\n\nbinom.test(x = 57, n = 500, p = 0.1, alternative = 'two.sided')\n\n\n    Exact binomial test\n\ndata:  57 and 500\nnumber of successes = 57, number of trials = 500, p-value = 0.2964\nalternative hypothesis: true probability of success is not equal to 0.1\n95 percent confidence interval:\n 0.0874949 0.1451685\nsample estimates:\nprobability of success \n                 0.114 \n\n\nInputs:\n\nx gives the number of occurrences of the category of interest\nn gives the sample size"
  },
  {
    "objectID": "content/week8-proportions.html#two-way-tables",
    "href": "content/week8-proportions.html#two-way-tables",
    "title": "Inference for population proportions",
    "section": "Two-way tables",
    "text": "Two-way tables\n\nTwo-way tables or “contingency” tables compare two categorical variables.\n\n\n\n\n\n\nVitamin C experiment\n\n\n\n\n\n\n\n\n \nCold\nNoCold\nn\n\n\n\n\nPlacebo\n335\n76\n411\n\n\nVitC\n302\n105\n407\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvitamin C and placebo treatments were randomly allocated to 818 volunteers\nvolunteers took treatments daily for a cold season\nstudy recorded how many volunteers came down with a cold\n\nIs vitamin C effective at preventing common cold?"
  },
  {
    "objectID": "content/week8-proportions.html#inference-for-two-proportions",
    "href": "content/week8-proportions.html#inference-for-two-proportions",
    "title": "Inference for population proportions",
    "section": "Inference for two proportions",
    "text": "Inference for two proportions\nWe can first consider inferences on the difference in proportions:\n\\[\\delta = p_\\text{placebo} - p_\\text{vitC}\\]\n\nInferences are based on groupwise estimates:\n\npoint estimate: \\(\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC}\\)\nstandard error: \\(\\sqrt{SE^2(\\hat{p}_\\text{placebo}) + SE^2(\\hat{p}_\\text{vitC})}\\)\n\nWhen both groups meet the conditions for inference for one proportion, the statistic\n\\[\nZ = \\frac{\\hat{p}_1 - \\hat{p}_2 - \\delta}{SE(\\hat{p}_1 - \\hat{p}_2)}\n\\] has a sampling distribution well-approximated by a normal model."
  },
  {
    "objectID": "content/week8-proportions.html#confidence-interval-for-the-difference",
    "href": "content/week8-proportions.html#confidence-interval-for-the-difference",
    "title": "Inference for population proportions",
    "section": "Confidence interval for the difference",
    "text": "Confidence interval for the difference\n\\[\n\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC} \\pm c\\times SE(\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC})\n\\] For a \\((1 - \\alpha)\\times 100\\%\\) confidence interval the critical value \\(c\\) is chosen to be the \\(\\left(1 - \\frac{\\alpha}{2}\\right)\\) quantile of the normal model.\n\npoint estimate: \\(\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC} = 0.0731\\)\nstandard error: \\(\\sqrt{SE^2(\\hat{p}_\\text{placebo}) + SE^2(\\hat{p}_\\text{vitC})} = 0.0289\\)\ncritical value for 95% interval: qnorm(1 - 0.05/2) = 1.959964\n\n95% confidence interval: (0.0164, 0.1298)\n\nWith 95% confidence, the prevalence of common cold is estimated to be between 1.64% and 12.98% lower among adults who take daily vitamin C supplements."
  },
  {
    "objectID": "content/week8-proportions.html#sampling-and-two-way-tables",
    "href": "content/week8-proportions.html#sampling-and-two-way-tables",
    "title": "Inference for population proportions",
    "section": "Sampling and two-way tables",
    "text": "Sampling and two-way tables\n\n\nConsider this case-control study:\n\n\n\n\n\n\n\n\n\n\n\n \nSmokers\nNonSmokers\nn\n\n\n\n\nCancer\n83\n3\n86\n\n\nControl\n72\n14\n86\n\n\n\n\n\nThis is an example of outcome-based sampling:\n\n86 lung cancer patients and 86 controls\ncan’t estimate cancer prevalence\n\n\nA different approach to inference is needed to analyze this data. Next time:\n\ntests of association in two-way tables\ninference for risk and odds ratios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/test3.html",
    "href": "content/test3.html",
    "title": "Test 3",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/24. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test3.html#instructions",
    "href": "content/test3.html#instructions",
    "title": "Test 3",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the test 2 form posted on the course website. The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/24. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test3.html#problems",
    "href": "content/test3.html#problems",
    "title": "Test 3",
    "section": "Problems",
    "text": "Problems\n\n[L9] Recall that the FAMuSS study, which we have used for several in-class examples, sought to determine whether the ACTN3 gene is associated with differential physical response to strength training. Here you’ll answer that question.\n\nUse an appropriate graphical summary to check whether assumptions are met for an ANOVA to test for differences in mean change in nondominant arm strength by genotype.\nFit the ANOVA model, construct the ANOVA table, and determine whether the study provides evidence that mean change in nondominant arm strength differs by genotype. Carry out your inference at the 5% significance level.\nIf your test in (b) indicated significant differences at the 5% level, carry out tests for pairwise differences to determine which genotypes differ. Provide interval estimates for any significant differences.\nCheck whether there are any significant differences in mean change in dominant arm strength by genotype at the 5% significance level and use pairwise comparisons to identify any such differences; do your conclusions match the analysis of change in nondominant arm strength?\n\n[L9] The output below shows: (1) an ANOVA model fit to data on the average number of flowers per meadowfoam plant grown in an experimental plot and the light intensity (μmol/m^2/sec) that the plot received during the experiment; (2) tests for contrasts with the lowest intensity level. The intensity levels were randomly allocated among the plots.\n\nConstruct the ANOVA table to test for an effect of light intensity on meadowfoam flowering. Interpret the result of the test in context.\nHow many observations and treatments were there in the expriment?\nWhat is the estimated standard deviation of the average number of flowers per plant across plots?\nBased on the output beneath the model fit summary, what is the effect of increasing intensity on flowering? Explain.\n\n\n\n\nCall:\n   aov(formula = flowers ~ intensity, data = meadow)\n\nTerms:\n                intensity Residuals\nSum of Squares   2683.514  1654.422\nDeg. of Freedom         5        18\n\nResidual standard error: 9.587093\nEstimated effects are balanced\n\n\n contrast                    estimate   SE df t.ratio p.value\n intensity300 - intensity150    -9.12 6.78 18  -1.346  0.5357\n intensity450 - intensity150   -13.38 6.78 18  -1.973  0.2234\n intensity600 - intensity150   -23.23 6.78 18  -3.426  0.0130\n intensity750 - intensity150   -27.75 6.78 18  -4.093  0.0030\n intensity900 - intensity150   -29.35 6.78 18  -4.329  0.0018\n\nP value adjustment: dunnettx method for 5 tests \n\n\n\n[L9] To study the influence of ocean grazers on regeneration rates of seaweed in the intertidal zone, a researcher scraped rock plots free of seaweed and observed the degree of regeneration when certain types of seaweed-grazing animals were denied access. The grazers were limpets (L), small fishes (f) and large fishes (F). Each plot received one of six treatments named by which grazers were allowed access, or a control (C) in which no grazers were allowed access. The grazers dataset contains observations of percent of regenerated seaweed for 96 plots along with which treatment the plot received.\n\nConstruct boxplots to inspect the distributions of percent cover regeneration among plots by treatment group. Assess whether assumptions for ANOVA seem to be met.\nHow many replicates (i.e., plots) per treatment group are there?\nFit an ANOVA model to test for an effect of grazers on seaweed regeneration. Carry out your inference at the 1% significance level.\nTest the appropriate contrasts to determine which grazers have a significant effect (at the 1% level) on seaweed regeneration relative to no grazers.\nEstimate any significant effects identified in (d) with confidence intervals at the appropriate confidence level for the tests you performed.\n\n\n\nExtra credit\n\n[L5] The logging dataset contains measurements on the number of tree seedlings lost per transect in nine logged (L) and seven unlogged (U) plots affected by the Oregon Biscuit Fire.\n\nUse an appropriate nonparametric rank test to assess whether the percentage of seedlings lost differed between logged and unlogged areas.\nWhich type of area saw the lesser impact of the fire? Support your answer quantitatively with the inference from (a)."
  },
  {
    "objectID": "content/lab11-proportions.html",
    "href": "content/lab11-proportions.html",
    "title": "Lab 11: Inference for binomial proportions",
    "section": "",
    "text": "The goal of this lab is to learn how to implement:\n\ntests and intervals for binomial proportions from one sample\ntests and intervals comparing two proportions from independent samples\n\nThe activity represents our first foray into categorical data analysis. You’ll reproduce examples from lecture using the NHANES data to estimate diabetets prevalence and the Vitamin C experiment; you’ll practice on a few additional datasets.\n\nlibrary(tidyverse)\nload('data/vitamin.RData')\nload('data/nhanes500.RData')\nload('data/obesity.RData')\n\n\nInference for one proportion\n\nRefresher: categorical frequency distributions\nInference for proportions – and for that matter, future material on inference for categorical data – will leverage frequency distributions to perform calculations.\nYou learned how to make these descriptive summaries at the beginning of the quarter, but a refresher may be helpful. Recall that a frequency distribution, for a categorical variable, is simply a set of counts of observations of each unique value of the variable. We made these using table(...).\n\n# extract variable of interest\ndia &lt;- nhanes$diabetes\n\n# construct table of counts (frequency distribution)\ntable(dia)\n\ndia\nYes  No \n 57 443 \n\n# render as proportions (still frequency distribution, but normalized)\ntable(dia) |&gt; prop.table()\n\ndia\n  Yes    No \n0.114 0.886 \n\n# barplot\ntable(dia) |&gt; prop.table() |&gt; barplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn 1\n\n\n\nCompute the frequency distribution, in both counts and proportions, for the sleeptrouble variable in the NHANES dataset, which records whether the participant experiences sleep trouble. Also construct a barplot to visualize the frequency distribution\n\n\n\n\nPoint estimation\nThe point estimate for a population proportion is simply the corresponding sample proportion:\n\\[\n\\hat{p} = \\frac{\\#\\text{ observations of category of interest}}{\\text{sample size }n}\n\\] This can be ascertained directly from the frequency distribution. In the diabetes example, \\(\\hat{p}=57\\).\nThe standard error for \\(\\hat{p}\\) is: \\[\nSE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\] This measures the sample-to-sample variability of \\(\\hat{p}\\). To compute the standard error for a binomial proportion, take the product of the two proportions and divide by the sample size, as shown below.\n\n# point estimates *are* sample proportions in the frequency distribution table\ndia.p &lt;- table(dia) |&gt; prop.table()\n\n# point estimate (sample proportion of interest)\ndia.p.hat &lt;- dia.p[1]\ndia.p.hat\n\n  Yes \n0.114 \n\n# standard error\ndia.n &lt;- length(dia)\ndia.p.hat.se &lt;- sqrt(prod(dia.p)/dia.n)\ndia.p.hat.se\n\n[1] 0.01421295\n\n\nThis is interpreted as follows:\n\nThe proportion of U.S. adults with diagnosed diabetes is estimated to be 0.114 (SE = 0.0142).\n\n\n\n\n\n\n\nYour turn 2\n\n\n\nUsing the NHANES data, compute a point estimate and standard error for the proportion of U.S. adults who have sleep trouble. Interpret the estimate in context as shown above.\n\n\n\n\nConfidence intervals (manually)\nThe confidence interval is straightforward to compute by hand: \\[\n\\hat{p} \\pm c\\times SE(\\hat{p})\n\\] Once estimates are in hand, the calculation requires only finding an appropriate critical value \\(c\\). For a \\(1 - \\alpha\\) confidence interval, this is the \\(1 - \\frac{\\alpha}{2}\\) quantile of the normal model, i.e., the value satisfying:\n\\[\nP(Z \\leq c) = 1 - \\frac{\\alpha}{2}\n\\]\nHere are several examples:\n\n# 90% interval\ncval &lt;- qnorm(1 - 0.10/2)\ndia.p.hat + c(-1, 1)*cval*dia.p.hat.se\n\n[1] 0.09062177 0.13737823\n\n# 95% interval\ncval &lt;- qnorm(1 - 0.05/2)\ndia.p.hat + c(-1, 1)*cval*dia.p.hat.se\n\n[1] 0.08614313 0.14185687\n\n# 99% interval\ncval &lt;- qnorm(1 - 0.01/2)\ndia.p.hat + c(-1, 1)*cval*dia.p.hat.se\n\n[1] 0.07738986 0.15061014\n\n# 99.9% interval\ncval &lt;- qnorm(1 - 0.001/2)\ndia.p.hat + c(-1, 1)*cval*dia.p.hat.se\n\n[1] 0.0672319 0.1607681\n\n\nTo interpret the last interval:\n\nWith 99.9% confidence, the proportion of U.S. adults with diagnosed diabetes is estimated to be between 0.067 and 0.161, with a point estimate of 0.114 (SE 0.0142).\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nUsing the NHANES data, construct and interpret a 98% confidence interval for the proportion of U.S. adults who have sleep trouble.\n\n\n\n\nHypothesis tests\nTo test the hypothesis that a population proportion is some null value \\(p_0\\) against a non-directional (two-sided) alternative, i.e.,\n\\[\n\\begin{cases}\nH_0: &p = p_0 \\\\\nH_A: &p \\neq p_0\n\\end{cases}\n\\]\nwe use a test statistic that is almost the estimation error divided by the standard error, but we substitute the hypothetical value for the estimate in the standard error:\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\n\\]\nThe \\(p\\)-value is simply the proportion of the normal model for the sampling distribution of \\(Z\\) that exceeds the observed statistic in either direction: \\(P(|Z| &gt; Z_obs)\\). This can be computed by hand as shown below.\n\n# test stat\ndia.z &lt;- (dia.p.hat - 0.1)/sqrt(0.1*0.9/dia.n)\n\n# two sided p value\ndia.pval &lt;- 2*pnorm(abs(dia.z), lower.tail = F)\ndia.pval\n\n      Yes \n0.2967175 \n\n\nThe result is interpreted as follows in standard narrative style:\n\nThe data provide no evidence that the proportion of U.S. adults with diagnosed diabetes differs from 0.1 (Z = 1.04, p = 0.2967).\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nTest the hypothesis that the proportion of U.S. adults who have sleep trouble is 0.3 against a two-sided alternative at the 5% significance level. Interpret the result in context.\n\n# test stat\n\n# two sided p value\n\n\n\nA more straightforward way to perform the inference – both the test and interval – utilizes prop.test(...). The command below matches our prior results:\n\n# pass table (counts) to prop.test\ntable(dia) |&gt; \n  prop.test(p = 0.1, alternative = 'two.sided', \n            conf.level = 0.95, correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 1.0889, df = 1, p-value = 0.2967\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.0890369 0.1448491\nsample estimates:\n    p \n0.114 \n\n\nThe resulting test and interval would be reported jointly:\n\nThe data provide no evidence that the proportion of U.S. adults with diagnosed diabetes differs from 0.1 (Z = 1.04, p = 0.2967). &gt; With 95% confidence, the proportion of U.S. adults with diagnosed diabetes is estimated to be between 0.089 and 0.145, with a point estimate of 0.114 (SE 0.0142).\n\nThe prop.test(...) function also makes doing directional tests easier. Below are two examples:\n\n# upper sided test/interval: does p exceed 0.09?\ntable(dia) |&gt; \n  prop.test(p = 0.09, alternative = 'greater', \n            conf.level = 0.95, correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.09\nX-squared = 3.5165, df = 1, p-value = 0.03038\nalternative hypothesis: true p is greater than 0.09\n95 percent confidence interval:\n 0.09266984 1.00000000\nsample estimates:\n    p \n0.114 \n\n# upper sided test/interval: is p under 0.14?\ntable(dia) |&gt; \n  prop.test(p = 0.14, alternative = 'less', \n            conf.level = 0.95, correct = F)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.14\nX-squared = 2.8073, df = 1, p-value = 0.04692\nalternative hypothesis: true p is less than 0.14\n95 percent confidence interval:\n 0.000000 0.139485\nsample estimates:\n    p \n0.114 \n\n\n\n\n\n\n\n\nYour turn 5\n\n\n\nTest whether the proportion of U.S. adults who experience sleep trouble is under 0.3 at the 1% significance level. Provide an upper confidence bound along with your test.\n\n# test whether the proportion with sleep trouble is under 0.3\n\n\n\nThe default approach is to apply the continuity correction (set correct = T or omit this argument). We omitted it so that results would match the manual calculations above. You should apply the correction in practice.\n\n\n\nInference for two proportions\nInference comparing two proportions proceeds from a two-way table or “contingency” table. You may recall that this is a bivariate frequency distribution of two categorical variables. Take a moment to refresh your memory on how to construct these tables:\n\n# variables of interest\ntrt &lt;- vitamin$treatment\nout &lt;- vitamin$outcome\n\n# construct contingency table\nvitamin.tbl &lt;- table(trt, out)\nvitamin.tbl\n\n         out\ntrt       cold nocold\n  Placebo  335     76\n  VitC     302    105\n\n\nFor inference to work appropriately with prop.test(...), it is important that the outcome be shown in the column dimension and the groups be shown in the row dimension. Further, the outcome of interest should be the first column (not the second).\n\n\n\n\n\n\nYour turn 6\n\n\n\nConstruct a contingency table for the obesity data.\n\n# variables of interest\n\n# contingency table\n\n\n\nThe contingency table can be used directly to perform inference on a difference in proportions:\n\n# test for difference in proportions\nprop.test(vitamin.tbl, \n          alternative = 'two.sided', \n          conf.level = 0.95)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  vitamin.tbl\nX-squared = 5.9196, df = 1, p-value = 0.01497\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.01391972 0.13222111\nsample estimates:\n   prop 1    prop 2 \n0.8150852 0.7420147 \n\n\n\n\n\n\n\n\nYour turn 7\n\n\n\nTest whether the rate of CHD deaths differs among obese and non-obese populations."
  },
  {
    "objectID": "content/project-guidelines.html",
    "href": "content/project-guidelines.html",
    "title": "Project guidelines",
    "section": "",
    "text": "In place of a final exam, you will work with a partner to find a case study relevant to one of your major fields of study that employs a method from the class (or a method closely related) to answer a research question. You’ll prepare a short summary and meet with me in person during the final exam time to briefly present your case study and field a few questions."
  },
  {
    "objectID": "content/project-guidelines.html#expectations",
    "href": "content/project-guidelines.html#expectations",
    "title": "Project guidelines",
    "section": "Expectations",
    "text": "Expectations\nYour case study should meet the following criteria:\n\naddress a research question in your field of study\nuse a statistical method we have discussed in class\nbe included in a published paper or report\n\nMost papers report multiple findings. Thus, you can expect to focus for your case study on a small portion of the full publication; you do not need to summarize a published study in its entirety. Rather, you are expected to identify one inferential analysis.\nKeep it simple in terms of the analysis – a few intervals/tests/estimates are fine, but don’t overcomplicate things by trying to summarize too many results at once. Keep in mind that you’ll need to be able to explain your case study in just a few minutes.\nYou have two options for the project deliverable:\n\n[option 1] prepare a written summary of the case study\n[option 2] replicate the analysis for the case study"
  },
  {
    "objectID": "content/project-guidelines.html#evaluation-criteria",
    "href": "content/project-guidelines.html#evaluation-criteria",
    "title": "Project guidelines",
    "section": "Evaluation criteria",
    "text": "Evaluation criteria\nYou will be evaluated on the appropriateness of the example you choose, the clarity of your summary/presentation, and your understanding of the statistical method(s) you discuss. Satisfactory work should:\n\ndemonstrate an understanding of the case study and its relevance\ndemonstrate an adequate understanding of the data and statistical method(s) involved\nbe free of obvious errors/misconceptions"
  },
  {
    "objectID": "content/project-guidelines.html#deliverables",
    "href": "content/project-guidelines.html#deliverables",
    "title": "Project guidelines",
    "section": "Deliverables",
    "text": "Deliverables\nYour deliverable should be submitted via file upload 24 hours in advance of your scheduled exam time.\nChoose ONE of the options below.\n\nOption 1: written summary\nPrepare a short 1-2 page summary addressing the following:\n\nWhat is the research question that the analysis addresses?\nWhat inference(s) are used to answer the question?\nWhat data were utilized for the inference(s)?\nWhat statistical method(s) are used to perform the inference(s)?\nWhat are the results?\n\nYour deliverable for this option should be uploaded as a .docx or .pdf file.\n\n\nOption 2: replicate an analysis\nPrepare an R script, data file, and 1-page summary of results. Your summary should include the following:\n\nA data description\nA data summary (plot or table)\nRelevant R output\nInterpretation of inference(s) in context\n\nYour deliverable for this option should be a .zip file containing one R script, one data file, and your written summary (as a .docx or .pdf file)."
  },
  {
    "objectID": "content/project-guidelines.html#logistics",
    "href": "content/project-guidelines.html#logistics",
    "title": "Project guidelines",
    "section": "Logistics",
    "text": "Logistics\nExam times are held in the usual classroom (186-C100) at:\n\n[12pm section] Wednesday 6/12 10:10am – 1:00pm\n[2pm section] Monday 6/10 1:10pm – 4:00pm\n\nYou can expect to schedule a 10-minute time slot in the window of the scheduled exam. A scheduling link will be provided at the end of week 10. Requests for alternate times must be made by the end of week 10 and include a motivation for the request. We will be on a tight schedule, so once you have your time slot, you should plan on arriving five minutes in advance.\nA few of you may be scheduled outside of the regular times at a location TBD. I will check with you first before scheduling you outside of the exam time."
  }
]