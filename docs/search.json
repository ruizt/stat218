[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics for Life Sciences",
    "section": "",
    "text": "Announcements\n\n\n\nPlease complete this short [midquarter feedback survey] by Friday 5/10.\nAssignment reminders:\n\nproblem set 7 due Monday 5/6; late submissions until Wednesday 5/8 5pm\ntest 2 upcoming: will be released Wednesday 5/8 5pm and due Friday 5/10 5pm\n\n\n\n\nCourse information\nRead the [course syllabus] for detailed information on content, materials, learning outcomes, assessments, and course policies.\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment]\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class.\n\nCompleting assignments:\nOne set of practice problems is included at the end of each lab; these problem sets are your homework assignments. You will often have some time to work on them during class, and they will be due by the following class period. To complete these assignments:\n\nReview the prompts included with the lab.\nDo your work (calculations, making plots, etc.) in the lab script provided in Posit Cloud.\nFollow the link that appears as [problem set N] with the class meeting outline for the period in which the problem set was assigned. This will direct you to a form where you’ll fill out select answers. Refer to your work in Posit Cloud as you complete the form.\n\nSome general remarks:\n\nproblem sets are due one hour before the next class meeting\nlate submissions are accepted until 5pm two days after the due date\nscore summaries will be posted once all deadlines pass\nonce scores are posted, you can see your individual responses using the link that you used to access the form\n\n\n\nWeek 1 (4/1/24)\nAcademic holiday 4/1/24\nIntroduction to statistical thinking and study designs\nWednesday class meeting\n\n[reading] Vu and Harrington 1.1\n[lecture] course introduction; study designs\n[activity] distinguishing types of studies\n[problem set 1] due Monday 4/8; late submissions until Wednesday 4/10 5pm\n[problem set 1 corrections] due by 5pm Friday 4/12\n\nResponse summary [PS1] [PS1 corrections]\n\n\nWeek 2 (4/8/24)\nData types and descriptive statistics\nMonday class meeting\n\nreading quiz [12pm section] [2pm section]\n[reading] Vu and Harrington 1.2\n[lecture] data types\n[lab] R basics\n[problem set 2] due Wednesday 4/10; late submissions until Friday 4/12 5pm\n\nResponse summary [PS2]\nWednesday class meeting\n\n[reading] Vu and Harrington 1.4 - 1.5\n[lecture] descriptive statistics\n[lab] descriptive statistics in R\n[problem set 3] due Monday 4/15; late submissions until Wednesday 4/17 5pm\n\nResponse summary [PS3]\n\n\nWeek 3 (4/15/24)\nDescriptive statistics and graphical summaries\nMonday class meeting\n\n[reading quiz] Vu and Harrington 1.6\n[lecture] descriptive statistics for relationships between two variables\n[lab] bivariate summaries in R\n[problem set 4] due Wednesday 4/17; late submissions until Friday 4/19 5pm\n\nResponse summary [PS4]\nWednesday class meeting\n\n[reading] review course notes and PS1, PS2, PS3 in detail\n[review] recap and Q&A\n[test 1 practice problems] in groups with short solution presentations\n[R cheatsheet] for easy reference\n\nTest 1 available Wednesday 4/17 5pm and due Friday 4/19 5:00pm PDT [prompts] [submission] [upload R script]\n\n\nWeek 4 (4/22/24)\nFoundations for inference\nMonday class meeting\n\n[reading] Vu and Harrington 4.1\n[lecture] point estimation, sampling variability, and interval estimation\n[lab] point and interval estimation for a population mean\n[problem set 5] due Wednesday 4/24; late submissions until Friday 4/26 5pm\n\nResponse summary [PS5]\nWednesday class meeting\n\n[reading quiz] Vu and Harrington 3.3.1, 3.3.2, and 3.3.3; and 4.2\n[lecture] constructing and interpreting confidence intervals\n[lab] computing confidence intervals\n\nTest 1 corrections due Friday 4/26 5pm [submit corrections]\n\n\nWeek 5 (4/29/24)\nOne-sample inference for numerical data\nMonday class meeting\n\n[reading] Vu and Harrington 4.3.1 & 4.3.2\n[lecture] the \\(t\\)-test for a population mean\n[lab] computing test statistics, critical values, and \\(p\\)-values\n[problem set 6] due Wednesday 5/1; late submissions until Friday 5/3 5pm\n\nResponse summary [PS6]\nWednesday class meeting\n\n[reading] Vu and Harrington 4.3.3 & 4.3.4\n[lecture] directional tests\n[lab] directional tests\n[problem set 7] due Monday 5/6; late submissions until Wednesday 5/8 5pm\n\nResponse summary [PS7]\n\n\nWeek 6 (5/6/24)\nTwo-sample inference for numerical data\nPlease complete this short [midquarter feedback survey] by Friday 5/10. Responses are anonymous.\nMonday class meeting\n\n[reading] Vu and Harrington 5.3\n[lecture] two-sample inference\n[lab] two-sample t tests in R\n[problem set 8] due Wednesday 5/8; late submissions until Friday 5/10\n\nWednesday class meeting\n\n[reading] Vu and Harrington 5.4\n[lecture] decision errors; statistical power\n[lab] post hoc and study design power analyses\n[test 2 practice problems] test 2 prep\n[R cheatsheet] for easy reference\n\nTest 2 available Wednesday 5/8 5pm and due Friday 5/10 5pm [prompts] [submission form] [upload R script]\n\n\nWeek 7 (5/13/24)\nNonparametric inference\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 (5/20/24)\nAnalysis of variance\n\n\n\n\n\n\n\n\nTest 3 due Friday 5/24 5:00pm PDT\n\n\nWeek 9 (5/27/24)\nAcademic holiday 5/27/24\nInference for categorical data\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 (6/3/24)\nSimple linear regression\n\n\n\n\n\n\n\n\n\nTest 4 due Friday 6/7 5:00pm PDT\n\n\nFinals week (6/10/24)\nOral exams to be held during scheduled exam time\nScheduled exam times:\n\n[12pm section] Wednesday 6/12 10:10am – 1:00pm\n[2pm section] Monday 6/10 1:10pm – 4:00pm"
  },
  {
    "objectID": "content/syllabus.html",
    "href": "content/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Statistics plays a crucial role in the sciences: statistical techniques provide a means of weighing quantitative evidence derived from observation and experimentation while accounting for uncertainty. Statistical thinking and data analysis also facilitate discovery, exploration, and hypothesis generation. This class aims to provide a hands-on introduction to common statistical methods used almost universally across the sciences — descriptive and graphical techniques, inferential methods for comparing population means, analysis of categorical data and contingency tables, and linear regression — while drawing on examples from the life sciences to help illuminate the potential for application in students’ chosen field(s) of study and providing basic training in the use of statistical software.\n\n\nCourse information\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nClass meetings will comprise a mixture of lecture, lab activities, class activities, and discussion.\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment].\nThese times are partitioned into 15 minute intervals that you can schedule via the appointment link above; this system is intended to minimize waiting times and guarantee one-on-one availability. Slots can be scheduled anywhere from 7 calendar days to 10 minutes in advance. While drop-ins are welcome, I can’t guarantee availability outside of scheduled times.\nCatalog Description: Data collection and experimental design, descriptive statistics, confidence intervals, parametric and non parametric one and two-sample hypothesis tests, analysis of variance, correlation, simple linear regression, chi-square tests. Applications of statistics to the life sciences. Substantial use of statistical software. Prerequisite: MATH 96; or MATH 115; or appropriate Math Placement Level. Fulfills GE Area B4 (GE Area B1 for students on the 2019-20 or earlier catalogs); a grade of C- or better is required in one course in this GE area.\n\n\nMaterials\nYou’ll need an internet-connected laptop or tablet (a keyboard is necessary since we will do some web-hosted computation and you will be expected to type assignments). You should expect to bring your laptop or tablet to every class meeting.\nComputing: use of R/RStudio will be hosted online via a posit.cloud workspace [link to join]. To access the workspace, you’ll need to create a posit.cloud account and purchase a $5/month student plan.\nTextbook: Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences, First edition. A PDF and tablet-friendly version are available for free online at the link above. This will be our primary reference and we will cover chapters 1 – 2, 4 – 6, and 8.\nCourse notes: course notes will be posted as slides on the course website.\nOther references:\n\nVan Belle, Fisher, Heagerty, and Lumley (2004). Biostatistics: a methodology for the health sciences. Wiley. A PDF can be obtained through the Kennedy Library via the link above. This text provides a thorough introduction to biostatistics (statistics for life sciences) and is an excellent reference for more depth of coverage. Select readings will be assigned from this book.\nDouglas et al. (2023). An Introduction to R. This online book covers a variety of introductory topics pertaining to R/RStudio: installation, packages, files and directories, objects, functions, data types, data structures, graphics, basic statistics, markdown, and version control. Select readings will be assigned from this book.\n\n\n\nLearning outcomes\nThis course aims to support you in developing the following abilities.\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques\n[L4] construct and interpret confidence intervals for means and differences between means for independent and paired samples\n[L5] conduct parametric and non-parametric two-sample hypothesis tests for means\n[L6] construct and interpret a confidence interval for a single proportion\n[L7] conduct Chi-square goodness-of-fit tests and tests for independence\n[L8] distinguish between case-control and cohort studies and compute relative-risk and odds in the appropriate settings\n[L9] perform analysis of variance tests and post-hoc comparisons for completely randomized designs\n[L10] use simple linear regression to describe relationships between variables\n[L11] apply one or more methods from the course to your major field of study\n\nEmphasis is placed on conceptual fluency, application, and interpretation. In addition, you will learn to perform simple statistical analyses in R and can expect to develop a basic familiarity with the software; however, as this is not a programming class, the R environment will not be discussed in any detail and you will only learn to use a handful of commands.\n\n\nAssessments\nAttainment of learning outcomes will be measured by performance on homework assignments, tests, and a short project with an oral assessment in lieu of a final exam.\n\nHomework assignments will be given at the end of every class meeting and will comprise two practice problems due by the next class meeting. These are your opportunity to practice applying course concepts and methods covered in class and will help you to keep current with the pace and content of the lectures.\nTests will be given every 2-3 weeks and will comprise roughly 10-20 problems each. These are your opportunity to demonstrate that you’ve synthesized course material and achieved learning outcomes, and you will have approximately 48 hours to complete each test. One round of revisions will be allowed for each test in which you can make up full credit for any problems answered incorrectly in your initial attempt.\nA project with an oral assessment will be given in place of a final exam. However, you will need to be available in person during the scheduled final exam time, as this is when the oral assessment will take place.\n\nEvery assessed problem will be matched to one of the learning outcomes L1-L10. All submitted work will be assessed on a question-by-question basis as satisfactory (S) or needing improvement (NI) according to whether responses are fully correct. The percentage of problems matched to a particular learning outcome for which you receive a satisfactory assessment provides a measure of your attainment of that learning outcome. These percentages form a basis for determining your course grade (see below).\nDue to limited resources we will only provide qualitative feedback on a small subset of assessed questions, and only when an assessment of NI is made. As such, it is your responsibility to seek the feedback you need to correct your understanding where needed via class engagement, office hours, peer consultation, further study, and [tutoring resources].\n\n\nLetter grades\nStudents will receive a score for each learning outcome representing the (possibly weighted) proportion of questions matched with that outcome that received a satisfactory assessment across all assignments. The outcome will be assessed as follows:\n\n‘fully met’ if the proportion is at least 0.8\n‘partly met’ if the proportion is between 0.5 and 0.8\n‘unmet’ otherwise\n\nYou will receive periodic email summaries of your progress on each learning outcome. To receive a passing grade in the class, at least six outcomes must be either partly or fully met. Subject to this condition, letter grades are then defined as follows:\n\n\n\nGrade\nNumber of fully met outcomes\n\n\n\n\nA\n10\n\n\nA-\n9\n\n\nB+\n8\n\n\nB\n7\n\n\nB-\n6\n\n\nC+\n5\n\n\nC\n4\n\n\nC-\n3\n\n\nD+\n2\n\n\nD\n1\n\n\nD-\n0\n\n\n\nPlease note that these definitions are tentative and potentially subject to change; however, I will not make the grading requirements more stringent under any circumstances.\nPlease also note that failure to adhere to course policies may result in a lower letter grade than would otherwise be assigned.\n\n\nTentative schedule\nSubject to change.\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings (V&H)\nAssessments\n\n\n\n\n1 (4/1/24)\nIntroduction to statistical thinking and study design\n1.1\n\n\n\n2 (4/8/24)\nData, data types, and data collection\n1.2\n\n\n\n3 (4/15/24)\nDescriptive statistics and graphical summaries\n1.4, 1.5, 1.6\nTest 1 [L1, L2, L3]\n\n\n4 (4/22/24)\nFoundations for inference\n4.1, 4.2\n\n\n\n5 (4/29/24)\nOne-sample inference for numerical data\n4.3, 5.1\n\n\n\n6 (5/6/24)\nTwo-sample inference for numerical data\n5.2, 5.3, 5.4\nTest 2 [L4, L5]\n\n\n7 (5/13/24)\nNonparametric tests; analysis of variance\n5.5\n\n\n\n8 (5/20/24)\nPost-hoc inference in ANOVA; intro to categorical data analysis\n8.1\nTest 3 [L6, L9]\n\n\n9 (5/27/24)\nCategorical data analysis and contingency tables\n8.3, 8.5.1, 8.5.3\n\n\n\n10 (6/3/24)\nSimple linear regression\n6.1, 6.2, 6.4, 6.5\nTest 4 [L7, L8, L10]\n\n\nFinals (6/10/24)\nN/A\nN/A\nOral project assessment [L11]\n\n\n\n\n\nCourse policies\n\nTime commitment\nSTAT218 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including lectures, reading, assignments, and study time. Some variability in workload by week should be expected, and most students will need to budget a few extra hours each week. However, students can expect to be able to meet course expectations with a time commitment of 12-16 hours per week. Considering that class meetings account for four hours per week, students should anticipate devoting 8-12 hours outside of class. If you are spending considerably more time than this on a regular basis, please let me know.\n\n\nAttendance\nRegular attendance is essential for success in the course and required per University policy. Each student may miss two class meetings without notice but additional absences should be excusable and students should notify the instructor. Unexcused absences may negatively impact course grades.\n\n\nDeadlines and extensions\nA one-hour grace period is applied to all deadlines. Work submitted more than one hour after a deadline is considered late. Policies regarding late work are as follows:\n\nYou may turn in as many as four homework assignments up to 48 hours late without penalty at any time during the quarter and without notice. Subsequently, late work may incur a penalty in final grade calculations.\nLate submissions are not allowed for tests. You are expected to plan ahead in order to meet test deadlines; I recommend putting the dates in your calendar at the beginning of the quarter.\nExceptions may be granted for significant and unforeseen challenges (medical absences, family emergencies, and the like).\n\nExtensions may be arranged as needed if warranted by the circumstances and should be requested by email. When requesting an extension, you should explain why it is needed; it is at my discretion to grant the extension or not based on the reason provided. Extensions must be arranged at least 24 hours in advance of the original deadline; requests made after this time will not be considered as a general rule.\nThese policies are intended to provide you with some flexibility to work around unforeseen circumstances while maintaining accountability for completing coursework in a timely manner. That said, if any circumstances arise that the policies do not accommodate well, please let me know and I will do my best to work with you to keep you on track in the course.\n\n\nAcademic integrity\nYou are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR). Particularly important course policies related to academic integrity are discussed below.\nCollaboration. Collaboration among enrolled students is allowed and encouraged on homework assignments subject to the condition that every collaborator must make material contributions. Material contributions might include participation in group discussions, critique or presentation of a proposed solution, comparing numerical answers, and the like. However, group submissions are not allowed and you are expected to write up your own work. Copying the work of another student outright, knowingly allowing another student to copy your work, or submitting a copy of a shared set of answers is not acceptable and amounts to a violation of University policy on academic integrity. The best way to adhere to this policy and ensure your collaborations are productive is to:\n\nattempt problems individually before consulting others\nwrite up your own solutions in private\n\nCollaboration is not permitted on tests and will result in loss of credit.\nUse of AI. Learning to use AI effectively and responsibly for problem-solving in an academic context is a skill unto itself. Submitting problem prompts directly to ChatGPT will, most of the time, return superfluous, tangential, and erroneous answers that do not meet assessment criteria for satisfactory work. Furthermore, even when AI-generated material is technically accurate, outputs rarely conform to the examples set forth in class or the solution strategies that you have been taught.\nSo in the best-case scenario, AI-generated material might be useful but only if you expend additional effort refine the prompts you use and subsequently to parse, understand, and integrate outputs with class content. In the worst-case scenario, AI-generated material will be wrong or irrelevant and simply confuse you. Considering you are learning material that is new to you, you will most likely not be able to distinguish correct from incorrect outputs – if you could, you would have had no need to query in the first place – and it will therefore be difficult if not impossible to use AI effectively. Thus, using AI is more likely to hinder than to help your learning, and for this reason I do not recommend it.\nShould you choose to use AI you must use it as an aid only and not as a substitute for doing your own work. You will be responsible for using it thoughtfully and judiciously. That means critically assessing any outputs and continuing to prepare work to be submitted in your own words and using your own analyses. Submitting AI-generated outputs directly is never acceptable — doing so amounts to falsely representing material that you did not create as your own work and is a violation of University academic integrity policy. I will respond to such violations as follows.\n\nsome AI-generated content detected: loss of credit and warning\nflagrant AI plagiarism, first offense: loss of credit and report to OSRR\nflagrant AI plagiarism, second offense: automatic course failure and report to OSRR\n\nIf you are unsure about where the line is between acceptable and unacceptable use in any particular situation, please discuss the situation with me – I’d much rather help you learn to navigate the issue without the use of penalties wherever possible.\n\n\nAssessments and final grades\nI make every effort to provide consistent, fair, and accurate evaluation of student work. Please notify me of any suspected errors or discrepancies in evaluation promptly on an assignment-by-assignment basis (i.e., not at the end of the quarter) to guarantee consideration. Final (letter) grades will only be changed in the case of clerical errors. Attempting to negotiate scores or final grades is not appropriate.\nPer University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. If you feel your grade is unfairly assigned at the end of the course, you have the right to appeal it according to the procedure outlined here.\n\n\nCommunication and email\nStudents are encouraged to use face-to-face means of communication (class meetings and office hours) when possible. Every effort is made to respond to email within 48 weekday hours; please be aware that a message sent Thursday or Friday may not receive a reply until Monday or Tuesday. Time-sensitive messages should be identified as such in the subject line. For non-time-sensitive messages, please wait one week before sending a reminder.\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nCopyright and distribution of course materials\nStudents are not permitted to share or distribute any course materials without the written consent of the instructor. This includes, in particular, uploading materials or prepared solutions to online services and sharing materials or prepared solutions with students who may take the course in a future term. Transgressions of this policy compromise the effectiveness of instruction and assessment and do a disservice to current and future students."
  },
  {
    "objectID": "content/week2-descriptive.html#todays-agenda",
    "href": "content/week2-descriptive.html#todays-agenda",
    "title": "Descriptive statistics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] frequency distributions; measures of spread and center\n[lab] descriptive statistics and simple graphics in R"
  },
  {
    "objectID": "content/week2-descriptive.html#last-time",
    "href": "content/week2-descriptive.html#last-time",
    "title": "Descriptive statistics",
    "section": "Last time",
    "text": "Last time\n\n\n\nData semantics\n\n\ncategorical data: ordinal (ordered) or nominal (unordered)\nnumeric data: continuous (no ‘gaps’) or discrete (‘gaps’)\n\n\nData types and data structures in R\n\n\nbasic types: numeric, character, logical, integer\na vector is a collection of values of one type\na data frame is a type-heterogeneous list of vectors of equal length\n\n\nVectors can store observations of one variable:\n\n# 4 observations of age\nages &lt;- c(18, 22, 18, 12)\nages\n\n[1] 18 22 18 12\n\n\nData frames can store observations of many variables:\n\n# 3 observations of 2 variables\ndata.frame(subject.id = c(11, 2, 31),\n           age = c(24, 31, 17),\n           sex = c('m', 'm', 'f'))\n\n  subject.id age sex\n1         11  24   m\n2          2  31   m\n3         31  17   f\n\n\n\n\nTechniques for summarizing data depend on the data type"
  },
  {
    "objectID": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "href": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "title": "Descriptive statistics",
    "section": "What are descriptive statistics?",
    "text": "What are descriptive statistics?\nWe learned last time that a statistic is a data summary, i.e., any function of a set of observations.\nDescriptive statistics refers to analysis of sample characteristics using summary statistics.\n\nthese are data analyses that uses statistics interpreted on face value\nin contrast to inferential statistics, which uses statistics interpreted relative to a broader population\n\nDescriptive statistics can be either numerical or graphical; we’ll discuss both."
  },
  {
    "objectID": "content/week2-descriptive.html#dataset-famuss-study",
    "href": "content/week2-descriptive.html#dataset-famuss-study",
    "title": "Descriptive statistics",
    "section": "Dataset: FAMuSS study",
    "text": "Dataset: FAMuSS study\nObservational study of 595 individuals comparing change in arm strength before and after resistance training between genotypes for a region of interest on the ACTN3 gene.\n\nPescatello, L. S., et al. (2013). Highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or FAMuSS study. BioMed research international.\n\n\n\n\nExample data rows\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n125\n0\nFemale\n40\nCaucasian\n68\n171\nCT\n26"
  },
  {
    "objectID": "content/week2-descriptive.html#categorical-frequency-distributions",
    "href": "content/week2-descriptive.html#categorical-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Categorical frequency distributions",
    "text": "Categorical frequency distributions\nFor categorical variables, the frequency distribution is simply an observation count by category. For example:\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\ngenotype\n\n\n\n\n494\nTT\n\n\n510\nTT\n\n\n216\nCT\n\n\n19\nTT\n\n\n278\nCT\n\n\n86\nTT\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n173\n261\n161"
  },
  {
    "objectID": "content/week2-descriptive.html#numeric-frequency-distributions",
    "href": "content/week2-descriptive.html#numeric-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Numeric frequency distributions",
    "text": "Numeric frequency distributions\nFrequency distributions of numeric variables are observation counts by range; a plot of a numeric frequency distribution is called a histogram.\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\nbmi\n\n\n\n\n194\n22.3\n\n\n141\n20.76\n\n\n313\n23.48\n\n\n522\n29.29\n\n\n504\n42.28\n\n\n273\n20.34\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\n\n(10,20]\n(20,30]\n(30,40]\n(40,50]\n\n\n\n\n69\n461\n58\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe operation of dividing a numeric variable into interval ranges is called binning."
  },
  {
    "objectID": "content/week2-descriptive.html#histograms-and-binning",
    "href": "content/week2-descriptive.html#histograms-and-binning",
    "title": "Descriptive statistics",
    "section": "Histograms and binning",
    "text": "Histograms and binning\nBinning has a big effect on the visual impression. Which one captures the shape best?"
  },
  {
    "objectID": "content/week2-descriptive.html#shapes",
    "href": "content/week2-descriptive.html#shapes",
    "title": "Descriptive statistics",
    "section": "Shapes",
    "text": "Shapes\nFor numeric variables, the histogram reveals the shape of the distribution:\n\nsymmetric if it shows left-right symmetry about a central value\nskewed if it stretches farther in one direction from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#modes",
    "href": "content/week2-descriptive.html#modes",
    "title": "Descriptive statistics",
    "section": "Modes",
    "text": "Modes\nHistograms also reveal the number of modes or local peaks of frequency distributions.\n\nuniform if there are zero peaks\nunimodal if there is one peak\nbimodal if there are two peaks\nmultimodal if there are two or more peaks"
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nConsider four variables from the FAMuSS study. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nHere are some made-up data. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#descriptive-measures",
    "href": "content/week2-descriptive.html#descriptive-measures",
    "title": "Descriptive statistics",
    "section": "Descriptive measures",
    "text": "Descriptive measures\nA descriptive measure is a summary statistic that captures a particular feature of the frequency distribution of a numeric variable.\nCommonly, measures capture either location or spread.\n\n\nMeasures of location:\n\nmean\nmedian\nmode\npercentiles/quantiles\n\n\nMeasures of spread:\n\nrange (min and max)\ninterquartile range\naverage deviation\nvariance\nstandard deviation\n\n\n\nIt is common practice to report multiple measures."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-center",
    "href": "content/week2-descriptive.html#measures-of-center",
    "title": "Descriptive statistics",
    "section": "Measures of center",
    "text": "Measures of center\nA measure of center is a statistic that reflects the typical value of a variable.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23"
  },
  {
    "objectID": "content/week2-descriptive.html#quick-example",
    "href": "content/week2-descriptive.html#quick-example",
    "title": "Descriptive statistics",
    "section": "Quick example",
    "text": "Quick example\nConsider the first 8 observations of change in nondominant arm strength from the FAMuSS study data:\n\n\n40, 25, 40, 125, 40, 75, 100 and 57.1\n\n\nCompute the mean, median, and mode."
  },
  {
    "objectID": "content/week2-descriptive.html#comparing-measures-of-center",
    "href": "content/week2-descriptive.html#comparing-measures-of-center",
    "title": "Descriptive statistics",
    "section": "Comparing measures of center",
    "text": "Comparing measures of center\nEach statistic is a little different, but often they roughly agree; for example, all are between 20 and 25, which seems to capture the typical BMI well enough.\n\nHow do you think the frequency distribution affects which one is “best”?"
  },
  {
    "objectID": "content/week2-descriptive.html#means-medians-and-skewness",
    "href": "content/week2-descriptive.html#means-medians-and-skewness",
    "title": "Descriptive statistics",
    "section": "Means, medians, and skewness",
    "text": "Means, medians, and skewness\nThe mean and median both get ‘pulled’ in the direction of skewness, but the mean is more sensitive:\n\nComparing means and medians captures information about skewness present since:\n\nmean \\(&gt;\\) median: right skew\nmean \\(&lt;\\) median: left skew\nmean \\(\\approx\\) median: symmetric"
  },
  {
    "objectID": "content/week2-descriptive.html#when-to-use-modes",
    "href": "content/week2-descriptive.html#when-to-use-modes",
    "title": "Descriptive statistics",
    "section": "When to use mode(s)",
    "text": "When to use mode(s)\nMode is rarely used unless extreme skewness or multiple modes are present; below are two examples."
  },
  {
    "objectID": "content/week2-descriptive.html#percentiles",
    "href": "content/week2-descriptive.html#percentiles",
    "title": "Descriptive statistics",
    "section": "Percentiles",
    "text": "Percentiles\nA percentile is a value with specified proportions of data lying both above and below that value.\n\nmeasure of location (but not center)\ndefined with reference to the percentage of data below\n\nFor example, the 20th percentile is the value with 20% of observations below and 80% of observations above. Suppose we have 5 observations:\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n19\n20\n21\n25\n31\n\n\nrank\n1\n2\n3\n4\n5\n\n\n\n\n\nThe 20th percentile is not unique! In fact any number between 19 and 20 is a 20th percentile since it would satisfy:\n\n20% below (19)\n80% above (20, 21, 25, 31)"
  },
  {
    "objectID": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "href": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "title": "Descriptive statistics",
    "section": "Cumulative frequency distribution",
    "text": "Cumulative frequency distribution\nThe cumulative frequency distribution is a data summary showing percentiles. Think of it as percentile (y) against value (x).\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of some specific values:\n\nabout 40% of the subjects are 20 or younger\nabout 80% of the subjects are 24 or younger\n\nYour turn:\n\nRoughly what percentage of subjects are 22 or younger?\nAbout what age is the 10th percentile?"
  },
  {
    "objectID": "content/week2-descriptive.html#common-percentiles",
    "href": "content/week2-descriptive.html#common-percentiles",
    "title": "Descriptive statistics",
    "section": "Common percentiles",
    "text": "Common percentiles\n\n\nThe five-number summary is a collection of five percentiles that succinctly describe the frequency distribution:\n\n\n\nStatistic name\nMeaning\n\n\n\n\nminimum\n0th percentile\n\n\nfirst quartile\n25th percentile\n\n\nmedian\n50th percentile\n\n\nthird quartile\n75th percentile\n\n\nmaximum\n100th percentile\n\n\n\n\nBoxplots provide a graphical display of the five-number summary."
  },
  {
    "objectID": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "href": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "title": "Descriptive statistics",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\nNotice how the two displays align, and also how they differ. The histogram shows shape in greater detail, but the boxplot is much more compact."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-spread",
    "href": "content/week2-descriptive.html#measures-of-spread",
    "title": "Descriptive statistics",
    "section": "Measures of spread",
    "text": "Measures of spread\nThe spread of observations refers to how concentrated or diffuse the values are.\n\nTwo ways to understand and measure spread:\n\nranges of values capturing much of the distribution\ndeviations of values from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#range-based-measures",
    "href": "content/week2-descriptive.html#range-based-measures",
    "title": "Descriptive statistics",
    "section": "Range-based measures",
    "text": "Range-based measures\nA simple way to understand and measure spread is based on ranges. Consider more ages, sorted and ranked:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\nrank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\nThe range is the minimum and maximum values: \\[\\text{range} = (\\text{min}, \\text{max}) = (16, 34)\\]\nThe interquartile range (IQR) is the difference [75th percentile] - [25th percentile] \\[\\text{IQR} = 29 - 19 = 10\\] When might you prefer IQR to range? Can you think of an example?"
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures",
    "href": "content/week2-descriptive.html#deviation-based-measures",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe average deviation is defined as the average of the absolute values of the deviations from the mean: \\[\\frac{8 + 6 + 5 + 4 + 3 + 2 + 1 + 2 + 4 + 5 + 6 + 10}{12}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#mathematical-notations",
    "href": "content/week2-descriptive.html#mathematical-notations",
    "title": "Descriptive statistics",
    "section": "Mathematical notations",
    "text": "Mathematical notations\nFollowing the convention from before, write a set of \\(n\\) observations as \\(x_1, x_2, \\dots, x_n\\).\n\n\nThe mean of the observations is written: \\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]\nThe average deviation is: \\[\\frac{1}{n} \\sum_i |x_i - \\bar{x}|\\]\n\nThe variance is: \\[s_x^2 = \\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2\\]\nThe standard deviation is: \\[s_x = \\sqrt{\\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#interpretations",
    "href": "content/week2-descriptive.html#interpretations",
    "title": "Descriptive statistics",
    "section": "Interpretations",
    "text": "Interpretations\nListed from largest to smallest, here are each of the measures of spread for the 12 ages:\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin\nmax\niqr\nvariance\nst.dev\navg.dev\n\n\n\n\n16\n34\n8.5\n30.55\n5.527\n4.667\n\n\n\n\n\nThe interpretations differ between these statistics:\n\n[range] all of the data lies on an between 16 and 34 years old on an interval 18 years in width\n[IQR] the middle half of the data lies on an interval 8.5 years in width\n[average deviation] the average distance from the mean is 4.67 years\n[variance] the average squared distance from the mean is 30.55 years\\(^2\\)\n[standard deviation] the average squared distance from the mean, rescaled to years, is 5.53 years\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-descriptive.html#lab-robustness",
    "href": "content/week2-descriptive.html#lab-robustness",
    "title": "Descriptive statistics",
    "section": "Lab: robustness",
    "text": "Lab: robustness\nFor this lab we’ll continue to work with the FAMuSS data as we have throughout lecture.\nThis lab has two objectives:\n\nTeach you to compute descriptive statistics and prepare graphical summaries for a single variable in R\nLearn when and why to use certain descriptive statistics in place of others\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab1-rbasics.html",
    "href": "content/lab1-rbasics.html",
    "title": "Lab 1: R Basics",
    "section": "",
    "text": "This lab is intended to introduce you to the basics in R that you will need for this class. Most of our analyses will consist of just a few steps:\n\nload a dataset\nidentify and select variable(s) of interest\nperform one or more calculations using variable(s) of interest as inputs\n\nWe will illustrate this process so that you can get used to the mechanics and familiarize yourself with how different data types appear in R.\n\nHow to do this lab\nI’ve provided you with a project on Posit Cloud containing data files and a script (a script is a plain text file containing R commands). The script contains all commands shown in this document, and some blank areas for you to fill in, with comment lines (the ones starting with #) to help you navigate.\nYou should refer back to this document for instructions and context, and fill in the script as you go:\n\nrun the codes provided as you read through the narrative in this document and inspect the results\nin the ‘your turn’ sections, refer to the prompt in this document and use the example commands provided immediately beforehand to determine which command to write\nwrite in your commands in the script the space below the corresponding comment, not in the console (otherwise you’ll have a hard time keeping track of your work)\n\nYour goal is to complete all of the “your turn” parts in the script. Two practice problems are given at the end of the lab as homework for you to complete on your own before next class.\n\n\nHow to use this lab\nThis lab (and the lab activities in general) are designed to provide you with a set of examples to learn initially in class and then follow on your own later when doing the homework problems given at the end of the lab.\nIf you can do the examples and ‘your turn’ activities in class, all you’ll need to do to complete the homeworks is copy commands from those examples and activities and adjust some small details (variable names, dataset names, etc.).\nIf you later need to figure out how to do something in R for a homework problem or test, all you’ll need to do is refer back to the labs.\n\n\nPackages in R\nA “package” is a bundle of functions, datasets, and other objects that can be imported into R for use in your working environment. Many scripts begin by loading packages that will be used throughout the script. Packages are loaded using the command library(&lt;PACKAGE NAME&gt;) where &lt;PACKAGE NAME&gt; is replaced by the actual name of the package. For example:\n\nlibrary(tidyverse)\n\nPackages do need to be installed before they can be loaded. One of the nice things about using Posit Cloud is that I can manage all of these installs for you. However, if you ever wish to install and use a package that’s not available (or if you use R on your own machine), you can install a package using the command install.packages(\"&lt;PACKAGE NAME&gt;\") after replacing &lt;PACKAGE NAME&gt; with the actual name of the package (but keeping the quotation marks!).\n\n\nLoading a dataset\nThere are several ways to load datasets in R. The strategy we’ll use most often is to load an .RData file, but you will encounter a few others here and there.\n\n# load nhanes data\nload('data/nhanes.RData')\n\nThis command looks for a file called nhanes.RData in a directory folder named data and reads the file.\nNotice that once you run the command, an object called nhanes appears in the “Environment” tab in the upper right hand panel of your RStudio window.\nIf you click the little blue carrot next to nhanes in the environment tab, you will then see a list of variables contained in the dataset. You can also see the first few rows of the dataset using head(...).\n\n# first few rows\nhead(nhanes)\n\n# A tibble: 6 × 9\n  subj.id gender   age poverty pulse bpsys1 bpdia1 totchol sleephrsnight\n    &lt;int&gt; &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;         &lt;int&gt;\n1       1 male      34    1.36    70    114     88    3.49             4\n2       2 male      34    1.36    70    114     88    3.49             4\n3       3 male      34    1.36    70    114     88    3.49             4\n4       5 female    49    1.91    86    118     82    6.7              8\n5       8 female    45    5       62    106     62    5.82             8\n6       9 female    45    5       62    106     62    5.82             8\n\n\nThis kind of object in R is called a data frame. Data frames are displayed in a tabular layout, like a spreadsheet. While data frames should be arranged so that observations are shown in rows and variables in columns, this is not guaranteed, so you should be in the habit of checking to make sure the layout is sensible; otherwise, you might accidentally perform bogus calculations and analyses.\nBeyond providing a sanity check, inspecting the data frame will show you three key pieces of information besides the values of the first few observations of each variable.\n\nData dimensions: how many observations (rows) and how many variables (columns)\nVariable names: subj.id, gender, age, etc.\nData types:\n\nint for integer (numerical data type)\nfct for factor (categorical data type)\nnum for numeric (numerical data type)\nchr for character (categorical data type)\n\n\nSo, for example, seeing that pulse is of data type int tells you that pulse is a discrete numerical variable. It also tells you what name to use to refer to the variable in subsequent R commands.\n\n\n\n\n\n\nYour turn\n\n\n\nThere is another data file in the data directory called famuss.RData. Load this into the environment, preview the first few observations, and check the variable names and data types.\n\n# load famuss dataset\n\n# preview first few rows\n\nTo check your understanding:\n\nhow many observations and variables?\nidentify a categorical variable\nwhat kind of variable is bmi?\n\n\n\n\n\nSelecting variables\nThe variable names in a dataset can be used to retrieve or refer to specific variables. For example, try running this command:\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# preview first few values\nhead(total.cholesterol)\n\n[1] 3.49 3.49 3.49 6.70 5.82 5.82\n\n\nThat command did the following:\n\nextracted the totchol column of nhanes (the nhanes$totchol part)\nassigned the result a new name total.cholesterol (the &lt;- part)\n\nAssignment (&lt;-) is a very important concept in R – you can store the result of any calculation as an object with a name of your choosing.\nYou’ll notice that total.cholesterol looks a bit different than the data frame in terms of its appearance. This is because it’s not a data frame but rather a different kind of object called a vector: a collection of values of the same data type.\n\n\n\n\n\n\nYour turn\n\n\n\nExtract the change in nondominant arm strength variable from the FAMuSS dataset, and store it as a vector called strength.\n\n# store the change in nondominant arm strength variable as a vector called 'strength'\n\n# preview the first few values\n\n\n\n\n\nPerforming calculations\nExtracting and storing variables as vectors isn’t strictly necessary, but does make it easier to perform many calculations. While you’re a beginner, I’d recommend using this strategy.\n\nNumeric summaries\nMost simple summary statistics can be calculated using simple functions in R that take a single vector argument. For example, to calculate the average, minimum, and maximum total cholesterol among the respondents in the sample:\n\n# average total cholesterol\nmean(total.cholesterol)\n\n[1] 5.042938\n\n# minimum\nmin(total.cholesterol)\n\n[1] 2.33\n\n# maximum\nmax(total.cholesterol)\n\n[1] 13.65\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nFind the average percent change in nondominant arm strength of participants in the FAMuSS study sample using the strength vector you created before.\n\n# compute mean change in nondominant arm strength\n\n\n\n\n\nCategorical summaries\nMost data summaries for categorical variables proceed from counts of the number of observations in each category. These counts can be obtained by passing a vector of observations to table(...):\n\n# retreive sex variable\nsex &lt;- nhanes$gender\n\n# counts\ntable(sex)\n\nsex\nfemale   male \n  1588   1591 \n\n\nTo obtain the proportion of observations in each category – the counts divided by the total number of observations – pass the table to the proportions(...) function:\n\n# proportions\ntable(sex) |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nThe character string |&gt; is a bit of syntax that you could read verbally as ‘then’: first make a table, then obtain proportions. It’s known as the pipe operator, because it ‘pipes’ the result of the command on its left into the command on its right.\nTo see another example of the pipe operator in action, you could rewrite the previous command as a chain of three steps:\n\n# same as above\nsex |&gt; table() |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nYou could interpret this as follows: start with sex, pass that to table(), then pass the result to proportions.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the FAMuSS dataset, calculate the genotype frequencies in the sample (i.e., find the proportion of observations of each genotype).\n\n# retrieve genotype\n\n# counts\n\n# proportions\n\n\n\nWhile the analyses you’ll learn will get more complex than computing summary statistics, the mechanics of performing the computations in R will be analogous to what you just did: executing a one-line command with a vector input.\n\n\n\nPutting together the pieces\nReflect for a moment on what you just did: you wrote a few lines of code to import a dataset, extract a variable, and compute a statistic. If you filled in the script as instructed, you now have a record of the commands you executed that you can use to retrace your steps.\nIn fact, anyone with your script and the data files (including future you) could easily reproduce your work. Reproducibility is a pillar of data-driven science; by storing analyses in the form of executable scripts, researchers can easily create and share records of their work.\nWe could put the steps above together in just a few lines as if it were a short script. Typical style is to provide line-by-line comments explaining what the commands do.\n\n# import nhanes data\nload('data/nhanes.RData')\n\n# inspect data\nhead(nhanes)\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# compute average total cholesterol\nmean(total.cholesterol)\n\n# extract sex\nsex &lt;- nhanes$gender\n\n# proportions of men and women in sample\ntable(sex) |&gt; proportions()\n\n\n\n\n\n\n\nYour turn\n\n\n\nFollow the example above and combine the previous exercises into a few lines of code with appropriate line comments.\n\n# load famuss dataset\n\n# inspect data\n\n# extract nondominant change in arm strength\n\n# compute average change in strength\n\n# extract genotype\n\n# compute genotype frequencies (proportions)\n\n\n\nIf this was all entirely new to you, congratulations on writing your first lines of code!\n\n\nExtras\n\nReading CSV files\nOften data are stored in spreadsheets, which can be easily converted to comma-separated values or CSV files (extension .csv). These are plain-text files that are a bit more lightweight than an Excel spreadsheet.\nR can read CSV (as well as other) files. The read.csv(...) function will parse the file and produce a data frame. The result can be assigned a name and stored as an object in the environment.\n\n# parse a csv file\nread.csv('data/gss.csv')\n\n# store the result in the environment\ngss &lt;- read.csv('data/gss.csv')\n\nMost of the time in class we’ll load .RData files or obtain datasets through packages (more on this later), but if you use R outside of class you may find it more common to manage data input via .csv files.\n\n\nMore about R\nWhile you will learn new commands going forward, we won’t go much more in depth with R than what you just saw. However, if you’re interested in understanding the above concepts in greater detail, or learning about R as a programming environment, see An Introduction to R.\n\n\n\nPractice problems\nDue before the next class meeting.\n\nThe census dataset contains a sample of data for 377 individuals included in the 2000 U.S. census. Load and inspect the dataset, and determine:\n\nthe youngest and oldest individual in the sample\nthe average total personal income\nthe average total family income\nhow many variables are in the dataset, not including census year and FIPS code\nhow many categorical variables are in the dataset, not including FIPS code\n\n\n\nThe cdc.samp dataset in the oibiostat package contains a sample of data for 60 individuals surveyed by the CDC’s Behavioral Risk Factors Surveillance System (BRFSS). Use the provided commands to load the dataset, and then inspect it the usual way. Notice that several of the variables are 1’s and 0’s. Use the command ?cdc.samp to view the data documentation.\n\nWhat do the values (1’s and 0’s) mean in the exerany variable?\nWhat proportion of the sample are men? What proportion are women?\nFor each general health category, find the proportion of respondents who rated themselves in that category.\nHow many of the respondents have health coverage? (Hint: sum(x) will add up the values in a vector x; adding up a collection of 1’s and 0’s is equivalent to counting the number of 1’s.) What percentage of the respondents have health coverage?"
  },
  {
    "objectID": "content/week1-studies.html#todays-agenda",
    "href": "content/week1-studies.html#todays-agenda",
    "title": "Welcome to STAT218",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nCourse logistics\n[lecture] Study designs\n[activity, if time] Distinguishing study types"
  },
  {
    "objectID": "content/week1-studies.html#icebreakers",
    "href": "content/week1-studies.html#icebreakers",
    "title": "Welcome to STAT218",
    "section": "Icebreakers",
    "text": "Icebreakers\nBy show of hands…\n\n\nFirst statistics class ever?\nLast statistics class ever?\nExpect to take STAT313?\nExpect to use statistics for your degree coursework or senior project?\nConsidering a statistics or data science minor?"
  },
  {
    "objectID": "content/week1-studies.html#class-composition",
    "href": "content/week1-studies.html#class-composition",
    "title": "Welcome to STAT218",
    "section": "Class composition",
    "text": "Class composition\nBy the numbers…"
  },
  {
    "objectID": "content/week1-studies.html#statistics-and-uncertainty",
    "href": "content/week1-studies.html#statistics-and-uncertainty",
    "title": "Welcome to STAT218",
    "section": "Statistics and uncertainty",
    "text": "Statistics and uncertainty\n\nLife is full of uncertainty, and this can make a lot of questions hard to answer, because similar situations do not always result in the same outcome.\n\nStatistical thinking: uncertainty is measurable.\nWhat statistics can offer:\n\nprinciples for designing studies and collecting data in order to capture outcome variability\ndata analytic tools to distinguish random from systematic variability\nheuristics to make inferences that account for uncertainty"
  },
  {
    "objectID": "content/week1-studies.html#course-goal-and-scope",
    "href": "content/week1-studies.html#course-goal-and-scope",
    "title": "Welcome to STAT218",
    "section": "Course goal and scope",
    "text": "Course goal and scope\nThe overarching goal of 218 is to introduce you to statistics in a hands-on way that is relevant to your major.\nSo we will focus on:\n\nstatistical thinking, study design, and data analysis\nclassical methods, mostly developed 1900-1940\ncase studies from life sciences"
  },
  {
    "objectID": "content/week1-studies.html#materials",
    "href": "content/week1-studies.html#materials",
    "title": "Welcome to STAT218",
    "section": "Materials",
    "text": "Materials\nComputer/tablet. You’ll need a laptop (preferred) or tablet with keyboard (workable).\nCourse website. All materials are hosted/linked on the course website. I won’t be using Canvas.\nTextbook. Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences. I suggest a $5-15 donation.\nStatistical software. R/RStudio hosted online via posit.cloud workspace. You will need to create an account and purchase a $5/month student subscription."
  },
  {
    "objectID": "content/week1-studies.html#class-meetings",
    "href": "content/week1-studies.html#class-meetings",
    "title": "Welcome to STAT218",
    "section": "Class meetings",
    "text": "Class meetings\nClass meetings will usually consist of a reading quiz, a lecture, a break, and a lab.\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class."
  },
  {
    "objectID": "content/week1-studies.html#assignments",
    "href": "content/week1-studies.html#assignments",
    "title": "Welcome to STAT218",
    "section": "Assignments",
    "text": "Assignments\nYou will have three categories of assignments:\n\nhomework problems: two per class due by next class\ntests: every 2-3 weeks, distributed Wednesday, due Friday\na project: find and present a case study\n\nDeadline policies:\n\none-hour grace period on all deadlines\nfour homework problem sets can be turned in up to 48 hours late without notice\nbesides free lates, extensions must be arranged 24 hours in advance of the deadline"
  },
  {
    "objectID": "content/week1-studies.html#grades",
    "href": "content/week1-studies.html#grades",
    "title": "Welcome to STAT218",
    "section": "Grades",
    "text": "Grades\nEvery graded question/problem is matched to one or more of the 11 course learning outcomes.\n\nQuestions/problems are evaluated as satisfactory (S), needs improvement (NI), or missing (M).\nFor each outcome, the percentage of questions/problems awarded a satisfactory mark is used to determine whether that outcome is fully met, partly met, or not met:\n\nfully met: 80% or more of matched questions satisfactory\npartly met: 50% – 80% of matched questions satisfactory\nnot met: less than 50% of matched questions satisfactory\n\n\nYour course grade is based on how many learning outcomes are fully met. To pass, you must partly or fully meet at least 6 outcomes; for a C-, you must fully meet at least 3 outcomes."
  },
  {
    "objectID": "content/week1-studies.html#important-policies",
    "href": "content/week1-studies.html#important-policies",
    "title": "Welcome to STAT218",
    "section": "Important policies",
    "text": "Important policies\n\nextensions must be confirmed (not simply requested) 24 hours in advance\ncollaboration on homework is encouraged, but everyone involved needs to…\n\nmake a contribution\nwrite up their own work\n\nsubmitting AI-generated content in place of your own work is not acceptable\n\nresponsible use is okay, but not recommended (GPT outputs are misleading)\npenalties for AI plagiarism depend on precedent and severity\n\n\n\n\n\nMinor offense\nMajor offense\nPenalty\n\n\n\n\nFirst\n\nloss of credit and warning\n\n\nSecond\nFirst\nloss of credit and OSRR report\n\n\nThird\nSecond\ncourse failure and second OSRR report"
  },
  {
    "objectID": "content/week1-studies.html#what-is-a-study",
    "href": "content/week1-studies.html#what-is-a-study",
    "title": "Welcome to STAT218",
    "section": "What is a study?",
    "text": "What is a study?\nA study is an effort to collect data in order to answer one or more research questions.\n\nstudies must be well-matched to research questions to provide good answers\nhow data are obtained is just as important as how the resulting data are analyzed\nno analysis, no matter how sophisticated will rescue a poorly conceived study\n\nA study unit is the smallest object or entity that is measured in a study; also called experimental unit or observational unit."
  },
  {
    "objectID": "content/week1-studies.html#two-types-of-studies",
    "href": "content/week1-studies.html#two-types-of-studies",
    "title": "Welcome to STAT218",
    "section": "Two types of studies",
    "text": "Two types of studies\nObservational studies collect data from an existing situation without intervention.\n\nAim is to detect associations and patterns\nCan’t be used to establish causal links\n\nExperiments collect data from a situation in which one or more interventions have been introduced by the investigator.\n\nAim is to draw conclusions about the causal effect of interventions\nStronger form of scientific evidence than an observational study\n\nOften, observational studies are used to explore/generate hypotheses prior to designing an experiment."
  },
  {
    "objectID": "content/week1-studies.html#comparing-study-types",
    "href": "content/week1-studies.html#comparing-study-types",
    "title": "Welcome to STAT218",
    "section": "Comparing study types",
    "text": "Comparing study types\nEither type of study can be used to address a question.\n\n\n\n\n\n\n\n\nQuestion\nObservational study\nExperiment\n\n\n\n\nAre diet and mood related?\nConduct surveys on diet, lifestyle, and affect\nRecruit study participants, assign diets, measure affect\n\n\nIs vaping safer than smoking?\nFollow groups of vapers and smokers over time and record health outcomes\nAmong a group of smokers, assign some to switch to vaping; compare health outcomes over time\n\n\nDo insecticide applications affect soil microbes?\nAnalyze soil samples from farms using different insecticides\n[Your turn]\n\n\n\nCan you think of pros and cons for each study type?"
  },
  {
    "objectID": "content/week1-studies.html#why-does-intervention-matter",
    "href": "content/week1-studies.html#why-does-intervention-matter",
    "title": "Welcome to STAT218",
    "section": "Why does intervention matter?",
    "text": "Why does intervention matter?\nControl over conditions allows a researcher to study causal effects resulting from interventions. This is not possible in observational studies due to the potential for confounding.\n\n\nConfounding: an unobserved condition is associated with both the study condition and the outcome.\n\nFailure to measure and account for confounders potentially distorts observed associations\nExample: a study finds that dog owners live longer, but doesn’t measure exercise; so it might just be the daily walks.\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved variable) --- B(study variable) & C(study outcome) \n\n\n\n\n\n\n\n\nThis is very common in observational studies, because you can’t measure every study condition."
  },
  {
    "objectID": "content/week1-studies.html#antidote-randomization",
    "href": "content/week1-studies.html#antidote-randomization",
    "title": "Welcome to STAT218",
    "section": "Antidote: randomization",
    "text": "Antidote: randomization\nThe ability to control study conditions allows researchers to randomly allocate interventions among study subjects.\n\nRandomization eliminates confounding by isolating the condition(s) of interest:\n\ninterventions are independent of extraneous conditions ⟹ no association possible\nif outcomes differ systematically according to the intervention, you can be certain that it is not an artifact\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved condition) x-.-x B(study condition)\n  A --- C(outcome)"
  },
  {
    "objectID": "content/week1-studies.html#practical-consequences",
    "href": "content/week1-studies.html#practical-consequences",
    "title": "Welcome to STAT218",
    "section": "Practical consequences",
    "text": "Practical consequences\nThe ability to randomize interventions in experiments means:\n\nobserved associations are independent of extraneous factors\nresults can support causal inferences\n\nThe absence of randomization in observational studies means:\n\nconfounding is always possible\nresults may be misleading"
  },
  {
    "objectID": "content/week1-studies.html#experimental-designs",
    "href": "content/week1-studies.html#experimental-designs",
    "title": "Welcome to STAT218",
    "section": "Experimental designs",
    "text": "Experimental designs\nA treatment is an experimental intervention; the design of an experiment refers to how treatments are allocated to study units.\nThe most basic design is:\n\n[balanced] each treatment is replicated an equal number of times\n[randomized] treatments are allocated completely at random to study units\n[no crossover] each study unit receives exactly one treatment\n\nWe’ll call this a completely randomized design. It’s the only kind of experimental design we’re going to consider in STAT218.\nThere are many other designs that we won’t discuss (but see STAT313); these are all about improving experimental efficiency by controlling extraneous variation."
  },
  {
    "objectID": "content/week1-studies.html#data-collection",
    "href": "content/week1-studies.html#data-collection",
    "title": "Welcome to STAT218",
    "section": "Data collection",
    "text": "Data collection\nStudy units should be chosen so as to represent a larger collection.\n\n\n\n\nA study population is a collection of all study units of interest.\nA sample is a subcollection from a population:\n\nrandom if study units have a known chance of inclusion in the sample\nnonrandom or convenience otherwise\n\n\n\nThe gold standard is the simple random sample: each study unit in the population has an equal chance of inclusion in the sample."
  },
  {
    "objectID": "content/week1-studies.html#leap-study",
    "href": "content/week1-studies.html#leap-study",
    "title": "Welcome to STAT218",
    "section": "LEAP Study",
    "text": "LEAP Study\n\n\nLearning early about peanut allergy (LEAP) study:\n\n640 infants in UK with eczema or egg allergy but no peanut allergy enrolled\neach infant randomly assigned to peanut consumption and peanut avoidance groups\n\npeanut consumption: fed 6g peanut protein daily until 5 years old\npeanut avoidance: no peanut consumption until 5 years old\n\nat 5 years old, oral food challenge (OFC) allergy test administered\n13.3% of the avoidance group developed allergies, compared with 1.9% of the consumption group\n\n\n\n\n\nStudy characteristics\n\n\nStudy type: experiment\nStudy population: UK infants with eczema or egg allergy but no peanut allergy\nSample: 640 infants from population\nStudy design: completely randomized design\nTreatments: peanut consumption; peanut avoidance\nStudy outcome: development of peanut allergy by 5 years of age\n\n\n\n\n\n\nStudy results\n\n\nModerated peanut consumption causes a reduction in the likelihood of developing an allergy."
  },
  {
    "objectID": "content/week1-studies.html#checklist-for-next-time",
    "href": "content/week1-studies.html#checklist-for-next-time",
    "title": "Welcome to STAT218",
    "section": "Checklist for next time",
    "text": "Checklist for next time\n\nObtain a copy of the textbook.\nCreate a posit.cloud account and purchase a student subscription. Ensure you can access the stat218-s24 workspace.\nComplete practice problems and reading before class.\nWrite down one question about the reading.\nPrint a paper or virtual copy of the slides."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account",
    "href": "content/week1-studies.html#posit-cloud-account",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nGo to: course webpage &gt; syllabus &gt; materials. Then look for the link to join the class workspace:"
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-1",
    "href": "content/week1-studies.html#posit-cloud-account-1",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nFollow prompts to create an account. Use your Cal Poly email."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-2",
    "href": "content/week1-studies.html#posit-cloud-account-2",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nOnce your email is verified, return to posit.cloud (or click the link in the syllabus again), and join the class workspace."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-3",
    "href": "content/week1-studies.html#posit-cloud-account-3",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nUpgrade your account to the student plan. Input payment details."
  },
  {
    "objectID": "content/week1-studies.html#printing-slides",
    "href": "content/week1-studies.html#printing-slides",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nOpen menu from lower left"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-1",
    "href": "content/week1-studies.html#printing-slides-1",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nNavigate to tools"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-2",
    "href": "content/week1-studies.html#printing-slides-2",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nSelect PDF export mode"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-3",
    "href": "content/week1-studies.html#printing-slides-3",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\n\n\nThen print from browser to PDF\n\n\nI suggest landscape layout and either 1, 2 or 4 slides per page\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-datatypes.html#todays-agenda",
    "href": "content/week2-datatypes.html#todays-agenda",
    "title": "Data semantics and data types",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\n[lecture] data semantics and data types\n[lab] R basics"
  },
  {
    "objectID": "content/week2-datatypes.html#data-semantics",
    "href": "content/week2-datatypes.html#data-semantics",
    "title": "Data semantics and data types",
    "section": "Data semantics",
    "text": "Data semantics\n\nData are a set of measurements.\nA variable is any measured attribute of study units.\nAn observation is a measurement of one or more variables taken on one particular study unit.\n\nIt is usually expedient to arrange data values in a table in which each row is an observation and each column is a variable:"
  },
  {
    "objectID": "content/week2-datatypes.html#leap-example",
    "href": "content/week2-datatypes.html#leap-example",
    "title": "Data semantics and data types",
    "section": "LEAP example",
    "text": "LEAP example\nA table showing the observations and variables for the LEAP study would look like this:\n\n\n\n\n\n\n\n\n\n\nparticipant.ID\ntreatment.group\nofc.test.result\n\n\n\n\nLEAP_100522\nPeanut Consumption\nPASS OFC\n\n\nLEAP_103358\nPeanut Consumption\nPASS OFC\n\n\nLEAP_105069\nPeanut Avoidance\nPASS OFC\n\n\nLEAP_105328\nPeanut Consumption\nPASS OFC\n\n\n\n\n\nThe table you saw in the reading was a summary of the data (not the data itself):\n\n\n\n\n\n\n\n\n\n\n \nFAIL OFC\nPASS OFC\n\n\n\n\nPeanut Avoidance\n36\n227\n\n\nPeanut Consumption\n5\n262"
  },
  {
    "objectID": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "href": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "title": "Data semantics and data types",
    "section": "Numeric and categorical variables",
    "text": "Numeric and categorical variables\nVariables are classified according to their values. Values can be one of two different types:\n\nA variable is numeric if its value is a number\nA variable is categorical if its value is a category, usually recorded as a name or label\n\nFor example:\n\nthe value of sex can be male or female, so it is categorical\nwhereas age (in years) can be any positive integer, so it is numeric"
  },
  {
    "objectID": "content/week2-datatypes.html#variable-subtypes",
    "href": "content/week2-datatypes.html#variable-subtypes",
    "title": "Data semantics and data types",
    "section": "Variable subtypes",
    "text": "Variable subtypes\nFurther distinctions are made based on the type of number or type of category used to measure an attribute. Can you match the subtypes to the variables at right?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nhispanic\ngrade\nweight\n\n\n\n\n15\nnot\n10\n78.02\n\n\n18\nhispanic\n12\n78.47\n\n\n17\nnot\n11\n95.26\n\n\n18\nnot\n12\n95.26\n\n\n\n\n\n\n\n\na numerical variable is discrete if there are ‘gaps’ between its possible values\na numerical variable is continuous if there are no such gaps\na categorical variable is nominal if its levels are not ordered\na categorical variable is ordinal if its levels are ordered"
  },
  {
    "objectID": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "href": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "title": "Data semantics and data types",
    "section": "Many ways to measure attributes",
    "text": "Many ways to measure attributes\nVariable type (or subtype) is not an inherent quality — attributes can often be measured in many different ways.\nFor instance, age might be measured as either a discrete, continuous, or ordinal variable, depending on the situation:\n\n\n\nAge (years)\nAge (minutes)\nAge (brackets)\n\n\n\n\n12\n6307518.45\n10-18\n\n\n8\n4209187.18\n5-10\n\n\n21\n11258103.08\n18-30\n\n\n\n\nNumeric variables can always be represented as categorical, but not the other way around."
  },
  {
    "objectID": "content/week2-datatypes.html#your-turn",
    "href": "content/week2-datatypes.html#your-turn",
    "title": "Data semantics and data types",
    "section": "Your turn",
    "text": "Your turn\nClassify each variable as nominal, ordinal, discrete, or continuous:\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ngenotype\nsex\nage\nrace\nbmi\n\n\n\n\n33.3\nCT\nFemale\n19\nCaucasian\n21.01\n\n\n71.4\nCT\nFemale\n18\nOther\n23.18\n\n\n37.5\nCC\nFemale\n21\nCaucasian\n28.92\n\n\n50\nCC\nFemale\n28\nAsian\n21.16\n\n\n\n\n\nData are from an observational study investigating demographic, physiological, and genetic characteristics associated with muscle strength.\n\nndrm.ch is change in strength in nondominant arm after resistance training\ngenotype indicates genotype at a particular location within the ACTN3 gene"
  },
  {
    "objectID": "content/week2-datatypes.html#common-summary-statistics",
    "href": "content/week2-datatypes.html#common-summary-statistics",
    "title": "Data semantics and data types",
    "section": "Common summary statistics",
    "text": "Common summary statistics\n\nA statistic is a data summary: in mathematical terms, a function of several observations\n\n\n\nFor numeric variables, the most common summary statistic is the average value:\n\\[\\text{average} = \\frac{\\text{sum of values}}{\\text{# observations}}\\]\nFor example, the average percent change in nondominant arm strength was 53.291%.\n\nFor categorical variables, the most common summary statistic is a proportion:\n\\[\\text{proportion}_i = \\frac{\\text{# observations in category } i}{\\text{# observations}}\\]\nFor example:\n\n\n\nGenotype proportions\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n0.2908\n0.4387\n0.2706"
  },
  {
    "objectID": "content/week2-datatypes.html#descriptive-analyses",
    "href": "content/week2-datatypes.html#descriptive-analyses",
    "title": "Data semantics and data types",
    "section": "Descriptive analyses",
    "text": "Descriptive analyses\nSometimes, a few clever summary statistics can be used to answer a research question.\n\nHow much does the average change in arm strength differ by genotype, if at all?\n\nComputing per-genotype averages provides an answer:\n\n\n\n\n\n\n\n\n\n\n\ngenotype\navg.change\nn.obs\nprop.obs\n\n\n\n\nTT\n58.08\n161\n0.2706\n\n\nCT\n53.25\n261\n0.4387\n\n\nCC\n48.89\n173\n0.2908\n\n\n\n\n\nNumber of observations and proportions are included because they provide information about genotype frequencies in the sample.\n\nconveys how many individuals were measured\nalso provides an estimate of genotype frequencies in the population"
  },
  {
    "objectID": "content/week2-datatypes.html#common-mathematical-notation",
    "href": "content/week2-datatypes.html#common-mathematical-notation",
    "title": "Data semantics and data types",
    "section": "Common mathematical notation",
    "text": "Common mathematical notation\nWhile we won’t use mathematical expressions too often in STAT218, it’s useful to be aware of some common notations.\nTypically, a set of observations is written as:\n\\[x_1, x_2, \\dots, x_n\\]\n\n\\(x\\) represents the variable (e.g., genotype, age, percent change, etc.)\nsubscript indexes observations: \\(x_i\\) is the \\(i\\)th observation\n\\(n\\) is the total number of observations\n\nThe sum of the observations is written \\(\\sum_i x_i\\), where the symbol \\(\\sum\\) stands for ‘summation’. This is useful for writing the formula for computing an average:\n\\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]"
  },
  {
    "objectID": "content/week2-datatypes.html#lab-data-basics-in-r",
    "href": "content/week2-datatypes.html#lab-data-basics-in-r",
    "title": "Data semantics and data types",
    "section": "Lab: data basics in R",
    "text": "Lab: data basics in R\nThe variable types we just discussed map pretty neatly (but not perfectly) onto the main “data types” in R:\n\nnumeric ➜ integer, numeric\ncategorical ➜ character, factor, logical\n\nThe primary way data are arranged in R is in a data frame. This lab will show you how to load, inspect, and use data frames.\nYour objectives in this lab are:\n\nlearn to load and inspect datasets\nlearn to recognize data types\nlearn to perform simple calculations (averages, etc.)"
  },
  {
    "objectID": "content/week2-datatypes.html#opening-the-lab-activity",
    "href": "content/week2-datatypes.html#opening-the-lab-activity",
    "title": "Data semantics and data types",
    "section": "Opening the lab activity",
    "text": "Opening the lab activity\nNavigate to posit.cloud. Then:\n\n\n\n\n\n\nMake sure the class workspace “stat218-s24” is highlighted at left. If “Your Workspace” is highlighted, you won’t see the example assignment.\nClick on the lab1-rbasics, then wait.\n\nOnce everyone is ready, we’ll have a look at the example files together.\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week1-activity-studydesigns.html",
    "href": "content/week1-activity-studydesigns.html",
    "title": "Distinguishing study designs",
    "section": "",
    "text": "Recall that the difference between an observational study and an experiment hinges on whether researchers intentionally intervene on the system of study (experiment) or passively record outcomes (observational study).\nIn this activity you’ll read abstracts from a few published studies and determine what kind of study is described in the abstract. You do not need to consider the examples in order — start with the ones that look most interesting.\nFor each example, identify the following:\n\nstudy type\nstudy population\nsample characteristics\nstudy outcome(s)\n\n\nExample 1: selenium exposure and Mediterranean diet\nThe following is from the abstract of a study investigating dietary mitigation of selenium exposure:\n\nSelenium is a trace element found in many chemical forms. Selenium and its species have nutritional and toxicologic properties, some of which may play a role in the etiology of neurological disease. We hypothesized that adherence to the Mediterranean-Dietary Approach to Stop Hypertension Intervention for Neurodegenerative Delay (MIND) diet could influence intake and endogenous concentrations of selenium and selenium species, thus contributing to the beneficial effects of this dietary pattern. We carried out a cross-sectional study of 137 non-smoking blood donors (75 females and 62 males) from the Reggio Emilia province, Northern Italy. We assessed MIND diet adherence using a semiquantitative food frequency questionnaire. We assessed selenium exposure through dietary intake and measurement of urinary and serum concentrations, including speciation of selenium compound in serum … Adherence to the MIND diet was positively associated with dietary selenium intake and urinary selenium excretion, whereas it was inversely associated with serum concentrations of overall selenium and organic selenium … Our results suggest that greater adherence to the MIND diet is non-linearly associated with lower circulating concentrations of selenium and of 2 potentially neurotoxic species of this element, selenoprotein P and selenate. This may explain why adherence to the MIND dietary pattern may reduce cognitive decline.\n\nUrbano, T., et al. (2023). Adherence to the Mediterranean-DASH Intervention for Neurodegenerative Delay (MIND) diet and exposure to selenium species: A cross-sectional study. Nutrition Research.\n\n\nExample 2: fermented kimchi and glucose metabolism\nThe following is from an abstract of a study investigating possible benefits of kimchi consumption among prediabetic individuals:\n\nWith the increased incidence of diabetes mellitus, the importance of early intervention in prediabetes has been emphasized … We hypothesized that kimchi and its fermented form would have beneficial effects on glucose metabolism in patients with prediabetes. A total of 21 participants with prediabetes were enrolled. During the first 8 weeks, they consumed either fresh (1-day-old) or fermented (10-day-old) kimchi. After a 4-week washout period, they switched to the other type of kimchi for the next 8 weeks. Consumption of both types of kimchi significantly decreased body weight, body mass index, and waist circumference. Fermented kimchi decreased insulin resistance, and increased insulin sensitivity … Systolic and diastolic blood pressure (BP) decreased significantly in the fermented kimchi group. The percentages of participants who showed improved glucose tolerance were 9.5 and 33.3% in the fresh and fermented kimchi groups, respectively.\n\nAn, S. Y., et al. (2013). Beneficial effects of fresh and fermented kimchi in prediabetic individuals. Annals of Nutrition and Metabolism, 63(1-2), 111-119."
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures-1",
    "href": "content/week2-descriptive.html#deviation-based-measures-1",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe variance is the average squared deviation from the mean (but divided by one less than the sample size): \\[\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}\\]\nThe standard deviation is the square root of the variance: \\[\\sqrt{\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-location",
    "href": "content/week2-descriptive.html#measures-of-location",
    "title": "Descriptive statistics",
    "section": "Measures of location",
    "text": "Measures of location\nOften location is specified by the “center” of a frequency distribution.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23"
  },
  {
    "objectID": "content/lab2-descriptive.html",
    "href": "content/lab2-descriptive.html",
    "title": "Lab 2: Descriptive statistics and simple graphics",
    "section": "",
    "text": "The objectives of this lab are to learn to:\n\nmake basic statistical graphics for visualizing frequency distributions\ncompute common measures of location and spread\ndiscern appropriate measures of location and spread based on presence of outliers and skewness\n\nWe’ll use the FAMuSS dataset, as in lecture.\n\nlibrary(tidyverse)\n\n# load famuss dataset \nload('data/famuss.RData')\n\n# inspect data frame\nhead(famuss)\n\n# A tibble: 6 × 9\n  ndrm.ch drm.ch sex      age race      height weight genotype   bmi\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;\n1      40     40 Female    27 Caucasian   65      199 CC        33.1\n2      25      0 Male      36 Caucasian   71.7    189 CT        25.8\n3      40      0 Female    24 Caucasian   65      134 CT        22.3\n4     125      0 Female    40 Caucasian   68      171 CT        26.0\n5      40     20 Female    32 Caucasian   61      118 CC        22.3\n6      75      0 Female    24 Hispanic    62.2    120 CT        21.8\n\n\nAs a quick refresher, you can extract a vector of the observations for any particular variable from the dataframe as follows: famuss$[variable name].\nWhile not strictly necessary, I recommend retrieving and storing the variable(s) you’ll use as separate objects, at least while you’re still a beginner. For example:\n\n# extract the age variable\nfamuss$age\n\n# store the age column as a vector\nage &lt;- famuss$age\n\n\nBasic statistical graphics\n\nCategorical variables\nFor categorical variables, as you saw in the last lab, table(...) will tabulate counts of the number of occurrences of each unique values of a categorical variable. The result can be passed to barplot(...) for a simple barplot to visualize the frequency distribution:\n\n# retrieve genotype\ngenotype &lt;- famuss$genotype\n\n# make a table, generate a barplot\ntable(genotype) |&gt; barplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nMake a barplot to visualize the frequency distribution of racial groups in the FAMuSS study.\n\n# retrieve race\n\n# make a table, generate a barplot\n\n\n\n\n\nNumerical variables\nIf a numeric variable is discrete without too many values, the frequency distribution could be visualized without any binning as a barplot. However, this is not recommended because it will result in a plot that is not scaled properly.\nInstead, it is better to make a histogram with one bin per unique possible value; this will scale the axis properly. However, it is also acceptable to make a histogram with binning that will result in aggregating some values. Both are shown below.\nThe approximate number of bins, and thus the amount of aggregation, is controlled by the argument breaks = ...:\n\n# retrieve age\nage &lt;- famuss$age\n\n# effectively, a bar plot of ages\nhist(age, breaks = 25)\n\n\n\n\n\n\n\n# fewer bins\nhist(age, breaks = 10)\n\n\n\n\n\n\n\n\nFor continuous variables, binning is a necessity. The second plot is better, because it shows the shape more clearly without obscuring too much detail.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a histogram of percent change in dominant arm strength. Experiment to see how the shape of the distribution appears at various binning resolutions; then pick a number of breaks that you feel reflects the data best.\n\n# retrieve dominant arm percent change\n\n# make a histogram; find a binning that captures the shape well\n\n\n\nTo store a graphic as a separate file for use in other documents, find the ‘export’ icon in the plot panel and select the ‘Save as image’ option; then follow prompts. Try this for the plot you just made.\n\n\n\nDescriptive statistics\nIn class we discussed several descriptive statistics for numeric variables. These statistics are so commonly used that they have their own functions in R.\n\nMeasures of location\nThe following functions return common measures of location for numeric variables:\n\nmean(...) returns an average\nmedian(...) returns a median\nquantile(...) returns a quantile\nmin(...) and max(...) return a minimum and a maximum, respectively\n\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age (middle value)\nmedian(age)\n\n[1] 22\n\n# 25th percentile of age (\"quantile\" is another term for percentile)\nquantile(age, probs = 0.25)\n\n25% \n 20 \n\n# 25th *and* 75th percentile of age\nquantile(age, probs = c(0.25, 0.75))\n\n25% 75% \n 20  27 \n\n# minimum age\nmin(age)\n\n[1] 17\n\n# maximum age\nmax(age)\n\n[1] 40\n\n\nNotice how the probs = ... argument to the quantile() function, which specifies which percentile R will calculate, can be used to calculate multiple percentiles at once.\nIf you want to inspect all of the location measures above (the five-number summary plus the mean) the summary(...) function will do just that.\n\n# all common location measures\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   17.0    20.0    22.0    24.4    27.0    40.0 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry computing the location measures above for percent change in dominant arm strength. Compare the mean and median. What does the comparison tell you about the skewness of this variable? Is this consistent with the histogram from the previous ‘your turn’?\n\n# compute the five-number summary for change in dominant arm strength\n\n\n\n\n\nMeasures of spread\nThe following functions return common measures of spread for numeric variables:\n\nrange(...) returns the range (min, max)\nIQR(...) returns the interquartile range (middle 50% of data)\nvar(...) returns the variance (average squared deviations from mean)\nsd(...) returns the standard deviation (variance, on original scale)\n\n\n# age range\nrange(age)\n\n[1] 17 40\n\n# interquartile range of ages (width of interval containing middle 50% of data)\nIQR(age)\n\n[1] 7\n\n# variance of age\nvar(age)\n\n[1] 33.79966\n\n# standard deviation of age\nsd(age)\n\n[1] 5.813748\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCompute the standard deviation of percent change in dominant arm strength.\n\n# standard deviation of percent change in dominant arm strength\n\nInterpret the value in context.\n\n\n\n\nRobustness\nWhen would you use median instead of mean? IQR instead of standard deviation? The answer has to do with robustness, which in statistics means sensitivity to outliers or extreme values.\nTo explore this idea, recall first the actual mean and median ages for the participants in the FAMuSS study as well as the age range:\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age\nmedian(age)\n\n[1] 22\n\n# range\nrange(age)\n\n[1] 17 40\n\n\nNow let’s add an artificial outlier – a few hypothetical participant who are in their 80’s and 90’s – and compute the measures of location again:\n\n# average age\nmean(c(age, 96, 92, 87, 91))\n\n[1] 24.84975\n\n# median age\nmedian(c(age, 96, 92, 87, 91))\n\n[1] 22\n\n\nThe mean increases while the median does not. More broadly, statistics based on percentiles are in general insensitive to outliers, unless there’s a large group of outlying observations. In this sense they are robust statistics.\nA similar difference can be observed between deviation-based measures and interquartile range. The original measures were:\n\n# variance of ages\nvar(age)\n\n[1] 33.79966\n\n# interquartile range of ages\nIQR(age)\n\n[1] 7\n\n\nNow adding in our artificial outliers:\n\n# age variance\nvar(c(age, 96, 92, 87, 91))\n\n[1] 63.55598\n\n# age iqr\nIQR(c(age, 96, 92, 87, 91))\n\n[1] 7\n\n\n\n\n\nGrouped summaries\nWhat if you wish to find the mean percent change in dominant arm strength separately for each genotype?\nThe tidyverse package loaded at the outset has a pair of functions, group_by and summarize, that allow you to do this efficiently. The steps are:\n\nStart with the data frame famuss\ngroup by the genotype variable\nsummarize\n\n\n# average dominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch))\n\n# A tibble: 3 × 2\n  genotype avg.drm.ch\n  &lt;fct&gt;         &lt;dbl&gt;\n1 CC            10.7 \n2 CT             8.49\n3 TT            13.0 \n\n\nThe summarize function can actually compute multiple summaries: each argument should specify a name for the summary and the calculation to perform in the format &lt;NAME&gt; = &lt;FUNCTION&gt;(&lt;COLUMN NAME&gt;):\n\n# average dominant and nondominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch),\n            avg.ndrm.ch = mean(ndrm.ch))\n\n# A tibble: 3 × 3\n  genotype avg.drm.ch avg.ndrm.ch\n  &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 CC            10.7         48.9\n2 CT             8.49        53.2\n3 TT            13.0         58.1\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate the median percent change in dominant arm strength separately for each genotype by modifying the first example above.\n\n# median percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\nThese problems may look lengthy at face value, but the calculations are rather brief. A suggestion is: determine which calculations are required for each part and focus on doing those calculations first; then look back over your results to interpret them and answer the prompts.\n\n[L3] Use the census data again from the previous problem set and carry out the following descriptive summaries.\n\nMake a histogram of total personal income. Choose the binning so as to capture the shape well but not obscure too much detail. Are there outliers?\nCompute the mean and five-number summary of total personal income. Which measure of location is most appropriate and why?\nCompute the interquartile range and standard deviation of total personal income and interpret them in context. Which measure is more appropriate and why?\nCompute the median total personal income separately for men and women.\n\n\n\n[L3] Data from Chen, W., et al., Maternal investment increases with altitude in a frog on the Tibetan Plateau. Journal of Evolutionary Biology 26-12 (2013) includes measurements pertaining to egg clutches of several populations of frog at breeding ponds (sites) in the eastern Tibetan Plateau.\n\nHow many samples were collected at each site?\nCompute the frequency distribution of site altitudes among samples collected in the study.\nMake a barplot of the frequency distribution from (a). Are samples collected more or less uniformily across altitudes? If not, which altitudes are most represented in the sample?\nMake a histogram of clutch volumes. Describe the shape and number of modes.\nCalculate the mean and five-number summary of clutch volume.\nCalculate and interpret the standard deviation, variance, and interquartile range.\nCalculate the average clutch volume separately for each altitude. Does average clutch volume seem to differ by altitude?\n[optional] Devise a way to calculate the average absolute deviation."
  },
  {
    "objectID": "content/week2-descriptive.html",
    "href": "content/week2-descriptive.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "[lecture] frequency distributions; measures of spread and center\n[lab] descriptive statistics and simple graphics in R"
  },
  {
    "objectID": "content/week3-multivariate.html#todays-agenda",
    "href": "content/week3-multivariate.html#todays-agenda",
    "title": "Bivariate summaries",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\nLoose end: robustness\nBivariate numeric and graphical summaries\nLab: bivariate graphics in R"
  },
  {
    "objectID": "content/week3-multivariate.html#robustness",
    "href": "content/week3-multivariate.html#robustness",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\n\nPercentile-based measures of location and spread are less sensitive to outliers\n\nConsider adding an observation of 94 to our 12 ages from last time. This is an outlier.\n\n\n\n# append an outlier\nages_add &lt;- c(ages, 94)\n\n# means\nc(original = mean(ages), with.outlier = mean(ages_add))\n\n    original with.outlier \n    24.00000     29.38462 \n\n# medians\nc(original = median(ages), with.outlier = median(ages_add))\n\n    original with.outlier \n        23.5         25.0 \n\n# IQR\nc(original = IQR(ages), with.outlier = IQR(ages_add))\n\n    original with.outlier \n         8.5          9.0 \n\n# SD\nc(original = sd(ages), with.outlier = sd(ages_add))\n\n    original with.outlier \n    5.526794    20.122701 \n\n\n\nThe effect of this outlier on each statistic is:\n\nmean increases by 22.44%\nmedian increases by 6.38%\nIQR increases by 5.88%\nSD increases by 264.09%\n\nRobustness refers to sensitivity to outliers. Mean and SD are less robust than median and IQR."
  },
  {
    "objectID": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "href": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "title": "Bivariate summaries",
    "section": "Limitations of univariate summaries",
    "text": "Limitations of univariate summaries\n\nUnivariate summaries aim to capture the distribution of values of a single variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nboth unimodal, no obvious outliers\nheights symmetric\nweights right-skewed\nbut these observations actually come in pairs\n\n\n\nUnivariate summaries don’t reflect how the variables might be related."
  },
  {
    "objectID": "content/week3-multivariate.html#bivariate-summaries",
    "href": "content/week3-multivariate.html#bivariate-summaries",
    "title": "Bivariate summaries",
    "section": "Bivariate summaries",
    "text": "Bivariate summaries\n\nBivariate summaries aim to capture a relationship between two variables.\n\n\n\nA simple example is a scatterplot:\n\n\n\n\n\n\n\n\n\n\nEach point represents a pair of values \\((h, w)\\) for one study participant.\n\nReveals a relationship: taller participants tend to be heavier\nBut no longer shows individual distributions clearly\n\nNotice, though, that the marginal means (dashed red lines) still capture the center well."
  },
  {
    "objectID": "content/week3-multivariate.html#summary-types",
    "href": "content/week3-multivariate.html#summary-types",
    "title": "Bivariate summaries",
    "section": "Summary types",
    "text": "Summary types\n\nBivariate summary techniques differ depending on the data types of the variables.\n\n\n\n\n\n\n\n\nQuestion\nComparison type\n\n\n\n\nDid genotype frequencies differ by race or sex among study participants?\ncategorical/categorical\n\n\nWere differential changes in arm strength observed according to genotype?\nnumeric/categorical\n\n\nDid change in arm strength appear related in any way to body size among study participants?\nnumeric/numeric\n\n\nDid study participants experience similar or different changes in arm strength depending on arm dominance?\n??"
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical",
    "href": "content/week3-multivariate.html#categoricalcategorical",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\n\nA contingency table is a bivariate tabular summary of two categorical variables; it shows the frequency of each pair of values. Usually the marginal totals are also shown.\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n106\n149\n98\n353\n\n\nMale\n67\n112\n63\n242\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\nThere are multiple ways to convert to proportions by using different denominators, and these yield proportions with distinct interpretations:\n\ngrand total – frequency of genotype/sex combination\nrow total – genotype frequency by sex\ncolumn total – sex frequency by genotype"
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-1",
    "href": "content/week3-multivariate.html#categoricalcategorical-1",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid genotype frequencies differ by sex among study participants?\nFor this question, the row totals should be used to convert to proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n0.3003\n0.4221\n0.2776\n1\n\n\nMale\n0.2769\n0.4628\n0.2603\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are quite close, suggesting minimal sex differences."
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-2",
    "href": "content/week3-multivariate.html#categoricalcategorical-2",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid sex frequencies differ by genotype among study participants?\nFor this question, the column totals should be used to compute proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.6127\n0.5709\n0.6087\n\n\nMale\n0.3873\n0.4291\n0.3913\n\n\ntotal\n1\n1\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are close, suggesting minimal genotype differences."
  },
  {
    "objectID": "content/week3-multivariate.html#numericnumeric",
    "href": "content/week3-multivariate.html#numericnumeric",
    "title": "Bivariate summaries",
    "section": "Numeric/numeric",
    "text": "Numeric/numeric\nDid change in arm strength appear related in any way to body size among study participants?\n\nPairwise scatterplots indicate no apparent relationships."
  },
  {
    "objectID": "content/week3-multivariate.html#correlation",
    "href": "content/week3-multivariate.html#correlation",
    "title": "Bivariate summaries",
    "section": "Correlation",
    "text": "Correlation\nIn addition to graphical techniques, for numeric/numeric comparisons, there are also quantiative measures of relationship.\n\n\n\n\nCorrelation measures the strength of linear relationship, and is defined as: \\[r_{xy} = \\frac{1}{n - 1}\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\\]\n\n\\(r \\rightarrow 1\\): positive relationship\n\\(r \\rightarrow -1\\): negative relationship\n\\(r \\rightarrow 0\\): no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "href": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "title": "Bivariate summaries",
    "section": "Uncorrelated \\(\\neq\\) no relationship",
    "text": "Uncorrelated \\(\\neq\\) no relationship\nCorrelation only captures linear relationships. Always do a graphical check.\n\nCommon misconceptions:\n\nstronger correlation \\(\\longleftrightarrow\\) greater slope\nweaker correlation \\(\\longleftrightarrow\\) no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-correlations",
    "href": "content/week3-multivariate.html#interpreting-correlations",
    "title": "Bivariate summaries",
    "section": "Interpreting correlations",
    "text": "Interpreting correlations\nDid change in arm strength appear related in any way to body size among study participants?\nHere are the correlations corresponding to the plots we checked earlier.\n\n\n\n\n\n\n\n\n\n\n\n \nheight\nweight\nbmi\n\n\n\n\ndrm.ch\n-0.1104\n-0.1159\n-0.07267\n\n\nndrm.ch\n-0.265\n-0.2529\n-0.1436\n\n\n\n\n\nSo there aren’t any linear relationships here. A rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\\(|r| = 1\\): either a mistake or not real data"
  },
  {
    "objectID": "content/week3-multivariate.html#numericcategorical",
    "href": "content/week3-multivariate.html#numericcategorical",
    "title": "Bivariate summaries",
    "section": "Numeric/categorical",
    "text": "Numeric/categorical\nSide-by-side boxplots are usually a good option. Avoid stacked histograms.\nWere differential changes in arm strength observed according to genotype?\n\n\n\n\n\n\n\n\n\n\n\n\nLook for differences:\n\nlocation shift\nspread\ncenter\n\nWhat do you think? Any notable relationships?"
  },
  {
    "objectID": "content/week3-multivariate.html#robustness-1",
    "href": "content/week3-multivariate.html#robustness-1",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\nThe effect of the outlier on each measure is captured by the ratio \\(\\frac{\\text{measure with outlier}}{\\text{measure without outlier}}\\), which shows:\n\nthe IQR increases by 5.88%\nthe standard deviation increases by 264%"
  },
  {
    "objectID": "content/week3-multivariate.html#choosing-appropriate-measures-of-spread",
    "href": "content/week3-multivariate.html#choosing-appropriate-measures-of-spread",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures of spread",
    "text": "Choosing appropriate measures of spread"
  },
  {
    "objectID": "content/week3-multivariate.html#choosing-appropriate-measures",
    "href": "content/week3-multivariate.html#choosing-appropriate-measures",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures",
    "text": "Choosing appropriate measures\n\nWhen outliers are present, use percentile-based measures; otherwise, use mean and standard deviation or variance\n\n\nCheck your understanding: which measures are most appropriate for each variable above?"
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-scatterplots",
    "href": "content/week3-multivariate.html#interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots\n\nScatterplots show the presence or absence of an association.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf there is an association (i.e., discernible pattern), it can be:\n\nlinear or nonlinear\n\nlinear if scatter roughly follows a straight line, nonlinear otherwise\n\npositive or negative\n\npositive if scatter is increasing from left to right, negative otherwise\n\n\nThe plot at left is an example of a positive and (slightly) nonlinear relationship."
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-scatterplots-1",
    "href": "content/week3-multivariate.html#interpreting-scatterplots-1",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots"
  },
  {
    "objectID": "content/week3-multivariate.html#practice-interpreting-scatterplots",
    "href": "content/week3-multivariate.html#practice-interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Practice interpreting scatterplots",
    "text": "Practice interpreting scatterplots"
  },
  {
    "objectID": "content/week3-multivariate.html#data-transformations",
    "href": "content/week3-multivariate.html#data-transformations",
    "title": "Bivariate summaries",
    "section": "Data transformations",
    "text": "Data transformations\nSometimes a simple transformation can reveal a linear relationship on an alternate scale.\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, weight)\n\n[1] 0.5308787\n\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356"
  },
  {
    "objectID": "content/lab3-multivariate.html",
    "href": "content/lab3-multivariate.html",
    "title": "Lab 3: Bivariate summaries",
    "section": "",
    "text": "This lab covers descriptive summaries and graphics for two variables. The goal of the activity is to learn to produce joint summaries of two variables for identifying relationships, in particular:\n\ncontingency tables\nproportional stacked bar plots\nscatterplots\nside-by-side boxplots\n\nWe will use the FAMuSS dataset again.\n\nlibrary(tidyverse)\nload('data/famuss.RData')\n\n\nContingency tables\n\nCounts\nA contingency table is a summary of two categorical variables. Specifically, it is a two-way table in which:\n\nrows correspond to the values of one variable\ncolumns correspond to the values of the other variable\nentries are counts of the numbers of observations for each combination of values\n\nIn fact, a contingency table is a frequency distribution for two variables. It can be constructed in R by retrieving the two variables of interest and providing them as arguments to table().\n\n# retrieve variables of interest\ngenotype &lt;- famuss$genotype\nsex &lt;- famuss$sex\n\n# make a contingency table\ntable(sex, genotype)\n\n        genotype\nsex       CC  CT  TT\n  Female 106 149  98\n  Male    67 112  63\n\n\nThe order of the arguments determines the row/column orientation of the table.\nThe values show the observation counts for each combination, so, for example, the number 106 in the upper-leftmost cell indicates that 106 participants were women with genotype CC.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a contingency table of race and genotype, with race as the row variable and genotype as the column variable.\n\n# retrieve variables of interest\n\n# make a contingency table\n\n\n\n\n\nProportions\nTo convert a contingency table to proportions, one can use either the grand total, row totals, or column totals. The resulting proportions have different meanings:\n\ngrand total – combination frequencies\nrow totals – column variable frequencies by row variable value\ncolumn totals – row variable frequencies by column variable value\n\nIn the sex/genotype contingency table, these proportions would be computed and interpreted as follows:\n\n# combination frequencies (using grand total as denominator)\ntable(sex, genotype) |&gt; proportions(margin = NULL)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.1781513 0.2504202 0.1647059\n  Male   0.1126050 0.1882353 0.1058824\n\n# genotype frequencies by sex (using row margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 1)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.3002833 0.4220963 0.2776204\n  Male   0.2768595 0.4628099 0.2603306\n\n# sex frequencies by genotype (using column margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 2)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.6127168 0.5708812 0.6086957\n  Male   0.3872832 0.4291188 0.3913043\n\n\nNotice that the margin = ... argument controls which totals are used as denominators in computing proportions. The syntax is:\n\nmargin = NULL specifies grand total\nmargin = 1 uses row totals\nmargin = 2 uses column totals\n\nThe interpretation of entries depends on which type of proportion is comupted. For instance, looking at only the upper-leftmost entry in each of the three tables above:\n\n[first table] 17.82% of participants were women with genotype CC\n[second table] 30.03% of female participants had genotype CC\n[third table] 61.27% of participants with genotype CC were women\n\nIf you lose track of which margin was which, you can always check yourself by looking at which values sum to one: if all the values add up to one, grand totals were used; if rows sum to one, row totals were used; and if columns add up to one, column totals were used.\n\n\n\n\n\n\nYour turn\n\n\n\nConvert your contingency table of genotype and race from the previous ‘your turn’ to a table showing genotype frequencies by race.\n\n# genotype frequencies by race\n\nThen, interpret the entry for Hispanic/CT in context.\n\n\n\n\nGraphics\nThe latter two proportion tables – those constructed using row or column totals – can be visualized graphically using barplot(...):\n\n# stacked bar plot showing genotype frequencies by sex\ntable(genotype, sex) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nThis function is a little finnicky about the table orientation, so the most straightforward way to approach making the plot is to adhere to this rule of thumb:\n\nalways use margin = 2\nspecify the order of the variables as they are input to table(...) so that the grouping variable appears second\n\nSo, for instance, to visualize the sex frequencies by genotype, we would do the following:\n\n# stacked bar plot showing sex frequencies by genotype\ntable(sex, genotype) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nIf you see uneven bar heights, that means you have not configured the table correctly before passing it to barplot(...).\n\n\n\n\n\n\nYour turn\n\n\n\nMake a plot showing the genotype frequencies by race.\n\n# stacked bar plot showing genotype frequencies by race\n\nCheck your understanding:\n\nFor which group is the CC frequency highest?\nFor which group is the CC frequency lowest?\n\n\n\n\n\n\nScatterplots and correlation\nTaller people tend to be heavier. We should expect a relationship between weight and height. The example below shows a scatterplot of the two variables, and computes the correlation (measure of linear relationship).\n\n# retrieve height and weight columns\nheight &lt;- famuss$height\nweight &lt;- famuss$weight\n\n# basic scatterplot\nplot(height, weight)\n\n\n\n\n\n\n\n# correlation\ncor(weight, height)\n\n[1] 0.5308787\n\n\nA rough rule of thumb for interpreting correlations is as follows:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case, the correlation of 0.53 indicates a moderate positive relationship.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there a relationship between nondominant and dominant percent change in arm strength? Make a scatterplot and compute the correlation.\n\n# retrieve the percent change variables\n\n# construct a scatterplot\n\n# compute the correlation\n\n\n\n\n\nSide-by-side boxplots\nConsider one of the main questions for the study: were differences on the ACTN gene region associated with differential change in arm strength after resistance training?\nThis is a comparison between a categorical variable (genotype) and numeric variable (percent change in arm strength). For this type of comparison, a set of side-by-side boxplots is best:\n\n# side-by-side boxplots for non-dominant arm\nboxplot(ndrm.ch ~ genotype, data = famuss)\n\n\n\n\n\n\n\n# change the orientation \nboxplot(ndrm.ch ~ genotype, data = famuss, horizontal = T)\n\n\n\n\n\n\n\n\nThis graphics summarizes the frequency distribution of percent change in non-dominant arm strength separately for each genotype. It is essentially a grouped summary.\n\n\n\n\n\n\nYour turn\n\n\n\nConstruct side-by-side boxplots of percent change in dominant arm strength by genotype.\n\n# make side-by-side boxplots of percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\n\nTrait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease (CHD); 12,986 participants were recruited for a study examining this hypothesis and followed for five years. The anger dataset includes data for the 8557 participants identified as having normal blood pressure (normotensives) and records, for each participant, whether they are classified as having high, moderate, or low trait anger, and whether they experienced a CHD event during the study period.\n\nMake a contingency table showing trait anger and CHD event occurrence.\nFind the proportion of participants who experienced a CHD event in each trait anger group.\nConstruct a stacked bar plot to visualize the frequencies you found in (b).\nFind the ratio \\(\\frac{\\text{CHD frequency in the high anger group}}{\\text{CHD frequency in the low anger group}}\\). This is called the “relative risk” of a CHD event. (You can calculate this quantity by hand based on your result in (b).)\n\n\n\nUse the nhanes data from earlier to answer the following questions.\n\nMake a scatterplot of systolic blood pressure and diastolic blood pressure. Put systolic blood pressure on the y axis. Characterize the association, if any, as positive/negative/non-associated and linear/nonlinear.\nCompute and interpret the correlation between systolic blood pressure and diastolic blood pressure.\nDoes there appear to be a substantial difference in the distribution of total cholesterol by sex? Construct side-by-side barplots to answer this question."
  },
  {
    "objectID": "content/week3-multivariate.html#interpretation",
    "href": "content/week3-multivariate.html#interpretation",
    "title": "Bivariate summaries",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356\n\n\n\nApplying these rules of thumb:\n\n\\(|r| &lt; 0.3\\): minimal association\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong\n\nAnd:\n\n\\(r &gt; 0\\): positive association\n\\(r &lt; 0\\): negative association\n\nThe interpretation is:\nThere is a moderately strong positive linear relationship between height and log weight.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week3-bivariate.html#todays-agenda",
    "href": "content/week3-bivariate.html#todays-agenda",
    "title": "Bivariate summaries",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\nLoose end: robustness\nBivariate numeric and graphical summaries\nLab: bivariate graphics in R"
  },
  {
    "objectID": "content/week3-bivariate.html#robustness",
    "href": "content/week3-bivariate.html#robustness",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\n\nPercentile-based measures of location and spread are less sensitive to outliers\n\nConsider adding an observation of 94 to our 12 ages from last time. This is an outlier.\n\n\n\n# append an outlier\nages_add &lt;- c(ages, 94)\n\n# means\nc(original = mean(ages), with.outlier = mean(ages_add))\n\n    original with.outlier \n    24.00000     29.38462 \n\n# medians\nc(original = median(ages), with.outlier = median(ages_add))\n\n    original with.outlier \n        23.5         25.0 \n\n# IQR\nc(original = IQR(ages), with.outlier = IQR(ages_add))\n\n    original with.outlier \n         8.5          9.0 \n\n# SD\nc(original = sd(ages), with.outlier = sd(ages_add))\n\n    original with.outlier \n    5.526794    20.122701 \n\n\n\nThe effect of this outlier on each statistic is:\n\nmean increases by 22.44%\nmedian increases by 6.38%\nIQR increases by 5.88%\nSD increases by 264.09%\n\nRobustness refers to sensitivity to outliers. Mean and SD are less robust than median and IQR."
  },
  {
    "objectID": "content/week3-bivariate.html#choosing-appropriate-measures",
    "href": "content/week3-bivariate.html#choosing-appropriate-measures",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures",
    "text": "Choosing appropriate measures\n\nWhen outliers are present, use percentile-based measures; otherwise, use mean and standard deviation or variance\n\n\nCheck your understanding: which measures are most appropriate for each variable above?"
  },
  {
    "objectID": "content/week3-bivariate.html#limitations-of-univariate-summaries",
    "href": "content/week3-bivariate.html#limitations-of-univariate-summaries",
    "title": "Bivariate summaries",
    "section": "Limitations of univariate summaries",
    "text": "Limitations of univariate summaries\n\nUnivariate summaries aim to capture the distribution of values of a single variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nboth unimodal, no obvious outliers\nheights symmetric\nweights right-skewed\nbut these observations actually come in pairs\n\n\n\nUnivariate summaries don’t reflect how the variables might be related."
  },
  {
    "objectID": "content/week3-bivariate.html#bivariate-summaries",
    "href": "content/week3-bivariate.html#bivariate-summaries",
    "title": "Bivariate summaries",
    "section": "Bivariate summaries",
    "text": "Bivariate summaries\n\nBivariate summaries aim to capture a relationship between two variables.\n\n\n\nA simple example is a scatterplot:\n\n\n\n\n\n\n\n\n\n\nEach point represents a pair of values \\((h, w)\\) for one study participant.\n\nReveals a relationship: taller participants tend to be heavier\nBut no longer shows individual distributions clearly\n\nNotice, though, that the marginal means (dashed red lines) still capture the center well."
  },
  {
    "objectID": "content/week3-bivariate.html#summary-types",
    "href": "content/week3-bivariate.html#summary-types",
    "title": "Bivariate summaries",
    "section": "Summary types",
    "text": "Summary types\n\nBivariate summary techniques differ depending on the data types of the variables.\n\n\n\n\n\n\n\n\nQuestion\nComparison type\n\n\n\n\nDid genotype frequencies differ by race or sex among study participants?\ncategorical/categorical\n\n\nWere differential changes in arm strength observed according to genotype?\nnumeric/categorical\n\n\nDid change in arm strength appear related in any way to body size among study participants?\nnumeric/numeric\n\n\nDid study participants experience similar or different changes in arm strength depending on arm dominance?\n??"
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical",
    "href": "content/week3-bivariate.html#categoricalcategorical",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\n\nA contingency table is a bivariate tabular summary of two categorical variables; it shows the frequency of each pair of values. Usually the marginal totals are also shown.\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n106\n149\n98\n353\n\n\nMale\n67\n112\n63\n242\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\nThere are multiple ways to convert to proportions by using different denominators, and these yield proportions with distinct interpretations:\n\ngrand total – frequency of genotype/sex combination\nrow total – genotype frequency by sex\ncolumn total – sex frequency by genotype"
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical-1",
    "href": "content/week3-bivariate.html#categoricalcategorical-1",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid genotype frequencies differ by sex among study participants?\nFor this question, the row totals should be used to convert to proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n0.3003\n0.4221\n0.2776\n1\n\n\nMale\n0.2769\n0.4628\n0.2603\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are quite close, suggesting minimal sex differences."
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical-2",
    "href": "content/week3-bivariate.html#categoricalcategorical-2",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid sex frequencies differ by genotype among study participants?\nFor this question, the column totals should be used to compute proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.6127\n0.5709\n0.6087\n\n\nMale\n0.3873\n0.4291\n0.3913\n\n\ntotal\n1\n1\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are close, suggesting minimal genotype differences."
  },
  {
    "objectID": "content/week3-bivariate.html#numericcategorical",
    "href": "content/week3-bivariate.html#numericcategorical",
    "title": "Bivariate summaries",
    "section": "Numeric/categorical",
    "text": "Numeric/categorical\nSide-by-side boxplots are usually a good option. Avoid stacked histograms.\nWere differential changes in arm strength observed according to genotype?\n\n\n\n\n\n\n\n\n\n\n\n\nLook for differences:\n\nlocation shift\nspread\ncenter\n\nWhat do you think? Any notable relationships?"
  },
  {
    "objectID": "content/week3-bivariate.html#numericnumeric",
    "href": "content/week3-bivariate.html#numericnumeric",
    "title": "Bivariate summaries",
    "section": "Numeric/numeric",
    "text": "Numeric/numeric\nDid change in arm strength appear related in any way to body size among study participants?\n\nPairwise scatterplots indicate no apparent relationships."
  },
  {
    "objectID": "content/week3-bivariate.html#interpreting-scatterplots",
    "href": "content/week3-bivariate.html#interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots\n\nScatterplots show the presence or absence of an association.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf there is an association (i.e., discernible pattern), it can be:\n\nlinear or nonlinear\n\nlinear if scatter roughly follows a straight line, nonlinear otherwise\n\npositive or negative\n\npositive if scatter is increasing from left to right, negative otherwise\n\n\nThe plot at left is an example of a positive and (slightly) nonlinear relationship."
  },
  {
    "objectID": "content/week3-bivariate.html#practice-interpreting-scatterplots",
    "href": "content/week3-bivariate.html#practice-interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Practice interpreting scatterplots",
    "text": "Practice interpreting scatterplots"
  },
  {
    "objectID": "content/week3-bivariate.html#correlation",
    "href": "content/week3-bivariate.html#correlation",
    "title": "Bivariate summaries",
    "section": "Correlation",
    "text": "Correlation\nIn addition to graphical techniques, for numeric/numeric comparisons, there are also quantiative measures of relationship.\n\n\n\n\nCorrelation measures the strength of linear relationship, and is defined as: \\[r_{xy} = \\frac{1}{n - 1}\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\\]\n\n\\(r \\rightarrow 1\\): positive relationship\n\\(r \\rightarrow -1\\): negative relationship\n\\(r \\rightarrow 0\\): no relationship"
  },
  {
    "objectID": "content/week3-bivariate.html#interpreting-correlations",
    "href": "content/week3-bivariate.html#interpreting-correlations",
    "title": "Bivariate summaries",
    "section": "Interpreting correlations",
    "text": "Interpreting correlations\nDid change in arm strength appear related in any way to body size among study participants?\nHere are the correlations corresponding to the plots we checked earlier.\n\n\n\n\n\n\n\n\n\n\n\n \nheight\nweight\nbmi\n\n\n\n\ndrm.ch\n-0.1104\n-0.1159\n-0.07267\n\n\nndrm.ch\n-0.265\n-0.2529\n-0.1436\n\n\n\n\n\nSo there aren’t any linear relationships here. A rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\\(|r| = 1\\): either a mistake or not real data"
  },
  {
    "objectID": "content/week3-bivariate.html#data-transformations",
    "href": "content/week3-bivariate.html#data-transformations",
    "title": "Bivariate summaries",
    "section": "Data transformations",
    "text": "Data transformations\nSometimes a simple transformation can reveal a linear relationship on an alternate scale.\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, weight)\n\n[1] 0.5308787\n\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356"
  },
  {
    "objectID": "content/week3-bivariate.html#interpretation",
    "href": "content/week3-bivariate.html#interpretation",
    "title": "Bivariate summaries",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356\n\n\n\nApplying these rules of thumb:\n\n\\(|r| &lt; 0.3\\): minimal association\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong\n\nAnd:\n\n\\(r &gt; 0\\): positive association\n\\(r &lt; 0\\): negative association\n\nThe interpretation is:\nThere is a moderately strong positive linear relationship between height and log weight.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab3-bivariate.html",
    "href": "content/lab3-bivariate.html",
    "title": "Lab 3: Bivariate summaries",
    "section": "",
    "text": "This lab covers descriptive summaries and graphics for two variables. The goal of the activity is to learn to produce joint summaries of two variables for identifying relationships, in particular:\n\ncontingency tables\nproportional stacked bar plots\nscatterplots\nside-by-side boxplots\n\nWe will use the FAMuSS dataset again.\n\nlibrary(tidyverse)\nload('data/famuss.RData')\n\n\nContingency tables\n\nCounts\nA contingency table is a summary of two categorical variables. Specifically, it is a two-way table in which:\n\nrows correspond to the values of one variable\ncolumns correspond to the values of the other variable\nentries are counts of the numbers of observations for each combination of values\n\nIn fact, a contingency table is a frequency distribution for two variables. It can be constructed in R by retrieving the two variables of interest and providing them as arguments to table().\n\n# retrieve variables of interest\ngenotype &lt;- famuss$genotype\nsex &lt;- famuss$sex\n\n# make a contingency table\ntable(sex, genotype)\n\n        genotype\nsex       CC  CT  TT\n  Female 106 149  98\n  Male    67 112  63\n\n\nThe order of the arguments determines the row/column orientation of the table.\nThe values show the observation counts for each combination, so, for example, the number 106 in the upper-leftmost cell indicates that 106 participants were women with genotype CC.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a contingency table of race and genotype, with race as the row variable and genotype as the column variable.\n\n# retrieve variables of interest\n\n# make a contingency table\n\n\n\n\n\nProportions\nTo convert a contingency table to proportions, one can use either the grand total, row totals, or column totals. The resulting proportions have different meanings:\n\ngrand total – combination frequencies\nrow totals – column variable frequencies by row variable value\ncolumn totals – row variable frequencies by column variable value\n\nIn the sex/genotype contingency table, these proportions would be computed and interpreted as follows:\n\n# combination frequencies (using grand total as denominator)\ntable(sex, genotype) |&gt; proportions(margin = NULL)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.1781513 0.2504202 0.1647059\n  Male   0.1126050 0.1882353 0.1058824\n\n# genotype frequencies by sex (using row margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 1)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.3002833 0.4220963 0.2776204\n  Male   0.2768595 0.4628099 0.2603306\n\n# sex frequencies by genotype (using column margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 2)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.6127168 0.5708812 0.6086957\n  Male   0.3872832 0.4291188 0.3913043\n\n\nNotice that the margin = ... argument controls which totals are used as denominators in computing proportions. The syntax is:\n\nmargin = NULL specifies grand total\nmargin = 1 uses row totals\nmargin = 2 uses column totals\n\nThe interpretation of entries depends on which type of proportion is comupted. For instance, looking at only the upper-leftmost entry in each of the three tables above:\n\n[first table] 17.82% of participants were women with genotype CC\n[second table] 30.03% of female participants had genotype CC\n[third table] 61.27% of participants with genotype CC were women\n\nIf you lose track of which margin was which, you can always check yourself by looking at which values sum to one: if all the values add up to one, grand totals were used; if rows sum to one, row totals were used; and if columns add up to one, column totals were used.\n\n\n\n\n\n\nYour turn\n\n\n\nConvert your contingency table of genotype and race from the previous ‘your turn’ to a table showing genotype frequencies by race.\n\n# genotype frequencies by race\n\nThen, interpret the entry for Hispanic/CT in context.\n\n\n\n\nGraphics\nThe latter two proportion tables – those constructed using row or column totals – can be visualized graphically using barplot(...):\n\n# stacked bar plot showing genotype frequencies by sex\ntable(genotype, sex) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nThis function is a little finnicky about the table orientation, so the most straightforward way to approach making the plot is to adhere to this rule of thumb:\n\nalways use margin = 2\nspecify the order of the variables as they are input to table(...) so that the grouping variable appears second\n\nSo, for instance, to visualize the sex frequencies by genotype, we would do the following:\n\n# stacked bar plot showing sex frequencies by genotype\ntable(sex, genotype) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nIf you see uneven bar heights, that means you have not configured the table correctly before passing it to barplot(...).\n\n\n\n\n\n\nYour turn\n\n\n\nMake a plot showing the genotype frequencies by race.\n\n# stacked bar plot showing genotype frequencies by race\n\nCheck your understanding:\n\nFor which group is the CC frequency highest?\nFor which group is the CC frequency lowest?\n\n\n\n\n\n\nScatterplots and correlation\nTaller people tend to be heavier. We should expect a relationship between weight and height. The example below shows a scatterplot of the two variables, and computes the correlation (measure of linear relationship).\n\n# retrieve height and weight columns\nheight &lt;- famuss$height\nweight &lt;- famuss$weight\n\n# basic scatterplot\nplot(height, weight)\n\n\n\n\n\n\n\n# correlation\ncor(weight, height)\n\n[1] 0.5308787\n\n\nA rough rule of thumb for interpreting correlations is as follows:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case, the correlation of 0.53 indicates a moderate positive relationship.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there a relationship between nondominant and dominant percent change in arm strength? Make a scatterplot and compute the correlation.\n\n# retrieve the percent change variables\n\n# construct a scatterplot\n\n# compute the correlation\n\n\n\n\n\nSide-by-side boxplots\nConsider one of the main questions for the study: were differences on the ACTN gene region associated with differential change in arm strength after resistance training?\nThis is a comparison between a categorical variable (genotype) and numeric variable (percent change in arm strength). For this type of comparison, a set of side-by-side boxplots is best:\n\n# side-by-side boxplots for non-dominant arm\nboxplot(ndrm.ch ~ genotype, data = famuss)\n\n\n\n\n\n\n\n# change the orientation \nboxplot(ndrm.ch ~ genotype, data = famuss, horizontal = T)\n\n\n\n\n\n\n\n\nThis graphics summarizes the frequency distribution of percent change in non-dominant arm strength separately for each genotype. It is essentially a grouped summary.\n\n\n\n\n\n\nYour turn\n\n\n\nConstruct side-by-side boxplots of percent change in dominant arm strength by genotype.\n\n# make side-by-side boxplots of percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\n\nTrait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease (CHD); 12,986 participants were recruited for a study examining this hypothesis and followed for five years. The anger dataset includes data for the 8557 participants identified as having normal blood pressure (normotensives) and records, for each participant, whether they are classified as having high, moderate, or low trait anger, and whether they experienced a CHD event during the study period.\n\nMake a contingency table showing trait anger and CHD event occurrence.\nFind the proportion of participants who experienced a CHD event in each trait anger group.\nConstruct a stacked bar plot to visualize the frequencies you found in (b).\nFind the ratio \\(\\frac{\\text{CHD frequency in the high anger group}}{\\text{CHD frequency in the low anger group}}\\). This is called the “relative risk” of a CHD event. (You can calculate this quantity by hand based on your result in (b).)\n\n\n\nUse the nhanes data from earlier to answer the following questions.\n\nMake a scatterplot of systolic blood pressure and diastolic blood pressure. Put systolic blood pressure on the y axis. Characterize the association, if any, as positive/negative/non-associated and linear/nonlinear.\nCompute and interpret the correlation between systolic blood pressure and diastolic blood pressure.\nDoes there appear to be a substantial difference in the distribution of total cholesterol by sex? Construct side-by-side boxplots to answer this question."
  },
  {
    "objectID": "content/r-cheatsheet.html",
    "href": "content/r-cheatsheet.html",
    "title": "R Cheatsheet",
    "section": "",
    "text": "Note\n\n\n\nThis document is a work in progress and will be updated prior to each test.\n\n\n\nBasics\nLoading data:\n\n[.RData file] load('&lt;FILEPATH&gt;')\n[from package] data(&lt;DATASET NAME&gt;, package = '&lt;PACKAGE NAME&gt;')\n[reading a CSV file] read.csv('&lt;FILEPATH&gt;')\n\nViewing dataframes:\n\n[preview] head(&lt;NAME&gt;)\n[data viewer] view(&lt;NAME&gt;)\n[check structure] str(&lt;NAME&gt;)\n\nExtracting a variable from a dataframe:\n\nDATAFRAME$VARIABLE\n\n\n\nSummary statistics\nIf x is a vector of values of a numeric variable…\n\nmean(x) computes the average\nmedian(x) computes the median\nmin(x) and max(x) compute the minimum and maximum\nquantile(x, probs = &lt;PERCENTILE&gt;) computes the percentile\nsummary(x) computes the five-number summary, plus the mean\nrange(x) computes the range (min, max)\nIQR(x) computes the interquartile range\nvar(x) computes the variance\nsd(x) computes the standard deviation\n\nIf df is a dataframe with a numeric variable y and a categorical variable x…\n\ndf |&gt; group_by(x) |&gt; summarize(&lt;OUTPUT.NAME&gt; = &lt;FUNCTION&gt;(y)) computes the statistic specified by &lt;FUNCTION&gt; separately for each category of the variable x (requires tidyverse package)\n\nSee especially Lab 2: Descriptive statistics.\n\n\nTables\nIf x and y are a vectors of values of two categorical variables…\n\ntable(x) computes the frequency distribution (counts)\ntable(x) |&gt; proportions() computes the frequency distribution (proportions)\ntable(x, y) computes a contingency table\ntable(x, y) |&gt; proportions(margin = NULL) computes proportions using grand total\ntable(x, y) |&gt; proportions(margin = 1) computes proportions using row total (group by x)\ntable(x, y) |&gt; proportions(margin = 2) computes proportions using column total (group by y)\n\nSee especially Lab 1: R basics and Lab 3: Bivariate summaries.\n\n\nGraphics\nIf x and y are vectors of values of two numeric variables…\n\nhist(x, breaks = &lt;NUMBER OF BINS&gt;) generates a histogram\nboxplot(x) generates a boxplot\nplot(x, y) generates a scatterplot\n\nIf x and y are vectors of values of two categorical variables…\n\ntable(x) |&gt; barplot() generates a bar plot (counts)\ntable(x) |&gt; proportions() |&gt; barplot() generates a bar plot (proportions)\ntable(x, y) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by y\ntable(y, x) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by x\n\nIf x is a vector of values of a categorical variable and y is a vector of values of a numeric variable…\n\nboxplot(y ~ x) generates a boxplot with x on the x axis (vertical orientation)\nboxplot(y ~ x, horizontal = T) generates a boxplot with y on the x axis (horizontal orientation)\n\nSee especially Lab 2: Descriptive statistics and Lab 3: Bivariate summaries.\n\n\nOne- and two-sample inference"
  },
  {
    "objectID": "content/week3-review.html#basics",
    "href": "content/week3-review.html#basics",
    "title": "Review session 1",
    "section": "Basics",
    "text": "Basics\nLoading data:\n\n[.RData file] load('&lt;FILEPATH&gt;')\n[from package] data(&lt;DATASET NAME&gt;, package = '&lt;PACKAGE NAME&gt;')\n[reading a CSV file] read.csv('&lt;FILEPATH&gt;')\n\nViewing dataframes:\n\n[preview] head(&lt;NAME&gt;)\n[data viewer] view(&lt;NAME&gt;)\n[check structure] str(&lt;NAME&gt;)\n\nExtracting a variable from a dataframe:\n\nDATAFRAME$VARIABLE"
  },
  {
    "objectID": "content/week3-review.html#summary-statistics",
    "href": "content/week3-review.html#summary-statistics",
    "title": "Review session 1",
    "section": "Summary statistics",
    "text": "Summary statistics\nIf x is a vector of values of a numeric variable…\n\n\n\nmean(x) computes the average\nmedian(x) computes the median\nmin(x) and max(x) compute the minimum and maximum\nquantile(x, probs = &lt;PERCENTILE&gt;) computes the percentile\nsummary(x) computes the five-number summary, plus the mean\n\n\n\nrange(x) computes the range (min, max)\nIQR(x) computes the interquartile range\nvar(x) computes the variance\nsd(x) computes the standard deviation\n\n\n\nSee especially Lab 2: Descriptive statistics."
  },
  {
    "objectID": "content/week3-review.html#tables",
    "href": "content/week3-review.html#tables",
    "title": "Review session 1",
    "section": "Tables",
    "text": "Tables\nIf x and y are a vectors of values of two categorical variables…\n\ntable(x) computes the frequency distribution (counts)\ntable(x) |&gt; proportions() computes the frequency distribution (proportions)\ntable(x, y) computes a contingency table\ntable(x, y) |&gt; proportions(margin = NULL) computes proportions using grand total\ntable(x, y) |&gt; proportions(margin = 1) computes proportions using row total (group by x)\ntable(x, y) |&gt; proportions(margin = 2) computes proportions using column total (group by y)\n\nSee especially Lab 3: Bivariate summaries."
  },
  {
    "objectID": "content/week3-review.html#graphics",
    "href": "content/week3-review.html#graphics",
    "title": "Review session 1",
    "section": "Graphics",
    "text": "Graphics\n\n\nIf x and y are vectors of values of two numeric variables…\n\nhist(x, breaks = &lt;NUMBER OF BINS&gt;) generates a histogram\nboxplot(x) generates a boxplot\nplot(x, y) generates a scatterplot\n\n\nIf x and y are vectors of values of two categorical variables…\n\ntable(x) |&gt; barplot() generates a bar plot (counts)\ntable(x) |&gt; proportions() |&gt; barplot() generates a bar plot (proportions)\ntable(x, y) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by y\ntable(y, x) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by x"
  },
  {
    "objectID": "content/test1.html",
    "href": "content/test1.html",
    "title": "Test 1",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 1 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 4/19. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/week3-review.html#study-design",
    "href": "content/week3-review.html#study-design",
    "title": "Review session 1",
    "section": "Study design",
    "text": "Study design"
  },
  {
    "objectID": "content/week3-review.html#data-types",
    "href": "content/week3-review.html#data-types",
    "title": "Review session 1",
    "section": "Data types",
    "text": "Data types"
  },
  {
    "objectID": "content/week3-review.html#descriptive-statistics",
    "href": "content/week3-review.html#descriptive-statistics",
    "title": "Review session 1",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics"
  },
  {
    "objectID": "content/test1.html#instructions",
    "href": "content/test1.html#instructions",
    "title": "Test 1",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 1 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 4/19. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test1.html#question-prompts",
    "href": "content/test1.html#question-prompts",
    "title": "Test 1",
    "section": "Question prompts",
    "text": "Question prompts\n\n[L2, L3] The yrbss dataset contains measurements on a small collection of variables from 10,587 survey responses collected as part of the CDC’s Youth Risk Behavior Surveillance System (YRBSS) from 1991-2013. The objective of the survey program is to track behaviors with potential negative physical and mental health impacts among adolescents. In this problem you’ll explore the amount of sleep that respondents get on school nights and the number of days per week respondents are physically active.\n\n[L2] Read briefly about the YRBSS on the CDC website: https://www.cdc.gov/healthyyouth/data/yrbs/overview.htm. Based on the overview and the description above, are the data observational or experimental?\n[L1] Based on the overview (link in part (a)), identify the study population.\n[LX] Load the dataset and identify the type of each variable. What kind of variable is sleep.hours?\n[LX] Examine the frequency distributions of age and grade level. Do any grades or ages seem over-represented in the sample?\n[L3] Make a bar plot showing the frequency distribution of hours of sleep on school nights. Based on the summary, what is the typical amount of sleep respondents get on school nights?\n[L3] Make a stacked bar plot showing levels of sleep by grade. Do older students sleep more on school nights than younger students?\n[L3] Visualize the frequency distribution of the number of days per week that survey participants are physically active. Describe the distribution and interpret any patterns.\n\n\n\n[L2, L3] Diet restriction and longevity. The longevity dataset contains data from a study in which 237 mice were randomly allocated to one of four diets at different levels of restriction: no restriction (NP), normal 85kCal diet before and after weaning (N/N85), normal diet before weaning and restricted 50kCal diet after weaning (N/R50), and normal diet before weaning and restricted 40kCal diet after weaning (N/R40). Researchers recorded the lifetime in months of each mouse in the study.\n\n[L2] Was this an experiment or observational study and why?\n[L3] Find the average lifetime of mice in each diet group.\n[L3] Find the standard deviation of lifetimes in each diet group.\n[L3] Make a plot comparing lifetimes by diet group.\n[L2] Write a short summary of the study results based on your work in (b)-(d). Indicate specifically whether there appears to be a relationship between dietary caloric intake and lifetime.\n\n\n\n[L3] Brain and body size. The mammals dataset contains average body weights (kg) and average brain weights (g) for 62 common species of mammal, as well as log-transformed versions of those weights.\n\n[L3] Make a scatterplot of log brain weights against log body weights and describe the apparent relationship, if any.\n[L3] Compute and interpret the correlation between log brain weight and log body weight.\n[L3] Make a histogram of brain weights (not on the log scale) with an appropriate number of bins. Describe the distribution. Are there outliers?\n[L3] Based on (c), compute an appropriate measure of center and spread for brain weight.\n[L3] Which species of mammal has the largest average brain weight? Inspect the data directly using view(...) to answer this question.\n[L3] Which species of mammal has the smallest \\(\\frac{\\text{brain weight}}{\\text{body weight}}\\) ratio? The largest? Inspect the data directly using view(...) to answer this question.\n\n\n\n[L1, L2] The following is an excerpt from the abstract of the study that reported the results of the Moderna Covid vaccine phase three clinical trial1: “Vaccines are needed to prevent coronavirus disease 2019 (Covid-19) and to protect persons who are at high risk for complications. The mRNA-1273 vaccine is a lipid nanoparticle–encapsulated mRNA-based vaccine that encodes the prefusion stabilized full-length spike protein of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes Covid-19. This [study] was conducted at 99 centers across the United States … The trial enrolled 30,420 [adult volunteers with no known history of SARS-CoV-2 infection and no circumstances that put them at high risk of infection or severe Covid-19 or both,] who were randomly assigned in a 1:1 ratio to receive either vaccine or placebo (15,210 participants in each group) … Symptomatic Covid-19 illness was confirmed in 185 participants in the placebo group and in 11 participants in the mRNA-1273 group; vaccine efficacy was 94.1%.”\n\n[L2] Is this an experiment or observational study? Explain.\n[L1] Identify the study population.\n[L1] Describe the study sample.\n[L1] What outcome(s) were measured in the study?\n[L3] The moderna dataset contains simulated observations according to the study description. Make a contingency table and use it to construct a table showing the proportions of volunteers infected and not infected in each group.\n[L3] Optional. Find the relative risk of illness in the vaccine group compared with the placebo group. Can you determine how efficacy is defined?\n\n\n\n[L1, L2, L3] The temps dataset contains physical data collected on a number of individuals. Explore the dataset and write a brief summary of your descriptive analysis. While open-ended, your analysis should include descriptions of the variables and their statistical properties, and descriptions of relationships between the variables. Include at least one graphic related to your summary. Your summary need not be exhaustive – in fact, it is better to pick 1-2 interesting findings and report those, rather than describe everything you tried."
  },
  {
    "objectID": "content/test1-practice.html",
    "href": "content/test1-practice.html",
    "title": "Extra practice problems",
    "section": "",
    "text": "The test will consist of four questions with multiple parts, and one less structured question in which your task is to explore and summarize a dataset based on descriptive statistics. You’ll have 48 hours to work on the test, and will submit your answers via a fillable form exactly as in the homework assignment. You can use any class resources (notes, textbook, lecture slides, past homeworks) but are required to work alone. You’ll be provided with a Posit cloud project in which to carry out your analyses, and will be expected to upload your script as a supporting document when you fill out the submission form. The practice problems below provide a sample of questions that approximate the test problems (but are slightly shorter)."
  },
  {
    "objectID": "content/test1-practice.html#test-information",
    "href": "content/test1-practice.html#test-information",
    "title": "Extra practice problems",
    "section": "",
    "text": "The test will consist of four questions with multiple parts, and one less structured question in which your task is to explore and summarize a dataset based on descriptive statistics. You’ll have 48 hours to work on the test, and will submit your answers via a fillable form exactly as in the homework assignment. You can use any class resources (notes, textbook, lecture slides, past homeworks) but are required to work alone. You’ll be provided with a Posit cloud project in which to carry out your analyses, and will be expected to upload your script as a supporting document when you fill out the submission form. The practice problems below provide a sample of questions that approximate the test problems (but are slightly shorter)."
  },
  {
    "objectID": "content/test1-practice.html#practice-problems",
    "href": "content/test1-practice.html#practice-problems",
    "title": "Extra practice problems",
    "section": "Practice problems",
    "text": "Practice problems\n\nAnother version of the frog dataset from earlier also includes measurements of egg size and body size. Use this dataset to practice visualizing and describing distributions of numeric variables.\n\nMake histograms of each of the four numeric variables, with appropriate numbers of bins, and describe the shape and number of modes.\nFor each variable, suggest an appropriate measure of center and measure spread and identify any measures that would not be appropriate.\nMake pairwise scatterplots of each of the four numeric variables and describe the association, if any. (Hint: try pairs(frog) for a more efficient way to generate these plots.)\nFor linear associations in (c), compute and interpret the correlation.\n\n\n\n# load and inspect data\nload('data/frog.RData')\nhead(frog)\n\n  site altitude clutch.size clutch.volume egg.size body.size\n1  040    3,462    181.9701      177.8279 1.949845  3.630781\n2  040    3,462    269.1535      257.0396 1.949845  3.630781\n3  040    3,462    158.4893      151.3561 1.949845  3.715352\n4  040    3,462    234.4229      223.8721 1.949845  3.801894\n5  040    3,462    245.4709      234.4229 1.949845  3.890451\n6  040    3,462    301.9952      288.4032 1.949845  3.890451\n\n# part a: histograms of each numeric variable; describe shape and modes\npar(mfrow = c(2, 2), mar = c(4, 4, 4, 1))\nhist(frog$clutch.size)\nhist(frog$clutch.volume)\nhist(frog$egg.size)\nhist(frog$body.size)\n\n\n\n\n\n\n\n# part c: pairwise scatterplots of clutch volume, egg size, body size, clutch size\npairs(frog)\n\n\n\n\n\n\n\n# part d: correlations for linear associations\ncor(frog$clutch.size, frog$clutch.volume)\n\n[1] 0.8077344\n\ncor(frog$clutch.volume, frog$egg.size)\n\n[1] 0.6462605\n\ncor(frog$body.size, frog$clutch.volume, use = 'complete.obs')\n\n[1] 0.6755435\n\ncor(frog$body.size, frog$clutch.size, use = 'complete.obs')\n\n[1] 0.6147564\n\n\n\nThe chick data data come from a study investigating the early growth of chicks on different diets. In the study, 47 chicks were randomly assigned one of four diets at birth and researchers measured body weight in grams daily. The data below show body weights at 18 days since birth for each chick. The question of interest is: which diet is best?\n\nIs this observational or experimental data? Explain your reasoning.\nProduce a visualization that compares body weight distributions by diet. For which diet have chicks grown the most? The least? Explain the statistic(s) or features of the distribution you used to make this determination.\nBased on your plot in (b), suggest a measure of center and measure of spread that would be appropriate for summarizing the data.\nCalculate the measures you suggested in (c) separately for each diet group.\nAssume that in the previous question you found that chicks on diet 3 grew the most, regardless of your actual answer. Can you conclude that diet 3 caused the fastest growth? Explain why or why not.\n\n\n\n# load and inspect data\nload('data/chick.RData')\nhead(chick)\n\n# A tibble: 6 × 3\n  chick.id weight diet  \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n1        1    171 diet 1\n2        2    187 diet 1\n3        3    187 diet 1\n4        4    154 diet 1\n5        5    199 diet 1\n6        6    160 diet 1\n\n# part b: visualize body weights by diet\nboxplot(weight ~ diet, data = chick)\n\n\n\n\n\n\n\n# part c-d: determine and compute appropriate measures of spread and center\nchick |&gt;\n  group_by(diet) |&gt;\n  summarize(avg.weight = mean(weight),\n            sd.weight = sd(weight))\n\n# A tibble: 4 × 3\n  diet   avg.weight sd.weight\n  &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 diet 1       159.      49.2\n2 diet 2       188.      63.3\n3 diet 3       233.      57.6\n4 diet 4       203.      33.6\n\n\n\nThe gss dataset contains observations for 500 respondents in the General Social Survey on a small number of demographic categorical variables. Use this to practice tabular and graphical summaries for categorical variables.\n\nFor each variable, determine whether the variable is nominal or ordinal.\nMake a contingency table of age bracket and whether participants have obtained a college degree.\nVisualize the relationship between age and having obtained a college degree.\nDoes the proportion of respondents with a college degree differ by sex?\nBy political party?\nBy socioeconomic class?\nMake one additional comparison of your choice and interpret the result.\n\n\n\n# load and inspect data\nload('data/gss.RData')\nhead(gss)\n\n# A tibble: 6 × 5\n  age     sex    college.degree political.party class        \n  &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;          &lt;fct&gt;           &lt;fct&gt;        \n1 (29,38] male   degree         ind             middle class \n2 (29,38] female no degree      rep             working class\n3 [18,29] male   degree         ind             working class\n4 (38,50] male   no degree      ind             working class\n5 (29,38] male   degree         rep             middle class \n6 (29,38] female no degree      rep             middle class \n\n# part b: contingency table of age and college degree\ntable(gss$college.degree, gss$age)\n\n           \n            [18,29] (29,38] (38,50] (50,87]\n  degree         36      44      60      34\n  no degree      99      74      64      89\n\n# part c: visualize relationship between age and college degree\ntable(gss$college.degree, gss$age) |&gt; \n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part d: does the proportion of respondents with a degree differ by sex?\ntable(gss$college.degree, gss$sex) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part e: by political party?\ntable(gss$college.degree, gss$political.party) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part f: by class?\ntable(gss$college.degree, gss$class) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part g: one additional comparison of your choosing\n\n\nLong COVID is a multi-systemic and often debilitating condition that develops in at least 10% of patients following a COVID infection. The following is an excerpt of the abstract from a recent study seeking to identify symptoms and risk factors associated with long COVID and published in Nature Medicine1: “We undertook a … study using a UK-based primary care database, Clinical Practice Research Datalink Aurum, to determine symptoms that are associated with confirmed SARS-CoV-2 infection beyond 12 weeks in non-hospitalized adults and the risk factors associated with developing persistent symptoms. We selected 486,149 adults with confirmed SARS-CoV-2 infection … Outcomes included 115 individual symptoms, as well as long COVID, defined as a composite outcome of 33 symptoms by the World Health Organization clinical case definition … Among the patients infected with SARS-CoV-2, risk factors for long COVID included female sex, belonging to an ethnic minority, socioeconomic deprivation, smoking, obesity and a wide range of comorbidities. The risk of developing long COVID was also found to be increased along a gradient of decreasing age.”\n\nIdentify the type of study.\nIdentify the study population.\nDescribe the sample.\nList the study outcomes of interest.\nIdentify any non-outcome variables."
  },
  {
    "objectID": "content/test1-practice.html#footnotes",
    "href": "content/test1-practice.html#footnotes",
    "title": "Extra practice problems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSubramanian et al. (2022). Symptoms and risk factors for long COVID in non-hospitalized adults. Nature medicine, 28(8), 1706-1714.↩︎"
  },
  {
    "objectID": "content/test1.html#footnotes",
    "href": "content/test1.html#footnotes",
    "title": "Test 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBaden, L. R., et al. (2021). Efficacy and safety of the mRNA-1273 SARS-CoV-2 vaccine. New England journal of medicine, 384(5), 403-416.↩︎"
  },
  {
    "objectID": "content/week3-review.html",
    "href": "content/week3-review.html",
    "title": "Review session 1",
    "section": "",
    "text": "[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques"
  },
  {
    "objectID": "content/week3-review.html#outcomes",
    "href": "content/week3-review.html#outcomes",
    "title": "Review session 1",
    "section": "Outcomes",
    "text": "Outcomes\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques"
  },
  {
    "objectID": "content/week3-review.html#lecturelab-recap",
    "href": "content/week3-review.html#lecturelab-recap",
    "title": "Review session 1",
    "section": "Lecture/lab recap",
    "text": "Lecture/lab recap\n\n\n\n\n\n\n\n\nWeek/day\nLecture\nLab\n\n\n\n\n1/W\nExperiments and observational studies\nReading abstracts\n\n\n2/M\nData vocabulary, proportions and means, common notation\nLoading data, extracting variables, computing means and proportions\n\n\n2/W\nMeasures of location/center and spread, simple graphics\nHistograms, barplots, summary statistics\n\n\n3/M\nGraphics and tables for two variables, interpreting relationships, correlation\nStacked bar plots, side-by-side boxplots, scatterplots"
  },
  {
    "objectID": "content/week3-review.html#assignment-recap",
    "href": "content/week3-review.html#assignment-recap",
    "title": "Review session 1",
    "section": "Assignment recap",
    "text": "Assignment recap\n\n\n\n\n\n\n\nProblem set\nTopics\n\n\n\n\nPS1\nInterpreting study descriptions\n\n\nPS2\nSummary statistics (mean, proportion) for one variable; identifying variable types\n\n\nPS3\nMeasures of location and spread for one variable; visualizing frequency distributions\n\n\nPS4\nVisualizing relationships between two variables (C/C, C/N, N/N)"
  },
  {
    "objectID": "content/week3-review.html#question-types-l2-study-design",
    "href": "content/week3-review.html#question-types-l2-study-design",
    "title": "Review session 1",
    "section": "Question types: [L2] study design",
    "text": "Question types: [L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#question-types-l3-descriptive-statistics",
    "href": "content/week3-review.html#question-types-l3-descriptive-statistics",
    "title": "Review session 1",
    "section": "Question types: [L3] descriptive statistics",
    "text": "Question types: [L3] descriptive statistics\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week3-review.html#questions-l2-study-design",
    "href": "content/week3-review.html#questions-l2-study-design",
    "title": "Review session 1",
    "section": "Questions: [L2] study design",
    "text": "Questions: [L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#questions-l3-descriptive-statistics",
    "href": "content/week3-review.html#questions-l3-descriptive-statistics",
    "title": "Review session 1",
    "section": "Questions: [L3] descriptive statistics",
    "text": "Questions: [L3] descriptive statistics\n\nCompute/interpret mean/median/percentiles/IQR/variance/SD\nCompute/interpret a grouped summary with one or more of the above measures\nDetermine appropriate measures of location/center/spread\nMake/interpret a table of proportions for values of a categorical variable\nMake/interpret a contingency table\nMake/interpret a histogram\nMake/interpret two-way tables of proportions\nMake/interpret stacked bar plots representing proportions\nMake/interpret side-by-side boxplots\nMake/interpret scatterplots\nCompute/interpret correlations"
  },
  {
    "objectID": "content/week3-review.html#questions-l1-l2-study-design",
    "href": "content/week3-review.html#questions-l1-l2-study-design",
    "title": "Review session 1",
    "section": "Questions: [L1, L2] study design",
    "text": "Questions: [L1, L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#commonly-missed-question-types",
    "href": "content/week3-review.html#commonly-missed-question-types",
    "title": "Review session 1",
    "section": "Commonly missed question types",
    "text": "Commonly missed question types\nIdentifying appropriate measures of center/spread (i.e., understanding robustness) Grouped summaries (getting syntax right) Determining"
  },
  {
    "objectID": "content/week3-review.html#key-concepts",
    "href": "content/week3-review.html#key-concepts",
    "title": "Review session 1",
    "section": "Key concepts",
    "text": "Key concepts\n\n[L1, L2] Experiments and observational studies\n[L1, L2] Samples and populations\n[L2] Variable types: categorical (nominal/ordinal) and numeric (discrete/continuous)\n[L3] Summary statistics: mean, median, percentile, IQR, SD, variance, correlation\n[L3] Distribution properties: skewness, outliers, modes\n[L3] Robustness of common summary statistics\n[L3] Relationships: positive/negative; linear/nonlinear"
  },
  {
    "objectID": "content/week3-review.html#about-the-test",
    "href": "content/week3-review.html#about-the-test",
    "title": "Review session 1",
    "section": "About the test",
    "text": "About the test\n\n4-5 questions\nExpect one challenge part, but otherwise very similar to homework problems and practice questions\n48 hours, open book, open note\nPosit cloud project + fillable form\nRevisions will be allowed to earn back credit\nNo collaboration\nDo your own analysis/writing (no AI plagiarism)\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#todays-agenda",
    "href": "content/week4-sampling.html#todays-agenda",
    "title": "Introduction to inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nTest 1 discussion\n[lecture] Inference vs. description, point estimation, interval estimation\n[lab] Exploring interval coverage"
  },
  {
    "objectID": "content/week4-sampling.html#inferential-statistics",
    "href": "content/week4-sampling.html#inferential-statistics",
    "title": "Introduction to inference",
    "section": "Inferential statistics",
    "text": "Inferential statistics\nDescriptive findings are statements about sample statistics. For instance:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nBy contrast, statistical inferences are statements about population statistics. For example:\n\nIndividuals with genotype TT exhibit the largest median percent change in strength."
  },
  {
    "objectID": "content/week4-sampling.html#sampling-variability",
    "href": "content/week4-sampling.html#sampling-variability",
    "title": "Introduction to inference",
    "section": "Sampling variability",
    "text": "Sampling variability\nA population model represents the distribution of values you’d see if you measured every individual in your population (a census).\nBut if you don’t measure every unit, results are subject to sampling variation: sample statistics change depending on which individuals you measure."
  },
  {
    "objectID": "content/week4-sampling.html#random-sampling",
    "href": "content/week4-sampling.html#random-sampling",
    "title": "Introduction to inference",
    "section": "Random sampling",
    "text": "Random sampling\n\nSampling establishes the link (or lack thereof) between a sample and a population.\n\n\n\n\n\nIn a simple random sample, units are chosen in such a way that every individual in the population has an equal probability of inclusion. For the SRS:\n\nsample statistics mirror population statistics (sample is representative)\nsampling variability depends only on population variability and sample size"
  },
  {
    "objectID": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "href": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "title": "Introduction to inference",
    "section": "A pretend population: NHANES data",
    "text": "A pretend population: NHANES data\nThe National Health and Nutrition Esamination Survey (NHANES) is an annual CDC program to collect health and nutrition data on the non-institutionalized civilian resident population of the United States. Here are a few variables:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubj.id\ngender\nage\npoverty\npulse\nbpsys1\nbpdia1\ntotchol\nsleephrsnight\n\n\n\n\n1\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n2\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n3\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n5\nfemale\n49\n1.91\n86\n118\n82\n6.7\n8\n\n\n\n\n\nI’ve selected 3,179 responses from the 2009-2010 survey; let’s pretend the corresponding individuals form a population of interest."
  },
  {
    "objectID": "content/week4-sampling.html#population-distribution-of-a-variable",
    "href": "content/week4-sampling.html#population-distribution-of-a-variable",
    "title": "Introduction to inference",
    "section": "Population distribution of a variable",
    "text": "Population distribution of a variable\nConsider the totchol variable: total HDL cholesterol in mmol/L. It has a certain frequency distribution among the population that we’ll call its population distribution.\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we draw a random sample of 50 individuals…\n\nhow closely will the sample align with the population distribution?\nhow much will alignment change if we select a new sample?"
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability",
    "href": "content/week4-sampling.html#simulating-sampling-variability",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\n\n\n\n\n\n\n\n\n\n\n\n\nThese are 20 random samples with the sample mean indicated by the dashed line and the population distribution and mean overlaid in red.\n\nsample size \\(n = 20\\)\nfrequency distributions differ a lot\nsample means differ some\n\nWe can actually measure this variability!"
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability-1",
    "href": "content/week4-sampling.html#simulating-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\nIf we had means calculated from a much larger number of samples, we could make a frequency distribution for the values of the sample mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample\n1\n2\n\\(\\cdots\\)\n10,000\n\n\nmean\n4.957\n5.039\n\\(\\cdots\\)\n5.24\n\n\n\n\nWe could then use the usual measures of center and spread to characterize the distribution of sample means.\n\nmean of \\(\\bar{x}\\): 5.0373228\nstandard deviation of \\(\\bar{x}\\): 0.2387869\n\n\nAcross 10,000 random samples of size 20, the typical sample mean was 5.04 and the root average squared distance of the sample mean from its typical value was 0.239."
  },
  {
    "objectID": "content/week4-sampling.html#point-estimation",
    "href": "content/week4-sampling.html#point-estimation",
    "title": "Introduction to inference",
    "section": "Point estimation",
    "text": "Point estimation\nSample statistics, viewed as guesses of population statistics, are called ‘point estimates’.\n\n\n\nParameter name\nParameter notation\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)\n\n\n\nNow we can more formally describe statistical inference:\n\na population parameter is any numeric characteristic of a population distribution\nan inference is a conclusion about the value of a population parameter based on point estimates and their sampling variability\n\nWe will focus initially on inferences about the mean \\(\\mu\\) based on the point estimate \\(\\bar{x}\\)."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability",
    "href": "content/week4-sampling.html#measuring-sampling-variability",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nIn practice \\(\\sigma\\) is not known so we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared error of the sample mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability-1",
    "href": "content/week4-sampling.html#measuring-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nTheory indicates the standard deviation of the sample mean under random sampling is: \\[\nSD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{population SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor totchol, the theoretical standard deviation is \\(SD(\\bar{x}) = \\frac{1.0747}{\\sqrt{50}} =\\) 0.1519822.\nWe can estimate this quantity by replacing \\(\\sigma\\) with the point estimate \\(s_x\\), resulting in a standard error (estimated standard deviation): \\[SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\\]"
  },
  {
    "objectID": "content/week4-sampling.html#example-with-one-sample",
    "href": "content/week4-sampling.html#example-with-one-sample",
    "title": "Introduction to inference",
    "section": "Example with one sample",
    "text": "Example with one sample\nThe simulations we’ve done so far have been a means of understanding just what a standard error is meant to capture; these are not a practicable method for measuring sampling variation.\nIn practice we’d simply compute a point estimate and standard error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.031\n0.1396\n\n\n\n\n\n\nThe estimated mean total HDL cholesterol among the population is 5.031 mmol/L.\nThe point estimate is expected to deviate by 0.1396 mmol/L on average from the population mean."
  },
  {
    "objectID": "content/week4-sampling.html#effect-of-sample-size",
    "href": "content/week4-sampling.html#effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\) is inversely proportional to sample size.\n\n\n\n\n\n\n\n\n\n\n\n\nAs sample size increases…\n\naccuracy remains the same\nestimates get more precise\nskewness vanishes"
  },
  {
    "objectID": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "href": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Visualizing effect of sample size",
    "text": "Visualizing effect of sample size\nGenerating a large number (10K) of samples allows us to approximate the sampling distribution for a variety of sample sizes.\n\n\nspread diminishes with sample size\nless variability among estimates from larger samples\n\nSo estimates are more accurate with more data, assuming data are from a random sample."
  },
  {
    "objectID": "content/week4-sampling.html#normal-model",
    "href": "content/week4-sampling.html#normal-model",
    "title": "Introduction to inference",
    "section": "Normal model",
    "text": "Normal model\nNotice that each simulated sampling distribution has produced a unimodal, symmetric, bell-shaped histogram.\n\n\nThe normal model is a theoretical frequency distribution characterized by two parameters:\n\na mean (center)\na standard deviation (spread)\n\nTheory dictates that the sampling distribution of the sample mean is well-approximated by a normal model under simple random sampling.\n\n\n\n\n\n\n\n\n\n\n\n\nBased on discussion thus far, what do you think the model parameters might be?\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#description-vs.-inference",
    "href": "content/week4-sampling.html#description-vs.-inference",
    "title": "Introduction to inference",
    "section": "Description vs. inference",
    "text": "Description vs. inference\n\nStatistical inferences are statements about population statistics based on samples.\n\nTo appreciate the meaning of this, consider the contrast with descriptive findings, which are statements about sample statistics:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nA descriptive finding is:\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nThe corresponding inference would be:\nThe median percent change in strength is highest among adults with genotype TT."
  },
  {
    "objectID": "content/week4-sampling.html#a-simpler-question",
    "href": "content/week4-sampling.html#a-simpler-question",
    "title": "Introduction to inference",
    "section": "A simpler question",
    "text": "A simpler question\n\n\nConsider total HDL cholesterol in mmol/L. We already know how to summarize the distribution of sample values:\n\n\n\n\n\n\n\n\n\nSample mean\nSample SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.043\n1.117"
  },
  {
    "objectID": "content/week4-sampling.html#population-model",
    "href": "content/week4-sampling.html#population-model",
    "title": "Introduction to inference",
    "section": "Population model",
    "text": "Population model\n\n\nConsider total HDL cholesterol in mmol/L. We already know how to summarize the distribution of sample values:\n\n\n\n\n\n\n\n\n\nSample mean\nSample SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.067\n1.126"
  },
  {
    "objectID": "content/week4-sampling.html#population-models",
    "href": "content/week4-sampling.html#population-models",
    "title": "Introduction to inference",
    "section": "Population models",
    "text": "Population models\n\nInference consists in using statistics of a random sample to ascertain population statistics under a population model.\n\nA population model represents the distribution of values you’d see if you measured every individual in the study population. We think of the sample values as a random draw.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Density is an alternative scale to frequency that is independent of population size.)"
  },
  {
    "objectID": "content/week4-sampling.html#sampling-variation",
    "href": "content/week4-sampling.html#sampling-variation",
    "title": "Introduction to inference",
    "section": "Sampling variation",
    "text": "Sampling variation\nA population model represents the distribution of values you’d see if you measured every individual in the study population (a census).\nThink of a sample as a random draw from the population:\n\ndifferent samples comprise different sets of individuals\nsample statistics depend on which individuals you measure"
  },
  {
    "objectID": "content/week4-sampling.html#sampling-distributions",
    "href": "content/week4-sampling.html#sampling-distributions",
    "title": "Introduction to inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nWhat we are simulating is known as a sampling distribution: the frequency of values of a statistic across all possible random samples.\n\n\n\n\n\n\n\n\n\n\n\n\nProvided data are from a random sample, the sample mean \\(\\bar{x}\\) has a sampling distribution with\n\nmean \\(\\color{red}{\\mu}\\) (population mean)\nstandard deviation \\(\\color{red}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\nregardless of its exact form.\n\n\nIn other words, across all random samples of a fixed size…\n\nThe average value of the sample mean is the population mean.\nThe average squared error (sample mean - population mean)\\(^2\\) is \\(\\frac{\\sigma^2}{n}\\)"
  },
  {
    "objectID": "content/week4-sampling.html#stuff",
    "href": "content/week4-sampling.html#stuff",
    "title": "Introduction to inference",
    "section": "Stuff",
    "text": "Stuff\nGenerating a large number (10K) of samples allows us to approximate the sampling distribution for a variety of sample sizes.\n\n\n\n\n\n\n\n\n\n\nspread diminishes with sample size\nless variability among estimates from larger samples\n\nSo estimates are more precise with more data.\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#standard-error",
    "href": "content/week4-sampling.html#standard-error",
    "title": "Introduction to inference",
    "section": "Standard error",
    "text": "Standard error\nIn practice we only have one sample so instead of a direct measure we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared distance of the sample mean from the population mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-sampling.html#point-estimates",
    "href": "content/week4-sampling.html#point-estimates",
    "title": "Introduction to inference",
    "section": "Point estimates",
    "text": "Point estimates\n\nSample statistics, viewed as guesses for the values of population statistics, are called ‘point estimates’.\n\nWe’ll focus on inferences involving the following:\n\n\n\nPopulation statistic\nParameter\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)"
  },
  {
    "objectID": "content/week4-sampling.html#standard-errors",
    "href": "content/week4-sampling.html#standard-errors",
    "title": "Introduction to inference",
    "section": "Standard errors",
    "text": "Standard errors\nIn practice we only have one sample so instead of a direct measure we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\n\n\n\n\n5.381\n1.073\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]"
  },
  {
    "objectID": "content/week4-sampling.html#reporting-point-estimates",
    "href": "content/week4-sampling.html#reporting-point-estimates",
    "title": "Introduction to inference",
    "section": "Reporting point estimates",
    "text": "Reporting point estimates\nIt is common style to report the value of a point estimate with a standard error given parenthetically.\n\n\nStatistics from full NHANES sample:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\n\n\n\n\n5.043\n1.075\n3179\n\n\n\n\n\n\n\nThe mean total cholesterol among the population is estimated to be 5.043 mmol/L (SE 0.019)\n\n\n\nThis style of report communicates:\n\nparameter of interest\nvalue of point estimate\nerror/variability of point estimate"
  },
  {
    "objectID": "content/week4-sampling.html#interval-estimation",
    "href": "content/week4-sampling.html#interval-estimation",
    "title": "Introduction to inference",
    "section": "Interval estimation",
    "text": "Interval estimation\n\nAn interval estimate is a range of plausible values for a population parameter.\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.043 \\pm 2\\times 0.0191 = (5.005, 5.081)\\]\n\nIn R:\n\navg.totchol &lt;- mean(totchol)\nse.totchol &lt;- sd(totchol)/sqrt(length(totchol))\navg.totchol + c(-2, 2)*se.totchol\n\n[1] 5.004817 5.081059\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#interval-construction",
    "href": "content/week4-sampling.html#interval-construction",
    "title": "Introduction to inference",
    "section": "Interval construction",
    "text": "Interval construction\nIn general, an interval estimate is constructed from two main ingredients:\n\npoint estimate\nstandard error\n\nAnd one secret ingredient:\n\na model for the sampling distribution of the point estimate\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]"
  },
  {
    "objectID": "content/week4-sampling.html#precision-and-coverage",
    "href": "content/week4-sampling.html#precision-and-coverage",
    "title": "Introduction to inference",
    "section": "Precision and coverage",
    "text": "Precision and coverage\n\n\nIntervals have two main and attributes:\n\nprecision refers to how wide or narrow the interval is\ncoverage refers to how often, under random sampling, the interval captures the parameter of interest\n\n\nThese properties are inversely related:\n\nif I say mean cholesterol is between 0 and 50 I’m almost certainly right, but the estimate is useless\nif I say mean cholesterol is between 5.0299 and 5.0301, I’ve made a very precise guess, but I’m likely wrong (think about sampling variability)\n\n\n\n\nAn accurate interval should maintain high coverage while achieving practically useful precision. This isn’t always possible!\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#inference-or-description",
    "href": "content/week4-sampling.html#inference-or-description",
    "title": "Introduction to inference",
    "section": "Inference or description?",
    "text": "Inference or description?\nSee if you can tell the difference:\n\nThe proportion of children who developed a peanut allergy was 0.133 in the avoidance group.\nThe proportion of children who develop peanut allergies is estimated to be 0.133 when peanut protein is avoided in infancy.\nThe average lifetime of mice on normal 85kCal diets is estimated to be 32.7 months.\nThe 57 mice on the normal 85kCal diet lived 32.7 months on average.\nThe relative risk of a CHD event in the high trait anger group compared with the low trait anger group was 2.5.\nThe relative risk of a CHD event among adults with high trait anger compared with adults with low trait anger is estimated to be 2.5."
  },
  {
    "objectID": "content/week4-sampling.html#a-difficulty",
    "href": "content/week4-sampling.html#a-difficulty",
    "title": "Introduction to inference",
    "section": "A difficulty",
    "text": "A difficulty\n\nDifferent samples yield different estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample means:\n\n\n\n\n\n\n\n\n\nsample.1\nsample.2\n\n\n\n\n5.093\n5.136\n\n\n\n\n\n\nestimates are close but not identical\nthe population mean can’t be both 5.093 and 5.136\nprobably neither estimate is exactly correct\n\nEstimation error and sample-to-sample variability are inherent to point estimation."
  },
  {
    "objectID": "content/week4-sampling.html#interpreting-standard-errors",
    "href": "content/week4-sampling.html#interpreting-standard-errors",
    "title": "Introduction to inference",
    "section": "Interpreting standard errors",
    "text": "Interpreting standard errors\nThe standard error is a point estimate of the (population) standard deviation of sample means across all possible random samples:\n\\[\nSE(\\bar{x}) \\text{ estimates } \\sqrt{\\text{average value of } (\\bar{x} - \\mu)^2}\n\\] Two phrasings for an interpretation:\n\nEstimated root average squared deviation of the sample mean from the population mean.\nEstimated root mean square error."
  },
  {
    "objectID": "content/lab4-sampling.html",
    "href": "content/lab4-sampling.html",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "",
    "text": "library(tidyverse)\nload('data/nhanes.RData')\nload('data/temps.RData')\nThe goal of this lab is to learn to compute a point estimate, standard error, and interval estimate for a population mean “by hand” by performing the arithmetic directly in R. You will have an opportunity to practice interpreting these quantities as you go.\nWe will also use this lab for a short class activity to explore how often the interval estimate we introduced is “correct”."
  },
  {
    "objectID": "content/lab4-sampling.html#your-turn-1",
    "href": "content/lab4-sampling.html#your-turn-1",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "Your turn",
    "text": "Your turn\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error"
  },
  {
    "objectID": "content/lab4-estimation.html",
    "href": "content/lab4-estimation.html",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "",
    "text": "library(tidyverse)\nload('data/nhanes.RData')\nload('data/temps.RData')\n\nThe goal of this lab is to learn to compute a point estimate, standard error, and interval estimate for a population mean “by hand” by performing the arithmetic directly in R. You will have an opportunity to practice interpreting these quantities as you go.\nWe will also use this lab for a short class activity to explore how often the interval estimate we introduced is “correct”.\n\nPoint estimation\n\nEstimate for the population mean\nSince the point estimate for the population mean of a numeric variable is the sample mean, you already know how to perform the calculation in R. We’ll store this for later use:\n\n# retrieve total cholesterol variable\ntotchol &lt;- nhanes$totchol\n\n# compute and store sample mean\ntotchol.mean &lt;- mean(totchol)\ntotchol.mean\n\n[1] 5.042938\n\n\nThe only novelty here is that we now interpret this as a point estimate of the population mean total cholesterol:\n\nThe mean total cholesterol of U.S. adults is estimated to be 5.043 mmol/L.\n\nThis is in contrast to the interpretation as a descriptive summary:\n\nThe average total cholesterol among the respondents in the NHANES survey was 5.043 mmol/L.\n\nBoth interpretations are valid; just different. By interpreting the sample mean as a point estimate, we are implicitly assuming that the data are a random sample from the U.S. adult population.\n\n\n\n\n\n\nYour turn\n\n\n\nUse the temps data to estimate mean body temperature.\n\n# retrieve variable of interest\n\n# compute and store sample mean\n\nCheck your understanding:\n\ninterpret the result as a descriptive summary\ninterpret the result as a point estimate\n\n\n\n\n\nStandard error for the sample mean\nA standard error is a measure of the sampling variability of a point estimate. Technically, it’s an estimate of the point estimate’s standard deviation across all possible random samples of a fixed size.\nThe standard error for the sample mean is calculated according to the formula: \\[SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\] Where:\n\n\\(s_x\\) is the sample standard deviation\n\\(n\\) is the sample size\n\nTo calculate this in R, we perform the arithmetic by hand (for now):\n\n# store sample sd and sample size\ntotchol.sd &lt;- sd(totchol)\ntotchol.n &lt;- length(totchol)\n\n# compute standard error\ntotchol.se &lt;- totchol.sd/sqrt(totchol.n)\ntotchol.se\n\n[1] 0.01906042\n\n\nThis result is interpreted as follows:\n\nThe root average deviation of the sample mean from the population mean is estimated to be 0.0191 mmol/L.\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error\n\n\n\n\n\n\nInterval estimation\n\nInterval estimate for the mean\nA common interval for the population mean is:\n\\[\\bar{x} \\pm \\underbrace{2\\times SE(\\bar{x})}_{\\text{margin of error}}\\]\nFor now, we’ll calculate this by directly performing the arithmetic. Later, you’ll use commands that return interval estimates by default.\n\n# add/subtract two standard errors from the mean\ntotchol.mean + c(-2, 2)*totchol.se\n\n[1] 5.004817 5.081059\n\n\nWe’ll talk more about the exact interpretation later; for now, you should think of this as a range of plausible values for the population mean.\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate an interval estimate for the mean body temperature using the body temperature data.\n\n# interval estimate for mean body temp\n\n\n\n\n\nHow often is the interval correct?\nAn interval “covers” the population mean if the true value is between the interval endpoints.\nWe can explore how often the interval covers the parameter by having everyone in the class simulate their own sample from a population with a known mean and check whether the interval they obtain from the sample covers the population mean or not.\nThe commands below simulate a sample and then compute an interval.\n\n# function to simulate body temp data from a population with mean 98.6\nsample.bodytemps &lt;- function(n){\n  rnorm(n, mean = 98.6, sd = 1)\n}\n\n# simulate a sample of body temperatures\nbodytemp &lt;- sample.bodytemps(n = 150)\n\n# compute interval 'ingredients'\nbodytemp.mean &lt;- mean(bodytemp)\nbodytemp.sd &lt;- sd(bodytemp)\nbodytemp.n &lt;- length(bodytemp)\nbodytemp.se &lt;- bodytemp.sd/sqrt(bodytemp.n)\n\n# compute interval estimate\nbodytemp.mean + c(-2, 2)*bodytemp.se\n\n[1] 98.41325 98.73135\n\n# margin of error\n2*bodytemp.se\n\n[1] 0.1590505\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nUse the example above to generate a sample of size 20 and compute an interval estimate for the mean body temperature.\nThen:\n\nDetermine whether your interval covers the population mean.\nCompute the margin of error used in your interval (\\(2\\times SE(\\bar{x})\\)).\n\nRepeat with \\(n = 150\\). Then fill out this form.\n\n\n\n\n\nPractice problems\n\nVu and Harrington exercise 4.1. Additionally:\n\nCompute an interval estimate for the mean BGC of nests.\nSupposing a sample of 30 nests returned exactly the same summary statistics, recompute your interval in (e). Is the margin of error smaller or larger?\n\n\n\nThe brfss dataset contains a measurement of body weight, weight, as well as a variable, wtdesire, that is the desired weight reported by respondents.\n\nEstimate the mean difference between actual and desired weight. Report the point estimate and standard error.\nDoes the point estimate suggest that the average U.S. adult would prefer to lose or gain weight?\nCompute an interval estimate for the mean difference between actual and desired weight."
  },
  {
    "objectID": "content/lab4-estimation.html#your-turn-1",
    "href": "content/lab4-estimation.html#your-turn-1",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "Your turn",
    "text": "Your turn\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error"
  },
  {
    "objectID": "content/week4-intervals.html",
    "href": "content/week4-intervals.html",
    "title": "Interval estimation",
    "section": "",
    "text": "HW2 remarks/discussion\n[Lecture] A basic interval estimate for the mean\n[Lecture/lab] Exploring interval coverage\n[Lecture/lab] Comparing normal and \\(t\\) models"
  },
  {
    "objectID": "content/week4-intervals.html#todays-agenda",
    "href": "content/week4-intervals.html#todays-agenda",
    "title": "Confidence intervals",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] \\(t\\) confidence intervals for the mean\n[lab] computing and interpreting confidence intervals"
  },
  {
    "objectID": "content/week4-intervals.html#from-last-time",
    "href": "content/week4-intervals.html#from-last-time",
    "title": "Confidence intervals",
    "section": "From last time",
    "text": "From last time\n\n\nUnder simple random sampling:\n\nthe sample mean \\(\\bar{x}\\) provides a good point estimate of the population mean \\(\\mu\\)\nits estimated sampling variability is given by the standard error \\(SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}} = \\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\nse\n\n\n\n\n5.043\n1.075\n3179\n0.01906\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean total HDL cholesterol among the U.S. adult population is estimated to be 5.043 mmol/L (SE 0.0191)."
  },
  {
    "objectID": "content/week4-intervals.html#interval-estimation",
    "href": "content/week4-intervals.html#interval-estimation",
    "title": "Confidence intervals",
    "section": "Interval estimation",
    "text": "Interval estimation\nA common interval estimate for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\nA range of plausible values for the mean total cholesterol among U.S. adults is 5.005 to 5.081 mmol/L.\n\nTwo related questions:\n\nWhat do we mean by “plausible”?\nWhere did the number 2 come from?"
  },
  {
    "objectID": "content/week4-intervals.html#precision-and-coverage",
    "href": "content/week4-intervals.html#precision-and-coverage",
    "title": "Confidence intervals",
    "section": "Precision and coverage",
    "text": "Precision and coverage\n\n\nIntervals have two main and attributes:\n\nprecision refers to how wide or narrow the interval is\ncoverage refers to how often, under random sampling, the interval captures the parameter of interest\n\n\nThese properties are inversely related:\n\nif I say mean cholesterol is between 0 and 50 I’m almost certainly right, but the estimate is useless\nif I say mean cholesterol is between 5.0299 and 5.0301, I’ve made a very precise guess, but I’m likely wrong (think about sampling variability)\n\n\n\n\nAn accurate interval should maintain high coverage while achieving practically useful precision. This isn’t always possible!"
  },
  {
    "objectID": "content/week4-intervals.html#an-interval-for-the-mean",
    "href": "content/week4-intervals.html#an-interval-for-the-mean",
    "title": "Confidence intervals",
    "section": "An interval for the mean",
    "text": "An interval for the mean\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.031 \\pm 2\\times 0.1396 = (4.75, 5.31)\\]\n\nIn R:\n\nc(lwr = mean(cholesterol) - 2*sd(cholesterol)/sqrt(50), \n  upr = mean(cholesterol) + 2*sd(cholesterol)/sqrt(50))\n\n     lwr      upr \n4.738974 5.346902 \n\n\n\n\nThe precision is evident from the interval width (0.5611). But what about coverage?"
  },
  {
    "objectID": "content/week4-intervals.html#exploring-interval-coverage",
    "href": "content/week4-intervals.html#exploring-interval-coverage",
    "title": "Confidence intervals",
    "section": "Exploring interval coverage",
    "text": "Exploring interval coverage\nLet’s carry on pretending that the NHANES data comprise a population.\nThe first section of lab5-intervals contains some simple commands to draw a sample and calculate an interval estimate.\n\nEach of you will generate an interval based on a different sample\nWe’ll tally how many of you obtained intervals capturing the population mean\n\nOur tally will give an approximate idea of the coverage."
  },
  {
    "objectID": "content/week4-intervals.html#more-simulation",
    "href": "content/week4-intervals.html#more-simulation",
    "title": "Confidence intervals",
    "section": "More simulation",
    "text": "More simulation\n\n\nArtificially simulating a larger number of intervals provides a slightly better approximation of coverage.\n\nat right, 100 intervals\n97% cover the population mean (vertical dashed line)\n\nWhat do you expect would happen to coverage if, for the same samples…\n\na wider margin of error (say, \\(3\\times SE\\)) were used?\na narrower margin of error (say, \\(1\\times SE\\)) were used?"
  },
  {
    "objectID": "content/week4-intervals.html#so-why-2-standard-errors",
    "href": "content/week4-intervals.html#so-why-2-standard-errors",
    "title": "Confidence intervals",
    "section": "So why 2 standard errors?",
    "text": "So why 2 standard errors?\n\n\nThe margin of error of \\(2\\times SE\\) comes from the so-called “empirical rule”.\n\nunder the normal model, 95% of values are within 2SD of center\nso for 95% of samples, the sample mean is within 2SD of the population mean\n\nSo in theory, according to the normal model, \\(\\bar{x} \\pm 2\\times SD\\) achieves 95% coverage.\n\n\n\n\nBut we are using standard error (SE), not standard deviation (SD). Do we still get the same coverage using the normal model?"
  },
  {
    "objectID": "content/week4-intervals.html#normal-model-coverage",
    "href": "content/week4-intervals.html#normal-model-coverage",
    "title": "Confidence intervals",
    "section": "Normal model coverage",
    "text": "Normal model coverage\n\n\nAt right, the misses are compared between intervals calculated with SD (left) and SE (right) using the multiplier from the normal model on the same 10,000 simulated datasets with sample size \\(n = 15\\).\n\nSE misses more often\nso the normal model produces under-coverage\n\n\n\n\n\n\n\n\n\n\ntype\ncoverage\n\n\n\n\nsd\n0.954\n\n\nse\n0.9294\n\n\n\n\n\nWhat do you think: the multiplier should be [smaller/larger] to ensure 95% coverage."
  },
  {
    "objectID": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "href": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "title": "Confidence intervals",
    "section": "A closer look at the normal model",
    "text": "A closer look at the normal model\nAn alternate but equivalent way to understand the normal model for the sampling distribution of \\(\\bar{x}\\) is in terms of deviations. The following are equivalent:\n\nThe expression \\(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\\) measures the number of standard deviations from center."
  },
  {
    "objectID": "content/week4-intervals.html#simulating-deviations",
    "href": "content/week4-intervals.html#simulating-deviations",
    "title": "Confidence intervals",
    "section": "Simulating deviations",
    "text": "Simulating deviations\nAnother way to check normal model coverage is to use deviations:\n\nSimulate many samples\nCompute scaled deviations\nTally how many scaled deviations are between -2 and 2\n\nThe proportion of samples for which the scaled deviation is between -2 and 2 approximates the coverage.\nWe’ll try it in the next part of the lab5-intervals. Hypotheses:\n\ndeviations scaled by SD should be between -2 and 2 95% of the time\ndeviations scaled by SE should be between -2 and 2 [more/less] than 95% of the time"
  },
  {
    "objectID": "content/week4-intervals.html#the-t-model",
    "href": "content/week4-intervals.html#the-t-model",
    "title": "Confidence intervals",
    "section": "The \\(t\\) model",
    "text": "The \\(t\\) model\n\n\nConsider the statistic:\n\\[\nT = \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\]\nThe sampling distribution of \\(T\\) is well-approximated by a \\(t_{n - 1}\\) model whenever either:\n\nthe population model is symmetric and unimodal\n\nOR\n\nthe sample size is not too small"
  },
  {
    "objectID": "content/week4-intervals.html#model-specification",
    "href": "content/week4-intervals.html#model-specification",
    "title": "Confidence intervals",
    "section": "Model specification",
    "text": "Model specification\nThe \\(t\\) model is characterized by its degrees of freedom.\n\nfor interval estimates for the mean, \\(n - 1\\) is used\ndepending on the degrees of freedom (i.e., sample size), a different multiplier is applied to the standard error to obtain the margin of error\n\nThe multiplier is called a critical value, and can be found in R via:\n\n# pseudo code -- replace coverage with desired level, e.g., 0.95\nqt((1 - coverage)/2, df = (n - 1), lower.tail = F)\n\n\nchosen to ensure a specified nominal coverage level (usually 95%)\nhigher nominal coverage levels utilize larger critical values, producing wider intervals"
  },
  {
    "objectID": "content/week4-intervals.html#model-validation",
    "href": "content/week4-intervals.html#model-validation",
    "title": "Confidence intervals",
    "section": "Model validation",
    "text": "Model validation\nUsing the \\(t\\) model should produce coverage closer to the nominal level compared with the normal model. Let’s check through simulation.\n\n\nAt right, misses are compared between intervals using SE and critical values from the normal model (left) and \\(t\\) model (right) constructed on the same 10,000 simulated datasets with sample size \\(n = 10\\).\n\n\n\n\n\n\n\n\n\nmodel\ncoverage\n\n\n\n\nnormal\n0.9219\n\n\nt\n0.9461\n\n\n\n\n\nThe \\(t\\) model produces coverage much closer to the nominal level."
  },
  {
    "objectID": "content/week4-intervals.html#calculations",
    "href": "content/week4-intervals.html#calculations",
    "title": "Confidence intervals",
    "section": "Calculations",
    "text": "Calculations\nSo, to sum up, the general formula for an interval for a population mean is: \\[\\bar{x} \\pm c \\times SE(\\bar{x}) \\quad\\text{where}\\quad SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\]\n\n\nRules of thumb:\n\nfor moderate to large samples, use the normal model\n\n\\(c = 1\\) for 68% coverage\n\\(c = 2\\) for 95% coverage\n\\(c = 3\\) for 99.7% coverage\n\nfor small sample sizes, use the \\(t\\) model\nwhen in doubt, use the \\(t\\) model\n\n\nExact critical values in R:\n\n# normal critical value\nc &lt;- qnorm((1 - coverage)/2, lower.tail = F)\n\n# t critical value\nc &lt;- qt((1 - coverage)/2, df = n - 1, lower.tail = F)\n\nInterval calculation:\n\n# pseudo code\nmean(data_vec) + c(-1, 1)*c*sd(data_vec)/sqrt(n)"
  },
  {
    "objectID": "content/week4-intervals.html#interpretation",
    "href": "content/week4-intervals.html#interpretation",
    "title": "Confidence intervals",
    "section": "Interpretation",
    "text": "Interpretation\nAs we’ve seen, coverage pertains to how often an interval of a particular form captures the population parameter of interest across samples of a fixed size. Loosely speaking, this represents how often you’d be right if you were to fully replicate your study ad infinitum.\nThis leads to the following interpretation:\n\nWith [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units].\n\nFor this reason, statisticians call interval estimates confidence intervals."
  },
  {
    "objectID": "content/week4-intervals.html#revisiting-initial-example",
    "href": "content/week4-intervals.html#revisiting-initial-example",
    "title": "Confidence intervals",
    "section": "Revisiting initial example",
    "text": "Revisiting initial example\n\n\nSo in the example we began with:\n\n# calculate 95% interval\nmean(cholesterol) + c(-1, 1)*2*sd(cholesterol)/sqrt(50)\n\n[1] 4.738974 5.346902\n\n\nWith 95% confidence, the mean total HDL cholesterol is estimated to be between 4.739 and 5.347 mmol/L.\nRemember, “95% confidence” refers to coverage under sampling variation.\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.043\n0.01906\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-inference.html#todays-agenda",
    "href": "content/week4-inference.html#todays-agenda",
    "title": "Introduction to inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nTest 1 discussion\n[lecture] Inference vs. description, point estimation, interval estimation\n[lab] Exploring interval coverage"
  },
  {
    "objectID": "content/week4-inference.html#description-vs.-inference",
    "href": "content/week4-inference.html#description-vs.-inference",
    "title": "Introduction to inference",
    "section": "Description vs. inference",
    "text": "Description vs. inference\n\nStatistical inferences are statements about population statistics based on samples.\n\nTo appreciate the meaning of this, consider the contrast with descriptive findings, which are statements about sample statistics:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nA descriptive finding is:\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nThe corresponding inference would be:\nThe median percent change in strength is highest among adults with genotype TT."
  },
  {
    "objectID": "content/week4-inference.html#inference-or-description",
    "href": "content/week4-inference.html#inference-or-description",
    "title": "Introduction to inference",
    "section": "Inference or description?",
    "text": "Inference or description?\nSee if you can tell the difference:\n\nThe proportion of children who developed a peanut allergy was 0.133 in the avoidance group.\nThe proportion of children who develop peanut allergies is estimated to be 0.133 when peanut protein is avoided in infancy.\nThe average lifetime of mice on normal 85kCal diets is estimated to be 32.7 months.\nThe 57 mice on the normal 85kCal diet lived 32.7 months on average.\nThe relative risk of a CHD event in the high trait anger group compared with the low trait anger group was 2.5.\nThe relative risk of a CHD event among adults with high trait anger compared with adults with low trait anger is estimated to be 2.5."
  },
  {
    "objectID": "content/week4-inference.html#random-sampling",
    "href": "content/week4-inference.html#random-sampling",
    "title": "Introduction to inference",
    "section": "Random sampling",
    "text": "Random sampling\n\nSampling establishes the link (or lack thereof) between a sample and a population.\n\n\n\n\n\nIn a simple random sample, units are chosen in such a way that every individual in the population has an equal probability of inclusion. For the SRS:\n\nsample statistics mirror population statistics (sample is representative)\nsampling variability depends only on population variability and sample size"
  },
  {
    "objectID": "content/week4-inference.html#population-models",
    "href": "content/week4-inference.html#population-models",
    "title": "Introduction to inference",
    "section": "Population models",
    "text": "Population models\n\nInference consists in using statistics of a random sample to ascertain population statistics under a population model.\n\nA population model represents the distribution of values you’d see if you measured every individual in the study population. We think of the sample values as a random draw.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Density is an alternative scale to frequency that is independent of population size.)"
  },
  {
    "objectID": "content/week4-inference.html#point-estimates",
    "href": "content/week4-inference.html#point-estimates",
    "title": "Introduction to inference",
    "section": "Point estimates",
    "text": "Point estimates\n\nSample statistics, viewed as guesses for the values of population statistics, are called ‘point estimates’.\n\nWe’ll focus on inferences involving the following:\n\n\n\nPopulation statistic\nParameter\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)"
  },
  {
    "objectID": "content/week4-inference.html#a-difficulty",
    "href": "content/week4-inference.html#a-difficulty",
    "title": "Introduction to inference",
    "section": "A difficulty",
    "text": "A difficulty\n\nDifferent samples yield different estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample means:\n\n\n\n\n\n\n\n\n\nsample.1\nsample.2\n\n\n\n\n5.093\n5.136\n\n\n\n\n\n\nestimates are close but not identical\nthe population mean can’t be both 5.093 and 5.136\nprobably neither estimate is exactly correct\n\nEstimation error and sample-to-sample variability are inherent to point estimation."
  },
  {
    "objectID": "content/week4-inference.html#simulating-sampling-variability",
    "href": "content/week4-inference.html#simulating-sampling-variability",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\n\n\n\n\n\n\n\n\n\n\n\n\nThese are 20 random samples with the sample mean indicated by the dashed line and the population distribution and mean overlaid in red.\n\nsample size \\(n = 20\\)\nfrequency distributions differ a lot\nsample means differ some\n\nWe can actually measure this variability!"
  },
  {
    "objectID": "content/week4-inference.html#simulating-sampling-variability-1",
    "href": "content/week4-inference.html#simulating-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\nIf we had means calculated from a much larger number of samples, we could make a frequency distribution for the values of the sample mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample\n1\n2\n\\(\\cdots\\)\n10,000\n\n\nmean\n4.957\n5.039\n\\(\\cdots\\)\n5.24\n\n\n\n\nWe could then use the usual measures of center and spread to characterize the distribution of sample means.\n\nmean of \\(\\bar{x}\\): 5.0373228\nstandard deviation of \\(\\bar{x}\\): 0.2387869\n\n\nAcross 10,000 random samples of size 20, the typical sample mean was 5.04 and the root average squared distance of the sample mean from its typical value was 0.239."
  },
  {
    "objectID": "content/week4-inference.html#sampling-distributions",
    "href": "content/week4-inference.html#sampling-distributions",
    "title": "Introduction to inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nWhat we are simulating is known as a sampling distribution: the frequency of values of a statistic across all possible random samples.\n\n\n\n\n\n\n\n\n\n\n\n\nProvided data are from a random sample, the sample mean \\(\\bar{x}\\) has a sampling distribution with\n\nmean \\(\\color{red}{\\mu}\\) (population mean)\nstandard deviation \\(\\color{red}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\nregardless of its exact form.\n\n\nIn other words, across all random samples of a fixed size…\n\nThe average value of the sample mean is the population mean.\nThe average squared error (sample mean - population mean)\\(^2\\) is \\(\\frac{\\sigma^2}{n}\\)"
  },
  {
    "objectID": "content/week4-inference.html#effect-of-sample-size",
    "href": "content/week4-inference.html#effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\) is inversely proportional to sample size.\n\n\n\n\n\n\n\n\n\n\n\n\nAs sample size increases…\n\naccuracy remains the same\nestimates get more precise\nskewness vanishes"
  },
  {
    "objectID": "content/week4-inference.html#measuring-sampling-variability",
    "href": "content/week4-inference.html#measuring-sampling-variability",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nIn practice \\(\\sigma\\) is not known so we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared error of the sample mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-inference.html#reporting-point-estimates",
    "href": "content/week4-inference.html#reporting-point-estimates",
    "title": "Introduction to inference",
    "section": "Reporting point estimates",
    "text": "Reporting point estimates\nIt is common style to report the value of a point estimate with a standard error given parenthetically.\n\n\nStatistics from full NHANES sample:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\n\n\n\n\n5.043\n1.075\n3179\n\n\n\n\n\n\n\nThe mean total cholesterol among the population is estimated to be 5.043 mmol/L (SE 0.019)\n\n\n\nThis style of report communicates:\n\nparameter of interest\nvalue of point estimate\nerror/variability of point estimate"
  },
  {
    "objectID": "content/week4-inference.html#interval-estimation",
    "href": "content/week4-inference.html#interval-estimation",
    "title": "Introduction to inference",
    "section": "Interval estimation",
    "text": "Interval estimation\n\nAn interval estimate is a range of plausible values for a population parameter.\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.043 \\pm 2\\times 0.0191 = (5.005, 5.081)\\]\n\nIn R:\n\navg.totchol &lt;- mean(totchol)\nse.totchol &lt;- sd(totchol)/sqrt(length(totchol))\navg.totchol + c(-2, 2)*se.totchol\n\n[1] 5.004817 5.081059\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-intervals.html#the-t-model-1",
    "href": "content/week4-intervals.html#the-t-model-1",
    "title": "Confidence intervals",
    "section": "The \\(t\\) model",
    "text": "The \\(t\\) model\nWe’re actually using \\(\\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\\) to construct intervals, because we don’t know \\(\\sigma\\).\nThese deviations are better approximated by a \\(t\\) model, which adjusts the normal model for the extra uncertainty that comes from estimating the standard deviation.\n\n\nThe difference between models depends mainly on sample size:\n\nbehaves almost exactly the same for moderate to large samples\nlarger deviations from center for small samples\nleads to larger multipliers for computing margin of error\n\n\n\n\n\nComparison of \\(t\\) model with normal model for various degrees of freedom."
  },
  {
    "objectID": "content/week4-intervals.html#model-interpretation",
    "href": "content/week4-intervals.html#model-interpretation",
    "title": "Confidence intervals",
    "section": "Model interpretation",
    "text": "Model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\n50% of samples give negative values of \\(T\\)\n\n\npt(0, df = 20 - 1) # area less than 0\n\n[1] 0.5"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation",
    "href": "content/week4-intervals.html#t-model-interpretation",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 50% of samples, \\(T &lt; 0\\)\n\n\n# area less than 0\npt(0, df = 20 - 1) \n\n[1] 0.5\n\n\n\nwritten as \\(P(T &lt; 0) = 0.5\\)"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-1",
    "href": "content/week4-intervals.html#t-model-interpretation-1",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 83.5% of samples, \\(T &lt; 1\\)\n\n\n# area less than 1\npt(1, df = 20 - 1) \n\n[1] 0.8350616\n\n\n\nwritten as \\(P(T &lt; 1) = 0.835\\)"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-2",
    "href": "content/week4-intervals.html#t-model-interpretation-2",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 97% of samples, \\(T &lt; 2\\)\n\n\n# area less than 2\npt(2, df = 20 - 1) \n\n[1] 0.969999\n\n\n\nwritten as \\(P(T &lt; 2) = 0.97\\)"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-3",
    "href": "content/week4-intervals.html#t-model-interpretation-3",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 3% of samples, \\(T &gt; 2\\)\n\n\n# area greater than 2\npt(2, df = 20 - 1, lower.tail = F) \n\n[1] 0.03000102\n\n\n\nnotice: \\[\n\\begin{align*}\nP(T &gt; 2) &= 1 - P(T &lt; 2) \\\\\n(0.03) &= 1 - (0.97)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-4",
    "href": "content/week4-intervals.html#t-model-interpretation-4",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 13.5% of samples, \\(1 &lt; T &lt; 2\\)\n\n\n# area between 1 and 2\npt(2, df = 20 - 1) - pt(1, df = 20 - 1) \n\n[1] 0.1349374\n\n\n\nnotice: \\[\n\\begin{align*}\nP(1 &lt; T &lt; 2) &= P(T &lt; 2) - P(T &lt; 1) \\\\\n(0.135) &= (0.97) - (0.835)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-interpretation-5",
    "href": "content/week4-intervals.html#t-model-interpretation-5",
    "title": "Confidence intervals",
    "section": "\\(t\\) model interpretation",
    "text": "\\(t\\) model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\nFor example:\n\nfor 94% of samples, \\(-2 &lt; T &lt; 2\\)\n\n\n# area between 1 and 2\npt(2, df = 20 - 1) - pt(-2, df = 20 - 1) \n\n[1] 0.939998\n\n\n\nwritten \\(P(-2 &lt; T &lt; 2) = 0.94\\)"
  },
  {
    "objectID": "content/week4-intervals.html#revisiting-intervals",
    "href": "content/week4-intervals.html#revisiting-intervals",
    "title": "Confidence intervals",
    "section": "Revisiting intervals",
    "text": "Revisiting intervals\nSo where did that 2 come from in the margin of error for our interval estimate?\n\\[\n\\bar{x} \\pm \\color{blue}{2}\\times SE(\\bar{x})\n\\]\nWell:\n\\[\n\\begin{align*}\n0.94 &= P(-\\color{blue}{2} &lt; T &lt; \\color{blue}{2}) \\\\\n&= P\\left(-\\color{blue}{2} &lt; \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}} &lt; \\color{blue}{2}\\right) \\\\\n&= P(\\underbrace{\\bar{x} - \\color{blue}{2}\\times SE(\\bar{x}) &lt; \\mu &lt; \\bar{x} + \\color{blue}{2}\\times SE(\\bar{x})}_{\\text{interval covers population mean}})\n\\end{align*}\n\\] So for 94% of all random samples, the interval covers the population mean."
  },
  {
    "objectID": "content/week4-intervals.html#a-closer-look-at-interval-construction",
    "href": "content/week4-intervals.html#a-closer-look-at-interval-construction",
    "title": "Confidence intervals",
    "section": "A closer look at interval construction",
    "text": "A closer look at interval construction\nSo where did that 2 come from in the margin of error for our interval estimate?\n\\[\n\\bar{x} \\pm \\color{blue}{2}\\times SE(\\bar{x})\n\\]\nWell:\n\n\n\\[\n\\begin{align*}\n0.94 &= P(-\\color{blue}{2} &lt; T &lt; \\color{blue}{2}) \\\\\n&= P\\left(-\\color{blue}{2} &lt; \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}} &lt; \\color{blue}{2}\\right) \\\\\n&= P(\\underbrace{\\bar{x} - \\color{blue}{2}\\times SE(\\bar{x}) &lt; \\mu &lt; \\bar{x} + \\color{blue}{2}\\times SE(\\bar{x})}_{\\text{interval covers population mean}})\n\\end{align*}\n\\]\n\n\nFor 94% of all random samples, the interval covers the population mean.\n\n\n\nSo the number 2 determines the proportion of samples for which the interval covers the mean, known as its coverage."
  },
  {
    "objectID": "content/week4-intervals.html#coverage",
    "href": "content/week4-intervals.html#coverage",
    "title": "Confidence intervals",
    "section": "Coverage",
    "text": "Coverage\nConsider a slightly more general expression for an interval for the mean:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nThe value of \\(c\\) determines the coverage rate, or simply “coverage”.\nA common empirical rule:\n\n\\(c = 1 \\longrightarrow\\) approximately 68% coverage\n\\(c = 2 \\longrightarrow\\) approximately 95% coverage\n\\(c = 3 \\longrightarrow\\) approximately 99.7% coverage"
  },
  {
    "objectID": "content/week4-intervals.html#effect-of-sample-size",
    "href": "content/week4-intervals.html#effect-of-sample-size",
    "title": "Confidence intervals",
    "section": "Effect of sample size",
    "text": "Effect of sample size\n\nThe sample size determines the exact shape of the \\(t\\) model through its ‘degrees of freedom’ \\(n - 1\\). This changes the areas slightly.\n\nThe exact coverage quickly converges to just over 95% as the sample size increases.\n\n\n\n\n\n\n\n\n\n\n\nn\ncoverage\n\n\n\n\n4\n0.8607\n\n\n8\n0.9144\n\n\n16\n0.9361\n\n\n32\n0.9457\n\n\n64\n0.9502\n\n\n128\n0.9524\n\n\n256\n0.9534"
  },
  {
    "objectID": "content/week4-intervals.html#empirical-rule",
    "href": "content/week4-intervals.html#empirical-rule",
    "title": "Confidence intervals",
    "section": "Empirical rule",
    "text": "Empirical rule\nConsider a slightly more general expression for an interval for the mean:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nThe value of \\(c\\) determines the coverage.\n\nlarger \\(c\\) \\(\\longrightarrow\\) higher coverage\nsmaller \\(c\\) \\(\\longrightarrow\\) lower coverage\n\nThe so-called “empirical rule” is that:\n\n\\(c = 1 \\longrightarrow\\) approximately 68% coverage\n\\(c = 2 \\longrightarrow\\) approximately 95% coverage\n\\(c = 3 \\longrightarrow\\) approximately 99.7% coverage"
  },
  {
    "objectID": "content/week4-intervals.html#effect-of-sample-size-on-coverage",
    "href": "content/week4-intervals.html#effect-of-sample-size-on-coverage",
    "title": "Confidence intervals",
    "section": "Effect of sample size on coverage",
    "text": "Effect of sample size on coverage\n\nThe sample size determines the exact shape of the \\(t\\) model through its ‘degrees of freedom’ \\(n - 1\\). This changes the areas slightly, but not by much.\n\nThe exact coverage rate, or simply “coverage”, quickly converges to just over 95%.\n\n\n\n\n\n\n\n\n\n\n\nn\ncoverage\n\n\n\n\n4\n0.8607\n\n\n8\n0.9144\n\n\n16\n0.9361\n\n\n32\n0.9457\n\n\n64\n0.9502\n\n\n128\n0.9524\n\n\n256\n0.9534"
  },
  {
    "objectID": "content/week4-intervals.html#changing-the-coverage",
    "href": "content/week4-intervals.html#changing-the-coverage",
    "title": "Confidence intervals",
    "section": "Changing the coverage",
    "text": "Changing the coverage\nConsider a slightly more general expression for an interval for the mean:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nThe number \\(c\\) is called a critical value. It determines the coverage.\n\nlarger \\(c\\) \\(\\longrightarrow\\) higher coverage\nsmaller \\(c\\) \\(\\longrightarrow\\) lower coverage\n\nThe so-called “empirical rule” is that:\n\n\\(c = 1 \\longrightarrow\\) approximately 68% coverage\n\\(c = 2 \\longrightarrow\\) approximately 95% coverage\n\\(c = 3 \\longrightarrow\\) approximately 99.7% coverage"
  },
  {
    "objectID": "content/week4-intervals.html#contrasting-coverage-with-precision",
    "href": "content/week4-intervals.html#contrasting-coverage-with-precision",
    "title": "Confidence intervals",
    "section": "Contrasting coverage with precision",
    "text": "Contrasting coverage with precision\n\nPrecision refers to how wide or narrow the interval is.\n\nPrecision depends on every component of the margin of error:\n\ncritical value used\nsample size\nvariability of values\n\nBy contrast, coverage depends only on the critical value used."
  },
  {
    "objectID": "content/week4-intervals.html#exact-coverage-using-t-quantiles",
    "href": "content/week4-intervals.html#exact-coverage-using-t-quantiles",
    "title": "Confidence intervals",
    "section": "Exact coverage using \\(t\\) quantiles",
    "text": "Exact coverage using \\(t\\) quantiles\nTo engineer an interval with a specific coverage, use the \\(p\\)th quantile where:\n\\[p = \\left[1 - \\left(\\frac{1 - \\text{coverage}}{2}\\right)\\right]\\] In R:\n\n# coverage 95% using t quantile\ncoverage &lt;- 0.95\nq.val &lt;- 1 - (1 - coverage)/2\ncrit.val &lt;- qt(q.val, df = 20 - 1)\ncrit.val\n\n[1] 2.093024\n\n\nThe effect of increasing/decreasing coverage on the quantile is:\n\nincrease coverage \\(\\longrightarrow\\) larger quantile \\(\\longrightarrow\\) wider interval\ndecrease coverage \\(\\longrightarrow\\) smaller quantile \\(\\longrightarrow\\) narrower interval"
  },
  {
    "objectID": "content/week4-intervals.html#t-model-quantiles",
    "href": "content/week4-intervals.html#t-model-quantiles",
    "title": "Confidence intervals",
    "section": "\\(t\\) model quantiles",
    "text": "\\(t\\) model quantiles\n\n\n\\[\nP(\\color{#FF6459}{-2 &lt; T &lt; 2}) = 1 - 2\\times P(\\color{blue}{T &gt; 2})\n\\]\nLook at how the areas add up so that: \\[\nP(\\color{blue}{T &gt; 2}) = 0.03\n\\] Moreover: \\[\nP(T &lt; 2) = 1 - 0.03 = 0.97\n\\]"
  },
  {
    "objectID": "content/week4-intervals.html#interpreting-critical-values",
    "href": "content/week4-intervals.html#interpreting-critical-values",
    "title": "Confidence intervals",
    "section": "Interpreting critical values",
    "text": "Interpreting critical values\n\n\n\\[\nP(\\color{#FF6459}{-2 &lt; T &lt; 2}) = 1 - 2\\times P(\\color{blue}{T &gt; 2})\n\\]\nLook at how the areas add up so that: \\[\nP(\\color{blue}{T &gt; 2}) = 0.03\n\\] Moreover: \\[\nP(T &lt; 2) = 1 - 0.03 = 0.97\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSo the critical value 2 is actually the 97th percentile of the sampling distribution of \\(T\\).\n\nalso called the 0.97 “quantile”\n(percentiles expressed in proportions are called quantiles)"
  },
  {
    "objectID": "content/week4-intervals.html#simulation-of-coverage",
    "href": "content/week4-intervals.html#simulation-of-coverage",
    "title": "Confidence intervals",
    "section": "Simulation of coverage",
    "text": "Simulation of coverage\n\n\nArtificially simulating a large number of intervals provides an empirical approximation of coverage.\n\nat right, 200 intervals\n94% cover the population mean (vertical dashed line)\npretty close to nominal coverage level 95%\n\nThis is also a handy way to remember the proper interpretation:\n\nIf I made a lot of intervals from independent samples, 95% of them would ‘get it right’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-intervals.html#confidence-intervals",
    "href": "content/week4-intervals.html#confidence-intervals",
    "title": "Confidence intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nInterval estimates constructed to achieve a specified coverage are called “confidence intervals”; the coverage is interpreted and reported as a “confidence level”.\n\n\n\n# ingredients\ncholesterol.mean &lt;- mean(cholesterol)\ncholesterol.sd &lt;- sd(cholesterol)\ncholesterol.n &lt;- length(cholesterol)\ncholesterol.se &lt;- cholesterol.sd/sqrt(cholesterol.n)\ncrit.val &lt;- qt(1 - (1 - 0.95)/2, df = cholesterol.n - 1)\n\n# interval\ncholesterol.mean + c(-1, 1)*crit.val*cholesterol.se\n\n[1] 5.005566 5.080310\n\n\n\n\nWith 95% confidence, the mean total cholesterol among U.S. adults is estimated to be between 5.0056 and 5.0803 mmol/L.\n\n\n\nThe general formula for a confidence interval for the population mean is\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\]\nwhere \\(c\\) is a critical value, obtained as a quantile of the \\(t_{n - 1}\\) model and chosen to ensure a specific coverage."
  },
  {
    "objectID": "content/week4-intervals.html#recap",
    "href": "content/week4-intervals.html#recap",
    "title": "Confidence intervals",
    "section": "Recap",
    "text": "Recap\nThe “common” interval estimate for the mean is actually an approximate 95% confidence interval:\n\\[\n\\bar{x} \\pm 2 \\times SE(\\bar{x})\n\\]\n\ncaptures the population mean \\(\\mu\\) for roughly 95% of random samples\nreplacing 2 with a \\(t_{n - 1}\\) quantile allows the analyst to adjust coverage\nthe \\(t_{n - 1}\\) model is an approximation for the sampling distribution of \\(\\frac{\\bar{x} - \\mu}{SE(\\bar{x})}\\)\n\napproximation improves with increasing sample size or symmetry\nusually good quality except in “extreme” situations\n\n\nInterval interpretation:\n\nWith [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units]."
  },
  {
    "objectID": "content/lab5-intervals.html",
    "href": "content/lab5-intervals.html",
    "title": "Lab 5: Confidence intervals",
    "section": "",
    "text": "The objective of this lab is to learn to compute confidence intervals for a population mean, and more specifically, to learn to adjust interval coverage by calculating appropriate critical values. Since you already learned to calculate an interval in the last lab, the basic mechanics of the arithmetic are familiar.\nWe’ll use data from a sample of 100 births in North Carolina in 2004. To change things up a little, the data is stored as a .csv file (not an .RData file). If you were to download and open this file on your computer, it would likely appear as a spreadsheet (try if you’re curious). Read in the data using the command below.\n\n# read in data and preview\nncbirths &lt;- read_csv('data/ncbirths.csv')\nhead(ncbirths)\n\n# A tibble: 6 × 4\n  mother.age weeks birth.weight sex   \n       &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt; \n1         36    39         7.69 male  \n2         35    40         8.88 male  \n3         40    40         9    female\n4         37    40         7.94 male  \n5         35    28         1.63 female\n6         25    40         8.75 female\n\n\nRecall that the general formula for an interval is:\n\\[\n\\bar{x} \\pm c\\times SE(\\bar{x})\n\\] Throughout this lab, you’ll manipulate the coverage by obtaining different values of the critical value \\(c\\). We’ll start the back-of-the-envelope approach following the empirical rule.\n\nIntervals using the empirical rule\nThe empirical rule allows us to construct intervals using whole number multiples of the standard error and obtain the following approximate coverages:\n\n\\(c = 1\\) gives 68% coverage\n\\(c = 2\\) gives 95% coverage\n\\(c = 3\\) gives 99.7% coverage\n\nAn approximate 95% confidence interval for the mean birth weight (lbs) in NC in 2004 is:\n\n# retrieve variable of interest\nbweight &lt;- ncbirths$birth.weight\n\n# interval ingredients\nbweight.mean &lt;- mean(bweight)\nbweight.sd &lt;- sd(bweight)\nbweight.n &lt;- length(bweight)\n\n# standard error\nbweight.se &lt;- bweight.sd/sqrt(bweight.n)\n\n# 95% interval using empirical rule\nbweight.mean + c(-2, 2)*bweight.se\n\n[1] 6.89267 7.46633\n\n\nFollowing class discussion, we’d interpret this as follows:\n\nWith 95% confidence, the mean birth weight of babies born in North Carolina in 2004 is estimated to be between 6.893 and 7.466 lbs.\n\nTo compute a 68% interval, we need only change the critical value. All of the above remains the same except the last command, which we change to:\n\n# 68% interval using empirical rule\nbweight.mean + c(-1, 1)*bweight.se\n\n[1] 7.036085 7.322915\n\n\nNotice that the interval got narrower: a more precise estimate can be given at a reduced coverage rate (which of course means the estimate is wrong more often).\n\nWith 68% confidence, the mean birth weight of babies born in North Carolina in 2004 is estimated to be between 7.036 and 7.323 lbs.\n\n\n\n\n\n\n\nYour turn 1\n\n\n\nCalculate and interpret a 99.7% confidence interval for the mean number of weeks at birth.\n\n# retrieve variable of interest (no. weeks at birth)\n\n# interval ingredients\n\n# standard error\n\n# 99.7% interval for mean number of weeks at birth using empirical rule\n\n\n\nBecause we’re only changing the critical value here, let’s save some work and write a simple function to calculate an interval from a vector of values and a critical value. (You don’t need to understand the syntax or be able to write functions in R, as this is a programming technique, but it may interest you to see how it can be done.)\n\n# run this before continuing, but ignore unless interested\nmake_ci &lt;- function(vec, cval){\n  vec.mean &lt;- mean(vec)\n  vec.mean.se &lt;- sd(vec)/sqrt(length(vec) - 1)\n  interval &lt;- vec.mean + c(-1, 1)*cval*vec.mean.se \n  names(interval) &lt;- c('lwr', 'upr')\n  return(interval)\n}\n\nWe can use this function to compute an interval a bit more efficiently. Check that the following give the same intervals as obtained above using fully manual calculations.\n\n# 95% interval\nmake_ci(bweight, cval = 2)\n\n     lwr      upr \n6.891225 7.467775 \n\n# 68% interval\nmake_ci(bweight, cval = 1)\n\n     lwr      upr \n7.035362 7.323638 \n\n\nTry it yourself to get the hang of using this function.\n\n\n\n\n\n\nYour turn 2\n\n\n\nUse make_ci(...) to compute 95% and 99.7% confidence intervals for the mean number of weeks at birth.\n\n# 99.7% interval for mean number of weeks at birth, using make_ci(...)\n\n# 95% interval for mean number of weeks at birth, using make_ci(...)\n\n\n\n\n\nIntervals using \\(t\\) critical values\nIf you want to construct an interval with a coverage other than 68%, 95%, or 99.7%, you’ll need to use a different critical value. Instead of a whole number, you’ll need the \\(p\\)th quantile from the \\(t_{n-1}\\) model where:\n\\[p = \\left[1 - \\left(\\frac{1 - \\text{coverage}}{2}\\right)\\right]\\] This is perhaps a little more complex than it looks. You could probably determine which quantile to use in your head – the quantile you want is just the midpoint between your coverage level and 1. Consider the following examples:\n\nfor a 95% interval, use \\(p = 0.975\\)\nfor an 80% interval, use \\(p = 0.9\\)\nfor a 99% interval, use \\(p = 0.995\\)\n\n\n\n\n\n\n\nYour turn 3\n\n\n\nWhich quantile would you use for…\n\nA 96% confidence interval?\nAn 85% confidence interval?\nA 98% confidence interval?\n\n\n\n\nCalculating quantiles\nThe qt(...) function in R will calculate quantiles for you. It takes two arguments: which quantile you want (\\(p\\)) and the degrees of freedom for the \\(t_{n - 1}\\) model. The degrees of freedom is one less than the sample size in this case (\\(n - 1\\)). The following commands illustrate the calculation.\n\n# for 80% interval from n = 15 observations, use this quantile\nqt(p = 0.9, df = 14)\n\n[1] 1.34503\n\n# for a 92% interval from n = 30 observations, use this quantile\nqt(p = 0.96, df = 29)\n\n[1] 1.814238\n\n\n\n\n\n\n\n\nYour turn 4\n\n\n\nCalculate \\(t\\) quantiles for the following scenarios:\n\n90% interval from 27 observations\n95% interval from 18 observations\n99% interval from 51 observations\n\n\n# quantile for a 90% interval from 27 observations\n\n# quantile for a 95% interval from 18 observations\n\n# quantile for a 99% interval from 51 observations\n\n\n\nThese quantiles are the critical values you’d use to construct an interval with the specified coverage and number of observations.\n\n\nConstructing intervals\nIf we want a confidence interval for the mean with a specific coverage, first determine which quantile is needed as above and compute it, and then construct the interval as usual using that quantile as the critical value.\nFor example, if we want confidence intervals for the mean birth weight:\n\n# 98% ci for mean birth weight\ncrit.val &lt;- qt(p = 0.99, df = 99)\nmake_ci(bweight, cval = crit.val)\n\n     lwr      upr \n6.838671 7.520329 \n\n# 95% ci for mean birth weight\ncrit.val &lt;- qt(p = 0.975, df = 99)\nmake_ci(bweight, cval = crit.val)\n\n     lwr      upr \n6.893499 7.465501 \n\n# 90% ci for mean birth weight\ncrit.val &lt;- qt(p = 0.95, df = 99)\nmake_ci(bweight, cval = crit.val)\n\n     lwr      upr \n6.940175 7.418825 \n\n\nAs a matter of interest, note that the critical value for the 95% interval is 1.9842. Technically, this is the value that provides an interval with 95% coverage; the approximation of 2 provided by the empirical rule is just that – an approximation.\n\n\n\n\n\n\nYour turn 5\n\n\n\nConstruct and interpret a 99% confidence interval for the mean number of weeks at birth.\n\n# 99% ci for mean number of weeks at birth\n\n\n\nNow that you have a sense of the critical value calculation, it’s helpful to remind yourself how to make the interval fully from scratch.\n\n\n\n\n\n\nYour turn 6\n\n\n\nRepeat the previous calculation, but without using the make_ci(...) function. You should obtain exactly the same numerical result.\n\n## repeat last calculation but fully 'by hand'\n\n# point estimate and standard error\n\n# critical value\n\n# interval\n\n\n\n\n\n\nWorking backwards to determine coverage\nNow let’s try doing the above backwards. If you’re given a confidence interval and you know the summary statistics, you can figure out the interval coverage by solving for the critical value and using the pt(...) function. Really, you only need to know the standard error (or sample size and standard deviation) to do this.\nFirst, find the margin of error by taking half the interval width.\n\\[\n\\text{margin of error} = \\frac{\\text{upr} - \\text{lwr}}{2}\n\\]\nThen, divide by the standard error to solve for the critical value:\n\\[\nc = \\frac{\\text{margin of error}}{SE(\\bar{x})}\n\\] Lastly, find the coverage as the area of the sampling distribution below the critical value: \\[\n\\text{coverage} = P(T &lt; c)\n\\]\nIn R:\n\n# example interval for mean birth weight\nbweight.ci &lt;- c(7.030077, 7.328923)\n\n# margin of error (half the width)\nbweight.ci.me &lt;- diff(bweight.ci)/2\n\n# divide out standard error to get critical value\ncrit.val &lt;- bweight.ci.me/bweight.se\n\n# coverage\npt(q = crit.val, df = bweight.n - 1)\n\n[1] 0.85\n\n\nTake a moment to align the R commands with the calculations shown above and check to make sure you see how this is consistent with the way we formed the interval in the first place. Then try it on your own.\n\n\n\n\n\n\nYour turn 7\n\n\n\nDetermine the coverage for the interval below for the mean number of weeks at birth.\n\n# interval for mean number of weeks at birth\nbweeks.ci &lt;- c(37.79485, 39.30515)\n\n# margin of error (half the width)\n\n# divide out standard error to get critical value\n\n# coverage"
  },
  {
    "objectID": "content/week5-hypothesis.html#todays-agenda",
    "href": "content/week5-hypothesis.html#todays-agenda",
    "title": "Introduction to hypothesis testing",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nLoose end: working backwards to determine interval coverage\n[lecture] the \\(t\\)-test for a population mean\n[lab] computing test statistics, critical values, and \\(p\\)-values"
  },
  {
    "objectID": "content/week5-hypothesis.html#ddt-data",
    "href": "content/week5-hypothesis.html#ddt-data",
    "title": "Hypothesis testing",
    "section": "DDT data",
    "text": "DDT data\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm). Each measurement was taken by a different laboratory.\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu \\leq 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]"
  },
  {
    "objectID": "content/week5-hypothesis.html#hypothesis-testing",
    "href": "content/week5-hypothesis.html#hypothesis-testing",
    "title": "Hypothesis testing",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nThis is an example of a hypothesis testing problem: we want to test the hypothesis that mean DDT in kale is within safe limits. Hypothesis testing is another form of statistical inference.\nThe general pattern for performing a hypothesis test is:\n\nFormulate the hypothesis to test in terms of the values of a population parameter.\nAssess the likelihood of the data under the hypothesis through use of a “test statistic”.\nConclude whether the data provide evidence favoring an alternative.\n\nToday we’ll cover each step in turn in the context of tests for a population mean."
  },
  {
    "objectID": "content/week5-hypothesis.html#formulating-hypotheses",
    "href": "content/week5-hypothesis.html#formulating-hypotheses",
    "title": "Hypothesis testing",
    "section": "1. Formulating hypotheses",
    "text": "1. Formulating hypotheses\nHypotheses cannot be tested in isolation, but must be considered relative to a specified alternative.\nTo articulate the hypotheses for a test, we need:\n\npopulation parameter of interest\nnull hypothesis \\(H_0\\): possible value(s) under the claim to be tested\nalternative hypothesis \\(H_A\\): possible value(s) if the claim is found to be false\n\nIn the context of the DDT example…\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\]"
  },
  {
    "objectID": "content/week5-hypothesis.html#test-statistic",
    "href": "content/week5-hypothesis.html#test-statistic",
    "title": "Hypothesis testing",
    "section": "2. Test statistic",
    "text": "2. Test statistic\nTest statistics are data summaries that:\n\ndepend on the null value of the population parameter\nhave a known sampling distribution\n\nFor a population mean, we use: \\[T = \\frac{\\bar{x} - \\mu_0}{s_x/\\sqrt{n}}\\]\nThis is well-described by a \\(t_{n - 1}\\) model when \\(\\mu = \\mu_0\\). It is useful for the test because:\n\nlarge (absolute) values of \\(T\\) are unlikely if \\(\\mu = \\mu_0\\)\nsmall (absolute) values of \\(T\\) are expected if \\(\\mu = \\mu_0\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#drawing-a-conclusion",
    "href": "content/week5-hypothesis.html#drawing-a-conclusion",
    "title": "Hypothesis testing",
    "section": "3. Drawing a conclusion",
    "text": "3. Drawing a conclusion\n\n\nIn the DDT example, \\(T\\) = 2.906. This favors \\(H_A\\), but by how much?\nAccording to the \\(t\\) model, less than 1% of samples would produce a result more favorable to \\(H_A\\).\n\n\n\n\n\n\n\n\n\n\nPoint estimate:\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n3.328\n0.1129\n\n\n\n\n\nTest statistic:\n\\[T = \\frac{\\bar{x} - \\mu_0}{SE(\\bar{x})} = \\]\n\nThis is strong evidence against the claim that the DDT level is 3ppm or less and in favor of the claim that the DDT level exceeds 3ppm."
  },
  {
    "objectID": "content/week5-hypothesis.html#recap-of-ddt-example",
    "href": "content/week5-hypothesis.html#recap-of-ddt-example",
    "title": "Hypothesis testing",
    "section": "Recap of DDT example",
    "text": "Recap of DDT example\n\nIs the mean DDT level in kale 3ppm or less?\n\n\n\nData are measurements of DDT levels in ppm from 15 labs.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n3.328\n0.4372\n0.1129\n\n\n\n\n\n\\(t_{14}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#another-example-sleep",
    "href": "content/week5-hypothesis.html#another-example-sleep",
    "title": "Hypothesis testing",
    "section": "Another example: sleep",
    "text": "Another example: sleep\n\nDoes the average U.S. adult sleep at least 7 hours per night?\n\n\n\nData are reported average hours of sleep per night from 135 NHANES respondents.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n6.896\n1.394\n0.12\n\n\n\n\n\n\\(t_{134}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#your-turn-body-temperatures",
    "href": "content/week5-hypothesis.html#your-turn-body-temperatures",
    "title": "Hypothesis testing",
    "section": "Your turn: body temperatures",
    "text": "Your turn: body temperatures\n\nIs mean body temperature actually 98.6 °F, or is it lower?\n\n\n\nData are 130 observations of body temperature (°F) derived from a JAMA study.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n98.25\n0.7332\n0.0643\n\n\n\n\n\n\\(t_{129}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#strength-of-evidence",
    "href": "content/week5-hypothesis.html#strength-of-evidence",
    "title": "Hypothesis testing",
    "section": "Strength of evidence",
    "text": "Strength of evidence\nThe result that 0.575% of samples would produce a test statistic more strongly favoring the alternative hypothesis is an example of a p-value:\n\nthe probability under \\(H_0\\) of obtaining a sample for which the test statistic is at least as favorable to \\(H_A\\) as the value actually observed\n\nIn other words, \\(p\\)-values assume the null hypothesis is true, and then ask, “what is the chance I’d obtain data at least as suggestive as what I have that the alternative is more likely than the null?”\n\nsmaller \\(p\\)-values: if \\(H_0\\) is true, equally or more favorable results are not expected often by chance\nlarger \\(p\\)-values: if \\(H_0\\) is true, equally or more favorable results are expected often by chance"
  },
  {
    "objectID": "content/week5-hypothesis.html#evidence-thresholds",
    "href": "content/week5-hypothesis.html#evidence-thresholds",
    "title": "Hypothesis testing",
    "section": "Evidence thresholds",
    "text": "Evidence thresholds\nIt remains to define an evidence threshold above which we decide to reject \\(H_0\\).\nA heuristic is to fix a significance level \\(\\alpha\\) and reject \\(H_0\\) whenever \\(p &lt; \\alpha\\).\n\nrepresents an evidence threshold\nconventionally, \\(\\alpha = 0.05\\)\ncontrols error rates\n\nImagine that indeed mean DDT in kale is 3ppm. Then 0.575% of samples produce test statistics at least as favorable to the alternative as what we saw in the study.\n\nso if we set the evidence threshold for rejecting \\(H_0\\) exactly here (\\(\\alpha = 0.00575\\)) we’ll be wrong 0.575% of the time\nif we set the evidence threshold lower (say \\(\\alpha = 0.01\\)) we’ll be wrong more than 0.575% of the time (in fact 1% of the time)"
  },
  {
    "objectID": "content/week5-hypothesis.html#interpreting-results",
    "href": "content/week5-hypothesis.html#interpreting-results",
    "title": "Introduction to hypothesis testing",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\nCalculations in R:\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# compute critical value for a 5% error tolerance\ncrit.val &lt;- qt(p = 0.975, df = 38)\ncrit.val\n\n[1] 2.024394\n\n# test decision\nabs(tstat) &gt; crit.val\n\n[1] FALSE\n\n# p-value\n2*pt(abs(tstat), df = 38, lower.tail = F)\n\n[1] 0.1920133\n\n\n\nConventional narrative summary style:\n\nThe data do not provide evidence that the mean body temperature differs from 98.6°F (T = -1.328 on 38 degrees of freedom, p = 0.192).\n\nConveys a lot of info succinctly:\n\ntest conclusion\nhypotheses tested\nnumber of standard errors from hypothesized value (\\(T\\))\nsample size (degrees of freedom + 1)\nstrength of evidence (\\(p\\)-value)"
  },
  {
    "objectID": "content/week5-hypothesis.html#composite-hypotheses",
    "href": "content/week5-hypothesis.html#composite-hypotheses",
    "title": "Hypothesis testing",
    "section": "Composite hypotheses",
    "text": "Composite hypotheses\nThe null hypothesis is a composite of values, so why did we choose just one (\\(\\mu_0 = 3\\)) to perform the test?\nAny null value farther from the alternative will produce stronger results.\n\n\n\n\n\n\n\n\nNull value\nTest statistic numerator\nProportion of samples more favorable to \\(H_A\\)\n\n\n\n\n\\(\\mu_0 = 3\\)\n0.328\n0.00575\n\n\n\\(\\mu_0 = 2.99\\)\n0.338\n0.00483\n\n\n\\(\\mu_0 = 2.95\\)\n0.378\n0.00239\n\n\n\nSo by using \\(\\mu_0 = 3\\), we are choosing the most conservative null value for the test."
  },
  {
    "objectID": "content/week5-hypothesis.html#components-of-a-test",
    "href": "content/week5-hypothesis.html#components-of-a-test",
    "title": "Introduction to hypothesis testing",
    "section": "Components of a test",
    "text": "Components of a test\n\n\n\n\n\n\n\n\nComponent\nExplanation\nExample\n\n\n\n\nPopulation parameter\nThe quantity of interest\nMean body temp \\(\\mu\\)\n\n\nNull hypothesis\nThe claim to be tested\n\\(\\mu = 98.6\\)\n\n\nAlternative hypothesis\nThe alternative claim\n\\(\\mu \\neq 98.6\\)\n\n\nTest statistic\nA function of the sample data and the hypothetical parameter value\n\\(T = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}} = -1.328\\)\n\n\nModel\nSampling distribution of the test statistic under \\(H_0\\)\n\\(t_{38}\\) model\n\n\n\\(p\\)-value\nProbability under \\(H_0\\) of obtaining a result at least as favorable to \\(H_A\\)\n19.2% of samples produce a test statistic at least as large\n\n\nDecision\nReject or fail to reject \\(H_0\\) in favor of \\(H_A\\)\nFail to reject\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-hypothesis.html#performing-tests-in-r",
    "href": "content/week5-hypothesis.html#performing-tests-in-r",
    "title": "Hypothesis testing",
    "section": "Performing tests in R",
    "text": "Performing tests in R\n\n\nInputs:\n\ndata vector\nnull value of parameter\nalternative hypothesis\n\nOutputs:\n\ntest statistic\ndegrees of freedom for \\(t\\) model\n\\(p\\)-value\nconfidence interval\npoint estimate\n\n\nt.test performs all calculations. Locate each input (1-3) and output (4-7) below:\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#directional-hypotheses",
    "href": "content/week5-hypothesis.html#directional-hypotheses",
    "title": "Hypothesis testing",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm). Each measurement was taken by a different laboratory.\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu \\leq 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]"
  },
  {
    "objectID": "content/week5-hypothesis.html#lab-t-tests-in-r",
    "href": "content/week5-hypothesis.html#lab-t-tests-in-r",
    "title": "Hypothesis testing",
    "section": "Lab: \\(t\\)-tests in R",
    "text": "Lab: \\(t\\)-tests in R\nOpen up lab6-hypotesting in the class workspace. The goals for this lab are:\n\nLearn how to implement \\(t\\) tests in R and interpret output\nPractice formulating and testing hypotheses from simple research questions\n(If time) Explore decision errors\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-hypothesis.html#from-last-time",
    "href": "content/week5-hypothesis.html#from-last-time",
    "title": "Hypothesis testing",
    "section": "From last time",
    "text": "From last time\n\n\nWe developed interval estimates based on the \\(t_{n - 1}\\) model for the sampling distribution of the statistic: \\[\nT = \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\]\n\n\\(T = 2\\) means we over-estimated the population mean by 2 standard errors\narea under the curve \\(=\\) proportion of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#body-temperatures",
    "href": "content/week5-hypothesis.html#body-temperatures",
    "title": "Introduction to hypothesis testing",
    "section": "Body temperatures",
    "text": "Body temperatures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\nse\n\n\n\n\n98.41\n0.9162\n39\n0.1467\n\n\n\n\n\n\nIs the true mean body temperature actually 98.6°F?\nSeems plausible given our data.\nBut what if the sample mean were instead…\n\n\n\n\\(\\bar{x}\\)\nconsistent with \\(\\mu = 98.6\\)?\n\n\n\n\n98.30\nprobably still yes\n\n\n98.15\nmaybe\n\n\n98.00\nhesitating\n\n\n97.85\nskeptical\n\n\n97.40\nunlikely\n\n\n\n\n\n\nIf the estimation error is “big enough” the hypothesis seems implausible."
  },
  {
    "objectID": "content/week5-hypothesis.html#an-intution",
    "href": "content/week5-hypothesis.html#an-intution",
    "title": "Hypothesis testing",
    "section": "An intution",
    "text": "An intution\n\nIf the estimation error from a hypothesized value is “big enough”, the hypothesis seems implausible."
  },
  {
    "objectID": "content/week5-hypothesis.html#statistical-hypotheses",
    "href": "content/week5-hypothesis.html#statistical-hypotheses",
    "title": "Hypothesis testing",
    "section": "Statistical hypotheses",
    "text": "Statistical hypotheses\n“The mean body temperature of adults 98.6°F” is an example of a statistical hypothesis: a definitive statement about the value of a population parameter.\n\\[H_0: \\mu = 98.6 \\qquad(\\text{population mean is 98.6°F})\\]\nAny hypothesis has an opposing or “alternative” hypothesis.\n\\[H_A: \\mu \\neq 98.6 \\qquad(\\text{population mean is not 98.6°F})\\]\n\nHypothesis testing means making a decision between a hypothesis and its alternative on the basis of a sample."
  },
  {
    "objectID": "content/week5-hypothesis.html#how-much-error-is-too-much",
    "href": "content/week5-hypothesis.html#how-much-error-is-too-much",
    "title": "Introduction to hypothesis testing",
    "section": "How much error is too much?",
    "text": "How much error is too much?\nConsider how many standard errors away from the hypothesized value we’d be:\n\n\n\n\\(\\bar{x}\\)\nestimation error\nno. SE’s\ninterpretation\n\n\n\n\n98.30\n-0.3\n2\ndouble the average error\n\n\n98.15\n-0.45\n3\ntriple the average error\n\n\n98.00\n-0.6\n4\nquadruple\n\n\n97.85\n-0.75\n5\nquintuple\n\n\n97.40\n-1.2\n8\noctuple!\n\n\n\nWe know from discussing confidence intervals that we’d estimate the mean temperature to be within about 2SE of the sample mean, and from interval coverage that:\n\nan error less than 2SE occurs for about 95% of samples\nan error greater than 2SE occurs for only about 5% of samples\n\nExactly how often would we see the error we did if the population mean is in fact 98.6°F?"
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model."
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-1",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-1",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-2",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-2",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate by more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-3",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-3",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate by more in 9.6% of samples\noverestimate by more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-4",
    "href": "content/week5-hypothesis.html#a-formal-test-using-the-t-model-4",
    "title": "Hypothesis testing",
    "section": "A formal test using the \\(t\\) model",
    "text": "A formal test using the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate by more in 9.6% of samples\noverestimate by more in 9.6% of samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’d see at least as much estimation error 19.2% of the time, assuming the hypothesis is true."
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model",
    "href": "content/week5-hypothesis.html#applying-the-t-model",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model."
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-1",
    "href": "content/week5-hypothesis.html#applying-the-t-model-1",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-2",
    "href": "content/week5-hypothesis.html#applying-the-t-model-2",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-3",
    "href": "content/week5-hypothesis.html#applying-the-t-model-3",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate more in 9.6% of samples\noverestimate more in 9.6% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#applying-the-t-model-4",
    "href": "content/week5-hypothesis.html#applying-the-t-model-4",
    "title": "Introduction to hypothesis testing",
    "section": "Applying the \\(t\\) model",
    "text": "Applying the \\(t\\) model\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nactual summary statistics give \\(T\\) = -1.328\nunderestimate more in 9.6% of samples\noverestimate more in 9.6% of samples\n\n\n\n\n\n\n\n\n\n\n\n\\[P(|T| &gt; 1.328) = 0.192\\]\n\n\n\nWe’d see at least as much (absolute) estimation error 19.2% of the time, assuming the hypothesis is true. So this amount of error isn’t surprising."
  },
  {
    "objectID": "content/week5-hypothesis.html#a-hypothetical-scenario",
    "href": "content/week5-hypothesis.html#a-hypothetical-scenario",
    "title": "Hypothesis testing",
    "section": "A hypothetical scenario",
    "text": "A hypothetical scenario\n\n\nNow if instead we had a sample with \\(\\bar{x} = 98.2\\) but all other summary statistics were the same, then if the population mean is in fact 98.6°F\n\n\\(T\\) = -2.726\nunderestimate by more in 0.48% of samples\noverestimate by more in 0.48% of samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’d see at least as much estimation error 0.96% of the time, assuming the hypothesis is true. So the hypothesis seems plausible given our data."
  },
  {
    "objectID": "content/week5-hypothesis.html#a-more-extreme-scenario",
    "href": "content/week5-hypothesis.html#a-more-extreme-scenario",
    "title": "Introduction to hypothesis testing",
    "section": "A more extreme scenario",
    "text": "A more extreme scenario\n\n\nIf the population mean is in fact 98.6°F then \\[\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\] has a sampling distribution that is well-approximated by a \\(t_{39 - 1}\\) model.\n\nsuppose instead \\(\\bar{x} = 98.2\\) so \\(T\\) = -2.726\nunderestimate more in 0.48% of samples\noverestimate more in 0.48% of samples\n\n\n\n\n\n\n\n\n\n\n\n\\[P(|T| &gt; 2.726) = 0.0096\\]\n\n\n\nWe’d see at least as much estimation error only 0.96% of the time. So if the hypothesis were true, this sample would be really unusual."
  },
  {
    "objectID": "content/week5-hypothesis.html#decisions-decisions",
    "href": "content/week5-hypothesis.html#decisions-decisions",
    "title": "Introduction to hypothesis testing",
    "section": "Decisions, decisions",
    "text": "Decisions, decisions\n\nWhat would happen if we decided that \\(T = -1.328\\) was unusual?\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\n\n\n98.2\n0.1467\n-2.726\n0.009632\n\n\n\n\n\nSuppose we drew a line at 20%. Then if in fact \\(\\mu = 98.6\\):\n\nerrors in the ‘reject’ regime occur by chance 20% of the time\nso we’ll reach the wrong conclusion for 1 in 5 samples\n\nThis error rate is too high."
  },
  {
    "objectID": "content/week5-hypothesis.html#formalizing-a-test-for-the-mean",
    "href": "content/week5-hypothesis.html#formalizing-a-test-for-the-mean",
    "title": "Introduction to hypothesis testing",
    "section": "Formalizing a test for the mean",
    "text": "Formalizing a test for the mean\nA statistical hypothesis is a statement about a population parameter. For every hypothesis there is an opposing or “alternative” hypothesis.\nA hypothesis test is a procedure for deciding between a hypothesis and its alternative.\n\n\nWe just tested the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = 98.6 \\quad(\\text{\"null\" hypothesis}) \\\\\nH_A: &\\mu \\neq 98.6 \\quad(\\text{\"alternative\" hypothesis})\n\\end{cases}\n\\]\nOur decision was based on the “test statistic”:\n\\[\nT = \\frac{\\bar{x} - 98.6}{SE(\\bar{x})}\n\\]\nIf \\(H_0\\) is true, the sampling distribution of \\(T\\) is well-approximated by a \\(t_{n-1}\\) model.\n\nWe reject \\(H_0\\) if it entails that the estimation error is unusually large relative to the standard error.\n\n‘unusual’ determined by considering error rate\ntwo equivalent approaches:\n\ncritical values\n\\(p\\)-values"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-critical-value-approach",
    "href": "content/week5-hypothesis.html#the-critical-value-approach",
    "title": "Introduction to hypothesis testing",
    "section": "The critical value approach",
    "text": "The critical value approach\n\nReject \\(H_0\\) if \\(|T|\\) exceeds the \\(1 - \\frac{\\alpha}{2}\\) quantile of the \\(t_{n - 1}\\) model\n\n\n\nSteps:\n\nDecide on an error tolerance \\(\\alpha\\).\nFind the \\(1 - \\frac{\\alpha}{2}\\) quantile \\(q\\).\nReject if \\(|T| &gt; q\\).\n\n\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# compute critical value for a 5% error tolerance\ncrit.val &lt;- qt(p = 0.975, df = 38)\ncrit.val\n\n[1] 2.024394\n\n# compare\nabs(tstat) &gt; crit.val \n\n[1] FALSE\n\n\n\n\nRationale: if \\(H_0\\) is true…\n\n\\(|T|\\) will be smaller than the quantile for \\((1 - \\alpha)\\times 100\\)% of samples\nso using this rule you’ll only make a mistake \\(\\alpha\\times 100\\)% of the time"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-unusualness-approach",
    "href": "content/week5-hypothesis.html#the-unusualness-approach",
    "title": "Hypothesis testing",
    "section": "The ‘unusualness’ approach",
    "text": "The ‘unusualness’ approach\n\nReject \\(H_0\\) if the observed value of \\(T\\) occurs for less than \\(100\\times\\alpha\\)% of samples"
  },
  {
    "objectID": "content/week5-hypothesis.html#directional-test",
    "href": "content/week5-hypothesis.html#directional-test",
    "title": "Hypothesis testing",
    "section": "Directional test",
    "text": "Directional test\n\n\nIn the DDT example, \\(T = 2.905\\); if the mean is 3 we overestimated, but how unusual is the error?\nAccording to the \\(t\\) model, less than 1% of samples would produce an error of this magnitude or more.\n\n\n\n\n\n\n\n\n\n\nPoint estimate:\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n3.328\n0.1129\n\n\n\n\n\nTest statistic:\n\\[T = \\frac{\\bar{x} - 3}{SE(\\bar{x})} = \\frac{0.328}{0.1129} = 2.905\\]\n\nThis is strong evidence against the claim that the DDT level is 3ppm or less and in favor of the claim that the DDT level exceeds 3ppm."
  },
  {
    "objectID": "content/week5-hypothesis.html#directional-hypotheses-1",
    "href": "content/week5-hypothesis.html#directional-hypotheses-1",
    "title": "Hypothesis testing",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nTests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.\n\n\n\n\n\n\n\n\n\nTest type\nNull\nAlternative\nFavors alternative\n\n\n\n\nUpper-sided\n\\(\\mu \\leq \\mu_0\\)\n\\(\\mu &gt; \\mu_0\\)\npositive \\(T\\)\n\n\nLower-sided\n\\(\\mu \\geq \\mu_0\\)\n\\(\\mu &lt; \\mu_0\\)\nnegative \\(T\\)\n\n\nTwo-sided\n\\(\\mu = \\mu_0\\)\n\\(\\mu \\neq \\mu_0\\)\nlarge \\(|T|\\)\n\n\n\nThe direction of the test affects the \\(p\\)-value calculation (and thus decision), but won’t change the test statistic.\n\n# upper-sided\nt.test(ddt, mu = mu_0, alternative = 'greater')\n\n# lower-sided\nt.test(ddt, mu = mu_0, alternative = 'less')\n\n# two-sided (default)\nt.test(ddt, mu = mu_0, alternative = 'two.sided')\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-hypothesis.html#significance-conventions",
    "href": "content/week5-hypothesis.html#significance-conventions",
    "title": "Hypothesis testing",
    "section": "Significance conventions",
    "text": "Significance conventions\n\n\nConvention 1: statistical significance\n\n\\(p &lt; 0.05\\): reject \\(H_0\\)\n\\(p \\geq 0.05\\): fail to reject \\(H_0\\)\n\n\n“The data provide significant evidence at level \\(\\alpha\\) = 0.05 against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (T = -5.4548 on 129 degrees of freedom, p = .0000002411).”\n\n\nConvention 2: weight of evidence against \\(H_0\\)\n\n\\(p &lt; 0.01\\): strong evidence\n\\(0.01 \\leq p &lt; 0.05\\): moderate evidence\n\\(0.05 \\leq p &lt; 0.1\\): weak evidence\n\\(0.1 \\leq p\\): no evidence\n\n\n“The data provide strong evidence against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (T = -5.4548 on 129 degrees of freedom, p = .0000002411).”\n\n\n\nYou may use either convention to interpret test results."
  },
  {
    "objectID": "content/week5-hypothesis.html#interpreting-results-1",
    "href": "content/week5-hypothesis.html#interpreting-results-1",
    "title": "Introduction to hypothesis testing",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\nCalculations in R:\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# compute critical value for a 5% error tolerance\ncrit.val &lt;- qt(p = 0.975, df = 38)\ncrit.val\n\n[1] 2.024394\n\n# test decision\nabs(tstat) &gt; crit.val\n\n[1] FALSE\n\n# p-value\n2*pt(abs(tstat), df = 38, lower.tail = F)\n\n[1] 0.1920133\n\n\n\nConventional narrative summary style:\n\nThe data do not provide evidence that the mean body temperature differs from 98.6°F (T = -1.328 on 38 degrees of freedom, p = 0.192).\n\nConveys:\n\nconclusion\nhypotheses tested\nnumber of standard errors from hypothesized value (\\(T\\))\nsample size (degrees of freedom + 1)\nstrength of evidence (\\(p\\)-value)"
  },
  {
    "objectID": "content/week5-hypothesis.html#narrative-summary",
    "href": "content/week5-hypothesis.html#narrative-summary",
    "title": "Hypothesis testing",
    "section": "Narrative summary",
    "text": "Narrative summary\n\n\n\n# retrieve body temp variable\nbody.temp &lt;- temps$body.temp\n\n# perform t test\nt.test(body.temp, mu = 98.6)\n\n\n    One Sample t-test\n\ndata:  body.temp\nt = -1.3283, df = 38, p-value = 0.192\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.10813 98.70213\nsample estimates:\nmean of x \n 98.40513 \n\n\nLocate above in the output:\n\nvalue of test statistic \\(T\\)\ndegrees of freedom for \\(t_{n - 1}\\) model\n\\(p\\)-value\npoint and interval estimates for \\(\\mu\\)\n\n\n\nThe data do not provide evidence that the mean body temperature differs from 98.6°F (T = -1.328 on 38 degrees of freedom, p = 0.192). With 95% confidence, the mean body temperature is estimated to be between 98.11°F and 98.70°F, with a point estimate of 98.41°F (SE 0.1467)."
  },
  {
    "objectID": "content/week5-hypothesis.html#the-logic-of-a-hypothesis-test",
    "href": "content/week5-hypothesis.html#the-logic-of-a-hypothesis-test",
    "title": "Hypothesis testing",
    "section": "The logic of a hypothesis test",
    "text": "The logic of a hypothesis test\nWhat if we want to decide whether mean body temp is 98.6°F?\nA decision can be based on whether the estimation error would be unusually large (i.e., infrequent) if the hypothesis were true.\n\nunusually large error \\(\\longrightarrow\\) hypothesis is implausible \\(\\longrightarrow\\) can be rejected\nnot unusually large error \\(\\longrightarrow\\) hypothesis is plausible \\(\\longrightarrow\\) can’t be rejected\n\nEach scenario we considered supports a different decision"
  },
  {
    "objectID": "content/week5-hypothesis.html#evaluating-a-hypothesis",
    "href": "content/week5-hypothesis.html#evaluating-a-hypothesis",
    "title": "Hypothesis testing",
    "section": "Evaluating a hypothesis",
    "text": "Evaluating a hypothesis\nWhat if we want to decide whether mean body temp is 98.6°F?\nTo evaluate the hypothesis that \\(\\mu = 98.6\\), we assume it is true and then consider whether the estimation error would be unusually large according to the \\(t\\) model:\n\nunusually large error \\(\\longrightarrow\\) hypothesis is implausible \\(\\longrightarrow\\) can be rejected\nnot unusually large error \\(\\longrightarrow\\) hypothesis is plausible \\(\\longrightarrow\\) can’t be rejected\n\nEach scenario we considered supports a different decision\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\nevaluation\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\nnot unusual\n\n\n98.2\n0.1467\n-2.726\n0.009632\nunusual"
  },
  {
    "objectID": "content/week5-hypothesis.html#evaluating-the-hypothesis",
    "href": "content/week5-hypothesis.html#evaluating-the-hypothesis",
    "title": "Introduction to hypothesis testing",
    "section": "Evaluating the hypothesis",
    "text": "Evaluating the hypothesis\nTo evaluate the hypothesis that \\(\\mu = 98.6\\), we assume it is true and then consider whether the estimation error would be unusually large purely by chance according to the \\(t\\) model:\n\nunusually large error \\(\\longrightarrow\\) hypothesis is implausible \\(\\longrightarrow\\) can be rejected\nnot unusually large error \\(\\longrightarrow\\) hypothesis is plausible \\(\\longrightarrow\\) can’t be rejected\n\nWe just made these assessments:\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\nevaluation\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\nnot unusual\n\n\n98.2\n0.1467\n-2.726\n0.009632\nunusual\n\n\n\n\n\nSeems reasonable, but why exactly isn’t 19.2% of the time ‘unusual’?"
  },
  {
    "objectID": "content/week5-hypothesis.html#decisions-decisions-1",
    "href": "content/week5-hypothesis.html#decisions-decisions-1",
    "title": "Hypothesis testing",
    "section": "Decisions, decisions",
    "text": "Decisions, decisions\n\nWhat would happen if we decided that \\(T = -2.726\\) was not unusual?\n\n\n\n\n\n\n\n\n\n\n\n\nsample.mean\nse\nt.stat\nhow.often\n\n\n\n\n98.41\n0.1467\n-1.328\n0.192\n\n\n98.2\n0.1467\n-2.726\n0.009632\n\n\n\n\n\nSuppose we drew a line at 0.5%. Then if in fact \\(\\mu = 98.6\\):\n\nerrors in the ‘reject’ regime occur by chance 0.5% of the time\nso we’ll reach the wrong conclusion for 1 in 200 samples\n\nThis error rate is good,"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-p-value-approach",
    "href": "content/week5-hypothesis.html#the-p-value-approach",
    "title": "Introduction to hypothesis testing",
    "section": "The \\(p\\)-value approach",
    "text": "The \\(p\\)-value approach\n\nReject \\(H_0\\) if \\(T\\) exceeds the observed value for less than \\(\\alpha\\times 100\\)% of samples: \\[2\\times P(T &gt; |T_\\text{obs}|) &lt; \\alpha\\]\n\n\n\nSteps:\n\nDecide on an error tolerance \\(\\alpha\\).\nCompute the proportion \\(p\\) of samples for which \\(T\\) exceeds observed value.\nReject if \\(p &lt; \\alpha\\).\n\n\n\n# compute test statistic\ntstat &lt;- (temp.mean - 98.6)/temp.mean.se\ntstat\n\n[1] -1.328265\n\n# proportion of samples where T exceeds observed value\np.val &lt;- 2*pt(abs(tstat), df = 38, lower.tail = F)\np.val\n\n[1] 0.1920133\n\n# decision with error rate controlled at 5%\np.val &lt; 0.05\n\n[1] FALSE\n\n\n\n\nRationale:\n\n\\(p\\)-value conveys exactly how unusual the test statistic is\n\\(p &lt; \\alpha\\) exactly when \\(|T| &gt; q\\), so this rule controls the error rate at \\(\\alpha\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#test-outcomes",
    "href": "content/week5-hypothesis.html#test-outcomes",
    "title": "Introduction to hypothesis testing",
    "section": "Test outcomes",
    "text": "Test outcomes\nThere are two possible findings for a test:\n\n[crosses decision threshold] reject \\(H_0\\) in favor of \\(H_A\\)\n[doesn’t cross decision threshold] fail to reject \\(H_0\\) in favor of \\(H_A\\)\n\nA reject decision is interpreted as:\n\nThe data provide evidence that… [against \\(H_0\\)/favoring \\(H_A\\)]\n\nA fail to reject decision is interpreted as:\n\nThe data do not provide evidence that… [against \\(H_0\\)/favoring \\(H_A\\)]"
  },
  {
    "objectID": "content/lab6-hypothesis.html",
    "href": "content/lab6-hypothesis.html",
    "title": "Lab 6: Hypothesis testing basics",
    "section": "",
    "text": "In class we discussed the \\(t\\) test for the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = \\mu_0 \\\\\nH_A: &\\mu \\neq \\mu_0\n\\end{cases}\n\\]\nThe objective of this lab is to learn to perform the basic calculations involved in this \\(t\\) test “by hand” (without the use of the function that we’ll apply later):\nWe’ll use the temps dataset to illustrate.\nlibrary(tidyverse)\nload('data/temps.RData')\nhead(temps)\n\n  body.temp    sex heart.rate\n1      98.8 female         69\n2      98.6 female         85\n3      98.4   male         68\n4      97.2 female         66\n5      99.5   male         75\n6      97.1   male         82"
  },
  {
    "objectID": "content/lab6-hypothesis.html#footnotes",
    "href": "content/lab6-hypothesis.html#footnotes",
    "title": "Lab 6: Hypothesis testing basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI usually think in the following terms for the \\(t\\)-test:\n\nsmall: \\(n \\leq 20\\)\nmodest: \\(20 &lt; n \\leq 50\\)\nlarge: \\(50 &lt; n\\)\n\nUnless I’m in the ‘small’ regime, I’m not too worried about skew or outliers. In the ‘modest’ regime, I’m not concerned unless I spot very pronounced skew or outliers. In the ‘large’ regime, I’m really only concerned about (strong) multimodality. Interestingly, in the latter case, the \\(t\\) test still works for multimodal populations, but the population mean isn’t meaningful.↩︎"
  },
  {
    "objectID": "content/week5-hypothesis.html",
    "href": "content/week5-hypothesis.html",
    "title": "Introduction to hypothesis testing",
    "section": "",
    "text": "Loose end: working backwards to determine interval coverage\n[lecture] the \\(t\\)-test for a population mean\n[lab] computing test statistics, critical values, and \\(p\\)-values"
  },
  {
    "objectID": "content/week5-directional.html#todays-agenda",
    "href": "content/week5-directional.html#todays-agenda",
    "title": "Tests for directional hypotheses",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nQuick review of decision criteria for the \\(t\\) test\n[lecture] test-interval relationship; directional \\(t\\) tests\n[lab] upper-sided, lower-sided, and two-sided tests for the population mean"
  },
  {
    "objectID": "content/week5-directional.html#from-last-time",
    "href": "content/week5-directional.html#from-last-time",
    "title": "Tests for directional hypotheses",
    "section": "From last time",
    "text": "From last time\nPractice problem: test the hypothesis that the average U.S. adult sleeps 8 hours.\n\n\n\n\n\n\n\n\n\n\n\n\n# calculations\nsleep.mean &lt;- mean(sleep) \nsleep.mean.se &lt;- sd(sleep)/sqrt(length(sleep))\ntstat &lt;- (sleep.mean - 8)/sleep.mean.se \ncrit.val &lt;- qt(0.975, df = 3178) \np.val &lt;- 2*pt(abs(tstat), df = 3178, lower.tail = F)\nci &lt;- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstd.err\ntstat\ncval\npval\n\n\n\n\n6.959\n0.02447\n-42.53\n1.961\n2.622e-313\n\n\n\n\n\n95% confidence interval: (6.91, 7.01)\n\nA complete narrative summary:\n\nThe data provide evidence that the average U.S. adult does not sleep 8 hours per night (T = -42.53 on 3178 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean nightly hours of sleep among U.S. adults is estimated to be between 6.91 and 7.01 hours, with a point estimate of 6.59 hours (SE: 0.0245)."
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals",
    "href": "content/week5-directional.html#tests-and-intervals",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\n\nTests and intervals are usually reported together\n\nConsider how the test and interval provide complementary information:\n\nthe test tells you U.S. adults don’t sleep 8 hours\nthe interval tells you how much they do sleep\n\nIt is reasonable that they should be consistent, and in fact they are.\n\n\n\n# calculations\nsleep.mean &lt;- mean(sleep) \nsleep.mean.se &lt;- sd(sleep)/sqrt(length(sleep))\ntstat &lt;- (sleep.mean - 8)/sleep.mean.se \ncrit.val &lt;- qt(0.975, df = 3178) \np.val &lt;- 2*pt(abs(tstat), df = 3178, lower.tail = F)\nci &lt;- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se\n\n\n\nNotice that the critical value in the 5% significance level test is exactly the same as that used in the 95% confidence interval."
  },
  {
    "objectID": "content/week5-directional.html#the-t.test...-function",
    "href": "content/week5-directional.html#the-t.test...-function",
    "title": "Tests for directional hypotheses",
    "section": "The t.test(...) function",
    "text": "The t.test(...) function\nSince tests and intervals go together, there is a single R function that computes both.\n\n\n\nt.test(sleep, mu = 8, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -42.533, df = 3178, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 8\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nNo critical value is reported, so you have to make the decision using the \\(p\\) value:\n\n\\(p &lt; \\alpha\\): reject\n\\(p &gt; \\alpha\\): fail to reject\n\n\n\nTake a moment to locate each component of the test and estimates from the output."
  },
  {
    "objectID": "content/week5-directional.html#decision-errors",
    "href": "content/week5-directional.html#decision-errors",
    "title": "Tests for directional hypotheses",
    "section": "Decision errors",
    "text": "Decision errors\n\n\nThere are two ways to make a mistake in a hypothesis test:\n\nreject a true \\(H_0\\)\nfail to reject a false \\(H_0\\)\n\nThese are known as type I and type II errors.\nBecause rejecting \\(H_0\\) is a stronger conclusion, type I errors are considered more severe.\n\n\n\nThe significance level of a test is a cap on the type I error rate."
  },
  {
    "objectID": "content/week5-directional.html#directional-hypotheses",
    "href": "content/week5-directional.html#directional-hypotheses",
    "title": "Tests for directional hypotheses",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nTests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.\n\n\n\nTest type\nAlternative\nDirection favoring alternative\n\n\n\n\nUpper-sided\n\\(\\mu &gt; \\mu_0\\)\nlarger \\(T\\)\n\n\nLower-sided\n\\(\\mu &lt; \\mu_0\\)\nsmaller \\(T\\)\n\n\nTwo-sided\n\\(\\mu \\neq \\mu_0\\)\nlarger \\(|T|\\)\n\n\n\nThe direction of the test affects the \\(p\\)-value calculation (and thus decision), but won’t change the test statistic.\nConceptually tricky, but easy in R:\n\n# upper-sided\nt.test(ddt, mu = mu_0, alternative = 'greater')\n\n# lower-sided\nt.test(ddt, mu = mu_0, alternative = 'less')\n\n# two-sided (default)\nt.test(ddt, mu = mu_0, alternative = 'two.sided')"
  },
  {
    "objectID": "content/week5-directional.html#directional-test",
    "href": "content/week5-directional.html#directional-test",
    "title": "Tests for directional hypotheses",
    "section": "Directional test",
    "text": "Directional test\n\n\nAccording to the \\(t\\) model, 0.58% of samples would produce an error of this magnitude or more in the direction of the alternative if in fact \\(\\mu = 3\\):\n\n\n\n\n\n\n\n\n\n\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328 \n\n\n\nThe data provide strong evidence that mean DDT in kale exceeds 3ppm (T = 2.9059 on 14 degrees of freedom, p = 0.0058)."
  },
  {
    "objectID": "content/week5-directional.html#composite-hypotheses",
    "href": "content/week5-directional.html#composite-hypotheses",
    "title": "Tests for directional hypotheses",
    "section": "Composite hypotheses",
    "text": "Composite hypotheses\nThe null hypothesis is a composite of values, so why did we choose just one (\\(\\mu_0 = 3\\)) to perform the test?\nAny null value farther from the alternative will produce stronger results.\n\n\n\n\n\n\n\n\nNull value\nTest statistic numerator\nProportion of samples more favorable to \\(H_A\\)\n\n\n\n\n\\(\\mu_0 = 3\\)\n0.328\n0.00575\n\n\n\\(\\mu_0 = 2.99\\)\n0.338\n0.00483\n\n\n\\(\\mu_0 = 2.95\\)\n0.378\n0.00239\n\n\n\nSo by using \\(\\mu_0 = 3\\), we are choosing the most conservative null value for the test."
  },
  {
    "objectID": "content/week5-directional.html#directional-hypotheses-1",
    "href": "content/week5-directional.html#directional-hypotheses-1",
    "title": "Tests for directional hypotheses",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm). Each measurement was taken by a different laboratory.\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu \\leq 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]"
  },
  {
    "objectID": "content/week5-directional.html#choosing-alternatives",
    "href": "content/week5-directional.html#choosing-alternatives",
    "title": "Tests for directional hypotheses",
    "section": "Choosing alternatives",
    "text": "Choosing alternatives\n\n\n\nIs the mean body temp less than 98.6?\n\nWhich test should you use? Consider the interpretations:\n\n[lower] evidence favoring lower temp\n[upper] no evidence against lower temp\n\nThe conclusions are consistent but not equivalent – (a) is a better answer.\nYour alternative should be the claim you hope to support with evidence, and your null the claim you hope to refute.\n\n\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -1.3283, df = 38, p-value = 0.09601\nalternative hypothesis: true mean is less than 98.6\n95 percent confidence interval:\n     -Inf 98.65248\nsample estimates:\nmean of x \n 98.40513 \n\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -1.3283, df = 38, p-value = 0.904\nalternative hypothesis: true mean is greater than 98.6\n95 percent confidence interval:\n 98.15778      Inf\nsample estimates:\nmean of x \n 98.40513"
  },
  {
    "objectID": "content/week5-directional.html#reporting-test-results",
    "href": "content/week5-directional.html#reporting-test-results",
    "title": "Tests for directional hypotheses",
    "section": "Reporting test results",
    "text": "Reporting test results\n\n\nTo report the result of a hypothesis test, you should:\n\nLead with your conclusion\nState the hypotheses tested\nInterpret the conclusion of the test in context\nProvide the \\(T\\) statistic, degrees of freedom, and \\(p\\)-value\nState and interpret the point estimate and interval for the mean\n\n\n\nOur results suggest mean body temperature is less than 98.6 °F. We tested the null hypothesis that mean body temperature is 98.6 °F or greater against the alternative that mean body temperature is less than 98.6 °F. The data provide sufficiently strong evidence against the hypothesis that mean body temperature is 98.6 °F or greater in favor of the alternative that mean body temperature is less than 98.6 °F (T = -5.4548 on 129 degrees of freedom, p-value = .0000001205). With 95% confidence, the mean nightly hours of sleep is estimated to be at most 98.36 °F, with a point estimate of 98.23 (SE = 0.0643).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-directional.html#your-turn-sleep-data",
    "href": "content/week5-directional.html#your-turn-sleep-data",
    "title": "Tests for directional hypotheses",
    "section": "Your turn: sleep data",
    "text": "Your turn: sleep data\nOpen lab7-moretests in the class workspace.\n\n\nWork with your group on just one of the questions below.\n\nDo US adults sleep 7.5 hours per night on average?\nDo US adults sleep less than 7.5 hours per night on average?\nDo US adults sleep more than 7.5 hours per night on average?\nDo US adults sleep more than 6.5 hours per night on average?\n\n\nYour task is to determine and carry out an appropriate test, and then write a complete report of the test outcome:\n\nanswer the question\nhypotheses tested\ntest conclusion, interpreted in context\ntest statistic, degrees of freedom, \\(p\\)-value\nconfidence interval and point estimate, interpreted in context\n\nThese elements should be summarized together in complete sentences."
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals-1",
    "href": "content/week5-directional.html#tests-and-intervals-1",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\nThe critical value used in a \\(\\alpha\\) significance level test is identical to the critical value used in a \\((1 - \\alpha)\\) interval. Consequently:\n\\[\n\\underbrace{\\bar{x} - c\\times SE(\\bar{x}) &lt; \\mu_0 &lt; \\bar{x} + c\\times SE(\\bar{x})}_\\text{hypothesized value is in the interval}\n\\quad\\Longleftrightarrow\\quad\n\\underbrace{-c &lt; \\frac{\\bar{x} - \\mu_{0}}{SE(\\bar{x})} &lt; c}_{|T| &lt; c}\n\\]\nMeaning: the interval includes exactly those values that the test fails to reject.\n\nSensible considering both use the same information: the distance between the point estimate and population mean, relative to the variability of the estimate"
  },
  {
    "objectID": "content/week5-directional.html#decision-errors-1",
    "href": "content/week5-directional.html#decision-errors-1",
    "title": "Tests for directional hypotheses",
    "section": "Decision errors",
    "text": "Decision errors\n\n\nThere are two ways to make a mistake in a hypothesis test:\n\nreject a true \\(H_0\\)\nfail to reject a false \\(H_0\\)\n\nThese are known as type I and type II errors.\nBecause rejecting \\(H_0\\) is a stronger conclusion, type I errors are considered more severe.\n\n\n\nThe significance level of a test is a cap on the type I error rate."
  },
  {
    "objectID": "content/week5-directional.html#exploring-type-i-error-rates",
    "href": "content/week5-directional.html#exploring-type-i-error-rates",
    "title": "Tests for directional hypotheses",
    "section": "Exploring type I error rates",
    "text": "Exploring type I error rates\nIn lab7-moretests, there are codes to draw a sample from a mock population and test a true null hypothesis. Run these and take note of your \\(p\\)-value.\n\n\nIn this situation, a type I error is rejecting \\(H_0\\).\n\nrule: reject if \\(p &lt; \\alpha\\).\nwe will tally rejections for various \\(\\alpha\\) values\n\n\n\n\n\nSignificance level\nError frequency\n\n\n\n\n\\(\\alpha = 0.2\\)\n\n\n\n\\(\\alpha = 0.1\\)\n\n\n\n\\(\\alpha = 0.05\\)\n\n\n\n\\(\\alpha = 0.02\\)\n\n\n\n\n\n\n\nTakeaway: using larger significance thresholds leads to [more/less] type 1 errors"
  },
  {
    "objectID": "content/week5-directional.html#exploring-type-ii-errors",
    "href": "content/week5-directional.html#exploring-type-ii-errors",
    "title": "Tests for directional hypotheses",
    "section": "Exploring type II errors",
    "text": "Exploring type II errors\nNow let’s test a false null hypothesis.\n\n\nA type II error is failing to reject \\(H_0\\).\n\nrule: \\(p &lt; 0.05\\)\nuse example commands to test each hypothesis at right\nuse a two-sided test\n\n\n\n\n\nNull value \\(\\mu_0\\)\nError frequency\n\n\n\n\n4.2\n\n\n\n4.6\n\n\n\n4.9\n\n\n\n5.1\n\n\n\n5.4\n\n\n\n5.7\n\n\n\n\n\n\n\nNotice that the type II error is quite high for null values near the true mean; this indicates the test has little power to detect such alternatives."
  },
  {
    "objectID": "content/week5-directional.html#swimsuit-data",
    "href": "content/week5-directional.html#swimsuit-data",
    "title": "Tests for directional hypotheses",
    "section": "Swimsuit data",
    "text": "Swimsuit data\nLet’s consider our first two-sample problem.\n\nAre swimmers faster in bodysuits than in regular swimsuits?\n\nBelow are the first few observations of the average velocity of competitive swimmers in a 1500m; one measurement was taken in a swimsuit, the other in a bodysuit.\n\n\n\n\n\n\n\n\n\n\nswimmer.number\nwet.suit.velocity\nswim.suit.velocity\n\n\n\n\n1\n1.57\n1.49\n\n\n2\n1.47\n1.37\n\n\n3\n1.42\n1.35\n\n\n\n\n\nCan you formulate a pair of hypotheses to test to answer this question using methods from last time?"
  },
  {
    "objectID": "content/week5-directional.html#comparing-two-means",
    "href": "content/week5-directional.html#comparing-two-means",
    "title": "Tests for directional hypotheses",
    "section": "Comparing two means",
    "text": "Comparing two means\nWe can formulate the question as a comparison of two means:\n\\[H_0: \\mu_\\text{bodysuit} \\leq \\mu_\\text{swimsuit}\\] \\[H_A: \\mu_\\text{bodysuit} &gt; \\mu_\\text{swimsuit}\\] It is common to express hypotheses of this form in terms of a difference in means: \\[H_0: \\underbrace{\\mu_\\text{bodysuit} - \\mu_\\text{swimsuit}}_\\delta \\leq 0\\] \\[H_A: \\underbrace{\\mu_\\text{bodysuit} - \\mu_\\text{swimsuit}}_\\delta &gt; 0\\]"
  },
  {
    "objectID": "content/week5-directional.html#pairing",
    "href": "content/week5-directional.html#pairing",
    "title": "Tests for directional hypotheses",
    "section": "Pairing",
    "text": "Pairing\nThe velocities in the swimsuit dataset are paired because every swimmer is measured in both suits.\n\nData are paired just in case the measurements in each group are taken on exactly the same study units or the study units can be placed in one-to-one correspondence.\n\nThis is the easiest situation to handle, because it reduces to a one-sample problem with the observed differences."
  },
  {
    "objectID": "content/week5-directional.html#inference-for-paired-data",
    "href": "content/week5-directional.html#inference-for-paired-data",
    "title": "Tests for directional hypotheses",
    "section": "Inference for paired data",
    "text": "Inference for paired data\n\nCalculate paired differences.\n\n\n\n\n\n\n\n\n\n\n\n\nswimmer.number\nwet.suit.velocity\nswim.suit.velocity\nvelocity.diff\n\n\n\n\n1\n1.57\n1.49\n0.08\n\n\n2\n1.47\n1.37\n0.1\n\n\n3\n1.42\n1.35\n0.07\n\n\n\n\n\n\n\n\nPerform test as before.\n\n\n# test for paired difference\nt.test(diffs, mu = 0, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  diffs\nt = 12.318, df = 11, p-value = 4.443e-08\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 0.06620114        Inf\nsample estimates:\nmean of x \n   0.0775 \n\n\n\n\nReport the result of the test. You try.\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals-2",
    "href": "content/week5-directional.html#tests-and-intervals-2",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\n\nThe level-\\(\\alpha\\) test rejects \\(H_0: \\mu = \\mu_0\\) exactly when \\(\\mu_0\\) is outside the \\((1 - \\alpha)\\times 100\\)% confidence interval for \\(\\mu\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft, \\(p\\)-values for a sequence of tests:\n\n\\(p &gt; 0.05\\) precisely for \\(\\mu_0\\) in 95% CI\n\\(p &gt; 0.01\\) precisely for \\(\\mu_0\\) in 99% CI\n\nIn other words:\n\\[\\text{level $\\alpha$ test rejects} \\Longleftrightarrow \\text{$1 - \\alpha$ CI excludes}\\]"
  },
  {
    "objectID": "content/week5-directional.html#tests-and-intervals-3",
    "href": "content/week5-directional.html#tests-and-intervals-3",
    "title": "Tests for directional hypotheses",
    "section": "Tests and intervals",
    "text": "Tests and intervals\nNotice that t.test produces a confidence interval.\n\nfor the two-sided test, the interval is exactly the one we calculated before\nfor the one-sided tests, the interval is a lower/upper confidence bound: \\[\\begin{align*}\n\\bar{x} - c \\times SE(\\bar{x}) &\\qquad \\text{lower confidence bound} \\quad\\Longleftrightarrow\\quad \\text{upper-sided test}\\\\\n\\bar{x} + c \\times SE(\\bar{x}) &\\qquad \\text{upper confidence bound} \\quad\\Longleftrightarrow\\quad \\text{lower-sided test}\n\\end{align*}\\] where \\(c\\) is chosen to ensure a specified coverage level\n\nThe tests and intervals correspond in the following sense:\n\nThe level \\(\\alpha\\) test rejects just in case the \\((1 - \\alpha)\\) confidence interval excludes \\(\\mu_0\\)\n\n\n# changing confidence level for interval\nt.test(body_temps, mu = 98.6, alternative = 'less', conf.level = 0.99)"
  },
  {
    "objectID": "content/week5-directional.html#interpreting-p-values",
    "href": "content/week5-directional.html#interpreting-p-values",
    "title": "Tests for directional hypotheses",
    "section": "Interpreting \\(p\\)-values",
    "text": "Interpreting \\(p\\)-values\n\n\\(p\\)-values measure the strength of evidence against \\(H_0\\) and favoring \\(H_A\\): smaller \\(p\\)-values indicate stronger evidence; larger \\(p\\)-values indicate weaker evidence.\n\n\n\nThe mathematical definition is: \\[p = P(|T| &gt; |T_\\text{observed}|)\\]\n\ntechnically, the probability under \\(H_0\\) that \\(T\\) exceeds the observed value in magnitude\ninformally, how unusual/rare your data are\n\n\nAs a measure of the strength of evidence favoring the alternative:\n\n\n\nvalue\nstrength of evidence\n\n\n\n\n\\(p &lt; 0.001\\)\nvery strong\n\n\n\\(0.001 &lt; p &lt; 0.01\\)\nstrong\n\n\n\\(0.01 &lt; p &lt; 0.05\\)\nmoderate\n\n\n\\(0.05 &lt; p &lt; 0.1\\)\nsuggestive\n\n\n\\(0.1 &lt; p\\)\nno evidence"
  },
  {
    "objectID": "content/week5-directional.html#a-different-test",
    "href": "content/week5-directional.html#a-different-test",
    "title": "Tests for directional hypotheses",
    "section": "A different test",
    "text": "A different test\n\nDoes the average U.S. adult sleep less than 7 hours?\n\n\n\nThis example leads to a directional test:\n\\[\n\\begin{cases}\nH_0: &\\mu = 8 \\\\\nH_0: &\\mu &lt; 8\n\\end{cases}\n\\]\nThe test statistic is the same as before:\n\\[\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n\\]\n\nThe upper-sided \\(p\\)-value is 0.9526:\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(p\\)-value, we look at how often \\(T\\) is larger in the direction of the alternative."
  },
  {
    "objectID": "content/week5-directional.html#ddt-data",
    "href": "content/week5-directional.html#ddt-data",
    "title": "Tests for directional hypotheses",
    "section": "DDT data",
    "text": "DDT data\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm).\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu = 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]\nWe choose this direction because we’re concerned with evidence that mean DDT exceeds the threshold."
  },
  {
    "objectID": "content/week5-directional.html#your-turn-interpret-these-p-values",
    "href": "content/week5-directional.html#your-turn-interpret-these-p-values",
    "title": "Tests for directional hypotheses",
    "section": "Your turn: interpret these \\(p\\)-values",
    "text": "Your turn: interpret these \\(p\\)-values\n\nDon’t just match the value to the table; add context, and state the test outcome.\n\n\n\n\n# example 1\nt.test(sleep, mu = 6.8)$p.value\n\n[1] 9.205865e-11\n\n# example 2\nt.test(sleep, mu = 6.9)$p.value\n\n[1] 0.01578191\n\n# example 3\nt.test(sleep, mu = 7)$p.value\n\n[1] 0.09482291\n\n# example 4\nt.test(sleep, mu = 7.1)$p.value\n\n[1] 9.366935e-09\n\n# example 5\nt.test(sleep, mu = 7.2)$p.value\n\n[1] 1.532371e-22\n\n\n\n\n\n\nvalue\nstrength of evidence\n\n\n\n\n\\(p &lt; 0.001\\)\nvery strong\n\n\n\\(0.001 &lt; p &lt; 0.01\\)\nstrong\n\n\n\\(0.01 &lt; p &lt; 0.05\\)\nmoderate\n\n\n\\(0.05 &lt; p &lt; 0.1\\)\nsuggestive\n\n\n\\(0.1 &lt; p\\)\nno evidence\n\n\n\n\n\nExample: “the data [DO/DO NOT] provide [STRENGTH] evidence that [ALTERNATIVE]”"
  },
  {
    "objectID": "content/week5-directional.html#a-directional-test",
    "href": "content/week5-directional.html#a-directional-test",
    "title": "Tests for directional hypotheses",
    "section": "A directional test",
    "text": "A directional test\n\nDoes the average U.S. adult sleep less than 7 hours?\n\n\n\nThis example leads to a directional test:\n\\[\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu &lt; 7\n\\end{cases}\n\\]\nThe test statistic is the same as before:\n\\[\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n\\]\n\nThe lower-sided \\(p\\)-value is 0.0474:\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(p\\)-value, we look at how often \\(T\\) is larger in the direction of the alternative.\n\nin this case, how often \\(T\\) is smaller (underestimate by more)"
  },
  {
    "objectID": "content/week5-directional.html#the-other-direction",
    "href": "content/week5-directional.html#the-other-direction",
    "title": "Tests for directional hypotheses",
    "section": "The other direction",
    "text": "The other direction\n\nDoes the average U.S. adult sleep more than 7 hours?\n\n\n\nNow the alternative is the opposite direction:\n\\[\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu &gt; 7\n\\end{cases}\n\\]\nThe test statistic is the same as before:\n\\[\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n\\]\n\nThe upper-sided \\(p\\)-value is 0.9526:\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(p\\)-value, we look at how often \\(T\\) is larger in the direction of the alternative.\n\nin this case, how often \\(T\\) is larger (overestimate by more)"
  },
  {
    "objectID": "content/week5-directional.html#another-example-ddt-data",
    "href": "content/week5-directional.html#another-example-ddt-data",
    "title": "Tests for directional hypotheses",
    "section": "Another example: DDT data",
    "text": "Another example: DDT data\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm).\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\\[\n\\begin{cases}\nH_0: &\\mu = 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu &gt; 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n\\]\nWe choose this direction because we’re concerned with evidence that mean DDT exceeds the threshold."
  },
  {
    "objectID": "content/week5-directional.html#another-example-ddt-data-1",
    "href": "content/week5-directional.html#another-example-ddt-data-1",
    "title": "Tests for directional hypotheses",
    "section": "Another example: DDT data",
    "text": "Another example: DDT data\n\n\nIf in fact \\(\\mu = 3\\), then according to the \\(t\\) model 0.58% of samples would produce an error of this magnitude or more in the direction of the alternative:\n\n\n\n\n\n\n\n\n\n\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328 \n\n\n\nThe data provide strong evidence that mean DDT in kale exceeds 3ppm (T = 2.9059 on 14 degrees of freedom, p = 0.0058). With 95% confidence, the mean DDT is estimated to be at least 3.129, with a point estimate of 3.32 (SE: 0.1168).\n\n\n\nNotice the one-sided interval! (Inf = \\(\\infty\\).) This is called a “lower confidence bound”."
  },
  {
    "objectID": "content/week5-directional.html#your-turn-which-alternative",
    "href": "content/week5-directional.html#your-turn-which-alternative",
    "title": "Tests for directional hypotheses",
    "section": "Your turn: which alternative?",
    "text": "Your turn: which alternative?\n\nWrite the hypotheses in notation and identify which test (upper/lower/two sided) should be used.\n\nUsing the temperature/heartrate data:\n\nIs mean body temperature less than 98.6°F?\nIs mean heart rate greater than 60 bpm?\nIs mean heart rate 65 bpm?\n\nUsing the NC births data:\n\nIs the mean number of weeks at birth 40?\nIs the mean birth weight at least 7 lbs?\nIs the mean birth weight under 8 lbs?\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-directional.html#three-t-tests",
    "href": "content/week5-directional.html#three-t-tests",
    "title": "Tests for directional hypotheses",
    "section": "Three \\(t\\)-tests",
    "text": "Three \\(t\\)-tests\n\n\nDo U.S. adults sleep 7 hours per night?\n\n# two sided test\nt.test(sleep, \n       mu = 7)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nSuggestive but insufficient evidence that U.S. adults don’t sleep 7 hours\n\n\nDo U.S. adults sleep less than 7 hours per night?\n\n# lower-sided test\nt.test(sleep, \n       mu = 7, \n       alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.04741\nalternative hypothesis: true mean is less than 7\n95 percent confidence interval:\n     -Inf 6.999372\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nModerate evidence that U.S. adults sleep less than 7 hours\n\n\nDo U.S. adults sleep more than 7 hours per night?\n\n# upper-sided test\nt.test(sleep, \n       mu = 7, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.9526\nalternative hypothesis: true mean is greater than 7\n95 percent confidence interval:\n 6.918841      Inf\nsample estimates:\nmean of x \n 6.959107 \n\n\n\nNo evidence that U.S. adults sleep more than 7 hours"
  },
  {
    "objectID": "content/week5-directional.html#recap-decision-criteria",
    "href": "content/week5-directional.html#recap-decision-criteria",
    "title": "Tests for directional hypotheses",
    "section": "Recap: decision criteria",
    "text": "Recap: decision criteria\n\nA hypothesis test boils down to deciding whether your estimate is too far from a hypothetical value for that hypothesis to be plausible.\n\n\n\nTo test the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = \\mu_0 \\\\\nH_A: &\\mu \\neq \\mu_0\n\\end{cases}\n\\]\nWe use the test statistic:\n\\[\nT = \\frac{\\bar{x} - \\mu_0}{SE(\\bar{x})}\n\\quad\\left(\\frac{\\text{estimation error under } H_0}{\\text{standard error}}\\right)\n\\]\n\nWe say \\(H_0\\) is implausible at level \\(\\alpha\\) if either:\n\n\\(|T| &gt; q\\) for the \\(\\alpha\\)-critical value \\(q\\)\n\n\\(q\\) is the \\(1 - \\frac{\\alpha}{2}\\) quantile of the \\(t_{n - 1}\\) model\n\n\\(\\underbrace{P(|T| &gt; |T_\\text{observed}|)}_\\text{p-value} &lt; \\alpha\\)\n\n\n\nThis procedure controls the error rate \\(\\alpha\\): the proportion of samples for which we’d make a false rejection."
  },
  {
    "objectID": "content/lab7-directional.html",
    "href": "content/lab7-directional.html",
    "title": "Lab 6: Directional \\(t\\)-tests",
    "section": "",
    "text": "This lab has two objectives:\n\nLearn to use the t.test(...) function\nLearn to discern the appropriate direction for a \\(t\\) test\n\nWe’ll use two familiar datasets: body temperature and heart rate measurements for 39 individuals; and data on birth weights and weeks at birth for a sample of 100 births in North Carolina in 2004.\n\nlibrary(tidyverse)\nload('data/temps.RData')\nload('data/nhanes.RData')\nncbirths &lt;- read_csv('data/ncbirths.csv')\n\n\nThe t.test(...) function\nThe t.test(...) function produces both a hypothesis test and an confidence interval, and can be used to obtain either or both in practice.\nLet’s demonstrate with the practice problem you completed most recently: inference on the mean nightly hours of sleep among U.S. adults based on NHANES data.\nThe default behavior of t.test(...) if given no arguments besides a vector of data is to test \\(H_0: \\mu = 0\\) against a two-sided alternative (\\(H_A: \\mu \\neq 0\\)) and provide a 95% confidence interval. There are three key arguments that allow you to adjust this behavior:\n\nmu = ... adjusts the value for the mean in the null hypothesis \\(H_0\\)\n\ndefault mu = 0\n\nalternative = ... adjusts the direction of the alternative, with options\n\n'less' for a lower-sided alternative\n'greater' for an upper-sided alternative\n'two.sided' (default) for a two-sided alternative\n\nconf.level = ... adjusts the confidence level for the interval estimate\n\ndefault conf.level = 0.95\n\n\nThe examples below illustrate this usage. Run each command and look at the output closely to determine what changes.\n\n# extract sleep variable\nsleep &lt;- nhanes$sleephrsnight\n\n# default behavior (these are equivalent)\nt.test(sleep)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = 284.36, df = 3178, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\nt.test(sleep, mu = 0, alternative = 'two.sided', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = 284.36, df = 3178, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n# change null value to 7 hours of sleep\nt.test(sleep, mu = 7, alternative = 'two.sided', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n\n# change the confidence level\nt.test(sleep, mu = 7, alternative = 'two.sided', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n99 percent confidence interval:\n 6.896032 7.022182\nsample estimates:\nmean of x \n 6.959107 \n\n# change the direction of the alternative\nt.test(sleep, mu = 7, alternative = 'less', conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.04741\nalternative hypothesis: true mean is less than 7\n99 percent confidence interval:\n     -Inf 7.016067\nsample estimates:\nmean of x \n 6.959107 \n\n\nFocus for a moment on the last example. In detail, this tests, at the 1% significance level, the hypotheses:\n\\[\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu &lt; 7\n\\end{cases}\n\\]\nWhile the conf.level argument doesn’t affect the \\(p\\)-value, it does imply a significance level – in this case, \\(\\alpha = 0.01\\). So, even though the \\(p\\)-value is less than the conventional level (\\(p &lt; 0.05\\)), it is not less than the implied significance level (here \\(p &gt; 0.01\\)), so the test output implies we’d fail to reject the hypothesis that adults sleep less than 7 hours.\nIn general, it’s important to set the confidence level to correspond to the significance level of the test you wish to perform, so that the test interpretation and interval provided match.\n\n\n\n\n\n\nYour turn 1\n\n\n\nAdjust the arguments of the t.test(...) function to achieve the following:\n\nfind a 90% CI for the mean\ntest whether mean sleep is 6.9 at the 5% level\ntest whether mean sleep is 6.9 at the 1% level\ntest whether mean sleep exceeds 6.9 at the 5% level\ntest whether mean sleep exceeds 6.9 at the 1% level\n\n\n# obtain a 90% confidence interval for the mean hours of sleep\n\n# test whether mean sleep is 6.9 at the 5% level\n\n# test whether mean sleep is 6.9 at the 1% level\n\n# test whether mean sleep exceeds 6.9 at the 5% level\n\n# test whether mean sleep exceeds 6.9 at the 1% level\n\nFor extra practice, write a short interpretation of the results of each test following the style introduced in class.\n\n\n\n\nDistinguishing directional alternatives\nHere we’ll use the temperature/heartrate data to illustrate a variety of directional tests based on questions of interest.\nAs you’re looking over the examples, focus on the correspondence between the questions and the direction of the alternative.\n\n# extract body temperature variable\nbodytemps &lt;- temps$body.temp\n\n# is mean temperature different from 98.6 at the 5% significance level?\nt.test(bodytemps, mu = 98.6, alternative = 'two.sided', conf.level = 0.95)\n\n# is mean temperature less than 98.6 at the 5% significance level?\nt.test(bodytemps, mu = 98.6, alternative = 'less', conf.level = 0.95)\n\n# is mean temperature greater than 98.1 at the 5% significance level?\nt.test(bodytemps, mu = 98.1, alternative = 'greater', conf.level = 0.95)\n\n# is mean temperature greater than 98.1 at the 1% significance level?\nt.test(bodytemps, mu = 98.1, alternative = 'greater', conf.level = 0.99)\n\n# is mean temperature less than 98.9 at the 5% significance level?\nt.test(bodytemps, mu = 98.9, alternative = 'less', conf.level = 0.95)\n\nAs an aside (but an important one!), performing all of these tests together is only meant to illustrate how the function works, not how to perform an analysis. Trying out many tests until you obtain significant results is known as “\\(p\\) hacking”, and is not an acceptable practice.\n\n\n\n\n\n\nYour turn 2\n\n\n\nUsing the heart.rate variable, test the following hypotheses:\n\nIs mean heart rate 65bpm at the 5% level?\nIs mean heart rate 70bpm at the 1% level?\nIs mean heart rate greater than 70bpm at the 1% level?\nIs mean heart rate less than 75bpm at the 10% level?\nIs mean heart rate greater than 75bpm at the 10% level?\n\n\n# extract heart rate variable\n\n# is mean heart rate 65bpm at the 5% level?\n\n# is mean heart rate 70bpm at the 1% level?\n\n# is mean heart rate greater than 70bpm at the 1% level?\n\n# is mean heart rate less than 75bpm at the 10% level?\n\n# is mean heart rate greater than 75bpm at the 10% level?\n\n\n\n\n\nA brief analysis\nNow that you’re familiar with using the t.test(...) function, let’s do something a bit more realistic. Suppose that, using the ncbirths data, you want to perform inference on the number of weeks at birth. We’re told that 40 weeks is typical.\n\nAdvance decisions\nIn advance of looking at the data (or perhaps even having data) we should determine:\n\nthe hypotheses to test\nthe level at which we’ll perform the test\n\nTo make these choices, first note that there’s no obvious directional question to ask here. So, we’ll test whether the mean number of weeks at birth is 40. A 5% significance level is conventional, so we’ll stick with that.\n\n\nAssessing assumptions\nBefore going ahead, let’s inspect the data.\n\n# extract variable of interest\nweeks &lt;- ncbirths$weeks\n\n# inspect distribution\nhist(weeks, breaks = 15)\n\n\n\n\n\n\n\n\nThis is an interesting case, because we do have a left-skewed distribution and there are a few outliers below 30 weeks. However, the sample size is large (\\(n = 100\\)), so the test should still work well regardless.\n\n\nPerforming the test\nFor inference we’ll want to report a test result and interval estimate. Both of these are obtained using t.test(...) as above, but of course we only perform one test/interval calculation.\n\n# inference\nt.test(weeks, mu = 40, alternative = 'two.sided', conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  weeks\nt = -5.0421, df = 99, p-value = 2.084e-06\nalternative hypothesis: true mean is not equal to 40\n95 percent confidence interval:\n 37.97938 39.12062\nsample estimates:\nmean of x \n    38.55 \n\n\nTake a moment to inspect the results.\n\n\nInterpreting results\nFollowing the format in class, a report of the results should interpret the test and interval in context, providing supporting statistics parenthetically:\n\nData from North Carolina in 2004 provide strong evidence that the mean number of weeks at birth differs from 40 (T = -5.0421 on 99 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean number of weeks at birth is estimated to be between 37.98 and 39.12 weeks, with a point estimate of 38.55 weeks (SE 0.289).\n\n\n\n\nPractice problems\n\nPerform and interpret the results of inference on the mean birth weight to investigate the claim that the typical birth weight is at least 7 lbs. Carry out inference at the 5% significance level.\n\nDetermine your hypotheses.\nCheck test assumptions.\nPerform the calculations.\nWrite a short report of the results.\n\n\n\n[REVISED] Using the BRFSS data, test whether actual body weight exceeds desired body weight and estimate the difference. Perform the test at the 1% level."
  },
  {
    "objectID": "content/test2.html",
    "href": "content/test2.html",
    "title": "Test 2",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 2 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/10. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test2.html#instructions",
    "href": "content/test2.html#instructions",
    "title": "Test 2",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 2 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 5/10. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test2.html#question-prompts",
    "href": "content/test2.html#question-prompts",
    "title": "Test 2",
    "section": "Question prompts",
    "text": "Question prompts\n\n[L3, L4, L5] The dataset lizards contains running speeds measured in a laboratory race track for two species of lizards, Western Fence (WF) and Sagebrush (S) lizards. Speeds are recorded in meters per second.\n\n[L3] Construct side-by-side boxplots of top speed by species. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Compute point estimates and standard errors for the mean top speed for each species.\n[L4] Compute and interpret 99.5% confidence intervals for the mean top speed for each species.\n[L5] Test for a difference in mean top speed between species at significance level 0.01. Interpret the test result following the style introduced in class.\n[L4] Construct and interpret a 99% confidence interval for the difference in mean top speed.\n\n[L3, L4] The tuition dataset contains in-state and out-of-state tuition at a random sample of 25 public universities from 2011-2012.\n\n[L3] Visualize the distribution of differences between in-state and out-of-state tuition. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Calculate and interpret a 95% confidence interval for the mean difference between in-state and out-of-state tuition.\n[L4] Interpret your interval in context following the style introduced in class.\n\n[L1, L2, L3, L4, L5] The creativity dataset contains data from an experiment on the effect of intrinsic vs. extrinsic motivation on creativity. A random sample of 47 creative writing students at an unnamed university were randomly assigned to one of two groups, extrinsic and intrinsic; each subject was instructed to write two short poems, but those in the extrinsic motivation group were primed on the task in a way that oriented them to external motivations for writing, and those in the intrinsic group were primed on the task in a way that oriented them to internal motivations for writing. Poems were scored by judges for creativity on a 40-point scale, and each subject received an average score.\n\n[L1] What is the study population? Based on the study description, is the sample representative, and if so, why?\n[L2] What type of study is this? Based on the study description, can the data support causal inferences about motivation and creativity, and if so, why?\n[L3] Construct an appropriate graphical summary comparing the distributions of average scores by treatment group.\n[L3] Provide appropriate summary statistics indicating the center, spread, and number of observations of average scores in each group.\n[L5] Test the hypothesis that motivational framing has no effect on creativity at the 1% significance level. Use your results from (c)-(d) to check assumptions.\n[L4] Compute an interval estimate for the difference in mean scores at the level corresponding to your test.\n[LX] Write a short narrative summary of your results in (e)-(f) following the style introduced in class. (Don’t forget to include a point estimate and standard error.)"
  },
  {
    "objectID": "content/week6-power.html",
    "href": "content/week6-power.html",
    "title": "Power analyses",
    "section": "",
    "text": "Reading quiz [2pm section] [4pm section]\nType II errors: review and further exploration\n[lecture/lab] Power analysis\n\nSample size calculations\nPost-hoc power analyses"
  },
  {
    "objectID": "content/week6-power.html#todays-agenda",
    "href": "content/week6-power.html#todays-agenda",
    "title": "Power analyses",
    "section": "",
    "text": "Reading quiz [2pm section] [4pm section]\nType II errors: review and further exploration\n[lecture/lab] Power analysis\n\nSample size calculations\nPost-hoc power analyses"
  },
  {
    "objectID": "content/week6-power.html#a-thought-experiment",
    "href": "content/week6-power.html#a-thought-experiment",
    "title": "Power analyses",
    "section": "A thought experiment",
    "text": "A thought experiment\n\n\nSuppose that for the cloud data you’d performed a two-sided test: \\[H_0: \\mu_\\text{seeded} = \\mu_\\text{unseeded}\\] \\[H_A: \\mu_\\text{seeded} \\neq \\mu_\\text{unseeded}\\]\n\n\n\n    Welch Two Sample t-test\n\ndata:  Rainfall by Treatment\nt = 1.9982, df = 33.855, p-value = 0.05377\nalternative hypothesis: true difference in means between group Seeded and group Unseeded is not equal to 0\n95 percent confidence interval:\n  -4.764295 559.556603\nsample estimates:\n  mean in group Seeded mean in group Unseeded \n              441.9846               164.5885 \n\n\nAlmost below the significance threshold but not quite.\n\n\nThe data do not provide sufficient evidence to reject the null hypothesis that seeding has no effect relative to the alternative of an increase or decrease in mean rainfall due to seeding (T = 1.998 on 33.86 degrees of freedom, p = 0.05377).\n\nThe point estimate for the difference is 277.4 acre-feet.\n\nThe test says this observed difference could plausibly be due to sampling variation\nBut is it also plausible that our test result is wrong if the difference is real?"
  },
  {
    "objectID": "content/week6-power.html#type-ii-error-rates",
    "href": "content/week6-power.html#type-ii-error-rates",
    "title": "Power analyses",
    "section": "Type II error rates",
    "text": "Type II error rates\n\nRecall: a type II error is failing to reject a false null hypothesis.\n\nIn the context of two-sample inference a type II error occurs when:\n\nthe true difference is \\(\\delta \\neq 0\\)\nwe test and fail to reject \\(H_0: \\delta \\neq 0\\)\n\nThe type II error rate depends on both known and unknown factors:\n\n[unknown] magnitude of \\(\\delta\\)\n[unknown] population variability \\(\\sigma\\)\n[known] significance level\n[known] sample sizes\n\nWhat was the type II error rate for the cloud seeding test?"
  },
  {
    "objectID": "content/week6-power.html#simulating-type-ii-errors",
    "href": "content/week6-power.html#simulating-type-ii-errors",
    "title": "Power analyses",
    "section": "Simulating type II errors",
    "text": "Simulating type II errors\n\n\nSummary stats for cloud data:\n\n\n\n\n\n\n\n\n\n\n\nTreatment\nmean\nsd\nn\n\n\n\n\nSeeded\n442\n650.8\n26\n\n\nUnseeded\n164.6\n278.4\n26\n\n\n\n\n\nWe can approximate the type II error rate by:\n\nsimulating datasets with matching summary statistics\nperforming two-sided tests of no difference\ncomputing the proportion of fail-to-reject decisions\n\n\ntype2sim(delta = 277, n = 26, sd = 650, alpha = 0.05)\n\n[1] 0.689\n\n\n\\(\\Rightarrow\\) if the true difference were exactly as estimated, our test result would be incorrect nearly 70% of the time!\n\nWhat would happen to the error rate if…\n\nthe true difference delta were bigger?\nthe significance level alpha were smaller?\nthe sample size n was larger?\nthe variability of rainfall sd were less?"
  },
  {
    "objectID": "content/week6-power.html#simulating-type-ii-errors-1",
    "href": "content/week6-power.html#simulating-type-ii-errors-1",
    "title": "Power analyses",
    "section": "Simulating type II errors",
    "text": "Simulating type II errors\n\n\nOpen the lab and use the simulation function type2sim to fill in the table by changing arguments accordingly.\n\ntry a few magnitudes of difference for each scenario\nrepeat runs for each setting once or twice to confirm effect\n\n\n\n\n\nFactor\nChange\nEffect on error rate\n\n\n\n\ntrue difference in means\nlarger\n\n\n\ntrue difference in means\nsmaller\n\n\n\npopulation variability\nlarger\n\n\n\npopulation variability\nsmaller\n\n\n\nsample size\nlarger\n\n\n\nsample size\nsmaller\n\n\n\nsignificance level\nlarger\n\n\n\nsignificance level\nsmaller\n\n\n\n\n\n\n\nBased on your explorations, do you think our original test decision was erroneous?"
  },
  {
    "objectID": "content/week6-power.html#statistical-power",
    "href": "content/week6-power.html#statistical-power",
    "title": "Power analyses",
    "section": "Statistical power",
    "text": "Statistical power\nThe power of a test refers to its true rejection rates across alternatives and is defined as: \\[\\beta(\\delta) = \\underbrace{(1 - \\text{type II error rate}_\\delta )}_\\text{correct decision rate when null is false}\\]\nPower is often interpreted as a detection rate for a specified alternative \\(\\delta\\):\n\nhigh type II error \\(\\longrightarrow\\) low power \\(\\longrightarrow\\) low detection rate\nlow type II error \\(\\longrightarrow\\) high power \\(\\longrightarrow\\) high detection rate\n\n\nIn general tests have low power for alternatives close to the null value (where “close” is relative to sampling variability).\n\nTheory allows a direct calculation of power, given sample size, significance level, population standard deviation, and population difference in means."
  },
  {
    "objectID": "content/week6-power.html#power-curves",
    "href": "content/week6-power.html#power-curves",
    "title": "Power analyses",
    "section": "Power curves",
    "text": "Power curves\n\nPower is usually construed as a curve depending on the true difference.\n\n\n\nPower curve for the test exactly as performed with the cloud seeding data:\n\n\n\n\n\n\n\n\n\n\nAll other attributes of the test are fixed to approximate the test performed:\n\nsample size \\(n = 26\\)\nsignificance level \\(\\alpha = 0.05\\)\npopulation standard deviation \\(\\sigma = 650\\) (larger of two group estimates)"
  },
  {
    "objectID": "content/week6-power.html#factors-affecting-power",
    "href": "content/week6-power.html#factors-affecting-power",
    "title": "Power analyses",
    "section": "Factors affecting power",
    "text": "Factors affecting power\n\nPower depends on all the same factors as type II error rates\n\n\n\n\n\n\n\n\n\n\nFactor\nChange\nEffect on error rate\nEffect on power\n\n\n\n\ntrue difference in means\nlarger\n\n\n\n\ntrue difference in means\nsmaller\n\n\n\n\npopulation variability\nlarger\n\n\n\n\npopulation variability\nsmaller\n\n\n\n\nsample size\nlarger\n\n\n\n\nsample size\nsmaller\n\n\n\n\nsignificance level\nlarger\n\n\n\n\nsignificance level\nsmaller"
  },
  {
    "objectID": "content/week6-power.html#two-common-power-analyses",
    "href": "content/week6-power.html#two-common-power-analyses",
    "title": "Power analyses",
    "section": "Two common power analyses",
    "text": "Two common power analyses\n\n\nPost hoc analysis: how much power does the test I conducted have if the true difference is exactly equal to my estimate?\nHelps to interpret negative results:\n\nlow power \\(\\rightarrow\\) failure to reject was likely\nhigh power \\(\\rightarrow\\) failure to reject was not likely\n\n\n\n\n\n\n\nDon’t over-interpret post-hoc analyses\n\n\n\nFailure to reject using a well-powered test does not confirm the null hypothesis.\n\n\n\nSample size determination: how much data do I need to collect to detect a difference of \\(\\delta\\) using a particular test?\nHelps avoid two potential issues:\n\ntoo little data \\(\\rightarrow\\) study not likely to yield significant results\ntoo much data \\(\\rightarrow\\) study is too likely to yield significant results"
  },
  {
    "objectID": "content/week6-power.html#post-hoc-analysis",
    "href": "content/week6-power.html#post-hoc-analysis",
    "title": "Power analyses",
    "section": "Post-hoc analysis",
    "text": "Post-hoc analysis\n\nCan we estimate the power of a test we already performed?\n\n\n\nFeasible if we assume (a) a population standard deviation and (b) test conditions are met.\nFor the cloud seeding test:\n\npower.t.test(delta = 250, # magnitude of difference\n             sd = 650, # largest population SD\n             n = 26, # smallest sample size\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n\n\n     Two-sample t test power calculation \n\n              n = 26\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.2743235\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nFor a conservative estimate, use:\n\nsmallest of the two sample sizes\nlargest of the two standard deviations\nsmaller difference than observed\n\n\n\\(\\Longrightarrow\\) our test would only reject in favor of a difference of the observed magnitude about 27% of the time\n\nFailure to reject doesn’t strongly rule out the alternative."
  },
  {
    "objectID": "content/week6-power.html#your-turn-post-hoc-analysis",
    "href": "content/week6-power.html#your-turn-post-hoc-analysis",
    "title": "Power analyses",
    "section": "Your turn: post-hoc analysis",
    "text": "Your turn: post-hoc analysis\n\n\nConsider testing whether body temperature differs by sex.\nSummary stats and test result:\n\n\n\n\n\n\n\n\n\n\n\nsex\nmean\nsd\nn\n\n\n\n\nfemale\n98.66\n0.9929\n19\n\n\nmale\n98.17\n0.7876\n20\n\n\n\n\n\n\nt.test(body.temp ~ sex, data = temps)\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 1.7118, df = 34.329, p-value = 0.09595\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.09204497  1.07783444\nsample estimates:\nmean in group female   mean in group male \n            98.65789             98.16500 \n\n\n\nAssume the true difference is actually 0.5 °F. Determine the power of the test above when:\n\nPopulation SD is the smaller of the two groups\nPopulation SD is the larger of the two groups\nA one-sided test is used instead\n\n\nBased on your answers, do you think the negative test result rules out the alternative?"
  },
  {
    "objectID": "content/week6-power.html#power-curve-for-body-temps",
    "href": "content/week6-power.html#power-curve-for-body-temps",
    "title": "Power analyses",
    "section": "Power curve for body temps",
    "text": "Power curve for body temps\n\n\nAssuming we underestimated the population standard deviation a bit, the power curve for a one-sided test would look like this:\n\n\n\n\n\n\n\n\n\n\nAssumptions:\n\nn = 19 per group\n\\(\\sigma = 1.2\\) per group\nsignificance level \\(\\alpha = 0.05\\)\none-sided test\n\n\nFairly low power for alternatives near the estimated difference (dashed line), so failure to reject doesn’t strongly rule out the alternative."
  },
  {
    "objectID": "content/week6-power.html#the-equal-variance-t-test",
    "href": "content/week6-power.html#the-equal-variance-t-test",
    "title": "Power analyses",
    "section": "The equal-variance \\(t\\)-test",
    "text": "The equal-variance \\(t\\)-test\nIf it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power by using a different standard error:\n\\[SE_\\text{pooled}(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{\\color{red}{s_p^2}}{n_x} + \\frac{\\color{red}{s_p^2}}{n_y}}\n\\quad\\text{where}\\quad \\color{red}{s_p} = \\underbrace{\\sqrt{\\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\\text{weighted average of } s_x^2 \\;\\&\\; s_y^2}\\]\nIn the case of the body temperature data, \\(s_p\\) = 0.8934. Check:\n\nHow much power do we gain if we assume a common SD of 0.89?\nDoes it change the outcome of the test (add var.equal = T)?\n\n\nProduces minimal gains and inflates type I error if not warranted, so better avoided unless you have a small sample size"
  },
  {
    "objectID": "content/week6-power.html#sample-size-calculation",
    "href": "content/week6-power.html#sample-size-calculation",
    "title": "Power analyses",
    "section": "Sample size calculation",
    "text": "Sample size calculation\n\nIf you were (re)designing the study, how much data should you collect to detect a specified effect size?\n\n\n\nTo detect a difference of 250 or more due to cloud seeding with power 0.9:\n\npower.t.test(power = 0.9, # target power level\n             delta = 250, # smallest difference\n             sd = 650, # largest population SD\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n\n\n     Two-sample t test power calculation \n\n              n = 143.0276\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nFor a conservative estimate, use:\n\noverestimate of the larger of the two standard deviations\nminimum difference of interest\n\n\n\\(\\Longrightarrow\\) we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time"
  },
  {
    "objectID": "content/week6-power.html#your-turn-sample-size-calculation",
    "href": "content/week6-power.html#your-turn-sample-size-calculation",
    "title": "Power analyses",
    "section": "Your turn: sample size calculation",
    "text": "Your turn: sample size calculation\nSuppose you are designing a follow-up study and wish to detect a difference of 0.4 °F at least 70% of the time. You know women have slightly higher body temperatures than men on average.\n\n\n\n\n\n\n\n\nKnown direction?\nPopulation SD\nMinimum \\(n\\)\n\n\n\n\nNo\nlarger of prior estimates\n\n\n\nNo\n1.2 times larger than larger of prior estimates\n\n\n\nYes\nlarger of prior estimates\n\n\n\nYes\n1.2 times larger than larger of prior estimates\n\n\n\n\n\nIf it costs $10 per participant to run the study, what’s the best power achievable within a $2K budget for the target detection magnitude?"
  },
  {
    "objectID": "content/week6-power.html#power-vs.-sample-size-curves",
    "href": "content/week6-power.html#power-vs.-sample-size-curves",
    "title": "Power analyses",
    "section": "Power vs. sample size curves",
    "text": "Power vs. sample size curves\n\n\nMinimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:\n\n\n\n\n\n\n\n\n\nAssumes \\(\\sigma = 1.2\\) for a conservative estimate.\n\nThe best power achievable within budget for the target detection range is 0.7593159.\n\nincreasing power to 0.8 will require n = 112 per group\n\n$240 over budget\n\nincreasing power to 0.9 will require n = 155 per group\n\n$1050 over budget"
  },
  {
    "objectID": "content/week6-twosample.html",
    "href": "content/week6-twosample.html",
    "title": "Two sample inference",
    "section": "",
    "text": "Reading quiz [2pm section] [4pm section]\n[lecture/lab] Two-sample inference for population means\n\nPaired data\nIndependent data\n\n[if time] Introduction to power analysis"
  },
  {
    "objectID": "content/week6-twosample.html#todays-agenda",
    "href": "content/week6-twosample.html#todays-agenda",
    "title": "Two sample inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] two sample inference for means\n[lab] two-sample \\(t\\) tests in R\n[test prep] practice problems"
  },
  {
    "objectID": "content/week6-twosample.html#from-last-time",
    "href": "content/week6-twosample.html#from-last-time",
    "title": "Two sample inference",
    "section": "From last time",
    "text": "From last time\nPractice problem: test whether actual body weight exceeds desired body weight.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubject\nactual\ndesired\ndifference\n\n\n\n\n1\n265\n225\n40\n\n\n2\n150\n150\n0\n\n\n3\n137\n150\n-13\n\n\n4\n159\n125\n34\n\n\n5\n145\n125\n20\n\n\n\n\n\n\n\nweight.diffs &lt;- brfss$weight - brfss$wtdesire\nt.test(weight.diffs, \n       mu = 0, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  weight.diffs\nt = 4.2172, df = 59, p-value = 4.311e-05\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 10.99824      Inf\nsample estimates:\nmean of x \n 18.21667 \n\n\n\n\n\nThe data provide very strong evidence that the average U.S. adult’s actual weight exceeds their desired weight (T = 4.2172 on 59 degrees of freedom, p &lt; 0.0001).\n\nInference is on the mean difference: \\(H_0: \\delta = 0\\) vs. \\(H_A: \\delta &gt; 0\\).\nCan we also do inference on a difference in means?"
  },
  {
    "objectID": "content/week6-twosample.html#inference-for-paired-data",
    "href": "content/week6-twosample.html#inference-for-paired-data",
    "title": "Two sample inference",
    "section": "Inference for paired data",
    "text": "Inference for paired data\n\nCalculate paired differences.\n\n\n\n\n\n\n\n\n\n\n\n\nswimmer\nbody.suit.velocity\nswim.suit.velocity\nvelocity.diff\n\n\n\n\n1\n1.57\n1.49\n0.08\n\n\n2\n1.47\n1.37\n0.1\n\n\n3\n1.42\n1.35\n0.07\n\n\n\n\n\n\n\n\nPerform test as before.\n\n\n# test for paired difference\nt.test(diffs, mu = 0, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  diffs\nt = 12.318, df = 11, p-value = 4.443e-08\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 0.06620114        Inf\nsample estimates:\nmean of x \n   0.0775 \n\n\n\n\nReport the result of the test. You try."
  },
  {
    "objectID": "content/week6-twosample.html#formulating-a-two-sample-problem",
    "href": "content/week6-twosample.html#formulating-a-two-sample-problem",
    "title": "Two sample inference",
    "section": "Formulating a two-sample problem",
    "text": "Formulating a two-sample problem\n\n\nTwo-sample problems are characterized by:\n\none variable of interest\ntwo groups of observations\nobjective to compare group means\n\nInference concerns the difference in means\n\\[\\delta = \\mu_1 - \\mu_2\\]\nWe just tested:\n\\[H_0: \\mu_\\text{body} \\leq \\mu_\\text{swim}\\] \\[H_A: \\mu_\\text{body} &gt; \\mu_\\text{swim}\\]\n\nRearranging the data to emphasize two-sample problem structure:\n\n\n\n\n\n\n\n\n\n\nswimmer\nsuit\nvelocity\n\n\n\n\n1\nbody\n1.57\n\n\n1\nswim\n1.49\n\n\n2\nbody\n1.47\n\n\n2\nswim\n1.37\n\n\n3\nbody\n1.42\n\n\n\n\n\n\nvariable of interest: velocity\ngrouping: suit\npairing: swimmer"
  },
  {
    "objectID": "content/week6-twosample.html#hypotheses-for-two-sample-tests",
    "href": "content/week6-twosample.html#hypotheses-for-two-sample-tests",
    "title": "Two sample inference",
    "section": "Hypotheses for two-sample tests",
    "text": "Hypotheses for two-sample tests\nWe can articulate two-sided and directional tests for the difference in means \\(\\delta = \\mu_1 - \\mu_2\\) and the corresponding interpretation in terms of the group means.\n\n\nDifference in means\n\\[\\text{two-sided}\\begin{cases} H_0: \\delta \\qquad 0 \\\\ H_A: \\delta \\qquad 0 \\end{cases}\\]\n\\[\\text{lower-sided}\\begin{cases} H_0: \\delta \\qquad 0 \\\\ H_A: \\delta \\qquad 0 \\end{cases}\\]\n\\[\\text{upper-sided}\\begin{cases} H_0: \\delta \\qquad 0 \\\\ H_A: \\delta \\qquad 0 \\end{cases}\\]\n\nGroup interpretation\n\\[\\begin{cases} H_0: \\mu_1 \\qquad \\mu_2 \\\\ H_A: \\mu_1 \\qquad \\mu_2 \\end{cases}\\]\n\\[\\begin{cases} H_0: \\mu_1 \\qquad \\mu_2 \\\\ H_A: \\mu_1 \\qquad \\mu_2 \\end{cases}\\]\n\\[\\begin{cases} H_0: \\mu_1 \\qquad \\mu_2 \\\\ H_A: \\mu_1 \\qquad \\mu_2 \\end{cases}\\]"
  },
  {
    "objectID": "content/week6-twosample.html#your-turn-famuss",
    "href": "content/week6-twosample.html#your-turn-famuss",
    "title": "Two sample inference",
    "section": "Your turn: FAMuSS",
    "text": "Your turn: FAMuSS\n\nDoes resistance training lead to greater strength gains on the nondominant arm?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n\n\n\nArticulate and test an appropriate hypothesis for \\(\\delta = \\mu_\\text{ndrm} - \\mu_\\text{drm}\\)\n\n\n\nHypotheses: \\[H_0: \\hspace{15cm}\\] \\[H_A: \\hspace{15cm}\\]\n\n\n\nResult:"
  },
  {
    "objectID": "content/week6-twosample.html#evolution-of-darwins-finches",
    "href": "content/week6-twosample.html#evolution-of-darwins-finches",
    "title": "Two sample inference",
    "section": "Evolution of Darwin’s finches",
    "text": "Evolution of Darwin’s finches\n\n\nPeter and Rosemary Grant caught and measured birds from more than 20 generations of finches on Daphne Major.\n\nsevere drought in 1977 limited food to large tough seeds\nselection pressure favoring larger and stronger beaks\nhypothesis: beak depth increased in 1978 relative to 1976\n\n\n\n\n\n\n\n\n\n\n\nyear\ndepth\n\n\n\n\n1976\n10.8\n\n\n1976\n7.4\n\n\n1978\n11.4\n\n\n1978\n10.6\n\n\n\n\n\n\n\nTo answer this, we need to test a hypothesis involving two means:\n\\[\n\\begin{cases}\nH_0: &\\mu_{1976} = \\mu_{1978} \\\\\nH_A: &\\mu_{1976} &lt; \\mu_{1978}\n\\end{cases}\n\\]\n\ncan’t do inference on a mean difference here (no pairing of observations)\ntreat each year as an independent sample"
  },
  {
    "objectID": "content/week6-twosample.html#inference-for-independent-data",
    "href": "content/week6-twosample.html#inference-for-independent-data",
    "title": "Two sample inference",
    "section": "Inference for independent data",
    "text": "Inference for independent data\n\nBeak depths exemplify independent data: the groups of observations are unrelated.\n\n\n\nInference is based on the difference in group means:\n\\[\nT = \\frac{\\bar{x} - \\bar{y}}{SE(\\bar{x} - \\bar{y})}\n\\]\n\n\\(\\bar{x}, \\bar{y}\\) are groupwise sample means\n\\(SE(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{s_x^2}{n_x} + \\frac{s_y^2}{n_y}}\\)\ndegrees of freedom for \\(t\\) model are approximated\n\n\n\\[H_0: \\mu_{1976} \\geq \\mu_{1978}\\] \\[H_A: \\mu_{1976} &lt; \\mu_{1978}\\]\n\nt.test(Depth ~ Year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  Depth by Year\nt = -4.5833, df = 172.98, p-value = 4.37e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n      -Inf -0.427321\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.469663          10.138202"
  },
  {
    "objectID": "content/week6-twosample.html#two-input-formats",
    "href": "content/week6-twosample.html#two-input-formats",
    "title": "Two sample inference",
    "section": "Two input formats",
    "text": "Two input formats\n\n\nThe formula format takes inputs:\n\nan R formula\na data frame\n\n\n# two-sample test (formula inputs)\nt.test(depth ~ year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n\n\nDepth ~ Year: “depth depends on year”\n\nThe vector format takes inputs:\n\nvector of observations for one group\nvector of observations for the other group\n\n\n# two-sample test (vector inputs)\nt.test(depth76, depth78,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth76 and depth78\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean of x mean of y \n 9.453448 10.190769"
  },
  {
    "objectID": "content/week6-twosample.html#interpreting-results",
    "href": "content/week6-twosample.html#interpreting-results",
    "title": "Two sample inference",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\n\nt.test(depth ~ year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-paired-or-independent",
    "href": "content/week6-twosample.html#cloud-data-paired-or-independent",
    "title": "Two sample inference",
    "section": "Cloud data: paired or independent?",
    "text": "Cloud data: paired or independent?\n\nDoes dropping silver iodide onto clouds increase rainfall?\n\n\n\nData are rainfall measurements in a target area from 26 days when clouds were seeded and 26 days when clouds were not seeded.\n\nrainfall gives volume of rainfall in acre-feet\ntreatment indicates whether clouds were seeded\n\nHypotheses to test: \\[H_0: \\mu_\\text{seeded} \\quad \\mu_\\text{unseeded}\\] \\[H_A: \\mu_\\text{seeded} \\quad \\mu_\\text{unseeded}\\]\n\n\n\n\n\n\n\n\n\n\nrainfall\ntreatment\n\n\n\n\n703.4\nSeeded\n\n\n17.5\nSeeded\n\n\n242.5\nSeeded\n\n\n1698\nSeeded\n\n\n830.1\nUnseeded\n\n\n11.5\nUnseeded\n\n\n4.9\nUnseeded\n\n\n26.3\nUnseeded"
  },
  {
    "objectID": "content/week6-twosample.html#sleep-drugs-paired-or-independent",
    "href": "content/week6-twosample.html#sleep-drugs-paired-or-independent",
    "title": "Two sample inference",
    "section": "Sleep drugs: paired or independent?",
    "text": "Sleep drugs: paired or independent?\n\nWhich (if either) of two soporific drugs is more effective?\n\n\n\nData are extra hours of sleep for 10 study participants when taking each of two drugs.\n\nextra.sleep gives hours of additional sleep relative to control\ndrug indicates which sleep drug was taken\nsubject indicates study participant id\n\nHypotheses to test: \\[H_0: \\mu_1 \\quad \\mu_2\\] \\[H_A: \\mu_1 \\quad \\mu_2\\]\n\n\n\n\n\n\n\n\n\n\n\nextra.sleep\ndrug\nsubject\n\n\n\n\n0.7\n1\n1\n\n\n1.9\n2\n1\n\n\n-1.6\n1\n2\n\n\n0.8\n2\n2\n\n\n-0.2\n1\n3\n\n\n1.1\n2\n3"
  },
  {
    "objectID": "content/week6-twosample.html#test-assumptions",
    "href": "content/week6-twosample.html#test-assumptions",
    "title": "Two sample inference",
    "section": "Test assumptions",
    "text": "Test assumptions\n\n\nInference relies on a \\(t\\) model providing a good approximation to the sampling distribution. This requires three assumptions:\n\nvariable of interest is numeric and not too discrete\nobservations are independent (besides pairing)\neither:\n\nsample sizes are not too small\nor distribution(s) are symmetric and unimodal\n\n\n\nCommon issues:\n\n\n\n\n\n\n\nIssue\nConsequence\n\n\n\n\nHighly discrete data\n\\(t\\) model not appropriate\n\n\nDependent observations\n\\(SE\\) is a biased estimate: nominal error rates and coverage are inaccurate\n\n\nSmall samples with heavy skew or extreme outliers\n\\(SE\\) too small: inflated type I error and under-coverage\n\n\n\n\nIn each of these scenarios, different inference procedures should be used."
  },
  {
    "objectID": "content/week6-twosample.html#power-calculations",
    "href": "content/week6-twosample.html#power-calculations",
    "title": "Two sample inference",
    "section": "Power calculations",
    "text": "Power calculations\n\nHow much data do you need to collect in order to detect a difference of \\(\\delta\\)?\n\n\n\nThe statistical power of a test captures how often it detects a specified alternative.\n\nmeasures how often the test correctly rejects (proportion of samples)\nvalue depends on…\n\nmagnitude of difference between null value and true value of parameter\nsignificance level\nsample size\n\n\n\n\npower.t.test(power = 0.95, \n             delta = 0.5, \n             sig.level = 0.05, \n             type = 'two.sample',\n             alternative = 'two.sided')\n\n\n     Two-sample t test power calculation \n\n              n = 104.928\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\\(\\Rightarrow\\) need 105 observations in each group to detect a difference of 0.5 standard deviations for 95% of samples with a 5% significance level test"
  },
  {
    "objectID": "content/week6-twosample.html#the-equal-variance-t-test",
    "href": "content/week6-twosample.html#the-equal-variance-t-test",
    "title": "Two sample inference",
    "section": "The equal-variance \\(t\\)-test",
    "text": "The equal-variance \\(t\\)-test\nIf it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power by using a different standard error:\n\\[SE_\\text{pooled}(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{\\color{red}{s_p^2}}{n_x} + \\frac{\\color{red}{s_p^2}}{n_y}}\n\\quad\\text{where}\\quad \\color{red}{s_p} = \\underbrace{\\sqrt{\\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\\text{weighted average of } s_x^2 \\;\\&\\; s_y^2}\\]\nImplement by adding var.equal = T as an argument to t.test().\n\nlarger df is used, hence more frequent rejections\navoid unless you have a small sample\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/test2.html#problems",
    "href": "content/test2.html#problems",
    "title": "Test 2",
    "section": "Problems",
    "text": "Problems\n\nProblem 1: gifted children\n\n[L3, L4, L5] The gifted dataset contains data on 36 children identified as gifted in a large city. Assume for the purpose of the problem that the data are from a random sample of gifted children in the metropolitan region where the data were collected.\n\n[L3] Is there a relationship between the mother’s IQ and the child’s test score? Construct a scatterplot and compute and interpret the correlation.\n[L3] Repeat but with the father’s IQ.\n[L5] Given your results in (a)-(b), formulate a hypothesis about which parent’s IQ is higher. Explain your reasoning.\n[L5] Construct a histogram of the pairwise differences between the mother’s IQ and father’s IQ for each child in the dataset and check the assumptions for inference using the \\(t\\) model.\n[L4, L5] Test the hypothesis you proposed in (c) at the 1% level and provide a corresponding interval estimate. Report your results in the narrative style introduced in class.\n[L4, L5] It’s thought that the mean age by which infants can count to 10 is around two years old. Test the hypothesis at the 5% level that gifted children do this sooner and provide a corresponding interval estimate. Interpret the test results and interval estimate in context following the narrative style introduced in class.\n\n\n\n\nProblem 2: lizard running speeds\n\n[L3, L4, L5] The dataset lizards contains running speeds measured in a laboratory race track for two species of lizards, Western Fence (WF) and Sagebrush (S) lizards. Speeds are recorded in meters per second.\n\n[L3] Construct side-by-side boxplots of top speed by species. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Compute point estimates and standard errors for the mean top speed for each species.\n[L4] Compute and interpret 99.5% confidence intervals for the mean top speed for each species.\n[L4, L5] Test for a difference in mean top speed between species at the 1% significance level and provide an interval estimate at the appropriate confidence level. Interpret the test and estimate following the narrative style introduced in class.\n\n\n\n\nProblem 3: self- and cross-fertilization and plant vigor\n\n[L3, L4, L5] Does self-fertilization produce less vigorous plants than cross-fertilization? The dataset plants contains measurements of plant heights in inches for 15 pairs of plants of the same age; one plant in each pair was grown from a seed from a cross-fertilized flower, and the other was grown from a seed from a self-fertilized flower.\n\n[L3] Visualize the distribution of differences in plant heights between the cross-fertilized and self-fertilized individuals. Does the plot alone suggest an answer to the question of interest?\n[L4, L5] Test, at the 2% level, whether mean height of plants grown from cross-fertilized seeds exceeds that of plants grown from self-fertilized seeds and provide a confidence bound for the difference at the level corresponding to your test. Report the results of your analysis in context following the narrative style introduced in class.\n[LX] Do the data provide any evidence of a difference? Explain.\n\n\n\n\nProblem 4: creativity and motivation\n\n[L1, L2, L3, L4, L5] The creativity dataset contains data from an experiment on the effect of intrinsic vs. extrinsic motivation on creativity. A random sample of 47 creative writing students at an unnamed university were randomly assigned to one of two groups, extrinsic and intrinsic; each subject was instructed to write two short poems, but those in the extrinsic motivation group were primed on the task in a way that oriented them to external motivations for writing, and those in the intrinsic group were primed on the task in a way that oriented them to internal motivations for writing. Poems were scored by judges for creativity on a 40-point scale, and each subject received an average score.\n\n[L1] What is the study population? Based on the study description, is the sample representative, and if so, why?\n[L2] What type of study is this? Based on the study description, can the data support causal inferences about motivation and creativity, and if so, why?\n[L3] Construct an appropriate graphical summary comparing the distributions of average scores by treatment group.\n[L3] Provide appropriate summary statistics indicating the center, spread, and number of observations of average scores in each group.\n[L5] Test the hypothesis that motivational framing has no effect on creativity at the 1% significance level. Compute an interval estimate for the difference in mean scores at the level corresponding to your test. Use your results from (c)-(d) to check assumptions.\n[LX] Write a short narrative summary of your results in (e)-(f) following the style introduced in class. (Don’t forget to include a point estimate and standard error.)"
  },
  {
    "objectID": "content/test2-practice.html",
    "href": "content/test2-practice.html",
    "title": "Test 2 practice problems",
    "section": "",
    "text": "Test 2 information\nThe test will comprise four problems focused on point estimation, interval estimation, and hypothesis tests for means (learning outcomes L4-L5). Each problem will have multiple parts, some of which may require skills from earlier (especially summary statistics and statistical graphics).\nYou will have 48 hours to complete the test; a Posit cloud project will be provided with comment outlines to help you organize your calculations. You’ll submit your work via an online form, and will be expected to also upload your R script from your Posit cloud project.\nThe problems below are intended to help you practice the skills and concepts that will be assessed in the test. An expandable “solution” is provided below each prompt that shows the calculations needed to answer the prompts; of course, resolving the problems satisfactorily also requires interpreting results accurately. You’re encouraged to ask about interpretations in class.\n\n\nPractice problems\n\n[L3, L4] The tuition dataset contains in-state and out-of-state tuition at a random sample of 25 public universities from 2011-2012.\n\n[L3] Visualize the distribution of differences between in-state and out-of-state tuition. Comment on whether the assumptions for inference using the \\(t\\) model seem appropriate.\n[L4] Calculate and interpret a 95% confidence interval for the mean difference between in-state and out-of-state tuition.\n[L4] Interpret your interval in context following the style introduced in class.\n\n\n\n\nSolution\n# load data\nload('data/tuition.RData')\n\n# part a: visualize distribution of differences; are assumptions for use of t model met?\ntuition.diffs &lt;- tuition$out.of.state - tuition$in.state\nhist(tuition.diffs, breaks = 10)\n\n# part b: 95% interval estimate for differences\nt.test(tuition.diffs)$conf.int\n\n\n\n[L3, L4, L5] [challenge problem] The dataset cancer contains skin cancer rates per 100,000 people in Connecticut each year from 1938 to 1972. Each year is also classified as following a period of higher than average or lower than average sunspot activity. The delta variable is the change in cancer rate relative to the previous year. In this problem, you’ll perform inference on the mean delta by sunspot activity level to determine whether higher than average sunspot activity is associated with an increase in mean skin cancer rates from the prior year.\n\n[L4] Estimate the mean delta (irrespective of sunspot activity level). Provide both a point estimate and standard error, and interpret the estimate in context. Does the estimate suggest that the cancer rate is increasing or decreasing? Explain.\n[L4, L5] Perform a test for mean delta to determine whether the mean cancer rate is increasing. Use a 5% significance level, and report your test result together with an interval estimate following the narrative style introduced in class.\n[L3] Plot the ‘raw’ cancer rate (i.e., not the delta) against year. (Add the argument type = 'b' to draw a path connecting the observations.) Is your answer in (b) consistent with any trend(s) you see?\n[L3] Make a side-by-side boxplot of the delta variable for each level of sunspot activity. Comment on the plot: does there seem to be a difference?\n[L5] Test whether the mean change in cancer rate is higher in years with higher than average sunspot activity. Carry out inference at the 5% significance level.\n\n\n\n\nSolution\n# load data\nload('data/cancer.RData')\n\n# part a: point estimate of mean delta and standard error\nmean(cancer$delta)\nsd(cancer$delta)/sqrt(length(cancer))\n\n# part b: is delta increasing? test at 5% level\nt.test(cancer$delta, mu = 0, alternative = 'greater', conf.level = 0.95)\n\n# part c: plot \nplot(cancer$year, cancer$rate, type = 'b')\n\n# part d: boxplots by activity level; different?\nboxplot(delta ~ sunspot, data = cancer, horizontal = T)\n\n# part e: test for a difference in mean delta by sunspot activity at the 5% level\nt.test(delta ~ sunspot, alternative = 'greater', data = cancer, conf.level = 0.95)\n\n\n\n[L4, L5] Studies have provided evidence that the hippocampus is smaller in schizophrenic patients on average. The dataset hippocampus contains data on volumes of the left hippocampus in cubic centimeters for 15 pairs of monozygotic twins; one twin in each pair was affected by schizophrenia and the other was not.\n\n[L5] Compute the pairwise differences in hippocampal volume by twin pair and inspect the distribution. Do assumptions for inference using the \\(t\\) model seem plausible?\n[L5] Formulate a hypothesis to test whether hippocampal volume is smaller among the affected twin on average. Write the hypotheses in notation.\n[L4, L5] Carry out the test in (b) at the 1% significance level. Report your test result along with a corresponding interval estimate following the narrative style introduced in class.\n\n\n\n\nSolution\n# load and inspect data\ntwins &lt;- read_csv('data/hippocampus.RData')\nhead(twins)\n\n# part a: compute differences; check distribution for t inference assumptions\nhvolume.diff &lt;- twins$affected - twins$unaffected\nhist(hvolume.diff, breaks = 5)\n\n# part b:\nt.test(hvolume.diff, mu = 0, alternative = 'less', conf.level = 0.99)"
  },
  {
    "objectID": "content/week6-twosample.html#paired-differences",
    "href": "content/week6-twosample.html#paired-differences",
    "title": "Two sample inference",
    "section": "Paired differences",
    "text": "Paired differences\nPractice problem from last time: test whether actual exceeds desired body weight.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubject\nactual\ndesired\ndifference\n\n\n\n\n1\n265\n225\n40\n\n\n2\n150\n150\n0\n\n\n3\n137\n150\n-13\n\n\n4\n159\n125\n34\n\n\n5\n145\n125\n20\n\n\n\n\n\n\n\nweight.diffs &lt;- brfss$weight - brfss$wtdesire\nt.test(weight.diffs, \n       mu = 0, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  weight.diffs\nt = 4.2172, df = 59, p-value = 4.311e-05\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 10.99824      Inf\nsample estimates:\nmean of x \n 18.21667 \n\n\n\n\n\nThe data provide very strong evidence that the average U.S. adult’s actual weight exceeds their desired weight.\n\nInference is on the mean difference: \\(H_0: \\delta = 0\\) vs. \\(H_A: \\delta &gt; 0\\).\nCan we also do inference on a difference in means based on two independent samples?"
  },
  {
    "objectID": "content/week6-twosample.html#two-sample-inference",
    "href": "content/week6-twosample.html#two-sample-inference",
    "title": "Two sample inference",
    "section": "Two-sample inference",
    "text": "Two-sample inference\nIf \\(x_1, \\dots, x_{58}\\) are the 1976 observations and \\(y_1, \\dots, y_{65}\\) are the 1978 observations:\n\n\\(\\bar{x}\\) is a point estimate for \\(\\mu_{1976}\\) with standard error \\(SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\)\n\\(\\bar{y}\\) is a point estimate for \\(\\mu_{1978}\\) with standard error \\(SE(\\bar{y}) = \\frac{s_y}{\\sqrt{n}}\\)\n\n\n\nInference uses a new \\(T\\) statistic:\n\\[\nT = \\frac{\\bar{x} - \\bar{y} - \\delta_0}{SE(\\bar{x} - \\bar{y})}\n\\]\n\n\\(\\delta_0\\) is the hypothesized difference in means\n\\(SE(\\bar{x} - \\bar{y}) = \\sqrt{SE(\\bar{x}) + SE(\\bar{y})}\\)\n\\(t_\\nu\\) model approximates the sampling distribution when each sample meets assumptions for one-sample inference"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data",
    "href": "content/week6-twosample.html#cloud-data",
    "title": "Two sample inference",
    "section": "Cloud data",
    "text": "Cloud data\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\nData are rainfall measurements in a target area from 26 days when clouds were seeded and 26 days when clouds were not seeded.\n\nrainfall gives volume of rainfall in acre-feet\ntreatment indicates whether clouds were seeded\n\nHypotheses to test: \\[\n\\begin{cases}\nH_0: &\\mu_\\text{seeded} = \\mu_\\text{unseeded} \\\\\nH_A: &\\mu_\\text{seeded} &gt; \\mu_\\text{unseeded}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\nrainfall\ntreatment\n\n\n\n\n334.1\nseeded\n\n\n489.1\nseeded\n\n\n200.7\nseeded\n\n\n40.6\nseeded\n\n\n21.7\nunseeded\n\n\n17.3\nunseeded\n\n\n68.5\nunseeded\n\n\n830.1\nunseeded"
  },
  {
    "objectID": "content/week6-twosample.html#difference-in-means",
    "href": "content/week6-twosample.html#difference-in-means",
    "title": "Two sample inference",
    "section": "Difference in means",
    "text": "Difference in means\nWhat if we want to compare two population means based on independent samples?"
  },
  {
    "objectID": "content/week6-twosample.html#interpreting-outputs-and-results",
    "href": "content/week6-twosample.html#interpreting-outputs-and-results",
    "title": "Two sample inference",
    "section": "Interpreting outputs and results",
    "text": "Interpreting outputs and results\n\n\n\nt.test(depth ~ year, data = finch,\n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n\n\n\n\nThe data provide very strong evidence that mean beak depth increased following the drought (T = -4.5727 on 111.79 degrees of freedom, p &lt; 0.0001). With 95% confidence, the mean increase is estimated to be at least 0.4699 mm, with a point estimate of 0.7373 (SE 0.1612).\n\n\n\nHighly similar, but notice:\n\ninput is a formula depth ~ year (“depth depends on year”) and data frame finch\nmu now indicates hypothesized difference in means\ndecimal degrees of freedom\nalternative is relative to the order in which groups appear"
  },
  {
    "objectID": "content/week6-twosample.html#directional-and-non-directional-hypotheses",
    "href": "content/week6-twosample.html#directional-and-non-directional-hypotheses",
    "title": "Two sample inference",
    "section": "Directional and non-directional hypotheses",
    "text": "Directional and non-directional hypotheses"
  },
  {
    "objectID": "content/week6-twosample.html#directional-and-nondirectional-hypotheses",
    "href": "content/week6-twosample.html#directional-and-nondirectional-hypotheses",
    "title": "Two sample inference",
    "section": "Directional and nondirectional hypotheses",
    "text": "Directional and nondirectional hypotheses"
  },
  {
    "objectID": "content/week6-twosample.html#directional-and-two-sided-tests",
    "href": "content/week6-twosample.html#directional-and-two-sided-tests",
    "title": "Two sample inference",
    "section": "Directional and two-sided tests",
    "text": "Directional and two-sided tests"
  },
  {
    "objectID": "content/week6-twosample.html#directional-and-nondirectional-tests",
    "href": "content/week6-twosample.html#directional-and-nondirectional-tests",
    "title": "Two sample inference",
    "section": "Directional and nondirectional tests",
    "text": "Directional and nondirectional tests"
  },
  {
    "objectID": "content/week6-twosample.html#checking-assumptions",
    "href": "content/week6-twosample.html#checking-assumptions",
    "title": "Two sample inference",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\nThe two-sample test is appropriate whenever two one-sample tests would be.\n\n\n\nIn other words, the test assumes that both samples are either:\n\nsufficiently large; or\nhave little skew and few outliers\n\nTo check, simply inspect each histogram.\n\nboth distributions unimodal\nboth a bit left skewed\nno extreme outliers\nlarge sample sizes (58, 65)"
  },
  {
    "objectID": "content/week6-twosample.html#checking-assumptions-alternative",
    "href": "content/week6-twosample.html#checking-assumptions-alternative",
    "title": "Two sample inference",
    "section": "Checking assumptions (alternative)",
    "text": "Checking assumptions (alternative)\n\nThe two-sample test is appropriate whenever two one-sample tests would be.\n\n\n\nIn other words, the test assumes that both samples are either:\n\nsufficiently large; or\nhave little skew and few outliers\n\nCould also check side-by-side boxplots for:\n\napproximate symmetry of boxes\noutliers far from whiskers\n\nThis is also a nice visualization of differences between samples."
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-which-test",
    "href": "content/week6-twosample.html#cloud-data-which-test",
    "title": "Two sample inference",
    "section": "Cloud data: which test?",
    "text": "Cloud data: which test?\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.9731\nalternative hypothesis: true difference in means between group Seeded and group Unseeded is less than 0\n95 percent confidence interval:\n     -Inf 512.1582\nsample estimates:\n  mean in group Seeded mean in group Unseeded \n              441.9846               164.5885 \n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group Seeded and group Unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group Seeded mean in group Unseeded \n              441.9846               164.5885"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-which-alternative",
    "href": "content/week6-twosample.html#cloud-data-which-alternative",
    "title": "Two sample inference",
    "section": "Cloud data: which alternative?",
    "text": "Cloud data: which alternative?\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'less')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.9731\nalternative hypothesis: true difference in means between group seeded and group unseeded is less than 0\n95 percent confidence interval:\n     -Inf 512.1582\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\n\nR always uses the first observation to determine which group comes first; you can tell from which estimate is printed first.\n\n'greater' is interpreted as [FIRST GROUP] &gt; [SECOND GROUP]\n'less' is interpreted as [FIRST GROUP] &lt; [SECOND GROUP]"
  },
  {
    "objectID": "content/week6-twosample.html#another-input-format",
    "href": "content/week6-twosample.html#another-input-format",
    "title": "Two sample inference",
    "section": "Another input format",
    "text": "Another input format\nIf you have trouble keeping track of directions, another option is to give t.test two vectors; it will always interpret the alternative in the order you specify.\n\n# extract observations in each group\nseeded &lt;- cloud |&gt; filter(treatment == 'seeded')\nunseeded &lt;- cloud |&gt; filter(treatment == 'unseeded') \n\n# perform t test\nt.test(seeded$rainfall, unseeded$rainfall, mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  seeded$rainfall and unseeded$rainfall\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\nmean of x mean of y \n 441.9846  164.5885"
  },
  {
    "objectID": "content/week6-twosample.html#cloud-data-interpretation",
    "href": "content/week6-twosample.html#cloud-data-interpretation",
    "title": "Two sample inference",
    "section": "Cloud data: interpretation",
    "text": "Cloud data: interpretation\n\nDoes seeding clouds with silver iodide increase mean rainfall?\n\n\n\n\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n\n\n    Welch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n\n\n\n\nThe data provide moderate evidence that cloud seeding increases mean rainfall (T = 1.9982 on 33.855 degrees of freedom, p = 0.02689). With 95% confidence, seeding is estimated to increase mean rainfall by at least 42.63 acre-feet, with a point estimate of 277.4 (SE 138.8199)."
  },
  {
    "objectID": "content/week6-twosample.html#body-temperatures-again",
    "href": "content/week6-twosample.html#body-temperatures-again",
    "title": "Two sample inference",
    "section": "Body temperatures (again)",
    "text": "Body temperatures (again)\n\nDoes mean body temperature differ between men and women?\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest \\(H_0: \\mu_F = \\mu_M\\) against \\(H_A: \\mu_F \\neq \\mu_M\\)\n\nt.test(body.temp ~ sex, data = temps, \n       mu = 0, alternative = 'two.sided')\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 1.7118, df = 34.329, p-value = 0.09595\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.09204497  1.07783444\nsample estimates:\nmean in group female   mean in group male \n            98.65789             98.16500 \n\n\n\n\nSuggestive but insufficient evidence that mean body temperature differs by sex.\nNotice: estimated difference (F - M) is 0.493 °F (SE 0.2879)"
  },
  {
    "objectID": "content/week6-twosample.html#what-if-we-had-more-data",
    "href": "content/week6-twosample.html#what-if-we-had-more-data",
    "title": "Two sample inference",
    "section": "What if we had more data?",
    "text": "What if we had more data?\nHere are estimates from two larger samples of 65 individuals each (compared with 19, 20):\n\n\n\n\n\n\n\n\n\n\n\nsex\nmean.temp\nse\nn\n\n\n\n\nfemale\n98.39\n0.09222\n65\n\n\nmale\n98.1\n0.08667\n65\n\n\n\n\n\n\nestimated difference (F - M) is smaller 0.2892 °F\nbut so is the standard error SE 0.1266 (recall more data \\(\\longleftrightarrow\\) better precision)\n\n\n\n\nt.test(body.temp ~ sex, data = temps.aug, \n       mu = 0, alternative = 'two.sided')\n\n\n    Welch Two Sample t-test\n\ndata:  body.temp by sex\nt = 2.2854, df = 127.51, p-value = 0.02394\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n 0.03881298 0.53964856\nsample estimates:\nmean in group female   mean in group male \n            98.39385             98.10462 \n\n\n\n\nThe data provide moderate evidence that mean body temperature differs by sex (T = 2.29 on 127.51 degrees of freedom, p = 0.02394)."
  },
  {
    "objectID": "content/week6-twosample.html#a-paradox",
    "href": "content/week6-twosample.html#a-paradox",
    "title": "Two sample inference",
    "section": "A paradox?",
    "text": "A paradox?\n\nIf you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo keep in mind:\n\nstatistical significance \\(\\neq\\) practical significance\nalways check your point estimates"
  },
  {
    "objectID": "content/week6-twosample.html#a-surprising-result",
    "href": "content/week6-twosample.html#a-surprising-result",
    "title": "Two sample inference",
    "section": "A surprising result",
    "text": "A surprising result\n\nIf you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo keep in mind:\n\nstatistical significance \\(\\neq\\) practical significance\nalways check your point estimates"
  },
  {
    "objectID": "content/week6-twosample.html#a-seeming-paradox",
    "href": "content/week6-twosample.html#a-seeming-paradox",
    "title": "Two sample inference",
    "section": "A seeming paradox",
    "text": "A seeming paradox\n\nIf you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo keep in mind:\n\nstatistical significance \\(\\neq\\) practical significance\nalways check your point estimates"
  },
  {
    "objectID": "content/week6-twosample.html#a-statistical-trap",
    "href": "content/week6-twosample.html#a-statistical-trap",
    "title": "Two sample inference",
    "section": "A statistical trap",
    "text": "A statistical trap\n\nIf you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo keep in mind:\n\nstatistical significance \\(\\neq\\) practical significance\nalways check your point estimates"
  }
]