[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics for Life Sciences",
    "section": "",
    "text": "Announcements\n\n\n\nAssignment reminders:\n\nPS2 due one hour before class Wednesday.\nPS1 corrections due by 5pm on Friday.\nPS3 due one hour before class next Monday (4/15).\n\n\n\n\nCourse information\nRead the [course syllabus] for detailed information on content, materials, learning outcomes, assessments, and course policies.\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment]\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class.\n\n\n\nWeek 1 (4/1/24)\nAcademic holiday 4/1/24\nIntroduction to statistical thinking and study designs\nWednesday class meeting: [slides] [activity] [problem set] [problem set corrections]\n\n[reading] Vu and Harrington 1.1\n[lecture/discussion] course introduction; study designs\n[activity] distinguishing types of studies\n\n\n\nWeek 2 (4/8/24)\nData types and descriptive statistics\nMonday class meeting [slides] [lab] [problem set]\n\nreading quiz [12pm section] [2pm section]\n[reading] Vu and Harrington 1.2\n[lecture/discussion] data types\n[lab] R basics\n\nWednesday class meeting [slides] [lab] [problem set]\n\n[reading] Vu and Harrington 1.4 - 1.5\n[lecture/discussion] descriptive statistics\n[lab] descriptive statistics in R\n\n\n\nWeek 3 (4/15/24)\nDescriptive statistics and graphical summaries\n\n\n\n\n\nTest 1 due Friday 4/19 11:59pm PDT\n\n\nWeek 4 (4/22/24)\nFoundations for inference\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5 (4/29/24)\nOne-sample inference for numerical data\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 (5/6/24)\nTwo-sample inference for numerical data\n\n\n\n\n\n\n\n\n\nTest 2 due Friday 5/10 11:59pm PDT\n\n\nWeek 7 (5/13/24)\nNonparametric tests; analysis of variance\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 (5/20/24)\nPost-hoc inference in ANOVA; introduction to inference for categorical data\n\n\n\n\n\n\n\n\nTest 3 due Friday 5/24 11:59pm PDT\n\n\nWeek 9 (5/27/24)\nAcademic holiday 5/27/24\nCategorical data analysis for contingency tables\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 (6/3/24)\nSimple linear regression\n\n\n\n\n\n\n\n\n\nTest 3 due Friday 6/7 11:59pm PDT\n\n\nFinals week (6/10/24)\nOral exams to be held during scheduled exam time\nScheduled exam times:\n\n[12pm section] Wednesday 6/12 10:10am – 1:00pm\n[2pm section] Monday 6/10 1:10pm – 4:00pm"
  },
  {
    "objectID": "content/syllabus.html",
    "href": "content/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Statistics plays a crucial role in the sciences: statistical techniques provide a means of weighing quantitative evidence derived from observation and experimentation while accounting for uncertainty. Statistical thinking and data analysis also facilitate discovery, exploration, and hypothesis generation. This class aims to provide a hands-on introduction to common statistical methods used almost universally across the sciences — descriptive and graphical techniques, inferential methods for comparing population means, analysis of categorical data and contingency tables, and linear regression — while drawing on examples from the life sciences to help illuminate the potential for application in students’ chosen field(s) of study and providing basic training in the use of statistical software.\n\n\nCourse information\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nClass meetings will comprise a mixture of lecture, lab activities, class activities, and discussion.\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment].\nThese times are partitioned into 15 minute intervals that you can schedule via the appointment link above; this system is intended to minimize waiting times and guarantee one-on-one availability. Slots can be scheduled anywhere from 7 calendar days to 10 minutes in advance. While drop-ins are welcome, I can’t guarantee availability outside of scheduled times.\nCatalog Description: Data collection and experimental design, descriptive statistics, confidence intervals, parametric and non parametric one and two-sample hypothesis tests, analysis of variance, correlation, simple linear regression, chi-square tests. Applications of statistics to the life sciences. Substantial use of statistical software. Prerequisite: MATH 96; or MATH 115; or appropriate Math Placement Level. Fulfills GE Area B4 (GE Area B1 for students on the 2019-20 or earlier catalogs); a grade of C- or better is required in one course in this GE area.\n\n\nMaterials\nYou’ll need an internet-connected laptop or tablet (a keyboard is necessary since we will do some web-hosted computation and you will be expected to type assignments). You should expect to bring your laptop or tablet to every class meeting.\nComputing: use of R/RStudio will be hosted online via a posit.cloud workspace [link to join]. To access the workspace, you’ll need to create a posit.cloud account and purchase a $5/month student plan.\nTextbook: Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences, First edition. A PDF and tablet-friendly version are available for free online at the link above. This will be our primary reference and we will cover chapters 1 – 2, 4 – 6, and 8.\nCourse notes: course notes will be posted as slides on the course website.\nOther references:\n\nVan Belle, Fisher, Heagerty, and Lumley (2004). Biostatistics: a methodology for the health sciences. Wiley. A PDF can be obtained through the Kennedy Library via the link above. This text provides a thorough introduction to biostatistics (statistics for life sciences) and is an excellent reference for more depth of coverage. Select readings will be assigned from this book.\nDouglas et al. (2023). An Introduction to R. This online book covers a variety of introductory topics pertaining to R/RStudio: installation, packages, files and directories, objects, functions, data types, data structures, graphics, basic statistics, markdown, and version control. Select readings will be assigned from this book.\n\n\n\nLearning outcomes\nThis course aims to support you in developing the following abilities.\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques\n[L4] construct and interpret confidence intervals for means and differences between means for independent and paired samples\n[L5] conduct parametric and non-parametric two-sample hypothesis tests for means\n[L6] construct and interpret a confidence interval for a single proportion\n[L7] conduct Chi-square goodness-of-fit tests and tests for independence\n[L8] distinguish between case-control and cohort studies and compute relative-risk and odds in the appropriate settings\n[L9] perform analysis of variance tests and post-hoc comparisons for completely randomized designs\n[L10] use simple linear regression to describe relationships between variables\n[L11] apply one or more methods from the course to your major field of study\n\nEmphasis is placed on conceptual fluency, application, and interpretation. In addition, you will learn to perform simple statistical analyses in R and can expect to develop a basic familiarity with the software; however, as this is not a programming class, the R environment will not be discussed in any detail and you will only learn to use a handful of commands.\n\n\nAssessments\nAttainment of learning outcomes will be measured by performance on homework assignments, tests, and a short project with an oral assessment in lieu of a final exam.\n\nHomework assignments will be given at the end of every class meeting and will comprise two practice problems due by the next class meeting. These are your opportunity to practice applying course concepts and methods covered in class and will help you to keep current with the pace and content of the lectures.\nTests will be given every 2-3 weeks and will comprise roughly 10-20 problems each. These are your opportunity to demonstrate that you’ve synthesized course material and achieved learning outcomes, and you will have approximately 48 hours to complete each test. One round of revisions will be allowed for each test in which you can make up full credit for any problems answered incorrectly in your initial attempt.\nA project with an oral assessment will be given in place of a final exam. However, you will need to be available in person during the scheduled final exam time, as this is when the oral assessment will take place.\n\nEvery assessed problem will be matched to one of the learning outcomes L1-L10. All submitted work will be assessed on a question-by-question basis as satisfactory (S) or needing improvement (NI) according to whether responses are fully correct. The percentage of problems matched to a particular learning outcome for which you receive a satisfactory assessment provides a measure of your attainment of that learning outcome. These percentages form a basis for determining your course grade (see below).\nDue to limited resources we will only provide qualitative feedback on a small subset of assessed questions, and only when an assessment of NI is made. As such, it is your responsibility to seek the feedback you need to correct your understanding where needed via class engagement, office hours, peer consultation, further study, and [tutoring resources].\n\n\nLetter grades\nStudents will receive a score for each learning outcome representing the (possibly weighted) proportion of questions matched with that outcome that received a satisfactory assessment across all assignments. The outcome will be assessed as follows:\n\n‘fully met’ if the proportion is at least 0.8\n‘partly met’ if the proportion is between 0.5 and 0.8\n‘unmet’ otherwise\n\nYou will receive periodic email summaries of your progress on each learning outcome. To receive a passing grade in the class, at least six outcomes must be either partly or fully met. Subject to this condition, letter grades are then defined as follows:\n\n\n\nGrade\nNumber of fully met outcomes\n\n\n\n\nA\n10\n\n\nA-\n9\n\n\nB+\n8\n\n\nB\n7\n\n\nB-\n6\n\n\nC+\n5\n\n\nC\n4\n\n\nC-\n3\n\n\nD+\n2\n\n\nD\n1\n\n\nD-\n0\n\n\n\nPlease note that these definitions are tentative and potentially subject to change; however, I will not make the grading requirements more stringent under any circumstances.\nPlease also note that failure to adhere to course policies may result in a lower letter grade than would otherwise be assigned.\n\n\nTentative schedule\nSubject to change.\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings (V&H)\nAssessments\n\n\n\n\n1 (4/1/24)\nIntroduction to statistical thinking and study design\n1.1\n\n\n\n2 (4/8/24)\nData, data types, and data collection\n1.2\n\n\n\n3 (4/15/24)\nDescriptive statistics and graphical summaries\n1.4, 1.5, 1.6\nTest 1 [L1, L2, L3]\n\n\n4 (4/22/24)\nFoundations for inference\n4.1, 4.2\n\n\n\n5 (4/29/24)\nOne-sample inference for numerical data\n4.3, 5.1\n\n\n\n6 (5/6/24)\nTwo-sample inference for numerical data\n5.2, 5.3, 5.4\nTest 2 [L4, L5]\n\n\n7 (5/13/24)\nNonparametric tests; analysis of variance\n5.5\n\n\n\n8 (5/20/24)\nPost-hoc inference in ANOVA; intro to categorical data analysis\n8.1\nTest 3 [L6, L9]\n\n\n9 (5/27/24)\nCategorical data analysis and contingency tables\n8.3, 8.5.1, 8.5.3\n\n\n\n10 (6/3/24)\nSimple linear regression\n6.1, 6.2, 6.4, 6.5\nTest 4 [L7, L8, L10]\n\n\nFinals (6/10/24)\nN/A\nN/A\nOral project assessment [L11]\n\n\n\n\n\nCourse policies\n\nTime commitment\nSTAT218 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including lectures, reading, assignments, and study time. Some variability in workload by week should be expected, and most students will need to budget a few extra hours each week. However, students can expect to be able to meet course expectations with a time commitment of 12-16 hours per week. Considering that class meetings account for four hours per week, students should anticipate devoting 8-12 hours outside of class. If you are spending considerably more time than this on a regular basis, please let me know.\n\n\nAttendance\nRegular attendance is essential for success in the course and required per University policy. Each student may miss two class meetings without notice but additional absences should be excusable and students should notify the instructor. Unexcused absences may negatively impact course grades.\n\n\nDeadlines and extensions\nA one-hour grace period is applied to all deadlines. Work submitted more than one hour after a deadline is considered late. Policies regarding late work are as follows:\n\nYou may turn in as many as four homework assignments up to 48 hours late without penalty at any time during the quarter and without notice. Subsequently, late work may incur a penalty in final grade calculations.\nLate submissions are not allowed for tests. You are expected to plan ahead in order to meet test deadlines; I recommend putting the dates in your calendar at the beginning of the quarter.\nExceptions may be granted for significant and unforeseen challenges (medical absences, family emergencies, and the like).\n\nExtensions may be arranged as needed if warranted by the circumstances and should be requested by email. When requesting an extension, you should explain why it is needed; it is at my discretion to grant the extension or not based on the reason provided. Extensions must be arranged at least 24 hours in advance of the original deadline; requests made after this time will not be considered as a general rule.\nThese policies are intended to provide you with some flexibility to work around unforeseen circumstances while maintaining accountability for completing coursework in a timely manner. That said, if any circumstances arise that the policies do not accommodate well, please let me know and I will do my best to work with you to keep you on track in the course.\n\n\nAcademic integrity\nYou are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR). Particularly important course policies related to academic integrity are discussed below.\nCollaboration. Collaboration among enrolled students is allowed and encouraged on homework assignments subject to the condition that every collaborator must make material contributions. Material contributions might include participation in group discussions, critique or presentation of a proposed solution, comparing numerical answers, and the like. However, group submissions are not allowed and you are expected to write up your own work. Copying the work of another student outright, knowingly allowing another student to copy your work, or submitting a copy of a shared set of answers is not acceptable and amounts to a violation of University policy on academic integrity. The best way to adhere to this policy and ensure your collaborations are productive is to:\n\nattempt problems individually before consulting others\nwrite up your own solutions in private\n\nCollaboration is not permitted on tests and will result in loss of credit.\nUse of AI. Learning to use AI effectively and responsibly for problem-solving in an academic context is a skill unto itself. Submitting problem prompts directly to ChatGPT will, most of the time, return superfluous, tangential, and erroneous answers that do not meet assessment criteria for satisfactory work. Furthermore, even when AI-generated material is technically accurate, outputs rarely conform to the examples set forth in class or the solution strategies that you have been taught.\nSo in the best-case scenario, AI-generated material might be useful but only if you expend additional effort refine the prompts you use and subsequently to parse, understand, and integrate outputs with class content. In the worst-case scenario, AI-generated material will be wrong or irrelevant and simply confuse you. Considering you are learning material that is new to you, you will most likely not be able to distinguish correct from incorrect outputs – if you could, you would have had no need to query in the first place – and it will therefore be difficult if not impossible to use AI effectively. Thus, using AI is more likely to hinder than to help your learning, and for this reason I do not recommend it.\nShould you choose to use AI you must use it as an aid only and not as a substitute for doing your own work. You will be responsible for using it thoughtfully and judiciously. That means critically assessing any outputs and continuing to prepare work to be submitted in your own words and using your own analyses. Submitting AI-generated outputs directly is never acceptable — doing so amounts to falsely representing material that you did not create as your own work and is a violation of University academic integrity policy. I will respond to such violations as follows.\n\nsome AI-generated content detected: loss of credit and warning\nflagrant AI plagiarism, first offense: loss of credit and report to OSRR\nflagrant AI plagiarism, second offense: automatic course failure and report to OSRR\n\nIf you are unsure about where the line is between acceptable and unacceptable use in any particular situation, please discuss the situation with me – I’d much rather help you learn to navigate the issue without the use of penalties wherever possible.\n\n\nAssessments and final grades\nI make every effort to provide consistent, fair, and accurate evaluation of student work. Please notify me of any suspected errors or discrepancies in evaluation promptly on an assignment-by-assignment basis (i.e., not at the end of the quarter) to guarantee consideration. Final (letter) grades will only be changed in the case of clerical errors. Attempting to negotiate scores or final grades is not appropriate.\nPer University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. If you feel your grade is unfairly assigned at the end of the course, you have the right to appeal it according to the procedure outlined here.\n\n\nCommunication and email\nStudents are encouraged to use face-to-face means of communication (class meetings and office hours) when possible. Every effort is made to respond to email within 48 weekday hours; please be aware that a message sent Thursday or Friday may not receive a reply until Monday or Tuesday. Time-sensitive messages should be identified as such in the subject line. For non-time-sensitive messages, please wait one week before sending a reminder.\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nCopyright and distribution of course materials\nStudents are not permitted to share or distribute any course materials without the written consent of the instructor. This includes, in particular, uploading materials or prepared solutions to online services and sharing materials or prepared solutions with students who may take the course in a future term. Transgressions of this policy compromise the effectiveness of instruction and assessment and do a disservice to current and future students."
  },
  {
    "objectID": "content/week2-descriptive.html#todays-agenda",
    "href": "content/week2-descriptive.html#todays-agenda",
    "title": "Descriptive statistics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] frequency distributions; measures of spread and center\n[lab] descriptive statistics and simple graphics in R"
  },
  {
    "objectID": "content/week2-descriptive.html#last-time",
    "href": "content/week2-descriptive.html#last-time",
    "title": "Descriptive statistics",
    "section": "Last time",
    "text": "Last time\n\n\n\nData semantics\n\n\ncategorical data: ordinal (ordered) or nominal (unordered)\nnumeric data: continuous (no ‘gaps’) or discrete (‘gaps’)\n\n\nData types and data structures in R\n\n\nbasic types: numeric, character, logical, integer\na vector is a collection of values of one type\na data frame is a type-heterogeneous list of vectors of equal length\n\n\nVectors can store observations of one variable:\n\n# 4 observations of age\nages &lt;- c(18, 22, 18, 12)\nages\n\n[1] 18 22 18 12\n\n\nData frames can store observations of many variables:\n\n# 3 observations of 2 variables\ndata.frame(subject.id = c(11, 2, 31),\n           age = c(24, 31, 17),\n           sex = c('m', 'm', 'f'))\n\n  subject.id age sex\n1         11  24   m\n2          2  31   m\n3         31  17   f\n\n\n\n\nTechniques for summarizing data depend on the data type"
  },
  {
    "objectID": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "href": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "title": "Descriptive statistics",
    "section": "What are descriptive statistics?",
    "text": "What are descriptive statistics?\nWe learned last time that a statistic is a data summary, i.e., any function of a set of observations.\nDescriptive statistics refers to analysis of sample characteristics using summary statistics.\n\nthese are data analyses that uses statistics interpreted on face value\nin contrast to inferential statistics, which uses statistics interpreted relative to a broader population\n\nDescriptive statistics can be either numerical or graphical; we’ll discuss both."
  },
  {
    "objectID": "content/week2-descriptive.html#dataset-famuss-study",
    "href": "content/week2-descriptive.html#dataset-famuss-study",
    "title": "Descriptive statistics",
    "section": "Dataset: FAMuSS study",
    "text": "Dataset: FAMuSS study\nObservational study of 595 individuals comparing change in arm strength before and after resistance training between genotypes for a region of interest on the ACTN3 gene.\n\nPescatello, L. S., et al. (2013). Highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or FAMuSS study. BioMed research international.\n\n\n\n\nExample data rows\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n125\n0\nFemale\n40\nCaucasian\n68\n171\nCT\n26"
  },
  {
    "objectID": "content/week2-descriptive.html#categorical-frequency-distributions",
    "href": "content/week2-descriptive.html#categorical-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Categorical frequency distributions",
    "text": "Categorical frequency distributions\nFor categorical variables, the frequency distribution is simply an observation count by category. For example:\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\ngenotype\n\n\n\n\n494\nTT\n\n\n510\nTT\n\n\n216\nCT\n\n\n19\nTT\n\n\n278\nCT\n\n\n86\nTT\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n173\n261\n161"
  },
  {
    "objectID": "content/week2-descriptive.html#numeric-frequency-distributions",
    "href": "content/week2-descriptive.html#numeric-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Numeric frequency distributions",
    "text": "Numeric frequency distributions\nFrequency distributions of numeric variables are observation counts by range; a plot of a numeric frequency distribution is called a histogram.\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\nbmi\n\n\n\n\n194\n22.3\n\n\n141\n20.76\n\n\n313\n23.48\n\n\n522\n29.29\n\n\n504\n42.28\n\n\n273\n20.34\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\n\n(10,20]\n(20,30]\n(30,40]\n(40,50]\n\n\n\n\n69\n461\n58\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe operation of dividing a numeric variable into interval ranges is called binning."
  },
  {
    "objectID": "content/week2-descriptive.html#histograms-and-binning",
    "href": "content/week2-descriptive.html#histograms-and-binning",
    "title": "Descriptive statistics",
    "section": "Histograms and binning",
    "text": "Histograms and binning\nBinning has a big effect on the visual impression. Which one captures the shape best?"
  },
  {
    "objectID": "content/week2-descriptive.html#shapes",
    "href": "content/week2-descriptive.html#shapes",
    "title": "Descriptive statistics",
    "section": "Shapes",
    "text": "Shapes\nFor numeric variables, the histogram reveals the shape of the distribution:\n\nsymmetric if it shows left-right symmetry about a central value\nskewed if it stretches farther in one direction from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#modes",
    "href": "content/week2-descriptive.html#modes",
    "title": "Descriptive statistics",
    "section": "Modes",
    "text": "Modes\nHistograms also reveal the number of modes or local peaks of frequency distributions.\n\nuniform if there are zero peaks\nunimodal if there is one peak\nbimodal if there are two peaks\nmultimodal if there are two or more peaks"
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nConsider four variables from the FAMuSS study. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nHere are some made-up data. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#descriptive-measures",
    "href": "content/week2-descriptive.html#descriptive-measures",
    "title": "Descriptive statistics",
    "section": "Descriptive measures",
    "text": "Descriptive measures\nA descriptive measure is a summary statistic that captures a particular feature of the frequency distribution of a numeric variable.\nCommonly, measures capture either location or spread.\n\n\nMeasures of location:\n\nmean\nmedian\nmode\npercentiles/quantiles\n\n\nMeasures of spread:\n\nrange (min and max)\ninterquartile range\naverage deviation\nvariance\nstandard deviation\n\n\n\nIt is common practice to report multiple measures."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-center",
    "href": "content/week2-descriptive.html#measures-of-center",
    "title": "Descriptive statistics",
    "section": "Measures of center",
    "text": "Measures of center\nA measure of center is a statistic that reflects the typical value of a variable.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23"
  },
  {
    "objectID": "content/week2-descriptive.html#quick-example",
    "href": "content/week2-descriptive.html#quick-example",
    "title": "Descriptive statistics",
    "section": "Quick example",
    "text": "Quick example\nConsider the first 8 observations of change in nondominant arm strength from the FAMuSS study data:\n\n\n40, 25, 40, 125, 40, 75, 100 and 57.1\n\n\nCompute the mean, median, and mode."
  },
  {
    "objectID": "content/week2-descriptive.html#comparing-measures-of-center",
    "href": "content/week2-descriptive.html#comparing-measures-of-center",
    "title": "Descriptive statistics",
    "section": "Comparing measures of center",
    "text": "Comparing measures of center\nEach statistic is a little different, but often they roughly agree; for example, all are between 20 and 25, which seems to capture the typical BMI well enough.\n\nHow do you think the frequency distribution affects which one is “best”?"
  },
  {
    "objectID": "content/week2-descriptive.html#means-medians-and-skewness",
    "href": "content/week2-descriptive.html#means-medians-and-skewness",
    "title": "Descriptive statistics",
    "section": "Means, medians, and skewness",
    "text": "Means, medians, and skewness\nThe mean and median both get ‘pulled’ in the direction of skewness, but the mean is more sensitive:\n\nComparing means and medians captures information about skewness present since:\n\nmean \\(&gt;\\) median: right skew\nmean \\(&lt;\\) median: left skew\nmean \\(\\approx\\) median: symmetric"
  },
  {
    "objectID": "content/week2-descriptive.html#when-to-use-modes",
    "href": "content/week2-descriptive.html#when-to-use-modes",
    "title": "Descriptive statistics",
    "section": "When to use mode(s)",
    "text": "When to use mode(s)\nMode is rarely used unless extreme skewness or multiple modes are present; below are two examples."
  },
  {
    "objectID": "content/week2-descriptive.html#percentiles",
    "href": "content/week2-descriptive.html#percentiles",
    "title": "Descriptive statistics",
    "section": "Percentiles",
    "text": "Percentiles\nA percentile is a value with specified proportions of data lying both above and below that value.\n\nmeasure of location (but not center)\ndefined with reference to the percentage of data below\n\nFor example, the 20th percentile is the value with 20% of observations below and 80% of observations above. Suppose we have 5 observations:\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n19\n20\n21\n25\n31\n\n\nrank\n1\n2\n3\n4\n5\n\n\n\n\n\nThe 20th percentile is not unique! In fact any number between 19 and 20 is a 20th percentile since it would satisfy:\n\n20% below (19)\n80% above (20, 21, 25, 31)"
  },
  {
    "objectID": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "href": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "title": "Descriptive statistics",
    "section": "Cumulative frequency distribution",
    "text": "Cumulative frequency distribution\nThe cumulative frequency distribution is a data summary showing percentiles. Think of it as percentile (y) against value (x).\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of some specific values:\n\nabout 40% of the subjects are 20 or younger\nabout 80% of the subjects are 24 or younger\n\nYour turn:\n\nRoughly what percentage of subjects are 22 or younger?\nAbout what age is the 10th percentile?"
  },
  {
    "objectID": "content/week2-descriptive.html#common-percentiles",
    "href": "content/week2-descriptive.html#common-percentiles",
    "title": "Descriptive statistics",
    "section": "Common percentiles",
    "text": "Common percentiles\n\n\nThe five-number summary is a collection of five percentiles that succinctly describe the frequency distribution:\n\n\n\nStatistic name\nMeaning\n\n\n\n\nminimum\n0th percentile\n\n\nfirst quartile\n25th percentile\n\n\nmedian\n50th percentile\n\n\nthird quartile\n75th percentile\n\n\nmaximum\n100th percentile\n\n\n\n\nBoxplots provide a graphical display of the five-number summary."
  },
  {
    "objectID": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "href": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "title": "Descriptive statistics",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\nNotice how the two displays align, and also how they differ. The histogram shows shape in greater detail, but the boxplot is much more compact."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-spread",
    "href": "content/week2-descriptive.html#measures-of-spread",
    "title": "Descriptive statistics",
    "section": "Measures of spread",
    "text": "Measures of spread\nThe spread of observations refers to how concentrated or diffuse the values are.\n\nTwo ways to understand and measure spread:\n\nranges of values capturing much of the distribution\ndeviations of values from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#range-based-measures",
    "href": "content/week2-descriptive.html#range-based-measures",
    "title": "Descriptive statistics",
    "section": "Range-based measures",
    "text": "Range-based measures\nA simple way to understand and measure spread is based on ranges. Consider more ages, sorted and ranked:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\nrank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\nThe range is the minimum and maximum values: \\[\\text{range} = (\\text{min}, \\text{max}) = (16, 34)\\]\nThe interquartile range (IQR) is the difference [75th percentile] - [25th percentile] \\[\\text{IQR} = 29 - 19 = 10\\] When might you prefer IQR to range? Can you think of an example?"
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures",
    "href": "content/week2-descriptive.html#deviation-based-measures",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe average deviation is defined as the average of the absolute values of the deviations from the mean: \\[\\frac{8 + 6 + 5 + 4 + 3 + 2 + 1 + 2 + 4 + 5 + 6}{12}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#mathematical-notations",
    "href": "content/week2-descriptive.html#mathematical-notations",
    "title": "Descriptive statistics",
    "section": "Mathematical notations",
    "text": "Mathematical notations\nFollowing the convention from before, write a set of \\(n\\) observations as \\(x_1, x_2, \\dots, x_n\\).\n\n\nThe mean of the observations is written: \\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]\nThe average deviation is: \\[\\frac{1}{n} \\sum_i |x_i - \\bar{x}|\\]\n\nThe variance is: \\[s_x^2 = \\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2\\]\nThe standard deviation is: \\[s_x = \\sqrt{\\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#interpretations",
    "href": "content/week2-descriptive.html#interpretations",
    "title": "Descriptive statistics",
    "section": "Interpretations",
    "text": "Interpretations\nListed from largest to smallest, here are each of the measures of spread for the 12 ages:\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin\nmax\niqr\nvariance\nst.dev\navg.dev\n\n\n\n\n16\n34\n8.5\n30.55\n5.527\n4.667\n\n\n\n\n\nThe interpretations differ between these statistics:\n\n[range] all of the data lies on an between 16 and 34 years old on an interval 18 years in width\n[IQR] the middle half of the data lies on an interval 8.5 years in width\n[average deviation] the average distance from the mean is 4.67 years\n[variance] the average squared distance from the mean is 30.55 years\\(^2\\)\n[standard deviation] the average squared distance from the mean, rescaled to years, is 5.53 years\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-descriptive.html#lab-robustness",
    "href": "content/week2-descriptive.html#lab-robustness",
    "title": "Descriptive statistics",
    "section": "Lab: robustness",
    "text": "Lab: robustness\nFor this lab we’ll continue to work with the FAMuSS data as we have throughout lecture.\nThis lab has two objectives:\n\nTeach you to compute descriptive statistics and prepare graphical summaries for a single variable in R\nLearn when and why to use certain descriptive statistics in place of others\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab1-rbasics.html",
    "href": "content/lab1-rbasics.html",
    "title": "Lab 1: R Basics",
    "section": "",
    "text": "This lab is intended to introduce you to the basics in R that you will need for this class. Most of our analyses will consist of just a few steps:\n\nload a dataset\nidentify and select variable(s) of interest\nperform one or more calculations using variable(s) of interest as inputs\n\nWe will illustrate this process so that you can get used to the mechanics and familiarize yourself with how different data types appear in R.\n\nHow to do this lab\nI’ve provided you with a project on Posit Cloud containing data files and a script (a script is a plain text file containing R commands). The script contains all commands shown in this document, and some blank areas for you to fill in, with comment lines (the ones starting with #) to help you navigate.\nYou should refer back to this document for instructions and context, and fill in the script as you go:\n\nrun the codes provided as you read through the narrative in this document and inspect the results\nin the ‘your turn’ sections, refer to the prompt in this document and use the example commands provided immediately beforehand to determine which command to write\nwrite in your commands in the script the space below the corresponding comment, not in the console (otherwise you’ll have a hard time keeping track of your work)\n\nYour goal is to complete all of the “your turn” parts in the script. Two practice problems are given at the end of the lab as homework for you to complete on your own before next class.\n\n\nHow to use this lab\nThis lab (and the lab activities in general) are designed to provide you with a set of examples to learn initially in class and then follow on your own later when doing the homework problems given at the end of the lab.\nIf you can do the examples and ‘your turn’ activities in class, all you’ll need to do to complete the homeworks is copy commands from those examples and activities and adjust some small details (variable names, dataset names, etc.).\nIf you later need to figure out how to do something in R for a homework problem or test, all you’ll need to do is refer back to the labs.\n\n\nPackages in R\nA “package” is a bundle of functions, datasets, and other objects that can be imported into R for use in your working environment. Many scripts begin by loading packages that will be used throughout the script. Packages are loaded using the command library(&lt;PACKAGE NAME&gt;) where &lt;PACKAGE NAME&gt; is replaced by the actual name of the package. For example:\n\nlibrary(tidyverse)\n\nPackages do need to be installed before they can be loaded. One of the nice things about using Posit Cloud is that I can manage all of these installs for you. However, if you ever wish to install and use a package that’s not available (or if you use R on your own machine), you can install a package using the command install.packages(\"&lt;PACKAGE NAME&gt;\") after replacing &lt;PACKAGE NAME&gt; with the actual name of the package (but keeping the quotation marks!).\n\n\nLoading a dataset\nThere are several ways to load datasets in R. The strategy we’ll use most often is to load an .RData file, but you will encounter a few others here and there.\n\n# load nhanes data\nload('data/nhanes.RData')\n\nThis command looks for a file called nhanes.RData in a directory folder named data and reads the file.\nNotice that once you run the command, an object called nhanes appears in the “Environment” tab in the upper right hand panel of your RStudio window.\nIf you click the little blue carrot next to nhanes in the environment tab, you will then see a list of variables contained in the dataset. You can also see the first few rows of the dataset using head(...).\n\n# first few rows\nhead(nhanes)\n\n# A tibble: 6 × 9\n  subj.id gender   age poverty pulse bpsys1 bpdia1 totchol sleephrsnight\n    &lt;int&gt; &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;         &lt;int&gt;\n1       1 male      34    1.36    70    114     88    3.49             4\n2       2 male      34    1.36    70    114     88    3.49             4\n3       3 male      34    1.36    70    114     88    3.49             4\n4       5 female    49    1.91    86    118     82    6.7              8\n5       8 female    45    5       62    106     62    5.82             8\n6       9 female    45    5       62    106     62    5.82             8\n\n\nThis kind of object in R is called a data frame. Data frames are displayed in a tabular layout, like a spreadsheet. While data frames should be arranged so that observations are shown in rows and variables in columns, this is not guaranteed, so you should be in the habit of checking to make sure the layout is sensible; otherwise, you might accidentally perform bogus calculations and analyses.\nBeyond providing a sanity check, inspecting the data frame will show you three key pieces of information besides the values of the first few observations of each variable.\n\nData dimensions: how many observations (rows) and how many variables (columns)\nVariable names: subj.id, gender, age, etc.\nData types:\n\nint for integer (numerical data type)\nfct for factor (categorical data type)\nnum for numeric (numerical data type)\nchr for character (categorical data type)\n\n\nSo, for example, seeing that pulse is of data type int tells you that pulse is a discrete numerical variable. It also tells you what name to use to refer to the variable in subsequent R commands.\n\n\n\n\n\n\nYour turn\n\n\n\nThere is another data file in the data directory called famuss.RData. Load this into the environment, preview the first few observations, and check the variable names and data types.\n\n# load famuss dataset\n\n# preview first few rows\n\nTo check your understanding:\n\nhow many observations and variables?\nidentify a categorical variable\nwhat kind of variable is bmi?\n\n\n\n\n\nSelecting variables\nThe variable names in a dataset can be used to retrieve or refer to specific variables. For example, try running this command:\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# preview first few values\nhead(total.cholesterol)\n\n[1] 3.49 3.49 3.49 6.70 5.82 5.82\n\n\nThat command did the following:\n\nextracted the totchol column of nhanes (the nhanes$totchol part)\nassigned the result a new name total.cholesterol (the &lt;- part)\n\nAssignment (&lt;-) is a very important concept in R – you can store the result of any calculation as an object with a name of your choosing.\nYou’ll notice that total.cholesterol looks a bit different than the data frame in terms of its appearance. This is because it’s not a data frame but rather a different kind of object called a vector: a collection of values of the same data type.\n\n\n\n\n\n\nYour turn\n\n\n\nExtract the change in nondominant arm strength variable from the FAMuSS dataset, and store it as a vector called strength.\n\n# store the change in nondominant arm strength variable as a vector called 'strength'\n\n# preview the first few values\n\n\n\n\n\nPerforming calculations\nExtracting and storing variables as vectors isn’t strictly necessary, but does make it easier to perform many calculations. While you’re a beginner, I’d recommend using this strategy.\n\nNumeric summaries\nMost simple summary statistics can be calculated using simple functions in R that take a single vector argument. For example, to calculate the average, minimum, and maximum total cholesterol among the respondents in the sample:\n\n# average total cholesterol\nmean(total.cholesterol)\n\n[1] 5.042938\n\n# minimum\nmin(total.cholesterol)\n\n[1] 2.33\n\n# maximum\nmax(total.cholesterol)\n\n[1] 13.65\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nFind the average percent change in nondominant arm strength of participants in the FAMuSS study sample using the strength vector you created before.\n\n# compute mean change in nondominant arm strength\n\n\n\n\n\nCategorical summaries\nMost data summaries for categorical variables proceed from counts of the number of observations in each category. These counts can be obtained by passing a vector of observations to table(...):\n\n# retreive sex variable\nsex &lt;- nhanes$gender\n\n# counts\ntable(sex)\n\nsex\nfemale   male \n  1588   1591 \n\n\nTo obtain the proportion of observations in each category – the counts divided by the total number of observations – pass the table to the proportions(...) function:\n\n# proportions\ntable(sex) |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nThe character string |&gt; is a bit of syntax that you could read verbally as ‘then’: first make a table, then obtain proportions. It’s known as the pipe operator, because it ‘pipes’ the result of the command on its left into the command on its right.\nTo see another example of the pipe operator in action, you could rewrite the previous command as a chain of three steps:\n\n# same as above\nsex |&gt; table() |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nYou could interpret this as follows: start with sex, pass that to table(), then pass the result to proportions.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the FAMuSS dataset, calculate the genotype frequencies in the sample (i.e., find the proportion of observations of each genotype).\n\n# retrieve genotype\n\n# counts\n\n# proportions\n\n\n\nWhile the analyses you’ll learn will get more complex than computing summary statistics, the mechanics of performing the computations in R will be analogous to what you just did: executing a one-line command with a vector input.\n\n\n\nPutting together the pieces\nReflect for a moment on what you just did: you wrote a few lines of code to import a dataset, extract a variable, and compute a statistic. If you filled in the script as instructed, you now have a record of the commands you executed that you can use to retrace your steps.\nIn fact, anyone with your script and the data files (including future you) could easily reproduce your work. Reproducibility is a pillar of data-driven science; by storing analyses in the form of executable scripts, researchers can easily create and share records of their work.\nWe could put the steps above together in just a few lines as if it were a short script. Typical style is to provide line-by-line comments explaining what the commands do.\n\n# import nhanes data\nload('data/nhanes.RData')\n\n# inspect data\nhead(nhanes)\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# compute average total cholesterol\nmean(total.cholesterol)\n\n# extract sex\nsex &lt;- nhanes$gender\n\n# proportions of men and women in sample\ntable(sex) |&gt; proportions()\n\n\n\n\n\n\n\nYour turn\n\n\n\nFollow the example above and combine the previous exercises into a few lines of code with appropriate line comments.\n\n# load famuss dataset\n\n# inspect data\n\n# extract nondominant change in arm strength\n\n# compute average change in strength\n\n# extract genotype\n\n# compute genotype frequencies (proportions)\n\n\n\nIf this was all entirely new to you, congratulations on writing your first lines of code!\n\n\nExtras\n\nReading CSV files\nOften data are stored in spreadsheets, which can be easily converted to comma-separated values or CSV files (extension .csv). These are plain-text files that are a bit more lightweight than an Excel spreadsheet.\nR can read CSV (as well as other) files. The read.csv(...) function will parse the file and produce a data frame. The result can be assigned a name and stored as an object in the environment.\n\n# parse a csv file\nread.csv('data/gss.csv')\n\n# store the result in the environment\ngss &lt;- read.csv('data/gss.csv')\n\nMost of the time in class we’ll load .RData files or obtain datasets through packages (more on this later), but if you use R outside of class you may find it more common to manage data input via .csv files.\n\n\nMore about R\nWhile you will learn new commands going forward, we won’t go much more in depth with R than what you just saw. However, if you’re interested in understanding the above concepts in greater detail, or learning about R as a programming environment, see An Introduction to R.\n\n\n\nPractice problems\nDue before the next class meeting.\n\nThe census dataset contains a sample of data for 377 individuals included in the 2000 U.S. census. Load and inspect the dataset, and determine:\n\nthe youngest and oldest individual in the sample\nthe average total personal income\nthe average total family income\nhow many variables are in the dataset, not including census year and FIPS code\nhow many categorical variables are in the dataset, not including FIPS code\n\n\n\nThe cdc.samp dataset in the oibiostat package contains a sample of data for 60 individuals surveyed by the CDC’s Behavioral Risk Factors Surveillance System (BRFSS). Use the provided commands to load the dataset, and then inspect it the usual way. Notice that several of the variables are 1’s and 0’s. Use the command ?cdc.samp to view the data documentation.\n\nWhat do the values (1’s and 0’s) mean in the exerany variable?\nWhat proportion of the sample are men? What proportion are women?\nFor each general health category, find the proportion of respondents who rated themselves in that category.\nHow many of the respondents have health coverage? (Hint: sum(x) will add up the values in a vector x; adding up a collection of 1’s and 0’s is equivalent to counting the number of 1’s.) What percentage of the respondents have health coverage?"
  },
  {
    "objectID": "content/week1-studies.html#todays-agenda",
    "href": "content/week1-studies.html#todays-agenda",
    "title": "Welcome to STAT218",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nCourse logistics\n[lecture] Study designs\n[activity, if time] Distinguishing study types"
  },
  {
    "objectID": "content/week1-studies.html#icebreakers",
    "href": "content/week1-studies.html#icebreakers",
    "title": "Welcome to STAT218",
    "section": "Icebreakers",
    "text": "Icebreakers\nBy show of hands…\n\n\nFirst statistics class ever?\nLast statistics class ever?\nExpect to take STAT313?\nExpect to use statistics for your degree coursework or senior project?\nConsidering a statistics or data science minor?"
  },
  {
    "objectID": "content/week1-studies.html#class-composition",
    "href": "content/week1-studies.html#class-composition",
    "title": "Welcome to STAT218",
    "section": "Class composition",
    "text": "Class composition\nBy the numbers…"
  },
  {
    "objectID": "content/week1-studies.html#statistics-and-uncertainty",
    "href": "content/week1-studies.html#statistics-and-uncertainty",
    "title": "Welcome to STAT218",
    "section": "Statistics and uncertainty",
    "text": "Statistics and uncertainty\n\nLife is full of uncertainty, and this can make a lot of questions hard to answer, because similar situations do not always result in the same outcome.\n\nStatistical thinking: uncertainty is measurable.\nWhat statistics can offer:\n\nprinciples for designing studies and collecting data in order to capture outcome variability\ndata analytic tools to distinguish random from systematic variability\nheuristics to make inferences that account for uncertainty"
  },
  {
    "objectID": "content/week1-studies.html#course-goal-and-scope",
    "href": "content/week1-studies.html#course-goal-and-scope",
    "title": "Welcome to STAT218",
    "section": "Course goal and scope",
    "text": "Course goal and scope\nThe overarching goal of 218 is to introduce you to statistics in a hands-on way that is relevant to your major.\nSo we will focus on:\n\nstatistical thinking, study design, and data analysis\nclassical methods, mostly developed 1900-1940\ncase studies from life sciences"
  },
  {
    "objectID": "content/week1-studies.html#materials",
    "href": "content/week1-studies.html#materials",
    "title": "Welcome to STAT218",
    "section": "Materials",
    "text": "Materials\nComputer/tablet. You’ll need a laptop (preferred) or tablet with keyboard (workable).\nCourse website. All materials are hosted/linked on the course website. I won’t be using Canvas.\nTextbook. Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences. I suggest a $5-15 donation.\nStatistical software. R/RStudio hosted online via posit.cloud workspace. You will need to create an account and purchase a $5/month student subscription."
  },
  {
    "objectID": "content/week1-studies.html#class-meetings",
    "href": "content/week1-studies.html#class-meetings",
    "title": "Welcome to STAT218",
    "section": "Class meetings",
    "text": "Class meetings\nClass meetings will usually consist of a reading quiz, a lecture, a break, and a lab.\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class."
  },
  {
    "objectID": "content/week1-studies.html#assignments",
    "href": "content/week1-studies.html#assignments",
    "title": "Welcome to STAT218",
    "section": "Assignments",
    "text": "Assignments\nYou will have three categories of assignments:\n\nhomework problems: two per class due by next class\ntests: every 2-3 weeks, distributed Wednesday, due Friday\na project: find and present a case study\n\nDeadline policies:\n\none-hour grace period on all deadlines\nfour homework problem sets can be turned in up to 48 hours late without notice\nbesides free lates, extensions must be arranged 24 hours in advance of the deadline"
  },
  {
    "objectID": "content/week1-studies.html#grades",
    "href": "content/week1-studies.html#grades",
    "title": "Welcome to STAT218",
    "section": "Grades",
    "text": "Grades\nEvery graded question/problem is matched to one or more of the 11 course learning outcomes.\n\nQuestions/problems are evaluated as satisfactory (S), needs improvement (NI), or missing (M).\nFor each outcome, the percentage of questions/problems awarded a satisfactory mark is used to determine whether that outcome is fully met, partly met, or not met:\n\nfully met: 80% or more of matched questions satisfactory\npartly met: 50% – 80% of matched questions satisfactory\nnot met: less than 50% of matched questions satisfactory\n\n\nYour course grade is based on how many learning outcomes are fully met. To pass, you must partly or fully meet at least 6 outcomes; for a C-, you must fully meet at least 3 outcomes."
  },
  {
    "objectID": "content/week1-studies.html#important-policies",
    "href": "content/week1-studies.html#important-policies",
    "title": "Welcome to STAT218",
    "section": "Important policies",
    "text": "Important policies\n\nextensions must be confirmed (not simply requested) 24 hours in advance\ncollaboration on homework is encouraged, but everyone involved needs to…\n\nmake a contribution\nwrite up their own work\n\nsubmitting AI-generated content in place of your own work is not acceptable\n\nresponsible use is okay, but not recommended (GPT outputs are misleading)\npenalties for AI plagiarism depend on precedent and severity\n\n\n\n\n\nMinor offense\nMajor offense\nPenalty\n\n\n\n\nFirst\n\nloss of credit and warning\n\n\nSecond\nFirst\nloss of credit and OSRR report\n\n\nThird\nSecond\ncourse failure and second OSRR report"
  },
  {
    "objectID": "content/week1-studies.html#what-is-a-study",
    "href": "content/week1-studies.html#what-is-a-study",
    "title": "Welcome to STAT218",
    "section": "What is a study?",
    "text": "What is a study?\nA study is an effort to collect data in order to answer one or more research questions.\n\nstudies must be well-matched to research questions to provide good answers\nhow data are obtained is just as important as how the resulting data are analyzed\nno analysis, no matter how sophisticated will rescue a poorly conceived study\n\nA study unit is the smallest object or entity that is measured in a study; also called experimental unit or observational unit."
  },
  {
    "objectID": "content/week1-studies.html#two-types-of-studies",
    "href": "content/week1-studies.html#two-types-of-studies",
    "title": "Welcome to STAT218",
    "section": "Two types of studies",
    "text": "Two types of studies\nObservational studies collect data from an existing situation without intervention.\n\nAim is to detect associations and patterns\nCan’t be used to establish causal links\n\nExperiments collect data from a situation in which one or more interventions have been introduced by the investigator.\n\nAim is to draw conclusions about the causal effect of interventions\nStronger form of scientific evidence than an observational study\n\nOften, observational studies are used to explore/generate hypotheses prior to designing an experiment."
  },
  {
    "objectID": "content/week1-studies.html#comparing-study-types",
    "href": "content/week1-studies.html#comparing-study-types",
    "title": "Welcome to STAT218",
    "section": "Comparing study types",
    "text": "Comparing study types\nEither type of study can be used to address a question.\n\n\n\n\n\n\n\n\nQuestion\nObservational study\nExperiment\n\n\n\n\nAre diet and mood related?\nConduct surveys on diet, lifestyle, and affect\nRecruit study participants, assign diets, measure affect\n\n\nIs vaping safer than smoking?\nFollow groups of vapers and smokers over time and record health outcomes\nAmong a group of smokers, assign some to switch to vaping; compare health outcomes over time\n\n\nDo insecticide applications affect soil microbes?\nAnalyze soil samples from farms using different insecticides\n[Your turn]\n\n\n\nCan you think of pros and cons for each study type?"
  },
  {
    "objectID": "content/week1-studies.html#why-does-intervention-matter",
    "href": "content/week1-studies.html#why-does-intervention-matter",
    "title": "Welcome to STAT218",
    "section": "Why does intervention matter?",
    "text": "Why does intervention matter?\nControl over conditions allows a researcher to study causal effects resulting from interventions. This is not possible in observational studies due to the potential for confounding.\n\n\nConfounding: an unobserved condition is associated with both the study condition and the outcome.\n\nFailure to measure and account for confounders potentially distorts observed associations\nExample: a study finds that dog owners live longer, but doesn’t measure exercise; so it might just be the daily walks.\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved variable) --- B(study variable) & C(study outcome) \n\n\n\n\n\n\n\n\nThis is very common in observational studies, because you can’t measure every study condition."
  },
  {
    "objectID": "content/week1-studies.html#antidote-randomization",
    "href": "content/week1-studies.html#antidote-randomization",
    "title": "Welcome to STAT218",
    "section": "Antidote: randomization",
    "text": "Antidote: randomization\nThe ability to control study conditions allows researchers to randomly allocate interventions among study subjects.\n\nRandomization eliminates confounding by isolating the condition(s) of interest:\n\ninterventions are independent of extraneous conditions ⟹ no association possible\nif outcomes differ systematically according to the intervention, you can be certain that it is not an artifact\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved condition) x-.-x B(study condition)\n  A --- C(outcome)"
  },
  {
    "objectID": "content/week1-studies.html#practical-consequences",
    "href": "content/week1-studies.html#practical-consequences",
    "title": "Welcome to STAT218",
    "section": "Practical consequences",
    "text": "Practical consequences\nThe ability to randomize interventions in experiments means:\n\nobserved associations are independent of extraneous factors\nresults can support causal inferences\n\nThe absence of randomization in observational studies means:\n\nconfounding is always possible\nresults may be misleading"
  },
  {
    "objectID": "content/week1-studies.html#experimental-designs",
    "href": "content/week1-studies.html#experimental-designs",
    "title": "Welcome to STAT218",
    "section": "Experimental designs",
    "text": "Experimental designs\nA treatment is an experimental intervention; the design of an experiment refers to how treatments are allocated to study units.\nThe most basic design is:\n\n[balanced] each treatment is replicated an equal number of times\n[randomized] treatments are allocated completely at random to study units\n[no crossover] each study unit receives exactly one treatment\n\nWe’ll call this a completely randomized design. It’s the only kind of experimental design we’re going to consider in STAT218.\nThere are many other designs that we won’t discuss (but see STAT313); these are all about improving experimental efficiency by controlling extraneous variation."
  },
  {
    "objectID": "content/week1-studies.html#data-collection",
    "href": "content/week1-studies.html#data-collection",
    "title": "Welcome to STAT218",
    "section": "Data collection",
    "text": "Data collection\nStudy units should be chosen so as to represent a larger collection.\n\n\n\n\nA study population is a collection of all study units of interest.\nA sample is a subcollection from a population:\n\nrandom if study units have a known chance of inclusion in the sample\nnonrandom or convenience otherwise\n\n\n\nThe gold standard is the simple random sample: each study unit in the population has an equal chance of inclusion in the sample."
  },
  {
    "objectID": "content/week1-studies.html#leap-study",
    "href": "content/week1-studies.html#leap-study",
    "title": "Welcome to STAT218",
    "section": "LEAP Study",
    "text": "LEAP Study\n\n\nLearning early about peanut allergy (LEAP) study:\n\n640 infants in UK with eczema or egg allergy but no peanut allergy enrolled\neach infant randomly assigned to peanut consumption and peanut avoidance groups\n\npeanut consumption: fed 6g peanut protein daily until 5 years old\npeanut avoidance: no peanut consumption until 5 years old\n\nat 5 years old, oral food challenge (OFC) allergy test administered\n13.3% of the avoidance group developed allergies, compared with 1.9% of the consumption group\n\n\n\n\n\nStudy characteristics\n\n\nStudy type: experiment\nStudy population: UK infants with eczema or egg allergy but no peanut allergy\nSample: 640 infants from population\nStudy design: completely randomized design\nTreatments: peanut consumption; peanut avoidance\nStudy outcome: development of peanut allergy by 5 years of age\n\n\n\n\n\n\nStudy results\n\n\nModerated peanut consumption causes a reduction in the likelihood of developing an allergy."
  },
  {
    "objectID": "content/week1-studies.html#checklist-for-next-time",
    "href": "content/week1-studies.html#checklist-for-next-time",
    "title": "Welcome to STAT218",
    "section": "Checklist for next time",
    "text": "Checklist for next time\n\nObtain a copy of the textbook.\nCreate a posit.cloud account and purchase a student subscription. Ensure you can access the stat218-s24 workspace.\nComplete practice problems and reading before class.\nWrite down one question about the reading.\nPrint a paper or virtual copy of the slides."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account",
    "href": "content/week1-studies.html#posit-cloud-account",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nGo to: course webpage &gt; syllabus &gt; materials. Then look for the link to join the class workspace:"
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-1",
    "href": "content/week1-studies.html#posit-cloud-account-1",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nFollow prompts to create an account. Use your Cal Poly email."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-2",
    "href": "content/week1-studies.html#posit-cloud-account-2",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nOnce your email is verified, return to posit.cloud (or click the link in the syllabus again), and join the class workspace."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-3",
    "href": "content/week1-studies.html#posit-cloud-account-3",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nUpgrade your account to the student plan. Input payment details."
  },
  {
    "objectID": "content/week1-studies.html#printing-slides",
    "href": "content/week1-studies.html#printing-slides",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nOpen menu from lower left"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-1",
    "href": "content/week1-studies.html#printing-slides-1",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nNavigate to tools"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-2",
    "href": "content/week1-studies.html#printing-slides-2",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nSelect PDF export mode"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-3",
    "href": "content/week1-studies.html#printing-slides-3",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\n\n\nThen print from browser to PDF\n\n\nI suggest landscape layout and either 1, 2 or 4 slides per page\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-datatypes.html#todays-agenda",
    "href": "content/week2-datatypes.html#todays-agenda",
    "title": "Data semantics and data types",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\n[lecture] data semantics and data types\n[lab] R basics"
  },
  {
    "objectID": "content/week2-datatypes.html#data-semantics",
    "href": "content/week2-datatypes.html#data-semantics",
    "title": "Data semantics and data types",
    "section": "Data semantics",
    "text": "Data semantics\n\nData are a set of measurements.\nA variable is any measured attribute of study units.\nAn observation is a measurement of one or more variables taken on one particular study unit.\n\nIt is usually expedient to arrange data values in a table in which each row is an observation and each column is a variable:"
  },
  {
    "objectID": "content/week2-datatypes.html#leap-example",
    "href": "content/week2-datatypes.html#leap-example",
    "title": "Data semantics and data types",
    "section": "LEAP example",
    "text": "LEAP example\nA table showing the observations and variables for the LEAP study would look like this:\n\n\n\n\n\n\n\n\n\n\nparticipant.ID\ntreatment.group\nofc.test.result\n\n\n\n\nLEAP_100522\nPeanut Consumption\nPASS OFC\n\n\nLEAP_103358\nPeanut Consumption\nPASS OFC\n\n\nLEAP_105069\nPeanut Avoidance\nPASS OFC\n\n\nLEAP_105328\nPeanut Consumption\nPASS OFC\n\n\n\n\n\nThe table you saw in the reading was a summary of the data (not the data itself):\n\n\n\n\n\n\n\n\n\n\n \nFAIL OFC\nPASS OFC\n\n\n\n\nPeanut Avoidance\n36\n227\n\n\nPeanut Consumption\n5\n262"
  },
  {
    "objectID": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "href": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "title": "Data semantics and data types",
    "section": "Numeric and categorical variables",
    "text": "Numeric and categorical variables\nVariables are classified according to their values. Values can be one of two different types:\n\nA variable is numeric if its value is a number\nA variable is categorical if its value is a category, usually recorded as a name or label\n\nFor example:\n\nthe value of sex can be male or female, so it is categorical\nwhereas age (in years) can be any positive integer, so it is numeric"
  },
  {
    "objectID": "content/week2-datatypes.html#variable-subtypes",
    "href": "content/week2-datatypes.html#variable-subtypes",
    "title": "Data semantics and data types",
    "section": "Variable subtypes",
    "text": "Variable subtypes\nFurther distinctions are made based on the type of number or type of category used to measure an attribute. Can you match the subtypes to the variables at right?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nhispanic\ngrade\nweight\n\n\n\n\n15\nnot\n10\n78.02\n\n\n18\nhispanic\n12\n78.47\n\n\n17\nnot\n11\n95.26\n\n\n18\nnot\n12\n95.26\n\n\n\n\n\n\n\n\na numerical variable is discrete if there are ‘gaps’ between its possible values\na numerical variable is continuous if there are no such gaps\na categorical variable is nominal if its levels are not ordered\na categorical variable is ordinal if its levels are ordered"
  },
  {
    "objectID": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "href": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "title": "Data semantics and data types",
    "section": "Many ways to measure attributes",
    "text": "Many ways to measure attributes\nVariable type (or subtype) is not an inherent quality — attributes can often be measured in many different ways.\nFor instance, age might be measured as either a discrete, continuous, or ordinal variable, depending on the situation:\n\n\n\nAge (years)\nAge (minutes)\nAge (brackets)\n\n\n\n\n12\n6307518.45\n10-18\n\n\n8\n4209187.18\n5-10\n\n\n21\n11258103.08\n18-30\n\n\n\n\nNumeric variables can always be represented as categorical, but not the other way around."
  },
  {
    "objectID": "content/week2-datatypes.html#your-turn",
    "href": "content/week2-datatypes.html#your-turn",
    "title": "Data semantics and data types",
    "section": "Your turn",
    "text": "Your turn\nClassify each variable as nominal, ordinal, discrete, or continuous:\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ngenotype\nsex\nage\nrace\nbmi\n\n\n\n\n33.3\nCT\nFemale\n19\nCaucasian\n21.01\n\n\n71.4\nCT\nFemale\n18\nOther\n23.18\n\n\n37.5\nCC\nFemale\n21\nCaucasian\n28.92\n\n\n50\nCC\nFemale\n28\nAsian\n21.16\n\n\n\n\n\nData are from an observational study investigating demographic, physiological, and genetic characteristics associated with muscle strength.\n\nndrm.ch is change in strength in nondominant arm after resistance training\ngenotype indicates genotype at a particular location within the ACTN3 gene"
  },
  {
    "objectID": "content/week2-datatypes.html#common-summary-statistics",
    "href": "content/week2-datatypes.html#common-summary-statistics",
    "title": "Data semantics and data types",
    "section": "Common summary statistics",
    "text": "Common summary statistics\n\nA statistic is a data summary: in mathematical terms, a function of several observations\n\n\n\nFor numeric variables, the most common summary statistic is the average value:\n\\[\\text{average} = \\frac{\\text{sum of values}}{\\text{# observations}}\\]\nFor example, the average percent change in nondominant arm strength was 53.291%.\n\nFor categorical variables, the most common summary statistic is a proportion:\n\\[\\text{proportion}_i = \\frac{\\text{# observations in category } i}{\\text{# observations}}\\]\nFor example:\n\n\n\nGenotype proportions\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n0.2908\n0.4387\n0.2706"
  },
  {
    "objectID": "content/week2-datatypes.html#descriptive-analyses",
    "href": "content/week2-datatypes.html#descriptive-analyses",
    "title": "Data semantics and data types",
    "section": "Descriptive analyses",
    "text": "Descriptive analyses\nSometimes, a few clever summary statistics can be used to answer a research question.\n\nHow much does the average change in arm strength differ by genotype, if at all?\n\nComputing per-genotype averages provides an answer:\n\n\n\n\n\n\n\n\n\n\n\ngenotype\navg.change\nn.obs\nprop.obs\n\n\n\n\nTT\n58.08\n161\n0.2706\n\n\nCT\n53.25\n261\n0.4387\n\n\nCC\n48.89\n173\n0.2908\n\n\n\n\n\nNumber of observations and proportions are included because they provide information about genotype frequencies in the sample.\n\nconveys how many individuals were measured\nalso provides an estimate of genotype frequencies in the population"
  },
  {
    "objectID": "content/week2-datatypes.html#common-mathematical-notation",
    "href": "content/week2-datatypes.html#common-mathematical-notation",
    "title": "Data semantics and data types",
    "section": "Common mathematical notation",
    "text": "Common mathematical notation\nWhile we won’t use mathematical expressions too often in STAT218, it’s useful to be aware of some common notations.\nTypically, a set of observations is written as:\n\\[x_1, x_2, \\dots, x_n\\]\n\n\\(x\\) represents the variable (e.g., genotype, age, percent change, etc.)\nsubscript indexes observations: \\(x_i\\) is the \\(i\\)th observation\n\\(n\\) is the total number of observations\n\nThe sum of the observations is written \\(\\sum_i x_i\\), where the symbol \\(\\sum\\) stands for ‘summation’. This is useful for writing the formula for computing an average:\n\\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]"
  },
  {
    "objectID": "content/week2-datatypes.html#lab-data-basics-in-r",
    "href": "content/week2-datatypes.html#lab-data-basics-in-r",
    "title": "Data semantics and data types",
    "section": "Lab: data basics in R",
    "text": "Lab: data basics in R\nThe variable types we just discussed map pretty neatly (but not perfectly) onto the main “data types” in R:\n\nnumeric ➜ integer, numeric\ncategorical ➜ character, factor, logical\n\nThe primary way data are arranged in R is in a data frame. This lab will show you how to load, inspect, and use data frames.\nYour objectives in this lab are:\n\nlearn to load and inspect datasets\nlearn to recognize data types\nlearn to perform simple calculations (averages, etc.)"
  },
  {
    "objectID": "content/week2-datatypes.html#opening-the-lab-activity",
    "href": "content/week2-datatypes.html#opening-the-lab-activity",
    "title": "Data semantics and data types",
    "section": "Opening the lab activity",
    "text": "Opening the lab activity\nNavigate to posit.cloud. Then:\n\n\n\n\n\n\nMake sure the class workspace “stat218-s24” is highlighted at left. If “Your Workspace” is highlighted, you won’t see the example assignment.\nClick on the lab1-rbasics, then wait.\n\nOnce everyone is ready, we’ll have a look at the example files together.\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week1-activity-studydesigns.html",
    "href": "content/week1-activity-studydesigns.html",
    "title": "Distinguishing study designs",
    "section": "",
    "text": "Recall that the difference between an observational study and an experiment hinges on whether researchers intentionally intervene on the system of study (experiment) or passively record outcomes (observational study).\nIn this activity you’ll read abstracts from a few published studies and determine what kind of study is described in the abstract. You do not need to consider the examples in order — start with the ones that look most interesting.\nFor each example, identify the following:\n\nstudy type\nstudy population\nsample characteristics\nstudy outcome(s)\n\n\n\n\n\n\nExample 1: selenium exposure and Mediterranean diet\nThe following is from the abstract of a study investigating dietary mitigation of selenium exposure:\n\nSelenium is a trace element found in many chemical forms. Selenium and its species have nutritional and toxicologic properties, some of which may play a role in the etiology of neurological disease. We hypothesized that adherence to the Mediterranean-Dietary Approach to Stop Hypertension Intervention for Neurodegenerative Delay (MIND) diet could influence intake and endogenous concentrations of selenium and selenium species, thus contributing to the beneficial effects of this dietary pattern. We carried out a cross-sectional study of 137 non-smoking blood donors (75 females and 62 males) from the Reggio Emilia province, Northern Italy. We assessed MIND diet adherence using a semiquantitative food frequency questionnaire. We assessed selenium exposure through dietary intake and measurement of urinary and serum concentrations, including speciation of selenium compound in serum … Adherence to the MIND diet was positively associated with dietary selenium intake and urinary selenium excretion, whereas it was inversely associated with serum concentrations of overall selenium and organic selenium … Our results suggest that greater adherence to the MIND diet is non-linearly associated with lower circulating concentrations of selenium and of 2 potentially neurotoxic species of this element, selenoprotein P and selenate. This may explain why adherence to the MIND dietary pattern may reduce cognitive decline.\n\nUrbano, T., et al. (2023). Adherence to the Mediterranean-DASH Intervention for Neurodegenerative Delay (MIND) diet and exposure to selenium species: A cross-sectional study. Nutrition Research.\n\n\nExample 2: fermented kimchi and glucose metabolism\nThe following is from an abstract of a study investigating possible benefits of kimchi consumption among prediabetic individuals:\n\nWith the increased incidence of diabetes mellitus, the importance of early intervention in prediabetes has been emphasized … We hypothesized that kimchi and its fermented form would have beneficial effects on glucose metabolism in patients with prediabetes. A total of 21 participants with prediabetes were enrolled. During the first 8 weeks, they consumed either fresh (1-day-old) or fermented (10-day-old) kimchi. After a 4-week washout period, they switched to the other type of kimchi for the next 8 weeks. Consumption of both types of kimchi significantly decreased body weight, body mass index, and waist circumference. Fermented kimchi decreased insulin resistance, and increased insulin sensitivity … Systolic and diastolic blood pressure (BP) decreased significantly in the fermented kimchi group. The percentages of participants who showed improved glucose tolerance were 9.5 and 33.3% in the fresh and fermented kimchi groups, respectively.\n\nAn, S. Y., et al. (2013). Beneficial effects of fresh and fermented kimchi in prediabetic individuals. Annals of Nutrition and Metabolism, 63(1-2), 111-119."
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures-1",
    "href": "content/week2-descriptive.html#deviation-based-measures-1",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe variance is the average squared deviation from the mean (but divided by one less than the sample size): \\[\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2}{12 - 1}\\]\nThe standard deviation is the square root of the variance: \\[\\sqrt{\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2}{12 - 1}}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-location",
    "href": "content/week2-descriptive.html#measures-of-location",
    "title": "Descriptive statistics",
    "section": "Measures of location",
    "text": "Measures of location\nOften location is specified by the “center” of a frequency distribution.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23"
  },
  {
    "objectID": "content/lab2-descriptive.html",
    "href": "content/lab2-descriptive.html",
    "title": "Lab 2: Descriptive statistics and simple graphics",
    "section": "",
    "text": "The objectives of this lab are to learn to:\n\nmake basic statistical graphics for visualizing frequency distributions\ncompute common measures of location and spread\ndiscern appropriate measures of location and spread based on presence of outliers and skewness\n\nWe’ll use the FAMuSS dataset, as in lecture.\n\nlibrary(tidyverse)\n\n# load famuss dataset \nload('data/famuss.RData')\n\n# inspect data frame\nhead(famuss)\n\n# A tibble: 6 × 9\n  ndrm.ch drm.ch sex      age race      height weight genotype   bmi\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;\n1      40     40 Female    27 Caucasian   65      199 CC        33.1\n2      25      0 Male      36 Caucasian   71.7    189 CT        25.8\n3      40      0 Female    24 Caucasian   65      134 CT        22.3\n4     125      0 Female    40 Caucasian   68      171 CT        26.0\n5      40     20 Female    32 Caucasian   61      118 CC        22.3\n6      75      0 Female    24 Hispanic    62.2    120 CT        21.8\n\n\nAs a quick refresher, you can extract a vector of the observations for any particular variable from the dataframe as follows: famuss$[variable name].\nWhile not strictly necessary, I recommend retrieving and storing the variable(s) you’ll use as separate objects, at least while you’re still a beginner. For example:\n\n# extract the age variable\nfamuss$age\n\n# store the age column as a vector\nage &lt;- famuss$age\n\n\nBasic statistical graphics\n\nCategorical variables\nFor categorical variables, as you saw in the last lab, table(...) will tabulate counts of the number of occurrences of each unique values of a categorical variable. The result can be passed to barplot(...) for a simple barplot to visualize the frequency distribution:\n\n# retrieve genotype\ngenotype &lt;- famuss$genotype\n\n# make a table, generate a barplot\ntable(genotype) |&gt; barplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nMake a barplot to visualize the frequency distribution of racial groups in the FAMuSS study.\n\n# retrieve race\n\n# make a table, generate a barplot\n\n\n\n\n\nNumerical variables\nIf a numeric variable is discrete without too many values, the frequency distribution could be visualized without any binning as a barplot. However, this is not recommended because it will result in a plot that is not scaled properly.\nInstead, it is better to make a histogram with one bin per unique possible value; this will scale the axis properly. However, it is also acceptable to make a histogram with binning that will result in aggregating some values. Both are shown below.\nThe approximate number of bins, and thus the amount of aggregation, is controlled by the argument breaks = ...:\n\n# retrieve age\nage &lt;- famuss$age\n\n# effectively, a bar plot of ages\nhist(age, breaks = 25)\n\n\n\n\n\n\n\n# fewer bins\nhist(age, breaks = 10)\n\n\n\n\n\n\n\n\nFor continuous variables, binning is a necessity. The second plot is better, because it shows the shape more clearly without obscuring too much detail.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a histogram of percent change in dominant arm strength. Experiment to see how the shape of the distribution appears at various binning resolutions; then pick a number of breaks that you feel reflects the data best.\n\n# retrieve dominant arm percent change\n\n# make a histogram; find a binning that captures the shape well\n\n\n\nTo store a graphic as a separate file for use in other documents, find the ‘export’ icon in the plot panel and select the ‘Save as image’ option; then follow prompts. Try this for the plot you just made.\n\n\n\nDescriptive statistics\nIn class we discussed several descriptive statistics for numeric variables. These statistics are so commonly used that they have their own functions in R.\n\nMeasures of location\nThe following functions return common measures of location for numeric variables:\n\nmean(...) returns an average\nmedian(...) returns a median\nquantile(...) returns a quantile\nmin(...) and max(...) return a minimum and a maximum, respectively\n\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age (middle value)\nmedian(age)\n\n[1] 22\n\n# 25th percentile of age (\"quantile\" is another term for percentile)\nquantile(age, probs = 0.25)\n\n25% \n 20 \n\n# 25th *and* 75th percentile of age\nquantile(age, probs = c(0.25, 0.75))\n\n25% 75% \n 20  27 \n\n# minimum age\nmin(age)\n\n[1] 17\n\n# maximum age\nmax(age)\n\n[1] 40\n\n\nNotice how the probs = ... argument to the quantile() function, which specifies which percentile R will calculate, can be used to calculate multiple percentiles at once.\nIf you want to inspect all of the location measures above (the five-number summary plus the mean) the summary(...) function will do just that.\n\n# all common location measures\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   17.0    20.0    22.0    24.4    27.0    40.0 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry computing the location measures above for percent change in dominant arm strength. Compare the mean and median. What does the comparison tell you about the skewness of this variable? Is this consistent with the histogram from the previous ‘your turn’?\n\n# compute the five-number summary for change in dominant arm strength\n\n\n\n\n\nMeasures of spread\nThe following functions return common measures of spread for numeric variables:\n\nrange(...) returns the range (min, max)\nIQR(...) returns the interquartile range (middle 50% of data)\nvar(...) returns the variance (average squared deviations from mean)\nsd(...) returns the standard deviation (variance, on original scale)\n\n\n# age range\nrange(age)\n\n[1] 17 40\n\n# interquartile range of ages (width of interval containing middle 50% of data)\nIQR(age)\n\n[1] 7\n\n# variance of age\nvar(age)\n\n[1] 33.79966\n\n# standard deviation of age\nsd(age)\n\n[1] 5.813748\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCompute the standard deviation of percent change in dominant arm strength.\n\n# standard deviation of percent change in dominant arm strength\n\nInterpret the value in context.\n\n\n\n\nRobustness\nWhen would you use median instead of mean? IQR instead of standard deviation? The answer has to do with robustness, which in statistics means sensitivity to outliers or extreme values.\nTo explore this idea, recall first the actual mean and median ages for the participants in the FAMuSS study as well as the age range:\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age\nmedian(age)\n\n[1] 22\n\n# range\nrange(age)\n\n[1] 17 40\n\n\nNow let’s add an artificial outlier – a few hypothetical participant who are in their 80’s and 90’s – and compute the measures of location again:\n\n# average age\nmean(c(age, 96, 92, 87, 91))\n\n[1] 24.84975\n\n# median age\nmedian(c(age, 96, 92, 87, 91))\n\n[1] 22\n\n\nThe mean increases while the median does not. More broadly, statistics based on percentiles are in general insensitive to outliers, unless there’s a large group of outlying observations. In this sense they are robust statistics.\nA similar difference can be observed between deviation-based measures and interquartile range. The original measures were:\n\n# variance of ages\nvar(age)\n\n[1] 33.79966\n\n# interquartile range of ages\nIQR(age)\n\n[1] 7\n\n\nNow adding in our artificial outliers:\n\n# age variance\nvar(c(age, 96, 92, 87, 91))\n\n[1] 63.55598\n\n# age iqr\nIQR(c(age, 96, 92, 87, 91))\n\n[1] 7\n\n\n\n\n\nGrouped summaries\nWhat if you wish to find the mean percent change in dominant arm strength separately for each genotype?\nThe tidyverse package loaded at the outset has a pair of functions, group_by and summarize, that allow you to do this efficiently. The steps are:\n\nStart with the data frame famuss\ngroup by the genotype variable\nsummarize\n\n\n# average dominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch))\n\n# A tibble: 3 × 2\n  genotype avg.drm.ch\n  &lt;fct&gt;         &lt;dbl&gt;\n1 CC            10.7 \n2 CT             8.49\n3 TT            13.0 \n\n\nThe summarize function can actually compute multiple summaries: each argument should specify a name for the summary and the calculation to perform in the format &lt;NAME&gt; = &lt;FUNCTION&gt;(&lt;COLUMN NAME&gt;):\n\n# average dominant and nondominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch),\n            avg.ndrm.ch = mean(ndrm.ch))\n\n# A tibble: 3 × 3\n  genotype avg.drm.ch avg.ndrm.ch\n  &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 CC            10.7         48.9\n2 CT             8.49        53.2\n3 TT            13.0         58.1\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate the median percent change in dominant arm strength separately for each genotype by modifying the first example above.\n\n# median percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\nThese problems may look lengthy at face value, but the calculations are rather brief. A suggestion is: determine which calculations are required for each part and focus on doing those calculations first; then look back over your results to interpret them and answer the prompts.\n\n[L3] Use the census data again from the previous problem set and carry out the following descriptive summaries.\n\nMake a histogram of total personal income. Choose the binning so as to capture the shape well but not obscure too much detail. Are there outliers?\nCompute the mean and five-number summary of total personal income. Which measure of location is most appropriate and why?\nCompute the interquartile range and standard deviation of total personal income and interpret them in context. Which measure is more appropriate and why?\nCompute the median total personal income separately for men and women.\n\n\n\n[L3] Data from Chen, W., et al., Maternal investment increases with altitude in a frog on the Tibetan Plateau. Journal of Evolutionary Biology 26-12 (2013) includes measurements pertaining to egg clutches of several populations of frog at breeding ponds (sites) in the eastern Tibetan Plateau.\n\nHow many samples were collected at each site?\nCompute the frequency distribution of site altitudes among samples collected in the study.\nMake a barplot of the frequency distribution from (a). Are samples collected more or less uniformily across altitudes? If not, which altitudes are most represented in the sample?\nMake a histogram of clutch volumes. Describe the shape and number of modes.\nCalculate the mean and five-number summary of clutch volume.\nCalculate and interpret the standard deviation, variance, and interquartile range.\nCalculate the average clutch volume separately for each altitude. Does average clutch volume seem to differ by altitude?\n[optional] Devise a way to calculate the average absolute deviation."
  },
  {
    "objectID": "content/week2-descriptive.html",
    "href": "content/week2-descriptive.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "[lecture] frequency distributions; measures of spread and center\n[lab] descriptive statistics and simple graphics in R"
  }
]