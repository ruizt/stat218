[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics for Life Sciences",
    "section": "",
    "text": "Announcements\n\n\n\nWeek 4 outlines posted.\nNo pending assignments.\n\n\n\nCourse information\nRead the [course syllabus] for detailed information on content, materials, learning outcomes, assessments, and course policies.\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment]\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class.\n\nCompleting assignments:\nOne set of practice problems is included at the end of each lab; these problem sets are your homework assignments. You will often have some time to work on them during class, and they will be due by the following class period. To complete these assignments:\n\nReview the prompts included with the lab.\nDo your work (calculations, making plots, etc.) in the lab script provided in Posit Cloud.\nFollow the link that appears as [problem set] with the class meeting outline for the period in which the problem set was assigned. This will direct you to a form where you’ll fill out select answers. Refer to your work in Posit Cloud as you complete the form.\n\nSome general remarks:\n\nproblem sets are due one hour before the next class meeting\nlate submissions are accepted until 5pm two days after the due date\nscore summaries will be posted once all deadlines pass\nonce scores are posted, you can see your individual responses using the link that you used to access the form\n\n\n\nWeek 1 (4/1/24)\nAcademic holiday 4/1/24\nIntroduction to statistical thinking and study designs\nWednesday class meeting\n\n[reading] Vu and Harrington 1.1\n[lecture] course introduction; study designs\n[activity] distinguishing types of studies\n[problem set 1] due Monday 4/8; late submissions until Wednesday 4/10 5pm\n[problem set 1 corrections] due by 5pm Friday 4/12\n\nResponse summary [PS1] [PS1 corrections]\n\n\nWeek 2 (4/8/24)\nData types and descriptive statistics\nMonday class meeting\n\nreading quiz [12pm section] [2pm section]\n[reading] Vu and Harrington 1.2\n[lecture] data types\n[lab] R basics\n[problem set 2] due Wednesday 4/10; late submissions until Friday 4/12 5pm\n\nResponse summary [PS2]\nWednesday class meeting\n\n[reading] Vu and Harrington 1.4 - 1.5\n[lecture] descriptive statistics\n[lab] descriptive statistics in R\n[problem set 3] due Monday 4/15; late submissions until Wednesday 4/17 5pm\n\nResponse summary [PS3]\n\n\nWeek 3 (4/15/24)\nDescriptive statistics and graphical summaries\nMonday class meeting\n\n[reading quiz] Vu and Harrington 1.6\n[lecture] descriptive statistics for relationships between two variables\n[lab] bivariate summaries in R\n[problem set 4] due Wednesday 4/17; late submissions until Friday 4/19 5pm\n\nResponse summary [PS4]\nWednesday class meeting\n\n[reading] review course notes and PS1, PS2, PS3 in detail\n[review] recap and Q&A\n[R cheatsheet] for easy reference\n[practice problems] in groups with short solution presentations\n\nTest 1 available Wednesday 4/17 5pm and due Friday 4/19 5:00pm PDT [prompts] [submission] [upload R script]\n\n\nWeek 4 (4/22/24)\nFoundations for inference\nMonday class meeting\n\n[reading] Vu and Harrington 4.1\n[lecture] point estimation, sampling variability, and interval estimation\n[lab] point and interval estimation for a population mean\n[problem set 5] due Wednesday 4/24; late submissions until Friday 4/26 5pm\n\nWednesday class meeting\n\n[reading] Vu and Harrington 3.3.1, 3.3.2, and 3.3.3; and 4.2\n[lecture] interval estimation for a population mean\n[lab] computing confidence intervals\n\n\n\nWeek 5 (4/29/24)\nOne-sample inference for numerical data\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 (5/6/24)\nTwo-sample inference for numerical data\n\n\n\n\n\n\n\n\n\nTest 2 due Friday 5/10 5:00pm PDT\n\n\nWeek 7 (5/13/24)\nNonparametric tests; analysis of variance\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 (5/20/24)\nPost-hoc inference in ANOVA; introduction to inference for categorical data\n\n\n\n\n\n\n\n\nTest 3 due Friday 5/24 5:00pm PDT\n\n\nWeek 9 (5/27/24)\nAcademic holiday 5/27/24\nCategorical data analysis for contingency tables\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 (6/3/24)\nSimple linear regression\n\n\n\n\n\n\n\n\n\nTest 4 due Friday 6/7 5:00pm PDT\n\n\nFinals week (6/10/24)\nOral exams to be held during scheduled exam time\nScheduled exam times:\n\n[12pm section] Wednesday 6/12 10:10am – 1:00pm\n[2pm section] Monday 6/10 1:10pm – 4:00pm"
  },
  {
    "objectID": "content/syllabus.html",
    "href": "content/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Statistics plays a crucial role in the sciences: statistical techniques provide a means of weighing quantitative evidence derived from observation and experimentation while accounting for uncertainty. Statistical thinking and data analysis also facilitate discovery, exploration, and hypothesis generation. This class aims to provide a hands-on introduction to common statistical methods used almost universally across the sciences — descriptive and graphical techniques, inferential methods for comparing population means, analysis of categorical data and contingency tables, and linear regression — while drawing on examples from the life sciences to help illuminate the potential for application in students’ chosen field(s) of study and providing basic training in the use of statistical software.\n\n\nCourse information\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05] 12:10pm — 2:00pm MW Construction Innovations Center Room C100\n[Section 06] 2:10pm — 4:00pm MW Construction Innovations Center Room C100\n\nClass meetings will comprise a mixture of lecture, lab activities, class activities, and discussion.\nOffice hours: 8:10am — 11:00am Mondays 25-236 or Zoom [by appointment].\nThese times are partitioned into 15 minute intervals that you can schedule via the appointment link above; this system is intended to minimize waiting times and guarantee one-on-one availability. Slots can be scheduled anywhere from 7 calendar days to 10 minutes in advance. While drop-ins are welcome, I can’t guarantee availability outside of scheduled times.\nCatalog Description: Data collection and experimental design, descriptive statistics, confidence intervals, parametric and non parametric one and two-sample hypothesis tests, analysis of variance, correlation, simple linear regression, chi-square tests. Applications of statistics to the life sciences. Substantial use of statistical software. Prerequisite: MATH 96; or MATH 115; or appropriate Math Placement Level. Fulfills GE Area B4 (GE Area B1 for students on the 2019-20 or earlier catalogs); a grade of C- or better is required in one course in this GE area.\n\n\nMaterials\nYou’ll need an internet-connected laptop or tablet (a keyboard is necessary since we will do some web-hosted computation and you will be expected to type assignments). You should expect to bring your laptop or tablet to every class meeting.\nComputing: use of R/RStudio will be hosted online via a posit.cloud workspace [link to join]. To access the workspace, you’ll need to create a posit.cloud account and purchase a $5/month student plan.\nTextbook: Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences, First edition. A PDF and tablet-friendly version are available for free online at the link above. This will be our primary reference and we will cover chapters 1 – 2, 4 – 6, and 8.\nCourse notes: course notes will be posted as slides on the course website.\nOther references:\n\nVan Belle, Fisher, Heagerty, and Lumley (2004). Biostatistics: a methodology for the health sciences. Wiley. A PDF can be obtained through the Kennedy Library via the link above. This text provides a thorough introduction to biostatistics (statistics for life sciences) and is an excellent reference for more depth of coverage. Select readings will be assigned from this book.\nDouglas et al. (2023). An Introduction to R. This online book covers a variety of introductory topics pertaining to R/RStudio: installation, packages, files and directories, objects, functions, data types, data structures, graphics, basic statistics, markdown, and version control. Select readings will be assigned from this book.\n\n\n\nLearning outcomes\nThis course aims to support you in developing the following abilities.\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques\n[L4] construct and interpret confidence intervals for means and differences between means for independent and paired samples\n[L5] conduct parametric and non-parametric two-sample hypothesis tests for means\n[L6] construct and interpret a confidence interval for a single proportion\n[L7] conduct Chi-square goodness-of-fit tests and tests for independence\n[L8] distinguish between case-control and cohort studies and compute relative-risk and odds in the appropriate settings\n[L9] perform analysis of variance tests and post-hoc comparisons for completely randomized designs\n[L10] use simple linear regression to describe relationships between variables\n[L11] apply one or more methods from the course to your major field of study\n\nEmphasis is placed on conceptual fluency, application, and interpretation. In addition, you will learn to perform simple statistical analyses in R and can expect to develop a basic familiarity with the software; however, as this is not a programming class, the R environment will not be discussed in any detail and you will only learn to use a handful of commands.\n\n\nAssessments\nAttainment of learning outcomes will be measured by performance on homework assignments, tests, and a short project with an oral assessment in lieu of a final exam.\n\nHomework assignments will be given at the end of every class meeting and will comprise two practice problems due by the next class meeting. These are your opportunity to practice applying course concepts and methods covered in class and will help you to keep current with the pace and content of the lectures.\nTests will be given every 2-3 weeks and will comprise roughly 10-20 problems each. These are your opportunity to demonstrate that you’ve synthesized course material and achieved learning outcomes, and you will have approximately 48 hours to complete each test. One round of revisions will be allowed for each test in which you can make up full credit for any problems answered incorrectly in your initial attempt.\nA project with an oral assessment will be given in place of a final exam. However, you will need to be available in person during the scheduled final exam time, as this is when the oral assessment will take place.\n\nEvery assessed problem will be matched to one of the learning outcomes L1-L10. All submitted work will be assessed on a question-by-question basis as satisfactory (S) or needing improvement (NI) according to whether responses are fully correct. The percentage of problems matched to a particular learning outcome for which you receive a satisfactory assessment provides a measure of your attainment of that learning outcome. These percentages form a basis for determining your course grade (see below).\nDue to limited resources we will only provide qualitative feedback on a small subset of assessed questions, and only when an assessment of NI is made. As such, it is your responsibility to seek the feedback you need to correct your understanding where needed via class engagement, office hours, peer consultation, further study, and [tutoring resources].\n\n\nLetter grades\nStudents will receive a score for each learning outcome representing the (possibly weighted) proportion of questions matched with that outcome that received a satisfactory assessment across all assignments. The outcome will be assessed as follows:\n\n‘fully met’ if the proportion is at least 0.8\n‘partly met’ if the proportion is between 0.5 and 0.8\n‘unmet’ otherwise\n\nYou will receive periodic email summaries of your progress on each learning outcome. To receive a passing grade in the class, at least six outcomes must be either partly or fully met. Subject to this condition, letter grades are then defined as follows:\n\n\n\nGrade\nNumber of fully met outcomes\n\n\n\n\nA\n10\n\n\nA-\n9\n\n\nB+\n8\n\n\nB\n7\n\n\nB-\n6\n\n\nC+\n5\n\n\nC\n4\n\n\nC-\n3\n\n\nD+\n2\n\n\nD\n1\n\n\nD-\n0\n\n\n\nPlease note that these definitions are tentative and potentially subject to change; however, I will not make the grading requirements more stringent under any circumstances.\nPlease also note that failure to adhere to course policies may result in a lower letter grade than would otherwise be assigned.\n\n\nTentative schedule\nSubject to change.\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings (V&H)\nAssessments\n\n\n\n\n1 (4/1/24)\nIntroduction to statistical thinking and study design\n1.1\n\n\n\n2 (4/8/24)\nData, data types, and data collection\n1.2\n\n\n\n3 (4/15/24)\nDescriptive statistics and graphical summaries\n1.4, 1.5, 1.6\nTest 1 [L1, L2, L3]\n\n\n4 (4/22/24)\nFoundations for inference\n4.1, 4.2\n\n\n\n5 (4/29/24)\nOne-sample inference for numerical data\n4.3, 5.1\n\n\n\n6 (5/6/24)\nTwo-sample inference for numerical data\n5.2, 5.3, 5.4\nTest 2 [L4, L5]\n\n\n7 (5/13/24)\nNonparametric tests; analysis of variance\n5.5\n\n\n\n8 (5/20/24)\nPost-hoc inference in ANOVA; intro to categorical data analysis\n8.1\nTest 3 [L6, L9]\n\n\n9 (5/27/24)\nCategorical data analysis and contingency tables\n8.3, 8.5.1, 8.5.3\n\n\n\n10 (6/3/24)\nSimple linear regression\n6.1, 6.2, 6.4, 6.5\nTest 4 [L7, L8, L10]\n\n\nFinals (6/10/24)\nN/A\nN/A\nOral project assessment [L11]\n\n\n\n\n\nCourse policies\n\nTime commitment\nSTAT218 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including lectures, reading, assignments, and study time. Some variability in workload by week should be expected, and most students will need to budget a few extra hours each week. However, students can expect to be able to meet course expectations with a time commitment of 12-16 hours per week. Considering that class meetings account for four hours per week, students should anticipate devoting 8-12 hours outside of class. If you are spending considerably more time than this on a regular basis, please let me know.\n\n\nAttendance\nRegular attendance is essential for success in the course and required per University policy. Each student may miss two class meetings without notice but additional absences should be excusable and students should notify the instructor. Unexcused absences may negatively impact course grades.\n\n\nDeadlines and extensions\nA one-hour grace period is applied to all deadlines. Work submitted more than one hour after a deadline is considered late. Policies regarding late work are as follows:\n\nYou may turn in as many as four homework assignments up to 48 hours late without penalty at any time during the quarter and without notice. Subsequently, late work may incur a penalty in final grade calculations.\nLate submissions are not allowed for tests. You are expected to plan ahead in order to meet test deadlines; I recommend putting the dates in your calendar at the beginning of the quarter.\nExceptions may be granted for significant and unforeseen challenges (medical absences, family emergencies, and the like).\n\nExtensions may be arranged as needed if warranted by the circumstances and should be requested by email. When requesting an extension, you should explain why it is needed; it is at my discretion to grant the extension or not based on the reason provided. Extensions must be arranged at least 24 hours in advance of the original deadline; requests made after this time will not be considered as a general rule.\nThese policies are intended to provide you with some flexibility to work around unforeseen circumstances while maintaining accountability for completing coursework in a timely manner. That said, if any circumstances arise that the policies do not accommodate well, please let me know and I will do my best to work with you to keep you on track in the course.\n\n\nAcademic integrity\nYou are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR). Particularly important course policies related to academic integrity are discussed below.\nCollaboration. Collaboration among enrolled students is allowed and encouraged on homework assignments subject to the condition that every collaborator must make material contributions. Material contributions might include participation in group discussions, critique or presentation of a proposed solution, comparing numerical answers, and the like. However, group submissions are not allowed and you are expected to write up your own work. Copying the work of another student outright, knowingly allowing another student to copy your work, or submitting a copy of a shared set of answers is not acceptable and amounts to a violation of University policy on academic integrity. The best way to adhere to this policy and ensure your collaborations are productive is to:\n\nattempt problems individually before consulting others\nwrite up your own solutions in private\n\nCollaboration is not permitted on tests and will result in loss of credit.\nUse of AI. Learning to use AI effectively and responsibly for problem-solving in an academic context is a skill unto itself. Submitting problem prompts directly to ChatGPT will, most of the time, return superfluous, tangential, and erroneous answers that do not meet assessment criteria for satisfactory work. Furthermore, even when AI-generated material is technically accurate, outputs rarely conform to the examples set forth in class or the solution strategies that you have been taught.\nSo in the best-case scenario, AI-generated material might be useful but only if you expend additional effort refine the prompts you use and subsequently to parse, understand, and integrate outputs with class content. In the worst-case scenario, AI-generated material will be wrong or irrelevant and simply confuse you. Considering you are learning material that is new to you, you will most likely not be able to distinguish correct from incorrect outputs – if you could, you would have had no need to query in the first place – and it will therefore be difficult if not impossible to use AI effectively. Thus, using AI is more likely to hinder than to help your learning, and for this reason I do not recommend it.\nShould you choose to use AI you must use it as an aid only and not as a substitute for doing your own work. You will be responsible for using it thoughtfully and judiciously. That means critically assessing any outputs and continuing to prepare work to be submitted in your own words and using your own analyses. Submitting AI-generated outputs directly is never acceptable — doing so amounts to falsely representing material that you did not create as your own work and is a violation of University academic integrity policy. I will respond to such violations as follows.\n\nsome AI-generated content detected: loss of credit and warning\nflagrant AI plagiarism, first offense: loss of credit and report to OSRR\nflagrant AI plagiarism, second offense: automatic course failure and report to OSRR\n\nIf you are unsure about where the line is between acceptable and unacceptable use in any particular situation, please discuss the situation with me – I’d much rather help you learn to navigate the issue without the use of penalties wherever possible.\n\n\nAssessments and final grades\nI make every effort to provide consistent, fair, and accurate evaluation of student work. Please notify me of any suspected errors or discrepancies in evaluation promptly on an assignment-by-assignment basis (i.e., not at the end of the quarter) to guarantee consideration. Final (letter) grades will only be changed in the case of clerical errors. Attempting to negotiate scores or final grades is not appropriate.\nPer University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. If you feel your grade is unfairly assigned at the end of the course, you have the right to appeal it according to the procedure outlined here.\n\n\nCommunication and email\nStudents are encouraged to use face-to-face means of communication (class meetings and office hours) when possible. Every effort is made to respond to email within 48 weekday hours; please be aware that a message sent Thursday or Friday may not receive a reply until Monday or Tuesday. Time-sensitive messages should be identified as such in the subject line. For non-time-sensitive messages, please wait one week before sending a reminder.\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nCopyright and distribution of course materials\nStudents are not permitted to share or distribute any course materials without the written consent of the instructor. This includes, in particular, uploading materials or prepared solutions to online services and sharing materials or prepared solutions with students who may take the course in a future term. Transgressions of this policy compromise the effectiveness of instruction and assessment and do a disservice to current and future students."
  },
  {
    "objectID": "content/week2-descriptive.html#todays-agenda",
    "href": "content/week2-descriptive.html#todays-agenda",
    "title": "Descriptive statistics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n[lecture] frequency distributions; measures of spread and center\n[lab] descriptive statistics and simple graphics in R"
  },
  {
    "objectID": "content/week2-descriptive.html#last-time",
    "href": "content/week2-descriptive.html#last-time",
    "title": "Descriptive statistics",
    "section": "Last time",
    "text": "Last time\n\n\n\nData semantics\n\n\ncategorical data: ordinal (ordered) or nominal (unordered)\nnumeric data: continuous (no ‘gaps’) or discrete (‘gaps’)\n\n\nData types and data structures in R\n\n\nbasic types: numeric, character, logical, integer\na vector is a collection of values of one type\na data frame is a type-heterogeneous list of vectors of equal length\n\n\nVectors can store observations of one variable:\n\n# 4 observations of age\nages &lt;- c(18, 22, 18, 12)\nages\n\n[1] 18 22 18 12\n\n\nData frames can store observations of many variables:\n\n# 3 observations of 2 variables\ndata.frame(subject.id = c(11, 2, 31),\n           age = c(24, 31, 17),\n           sex = c('m', 'm', 'f'))\n\n  subject.id age sex\n1         11  24   m\n2          2  31   m\n3         31  17   f\n\n\n\n\nTechniques for summarizing data depend on the data type"
  },
  {
    "objectID": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "href": "content/week2-descriptive.html#what-are-descriptive-statistics",
    "title": "Descriptive statistics",
    "section": "What are descriptive statistics?",
    "text": "What are descriptive statistics?\nWe learned last time that a statistic is a data summary, i.e., any function of a set of observations.\nDescriptive statistics refers to analysis of sample characteristics using summary statistics.\n\nthese are data analyses that uses statistics interpreted on face value\nin contrast to inferential statistics, which uses statistics interpreted relative to a broader population\n\nDescriptive statistics can be either numerical or graphical; we’ll discuss both."
  },
  {
    "objectID": "content/week2-descriptive.html#dataset-famuss-study",
    "href": "content/week2-descriptive.html#dataset-famuss-study",
    "title": "Descriptive statistics",
    "section": "Dataset: FAMuSS study",
    "text": "Dataset: FAMuSS study\nObservational study of 595 individuals comparing change in arm strength before and after resistance training between genotypes for a region of interest on the ACTN3 gene.\n\nPescatello, L. S., et al. (2013). Highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or FAMuSS study. BioMed research international.\n\n\n\n\nExample data rows\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n125\n0\nFemale\n40\nCaucasian\n68\n171\nCT\n26"
  },
  {
    "objectID": "content/week2-descriptive.html#categorical-frequency-distributions",
    "href": "content/week2-descriptive.html#categorical-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Categorical frequency distributions",
    "text": "Categorical frequency distributions\nFor categorical variables, the frequency distribution is simply an observation count by category. For example:\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\ngenotype\n\n\n\n\n494\nTT\n\n\n510\nTT\n\n\n216\nCT\n\n\n19\nTT\n\n\n278\nCT\n\n\n86\nTT\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n173\n261\n161"
  },
  {
    "objectID": "content/week2-descriptive.html#numeric-frequency-distributions",
    "href": "content/week2-descriptive.html#numeric-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Numeric frequency distributions",
    "text": "Numeric frequency distributions\nFrequency distributions of numeric variables are observation counts by range; a plot of a numeric frequency distribution is called a histogram.\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\nbmi\n\n\n\n\n194\n22.3\n\n\n141\n20.76\n\n\n313\n23.48\n\n\n522\n29.29\n\n\n504\n42.28\n\n\n273\n20.34\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\n\n(10,20]\n(20,30]\n(30,40]\n(40,50]\n\n\n\n\n69\n461\n58\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe operation of dividing a numeric variable into interval ranges is called binning."
  },
  {
    "objectID": "content/week2-descriptive.html#histograms-and-binning",
    "href": "content/week2-descriptive.html#histograms-and-binning",
    "title": "Descriptive statistics",
    "section": "Histograms and binning",
    "text": "Histograms and binning\nBinning has a big effect on the visual impression. Which one captures the shape best?"
  },
  {
    "objectID": "content/week2-descriptive.html#shapes",
    "href": "content/week2-descriptive.html#shapes",
    "title": "Descriptive statistics",
    "section": "Shapes",
    "text": "Shapes\nFor numeric variables, the histogram reveals the shape of the distribution:\n\nsymmetric if it shows left-right symmetry about a central value\nskewed if it stretches farther in one direction from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#modes",
    "href": "content/week2-descriptive.html#modes",
    "title": "Descriptive statistics",
    "section": "Modes",
    "text": "Modes\nHistograms also reveal the number of modes or local peaks of frequency distributions.\n\nuniform if there are zero peaks\nunimodal if there is one peak\nbimodal if there are two peaks\nmultimodal if there are two or more peaks"
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nConsider four variables from the FAMuSS study. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "href": "content/week2-descriptive.html#your-turn-characterizing-distributions-1",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nHere are some made-up data. Describe the shape and modality."
  },
  {
    "objectID": "content/week2-descriptive.html#descriptive-measures",
    "href": "content/week2-descriptive.html#descriptive-measures",
    "title": "Descriptive statistics",
    "section": "Descriptive measures",
    "text": "Descriptive measures\nA descriptive measure is a summary statistic that captures a particular feature of the frequency distribution of a numeric variable.\nCommonly, measures capture either location or spread.\n\n\nMeasures of location:\n\nmean\nmedian\nmode\npercentiles/quantiles\n\n\nMeasures of spread:\n\nrange (min and max)\ninterquartile range\naverage deviation\nvariance\nstandard deviation\n\n\n\nIt is common practice to report multiple measures."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-center",
    "href": "content/week2-descriptive.html#measures-of-center",
    "title": "Descriptive statistics",
    "section": "Measures of center",
    "text": "Measures of center\nA measure of center is a statistic that reflects the typical value of a variable.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23"
  },
  {
    "objectID": "content/week2-descriptive.html#quick-example",
    "href": "content/week2-descriptive.html#quick-example",
    "title": "Descriptive statistics",
    "section": "Quick example",
    "text": "Quick example\nConsider the first 8 observations of change in nondominant arm strength from the FAMuSS study data:\n\n\n40, 25, 40, 125, 40, 75, 100 and 57.1\n\n\nCompute the mean, median, and mode."
  },
  {
    "objectID": "content/week2-descriptive.html#comparing-measures-of-center",
    "href": "content/week2-descriptive.html#comparing-measures-of-center",
    "title": "Descriptive statistics",
    "section": "Comparing measures of center",
    "text": "Comparing measures of center\nEach statistic is a little different, but often they roughly agree; for example, all are between 20 and 25, which seems to capture the typical BMI well enough.\n\nHow do you think the frequency distribution affects which one is “best”?"
  },
  {
    "objectID": "content/week2-descriptive.html#means-medians-and-skewness",
    "href": "content/week2-descriptive.html#means-medians-and-skewness",
    "title": "Descriptive statistics",
    "section": "Means, medians, and skewness",
    "text": "Means, medians, and skewness\nThe mean and median both get ‘pulled’ in the direction of skewness, but the mean is more sensitive:\n\nComparing means and medians captures information about skewness present since:\n\nmean \\(&gt;\\) median: right skew\nmean \\(&lt;\\) median: left skew\nmean \\(\\approx\\) median: symmetric"
  },
  {
    "objectID": "content/week2-descriptive.html#when-to-use-modes",
    "href": "content/week2-descriptive.html#when-to-use-modes",
    "title": "Descriptive statistics",
    "section": "When to use mode(s)",
    "text": "When to use mode(s)\nMode is rarely used unless extreme skewness or multiple modes are present; below are two examples."
  },
  {
    "objectID": "content/week2-descriptive.html#percentiles",
    "href": "content/week2-descriptive.html#percentiles",
    "title": "Descriptive statistics",
    "section": "Percentiles",
    "text": "Percentiles\nA percentile is a value with specified proportions of data lying both above and below that value.\n\nmeasure of location (but not center)\ndefined with reference to the percentage of data below\n\nFor example, the 20th percentile is the value with 20% of observations below and 80% of observations above. Suppose we have 5 observations:\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n19\n20\n21\n25\n31\n\n\nrank\n1\n2\n3\n4\n5\n\n\n\n\n\nThe 20th percentile is not unique! In fact any number between 19 and 20 is a 20th percentile since it would satisfy:\n\n20% below (19)\n80% above (20, 21, 25, 31)"
  },
  {
    "objectID": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "href": "content/week2-descriptive.html#cumulative-frequency-distribution",
    "title": "Descriptive statistics",
    "section": "Cumulative frequency distribution",
    "text": "Cumulative frequency distribution\nThe cumulative frequency distribution is a data summary showing percentiles. Think of it as percentile (y) against value (x).\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of some specific values:\n\nabout 40% of the subjects are 20 or younger\nabout 80% of the subjects are 24 or younger\n\nYour turn:\n\nRoughly what percentage of subjects are 22 or younger?\nAbout what age is the 10th percentile?"
  },
  {
    "objectID": "content/week2-descriptive.html#common-percentiles",
    "href": "content/week2-descriptive.html#common-percentiles",
    "title": "Descriptive statistics",
    "section": "Common percentiles",
    "text": "Common percentiles\n\n\nThe five-number summary is a collection of five percentiles that succinctly describe the frequency distribution:\n\n\n\nStatistic name\nMeaning\n\n\n\n\nminimum\n0th percentile\n\n\nfirst quartile\n25th percentile\n\n\nmedian\n50th percentile\n\n\nthird quartile\n75th percentile\n\n\nmaximum\n100th percentile\n\n\n\n\nBoxplots provide a graphical display of the five-number summary."
  },
  {
    "objectID": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "href": "content/week2-descriptive.html#boxplots-vs.-histograms",
    "title": "Descriptive statistics",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\nNotice how the two displays align, and also how they differ. The histogram shows shape in greater detail, but the boxplot is much more compact."
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-spread",
    "href": "content/week2-descriptive.html#measures-of-spread",
    "title": "Descriptive statistics",
    "section": "Measures of spread",
    "text": "Measures of spread\nThe spread of observations refers to how concentrated or diffuse the values are.\n\nTwo ways to understand and measure spread:\n\nranges of values capturing much of the distribution\ndeviations of values from a central value"
  },
  {
    "objectID": "content/week2-descriptive.html#range-based-measures",
    "href": "content/week2-descriptive.html#range-based-measures",
    "title": "Descriptive statistics",
    "section": "Range-based measures",
    "text": "Range-based measures\nA simple way to understand and measure spread is based on ranges. Consider more ages, sorted and ranked:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\nrank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\nThe range is the minimum and maximum values: \\[\\text{range} = (\\text{min}, \\text{max}) = (16, 34)\\]\nThe interquartile range (IQR) is the difference [75th percentile] - [25th percentile] \\[\\text{IQR} = 29 - 19 = 10\\] When might you prefer IQR to range? Can you think of an example?"
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures",
    "href": "content/week2-descriptive.html#deviation-based-measures",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe average deviation is defined as the average of the absolute values of the deviations from the mean: \\[\\frac{8 + 6 + 5 + 4 + 3 + 2 + 1 + 2 + 4 + 5 + 6 + 10}{12}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#mathematical-notations",
    "href": "content/week2-descriptive.html#mathematical-notations",
    "title": "Descriptive statistics",
    "section": "Mathematical notations",
    "text": "Mathematical notations\nFollowing the convention from before, write a set of \\(n\\) observations as \\(x_1, x_2, \\dots, x_n\\).\n\n\nThe mean of the observations is written: \\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]\nThe average deviation is: \\[\\frac{1}{n} \\sum_i |x_i - \\bar{x}|\\]\n\nThe variance is: \\[s_x^2 = \\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2\\]\nThe standard deviation is: \\[s_x = \\sqrt{\\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#interpretations",
    "href": "content/week2-descriptive.html#interpretations",
    "title": "Descriptive statistics",
    "section": "Interpretations",
    "text": "Interpretations\nListed from largest to smallest, here are each of the measures of spread for the 12 ages:\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin\nmax\niqr\nvariance\nst.dev\navg.dev\n\n\n\n\n16\n34\n8.5\n30.55\n5.527\n4.667\n\n\n\n\n\nThe interpretations differ between these statistics:\n\n[range] all of the data lies on an between 16 and 34 years old on an interval 18 years in width\n[IQR] the middle half of the data lies on an interval 8.5 years in width\n[average deviation] the average distance from the mean is 4.67 years\n[variance] the average squared distance from the mean is 30.55 years\\(^2\\)\n[standard deviation] the average squared distance from the mean, rescaled to years, is 5.53 years\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-descriptive.html#lab-robustness",
    "href": "content/week2-descriptive.html#lab-robustness",
    "title": "Descriptive statistics",
    "section": "Lab: robustness",
    "text": "Lab: robustness\nFor this lab we’ll continue to work with the FAMuSS data as we have throughout lecture.\nThis lab has two objectives:\n\nTeach you to compute descriptive statistics and prepare graphical summaries for a single variable in R\nLearn when and why to use certain descriptive statistics in place of others\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab1-rbasics.html",
    "href": "content/lab1-rbasics.html",
    "title": "Lab 1: R Basics",
    "section": "",
    "text": "This lab is intended to introduce you to the basics in R that you will need for this class. Most of our analyses will consist of just a few steps:\n\nload a dataset\nidentify and select variable(s) of interest\nperform one or more calculations using variable(s) of interest as inputs\n\nWe will illustrate this process so that you can get used to the mechanics and familiarize yourself with how different data types appear in R.\n\nHow to do this lab\nI’ve provided you with a project on Posit Cloud containing data files and a script (a script is a plain text file containing R commands). The script contains all commands shown in this document, and some blank areas for you to fill in, with comment lines (the ones starting with #) to help you navigate.\nYou should refer back to this document for instructions and context, and fill in the script as you go:\n\nrun the codes provided as you read through the narrative in this document and inspect the results\nin the ‘your turn’ sections, refer to the prompt in this document and use the example commands provided immediately beforehand to determine which command to write\nwrite in your commands in the script the space below the corresponding comment, not in the console (otherwise you’ll have a hard time keeping track of your work)\n\nYour goal is to complete all of the “your turn” parts in the script. Two practice problems are given at the end of the lab as homework for you to complete on your own before next class.\n\n\nHow to use this lab\nThis lab (and the lab activities in general) are designed to provide you with a set of examples to learn initially in class and then follow on your own later when doing the homework problems given at the end of the lab.\nIf you can do the examples and ‘your turn’ activities in class, all you’ll need to do to complete the homeworks is copy commands from those examples and activities and adjust some small details (variable names, dataset names, etc.).\nIf you later need to figure out how to do something in R for a homework problem or test, all you’ll need to do is refer back to the labs.\n\n\nPackages in R\nA “package” is a bundle of functions, datasets, and other objects that can be imported into R for use in your working environment. Many scripts begin by loading packages that will be used throughout the script. Packages are loaded using the command library(&lt;PACKAGE NAME&gt;) where &lt;PACKAGE NAME&gt; is replaced by the actual name of the package. For example:\n\nlibrary(tidyverse)\n\nPackages do need to be installed before they can be loaded. One of the nice things about using Posit Cloud is that I can manage all of these installs for you. However, if you ever wish to install and use a package that’s not available (or if you use R on your own machine), you can install a package using the command install.packages(\"&lt;PACKAGE NAME&gt;\") after replacing &lt;PACKAGE NAME&gt; with the actual name of the package (but keeping the quotation marks!).\n\n\nLoading a dataset\nThere are several ways to load datasets in R. The strategy we’ll use most often is to load an .RData file, but you will encounter a few others here and there.\n\n# load nhanes data\nload('data/nhanes.RData')\n\nThis command looks for a file called nhanes.RData in a directory folder named data and reads the file.\nNotice that once you run the command, an object called nhanes appears in the “Environment” tab in the upper right hand panel of your RStudio window.\nIf you click the little blue carrot next to nhanes in the environment tab, you will then see a list of variables contained in the dataset. You can also see the first few rows of the dataset using head(...).\n\n# first few rows\nhead(nhanes)\n\n# A tibble: 6 × 9\n  subj.id gender   age poverty pulse bpsys1 bpdia1 totchol sleephrsnight\n    &lt;int&gt; &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;         &lt;int&gt;\n1       1 male      34    1.36    70    114     88    3.49             4\n2       2 male      34    1.36    70    114     88    3.49             4\n3       3 male      34    1.36    70    114     88    3.49             4\n4       5 female    49    1.91    86    118     82    6.7              8\n5       8 female    45    5       62    106     62    5.82             8\n6       9 female    45    5       62    106     62    5.82             8\n\n\nThis kind of object in R is called a data frame. Data frames are displayed in a tabular layout, like a spreadsheet. While data frames should be arranged so that observations are shown in rows and variables in columns, this is not guaranteed, so you should be in the habit of checking to make sure the layout is sensible; otherwise, you might accidentally perform bogus calculations and analyses.\nBeyond providing a sanity check, inspecting the data frame will show you three key pieces of information besides the values of the first few observations of each variable.\n\nData dimensions: how many observations (rows) and how many variables (columns)\nVariable names: subj.id, gender, age, etc.\nData types:\n\nint for integer (numerical data type)\nfct for factor (categorical data type)\nnum for numeric (numerical data type)\nchr for character (categorical data type)\n\n\nSo, for example, seeing that pulse is of data type int tells you that pulse is a discrete numerical variable. It also tells you what name to use to refer to the variable in subsequent R commands.\n\n\n\n\n\n\nYour turn\n\n\n\nThere is another data file in the data directory called famuss.RData. Load this into the environment, preview the first few observations, and check the variable names and data types.\n\n# load famuss dataset\n\n# preview first few rows\n\nTo check your understanding:\n\nhow many observations and variables?\nidentify a categorical variable\nwhat kind of variable is bmi?\n\n\n\n\n\nSelecting variables\nThe variable names in a dataset can be used to retrieve or refer to specific variables. For example, try running this command:\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# preview first few values\nhead(total.cholesterol)\n\n[1] 3.49 3.49 3.49 6.70 5.82 5.82\n\n\nThat command did the following:\n\nextracted the totchol column of nhanes (the nhanes$totchol part)\nassigned the result a new name total.cholesterol (the &lt;- part)\n\nAssignment (&lt;-) is a very important concept in R – you can store the result of any calculation as an object with a name of your choosing.\nYou’ll notice that total.cholesterol looks a bit different than the data frame in terms of its appearance. This is because it’s not a data frame but rather a different kind of object called a vector: a collection of values of the same data type.\n\n\n\n\n\n\nYour turn\n\n\n\nExtract the change in nondominant arm strength variable from the FAMuSS dataset, and store it as a vector called strength.\n\n# store the change in nondominant arm strength variable as a vector called 'strength'\n\n# preview the first few values\n\n\n\n\n\nPerforming calculations\nExtracting and storing variables as vectors isn’t strictly necessary, but does make it easier to perform many calculations. While you’re a beginner, I’d recommend using this strategy.\n\nNumeric summaries\nMost simple summary statistics can be calculated using simple functions in R that take a single vector argument. For example, to calculate the average, minimum, and maximum total cholesterol among the respondents in the sample:\n\n# average total cholesterol\nmean(total.cholesterol)\n\n[1] 5.042938\n\n# minimum\nmin(total.cholesterol)\n\n[1] 2.33\n\n# maximum\nmax(total.cholesterol)\n\n[1] 13.65\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nFind the average percent change in nondominant arm strength of participants in the FAMuSS study sample using the strength vector you created before.\n\n# compute mean change in nondominant arm strength\n\n\n\n\n\nCategorical summaries\nMost data summaries for categorical variables proceed from counts of the number of observations in each category. These counts can be obtained by passing a vector of observations to table(...):\n\n# retreive sex variable\nsex &lt;- nhanes$gender\n\n# counts\ntable(sex)\n\nsex\nfemale   male \n  1588   1591 \n\n\nTo obtain the proportion of observations in each category – the counts divided by the total number of observations – pass the table to the proportions(...) function:\n\n# proportions\ntable(sex) |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nThe character string |&gt; is a bit of syntax that you could read verbally as ‘then’: first make a table, then obtain proportions. It’s known as the pipe operator, because it ‘pipes’ the result of the command on its left into the command on its right.\nTo see another example of the pipe operator in action, you could rewrite the previous command as a chain of three steps:\n\n# same as above\nsex |&gt; table() |&gt; proportions()\n\nsex\n   female      male \n0.4995282 0.5004718 \n\n\nYou could interpret this as follows: start with sex, pass that to table(), then pass the result to proportions.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the FAMuSS dataset, calculate the genotype frequencies in the sample (i.e., find the proportion of observations of each genotype).\n\n# retrieve genotype\n\n# counts\n\n# proportions\n\n\n\nWhile the analyses you’ll learn will get more complex than computing summary statistics, the mechanics of performing the computations in R will be analogous to what you just did: executing a one-line command with a vector input.\n\n\n\nPutting together the pieces\nReflect for a moment on what you just did: you wrote a few lines of code to import a dataset, extract a variable, and compute a statistic. If you filled in the script as instructed, you now have a record of the commands you executed that you can use to retrace your steps.\nIn fact, anyone with your script and the data files (including future you) could easily reproduce your work. Reproducibility is a pillar of data-driven science; by storing analyses in the form of executable scripts, researchers can easily create and share records of their work.\nWe could put the steps above together in just a few lines as if it were a short script. Typical style is to provide line-by-line comments explaining what the commands do.\n\n# import nhanes data\nload('data/nhanes.RData')\n\n# inspect data\nhead(nhanes)\n\n# extract total cholesterol\ntotal.cholesterol &lt;- nhanes$totchol\n\n# compute average total cholesterol\nmean(total.cholesterol)\n\n# extract sex\nsex &lt;- nhanes$gender\n\n# proportions of men and women in sample\ntable(sex) |&gt; proportions()\n\n\n\n\n\n\n\nYour turn\n\n\n\nFollow the example above and combine the previous exercises into a few lines of code with appropriate line comments.\n\n# load famuss dataset\n\n# inspect data\n\n# extract nondominant change in arm strength\n\n# compute average change in strength\n\n# extract genotype\n\n# compute genotype frequencies (proportions)\n\n\n\nIf this was all entirely new to you, congratulations on writing your first lines of code!\n\n\nExtras\n\nReading CSV files\nOften data are stored in spreadsheets, which can be easily converted to comma-separated values or CSV files (extension .csv). These are plain-text files that are a bit more lightweight than an Excel spreadsheet.\nR can read CSV (as well as other) files. The read.csv(...) function will parse the file and produce a data frame. The result can be assigned a name and stored as an object in the environment.\n\n# parse a csv file\nread.csv('data/gss.csv')\n\n# store the result in the environment\ngss &lt;- read.csv('data/gss.csv')\n\nMost of the time in class we’ll load .RData files or obtain datasets through packages (more on this later), but if you use R outside of class you may find it more common to manage data input via .csv files.\n\n\nMore about R\nWhile you will learn new commands going forward, we won’t go much more in depth with R than what you just saw. However, if you’re interested in understanding the above concepts in greater detail, or learning about R as a programming environment, see An Introduction to R.\n\n\n\nPractice problems\nDue before the next class meeting.\n\nThe census dataset contains a sample of data for 377 individuals included in the 2000 U.S. census. Load and inspect the dataset, and determine:\n\nthe youngest and oldest individual in the sample\nthe average total personal income\nthe average total family income\nhow many variables are in the dataset, not including census year and FIPS code\nhow many categorical variables are in the dataset, not including FIPS code\n\n\n\nThe cdc.samp dataset in the oibiostat package contains a sample of data for 60 individuals surveyed by the CDC’s Behavioral Risk Factors Surveillance System (BRFSS). Use the provided commands to load the dataset, and then inspect it the usual way. Notice that several of the variables are 1’s and 0’s. Use the command ?cdc.samp to view the data documentation.\n\nWhat do the values (1’s and 0’s) mean in the exerany variable?\nWhat proportion of the sample are men? What proportion are women?\nFor each general health category, find the proportion of respondents who rated themselves in that category.\nHow many of the respondents have health coverage? (Hint: sum(x) will add up the values in a vector x; adding up a collection of 1’s and 0’s is equivalent to counting the number of 1’s.) What percentage of the respondents have health coverage?"
  },
  {
    "objectID": "content/week1-studies.html#todays-agenda",
    "href": "content/week1-studies.html#todays-agenda",
    "title": "Welcome to STAT218",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nCourse logistics\n[lecture] Study designs\n[activity, if time] Distinguishing study types"
  },
  {
    "objectID": "content/week1-studies.html#icebreakers",
    "href": "content/week1-studies.html#icebreakers",
    "title": "Welcome to STAT218",
    "section": "Icebreakers",
    "text": "Icebreakers\nBy show of hands…\n\n\nFirst statistics class ever?\nLast statistics class ever?\nExpect to take STAT313?\nExpect to use statistics for your degree coursework or senior project?\nConsidering a statistics or data science minor?"
  },
  {
    "objectID": "content/week1-studies.html#class-composition",
    "href": "content/week1-studies.html#class-composition",
    "title": "Welcome to STAT218",
    "section": "Class composition",
    "text": "Class composition\nBy the numbers…"
  },
  {
    "objectID": "content/week1-studies.html#statistics-and-uncertainty",
    "href": "content/week1-studies.html#statistics-and-uncertainty",
    "title": "Welcome to STAT218",
    "section": "Statistics and uncertainty",
    "text": "Statistics and uncertainty\n\nLife is full of uncertainty, and this can make a lot of questions hard to answer, because similar situations do not always result in the same outcome.\n\nStatistical thinking: uncertainty is measurable.\nWhat statistics can offer:\n\nprinciples for designing studies and collecting data in order to capture outcome variability\ndata analytic tools to distinguish random from systematic variability\nheuristics to make inferences that account for uncertainty"
  },
  {
    "objectID": "content/week1-studies.html#course-goal-and-scope",
    "href": "content/week1-studies.html#course-goal-and-scope",
    "title": "Welcome to STAT218",
    "section": "Course goal and scope",
    "text": "Course goal and scope\nThe overarching goal of 218 is to introduce you to statistics in a hands-on way that is relevant to your major.\nSo we will focus on:\n\nstatistical thinking, study design, and data analysis\nclassical methods, mostly developed 1900-1940\ncase studies from life sciences"
  },
  {
    "objectID": "content/week1-studies.html#materials",
    "href": "content/week1-studies.html#materials",
    "title": "Welcome to STAT218",
    "section": "Materials",
    "text": "Materials\nComputer/tablet. You’ll need a laptop (preferred) or tablet with keyboard (workable).\nCourse website. All materials are hosted/linked on the course website. I won’t be using Canvas.\nTextbook. Vu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences. I suggest a $5-15 donation.\nStatistical software. R/RStudio hosted online via posit.cloud workspace. You will need to create an account and purchase a $5/month student subscription."
  },
  {
    "objectID": "content/week1-studies.html#class-meetings",
    "href": "content/week1-studies.html#class-meetings",
    "title": "Welcome to STAT218",
    "section": "Class meetings",
    "text": "Class meetings\nClass meetings will usually consist of a reading quiz, a lecture, a break, and a lab.\nPreparing for class meetings:\n\nCheck the course website for posted reading, materials, and assignments.\nComplete readings in advance of the class meetings for which they are listed.\nWrite down one question you have about the reading and bring it to class.\nDownload and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class."
  },
  {
    "objectID": "content/week1-studies.html#assignments",
    "href": "content/week1-studies.html#assignments",
    "title": "Welcome to STAT218",
    "section": "Assignments",
    "text": "Assignments\nYou will have three categories of assignments:\n\nhomework problems: two per class due by next class\ntests: every 2-3 weeks, distributed Wednesday, due Friday\na project: find and present a case study\n\nDeadline policies:\n\none-hour grace period on all deadlines\nfour homework problem sets can be turned in up to 48 hours late without notice\nbesides free lates, extensions must be arranged 24 hours in advance of the deadline"
  },
  {
    "objectID": "content/week1-studies.html#grades",
    "href": "content/week1-studies.html#grades",
    "title": "Welcome to STAT218",
    "section": "Grades",
    "text": "Grades\nEvery graded question/problem is matched to one or more of the 11 course learning outcomes.\n\nQuestions/problems are evaluated as satisfactory (S), needs improvement (NI), or missing (M).\nFor each outcome, the percentage of questions/problems awarded a satisfactory mark is used to determine whether that outcome is fully met, partly met, or not met:\n\nfully met: 80% or more of matched questions satisfactory\npartly met: 50% – 80% of matched questions satisfactory\nnot met: less than 50% of matched questions satisfactory\n\n\nYour course grade is based on how many learning outcomes are fully met. To pass, you must partly or fully meet at least 6 outcomes; for a C-, you must fully meet at least 3 outcomes."
  },
  {
    "objectID": "content/week1-studies.html#important-policies",
    "href": "content/week1-studies.html#important-policies",
    "title": "Welcome to STAT218",
    "section": "Important policies",
    "text": "Important policies\n\nextensions must be confirmed (not simply requested) 24 hours in advance\ncollaboration on homework is encouraged, but everyone involved needs to…\n\nmake a contribution\nwrite up their own work\n\nsubmitting AI-generated content in place of your own work is not acceptable\n\nresponsible use is okay, but not recommended (GPT outputs are misleading)\npenalties for AI plagiarism depend on precedent and severity\n\n\n\n\n\nMinor offense\nMajor offense\nPenalty\n\n\n\n\nFirst\n\nloss of credit and warning\n\n\nSecond\nFirst\nloss of credit and OSRR report\n\n\nThird\nSecond\ncourse failure and second OSRR report"
  },
  {
    "objectID": "content/week1-studies.html#what-is-a-study",
    "href": "content/week1-studies.html#what-is-a-study",
    "title": "Welcome to STAT218",
    "section": "What is a study?",
    "text": "What is a study?\nA study is an effort to collect data in order to answer one or more research questions.\n\nstudies must be well-matched to research questions to provide good answers\nhow data are obtained is just as important as how the resulting data are analyzed\nno analysis, no matter how sophisticated will rescue a poorly conceived study\n\nA study unit is the smallest object or entity that is measured in a study; also called experimental unit or observational unit."
  },
  {
    "objectID": "content/week1-studies.html#two-types-of-studies",
    "href": "content/week1-studies.html#two-types-of-studies",
    "title": "Welcome to STAT218",
    "section": "Two types of studies",
    "text": "Two types of studies\nObservational studies collect data from an existing situation without intervention.\n\nAim is to detect associations and patterns\nCan’t be used to establish causal links\n\nExperiments collect data from a situation in which one or more interventions have been introduced by the investigator.\n\nAim is to draw conclusions about the causal effect of interventions\nStronger form of scientific evidence than an observational study\n\nOften, observational studies are used to explore/generate hypotheses prior to designing an experiment."
  },
  {
    "objectID": "content/week1-studies.html#comparing-study-types",
    "href": "content/week1-studies.html#comparing-study-types",
    "title": "Welcome to STAT218",
    "section": "Comparing study types",
    "text": "Comparing study types\nEither type of study can be used to address a question.\n\n\n\n\n\n\n\n\nQuestion\nObservational study\nExperiment\n\n\n\n\nAre diet and mood related?\nConduct surveys on diet, lifestyle, and affect\nRecruit study participants, assign diets, measure affect\n\n\nIs vaping safer than smoking?\nFollow groups of vapers and smokers over time and record health outcomes\nAmong a group of smokers, assign some to switch to vaping; compare health outcomes over time\n\n\nDo insecticide applications affect soil microbes?\nAnalyze soil samples from farms using different insecticides\n[Your turn]\n\n\n\nCan you think of pros and cons for each study type?"
  },
  {
    "objectID": "content/week1-studies.html#why-does-intervention-matter",
    "href": "content/week1-studies.html#why-does-intervention-matter",
    "title": "Welcome to STAT218",
    "section": "Why does intervention matter?",
    "text": "Why does intervention matter?\nControl over conditions allows a researcher to study causal effects resulting from interventions. This is not possible in observational studies due to the potential for confounding.\n\n\nConfounding: an unobserved condition is associated with both the study condition and the outcome.\n\nFailure to measure and account for confounders potentially distorts observed associations\nExample: a study finds that dog owners live longer, but doesn’t measure exercise; so it might just be the daily walks.\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved variable) --- B(study variable) & C(study outcome) \n\n\n\n\n\n\n\n\nThis is very common in observational studies, because you can’t measure every study condition."
  },
  {
    "objectID": "content/week1-studies.html#antidote-randomization",
    "href": "content/week1-studies.html#antidote-randomization",
    "title": "Welcome to STAT218",
    "section": "Antidote: randomization",
    "text": "Antidote: randomization\nThe ability to control study conditions allows researchers to randomly allocate interventions among study subjects.\n\nRandomization eliminates confounding by isolating the condition(s) of interest:\n\ninterventions are independent of extraneous conditions ⟹ no association possible\nif outcomes differ systematically according to the intervention, you can be certain that it is not an artifact\n\n\n\n\n\n\n\nflowchart TD\n  A(unobserved condition) x-.-x B(study condition)\n  A --- C(outcome)"
  },
  {
    "objectID": "content/week1-studies.html#practical-consequences",
    "href": "content/week1-studies.html#practical-consequences",
    "title": "Welcome to STAT218",
    "section": "Practical consequences",
    "text": "Practical consequences\nThe ability to randomize interventions in experiments means:\n\nobserved associations are independent of extraneous factors\nresults can support causal inferences\n\nThe absence of randomization in observational studies means:\n\nconfounding is always possible\nresults may be misleading"
  },
  {
    "objectID": "content/week1-studies.html#experimental-designs",
    "href": "content/week1-studies.html#experimental-designs",
    "title": "Welcome to STAT218",
    "section": "Experimental designs",
    "text": "Experimental designs\nA treatment is an experimental intervention; the design of an experiment refers to how treatments are allocated to study units.\nThe most basic design is:\n\n[balanced] each treatment is replicated an equal number of times\n[randomized] treatments are allocated completely at random to study units\n[no crossover] each study unit receives exactly one treatment\n\nWe’ll call this a completely randomized design. It’s the only kind of experimental design we’re going to consider in STAT218.\nThere are many other designs that we won’t discuss (but see STAT313); these are all about improving experimental efficiency by controlling extraneous variation."
  },
  {
    "objectID": "content/week1-studies.html#data-collection",
    "href": "content/week1-studies.html#data-collection",
    "title": "Welcome to STAT218",
    "section": "Data collection",
    "text": "Data collection\nStudy units should be chosen so as to represent a larger collection.\n\n\n\n\nA study population is a collection of all study units of interest.\nA sample is a subcollection from a population:\n\nrandom if study units have a known chance of inclusion in the sample\nnonrandom or convenience otherwise\n\n\n\nThe gold standard is the simple random sample: each study unit in the population has an equal chance of inclusion in the sample."
  },
  {
    "objectID": "content/week1-studies.html#leap-study",
    "href": "content/week1-studies.html#leap-study",
    "title": "Welcome to STAT218",
    "section": "LEAP Study",
    "text": "LEAP Study\n\n\nLearning early about peanut allergy (LEAP) study:\n\n640 infants in UK with eczema or egg allergy but no peanut allergy enrolled\neach infant randomly assigned to peanut consumption and peanut avoidance groups\n\npeanut consumption: fed 6g peanut protein daily until 5 years old\npeanut avoidance: no peanut consumption until 5 years old\n\nat 5 years old, oral food challenge (OFC) allergy test administered\n13.3% of the avoidance group developed allergies, compared with 1.9% of the consumption group\n\n\n\n\n\nStudy characteristics\n\n\nStudy type: experiment\nStudy population: UK infants with eczema or egg allergy but no peanut allergy\nSample: 640 infants from population\nStudy design: completely randomized design\nTreatments: peanut consumption; peanut avoidance\nStudy outcome: development of peanut allergy by 5 years of age\n\n\n\n\n\n\nStudy results\n\n\nModerated peanut consumption causes a reduction in the likelihood of developing an allergy."
  },
  {
    "objectID": "content/week1-studies.html#checklist-for-next-time",
    "href": "content/week1-studies.html#checklist-for-next-time",
    "title": "Welcome to STAT218",
    "section": "Checklist for next time",
    "text": "Checklist for next time\n\nObtain a copy of the textbook.\nCreate a posit.cloud account and purchase a student subscription. Ensure you can access the stat218-s24 workspace.\nComplete practice problems and reading before class.\nWrite down one question about the reading.\nPrint a paper or virtual copy of the slides."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account",
    "href": "content/week1-studies.html#posit-cloud-account",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nGo to: course webpage &gt; syllabus &gt; materials. Then look for the link to join the class workspace:"
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-1",
    "href": "content/week1-studies.html#posit-cloud-account-1",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nFollow prompts to create an account. Use your Cal Poly email."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-2",
    "href": "content/week1-studies.html#posit-cloud-account-2",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nOnce your email is verified, return to posit.cloud (or click the link in the syllabus again), and join the class workspace."
  },
  {
    "objectID": "content/week1-studies.html#posit-cloud-account-3",
    "href": "content/week1-studies.html#posit-cloud-account-3",
    "title": "Welcome to STAT218",
    "section": "Posit cloud account",
    "text": "Posit cloud account\nUpgrade your account to the student plan. Input payment details."
  },
  {
    "objectID": "content/week1-studies.html#printing-slides",
    "href": "content/week1-studies.html#printing-slides",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nOpen menu from lower left"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-1",
    "href": "content/week1-studies.html#printing-slides-1",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nNavigate to tools"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-2",
    "href": "content/week1-studies.html#printing-slides-2",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nSelect PDF export mode"
  },
  {
    "objectID": "content/week1-studies.html#printing-slides-3",
    "href": "content/week1-studies.html#printing-slides-3",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\n\n\nThen print from browser to PDF\n\n\nI suggest landscape layout and either 1, 2 or 4 slides per page\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week2-datatypes.html#todays-agenda",
    "href": "content/week2-datatypes.html#todays-agenda",
    "title": "Data semantics and data types",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\n[lecture] data semantics and data types\n[lab] R basics"
  },
  {
    "objectID": "content/week2-datatypes.html#data-semantics",
    "href": "content/week2-datatypes.html#data-semantics",
    "title": "Data semantics and data types",
    "section": "Data semantics",
    "text": "Data semantics\n\nData are a set of measurements.\nA variable is any measured attribute of study units.\nAn observation is a measurement of one or more variables taken on one particular study unit.\n\nIt is usually expedient to arrange data values in a table in which each row is an observation and each column is a variable:"
  },
  {
    "objectID": "content/week2-datatypes.html#leap-example",
    "href": "content/week2-datatypes.html#leap-example",
    "title": "Data semantics and data types",
    "section": "LEAP example",
    "text": "LEAP example\nA table showing the observations and variables for the LEAP study would look like this:\n\n\n\n\n\n\n\n\n\n\nparticipant.ID\ntreatment.group\nofc.test.result\n\n\n\n\nLEAP_100522\nPeanut Consumption\nPASS OFC\n\n\nLEAP_103358\nPeanut Consumption\nPASS OFC\n\n\nLEAP_105069\nPeanut Avoidance\nPASS OFC\n\n\nLEAP_105328\nPeanut Consumption\nPASS OFC\n\n\n\n\n\nThe table you saw in the reading was a summary of the data (not the data itself):\n\n\n\n\n\n\n\n\n\n\n \nFAIL OFC\nPASS OFC\n\n\n\n\nPeanut Avoidance\n36\n227\n\n\nPeanut Consumption\n5\n262"
  },
  {
    "objectID": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "href": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "title": "Data semantics and data types",
    "section": "Numeric and categorical variables",
    "text": "Numeric and categorical variables\nVariables are classified according to their values. Values can be one of two different types:\n\nA variable is numeric if its value is a number\nA variable is categorical if its value is a category, usually recorded as a name or label\n\nFor example:\n\nthe value of sex can be male or female, so it is categorical\nwhereas age (in years) can be any positive integer, so it is numeric"
  },
  {
    "objectID": "content/week2-datatypes.html#variable-subtypes",
    "href": "content/week2-datatypes.html#variable-subtypes",
    "title": "Data semantics and data types",
    "section": "Variable subtypes",
    "text": "Variable subtypes\nFurther distinctions are made based on the type of number or type of category used to measure an attribute. Can you match the subtypes to the variables at right?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nhispanic\ngrade\nweight\n\n\n\n\n15\nnot\n10\n78.02\n\n\n18\nhispanic\n12\n78.47\n\n\n17\nnot\n11\n95.26\n\n\n18\nnot\n12\n95.26\n\n\n\n\n\n\n\n\na numerical variable is discrete if there are ‘gaps’ between its possible values\na numerical variable is continuous if there are no such gaps\na categorical variable is nominal if its levels are not ordered\na categorical variable is ordinal if its levels are ordered"
  },
  {
    "objectID": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "href": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "title": "Data semantics and data types",
    "section": "Many ways to measure attributes",
    "text": "Many ways to measure attributes\nVariable type (or subtype) is not an inherent quality — attributes can often be measured in many different ways.\nFor instance, age might be measured as either a discrete, continuous, or ordinal variable, depending on the situation:\n\n\n\nAge (years)\nAge (minutes)\nAge (brackets)\n\n\n\n\n12\n6307518.45\n10-18\n\n\n8\n4209187.18\n5-10\n\n\n21\n11258103.08\n18-30\n\n\n\n\nNumeric variables can always be represented as categorical, but not the other way around."
  },
  {
    "objectID": "content/week2-datatypes.html#your-turn",
    "href": "content/week2-datatypes.html#your-turn",
    "title": "Data semantics and data types",
    "section": "Your turn",
    "text": "Your turn\nClassify each variable as nominal, ordinal, discrete, or continuous:\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ngenotype\nsex\nage\nrace\nbmi\n\n\n\n\n33.3\nCT\nFemale\n19\nCaucasian\n21.01\n\n\n71.4\nCT\nFemale\n18\nOther\n23.18\n\n\n37.5\nCC\nFemale\n21\nCaucasian\n28.92\n\n\n50\nCC\nFemale\n28\nAsian\n21.16\n\n\n\n\n\nData are from an observational study investigating demographic, physiological, and genetic characteristics associated with muscle strength.\n\nndrm.ch is change in strength in nondominant arm after resistance training\ngenotype indicates genotype at a particular location within the ACTN3 gene"
  },
  {
    "objectID": "content/week2-datatypes.html#common-summary-statistics",
    "href": "content/week2-datatypes.html#common-summary-statistics",
    "title": "Data semantics and data types",
    "section": "Common summary statistics",
    "text": "Common summary statistics\n\nA statistic is a data summary: in mathematical terms, a function of several observations\n\n\n\nFor numeric variables, the most common summary statistic is the average value:\n\\[\\text{average} = \\frac{\\text{sum of values}}{\\text{# observations}}\\]\nFor example, the average percent change in nondominant arm strength was 53.291%.\n\nFor categorical variables, the most common summary statistic is a proportion:\n\\[\\text{proportion}_i = \\frac{\\text{# observations in category } i}{\\text{# observations}}\\]\nFor example:\n\n\n\nGenotype proportions\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n0.2908\n0.4387\n0.2706"
  },
  {
    "objectID": "content/week2-datatypes.html#descriptive-analyses",
    "href": "content/week2-datatypes.html#descriptive-analyses",
    "title": "Data semantics and data types",
    "section": "Descriptive analyses",
    "text": "Descriptive analyses\nSometimes, a few clever summary statistics can be used to answer a research question.\n\nHow much does the average change in arm strength differ by genotype, if at all?\n\nComputing per-genotype averages provides an answer:\n\n\n\n\n\n\n\n\n\n\n\ngenotype\navg.change\nn.obs\nprop.obs\n\n\n\n\nTT\n58.08\n161\n0.2706\n\n\nCT\n53.25\n261\n0.4387\n\n\nCC\n48.89\n173\n0.2908\n\n\n\n\n\nNumber of observations and proportions are included because they provide information about genotype frequencies in the sample.\n\nconveys how many individuals were measured\nalso provides an estimate of genotype frequencies in the population"
  },
  {
    "objectID": "content/week2-datatypes.html#common-mathematical-notation",
    "href": "content/week2-datatypes.html#common-mathematical-notation",
    "title": "Data semantics and data types",
    "section": "Common mathematical notation",
    "text": "Common mathematical notation\nWhile we won’t use mathematical expressions too often in STAT218, it’s useful to be aware of some common notations.\nTypically, a set of observations is written as:\n\\[x_1, x_2, \\dots, x_n\\]\n\n\\(x\\) represents the variable (e.g., genotype, age, percent change, etc.)\nsubscript indexes observations: \\(x_i\\) is the \\(i\\)th observation\n\\(n\\) is the total number of observations\n\nThe sum of the observations is written \\(\\sum_i x_i\\), where the symbol \\(\\sum\\) stands for ‘summation’. This is useful for writing the formula for computing an average:\n\\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]"
  },
  {
    "objectID": "content/week2-datatypes.html#lab-data-basics-in-r",
    "href": "content/week2-datatypes.html#lab-data-basics-in-r",
    "title": "Data semantics and data types",
    "section": "Lab: data basics in R",
    "text": "Lab: data basics in R\nThe variable types we just discussed map pretty neatly (but not perfectly) onto the main “data types” in R:\n\nnumeric ➜ integer, numeric\ncategorical ➜ character, factor, logical\n\nThe primary way data are arranged in R is in a data frame. This lab will show you how to load, inspect, and use data frames.\nYour objectives in this lab are:\n\nlearn to load and inspect datasets\nlearn to recognize data types\nlearn to perform simple calculations (averages, etc.)"
  },
  {
    "objectID": "content/week2-datatypes.html#opening-the-lab-activity",
    "href": "content/week2-datatypes.html#opening-the-lab-activity",
    "title": "Data semantics and data types",
    "section": "Opening the lab activity",
    "text": "Opening the lab activity\nNavigate to posit.cloud. Then:\n\n\n\n\n\n\nMake sure the class workspace “stat218-s24” is highlighted at left. If “Your Workspace” is highlighted, you won’t see the example assignment.\nClick on the lab1-rbasics, then wait.\n\nOnce everyone is ready, we’ll have a look at the example files together.\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week1-activity-studydesigns.html",
    "href": "content/week1-activity-studydesigns.html",
    "title": "Distinguishing study designs",
    "section": "",
    "text": "Recall that the difference between an observational study and an experiment hinges on whether researchers intentionally intervene on the system of study (experiment) or passively record outcomes (observational study).\nIn this activity you’ll read abstracts from a few published studies and determine what kind of study is described in the abstract. You do not need to consider the examples in order — start with the ones that look most interesting.\nFor each example, identify the following:\n\nstudy type\nstudy population\nsample characteristics\nstudy outcome(s)\n\n\nExample 1: selenium exposure and Mediterranean diet\nThe following is from the abstract of a study investigating dietary mitigation of selenium exposure:\n\nSelenium is a trace element found in many chemical forms. Selenium and its species have nutritional and toxicologic properties, some of which may play a role in the etiology of neurological disease. We hypothesized that adherence to the Mediterranean-Dietary Approach to Stop Hypertension Intervention for Neurodegenerative Delay (MIND) diet could influence intake and endogenous concentrations of selenium and selenium species, thus contributing to the beneficial effects of this dietary pattern. We carried out a cross-sectional study of 137 non-smoking blood donors (75 females and 62 males) from the Reggio Emilia province, Northern Italy. We assessed MIND diet adherence using a semiquantitative food frequency questionnaire. We assessed selenium exposure through dietary intake and measurement of urinary and serum concentrations, including speciation of selenium compound in serum … Adherence to the MIND diet was positively associated with dietary selenium intake and urinary selenium excretion, whereas it was inversely associated with serum concentrations of overall selenium and organic selenium … Our results suggest that greater adherence to the MIND diet is non-linearly associated with lower circulating concentrations of selenium and of 2 potentially neurotoxic species of this element, selenoprotein P and selenate. This may explain why adherence to the MIND dietary pattern may reduce cognitive decline.\n\nUrbano, T., et al. (2023). Adherence to the Mediterranean-DASH Intervention for Neurodegenerative Delay (MIND) diet and exposure to selenium species: A cross-sectional study. Nutrition Research.\n\n\nExample 2: fermented kimchi and glucose metabolism\nThe following is from an abstract of a study investigating possible benefits of kimchi consumption among prediabetic individuals:\n\nWith the increased incidence of diabetes mellitus, the importance of early intervention in prediabetes has been emphasized … We hypothesized that kimchi and its fermented form would have beneficial effects on glucose metabolism in patients with prediabetes. A total of 21 participants with prediabetes were enrolled. During the first 8 weeks, they consumed either fresh (1-day-old) or fermented (10-day-old) kimchi. After a 4-week washout period, they switched to the other type of kimchi for the next 8 weeks. Consumption of both types of kimchi significantly decreased body weight, body mass index, and waist circumference. Fermented kimchi decreased insulin resistance, and increased insulin sensitivity … Systolic and diastolic blood pressure (BP) decreased significantly in the fermented kimchi group. The percentages of participants who showed improved glucose tolerance were 9.5 and 33.3% in the fresh and fermented kimchi groups, respectively.\n\nAn, S. Y., et al. (2013). Beneficial effects of fresh and fermented kimchi in prediabetic individuals. Annals of Nutrition and Metabolism, 63(1-2), 111-119."
  },
  {
    "objectID": "content/week2-descriptive.html#deviation-based-measures-1",
    "href": "content/week2-descriptive.html#deviation-based-measures-1",
    "title": "Descriptive statistics",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe variance is the average squared deviation from the mean (but divided by one less than the sample size): \\[\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}\\]\nThe standard deviation is the square root of the variance: \\[\\sqrt{\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}}\\]"
  },
  {
    "objectID": "content/week2-descriptive.html#measures-of-location",
    "href": "content/week2-descriptive.html#measures-of-location",
    "title": "Descriptive statistics",
    "section": "Measures of location",
    "text": "Measures of location\nOften location is specified by the “center” of a frequency distribution.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23"
  },
  {
    "objectID": "content/lab2-descriptive.html",
    "href": "content/lab2-descriptive.html",
    "title": "Lab 2: Descriptive statistics and simple graphics",
    "section": "",
    "text": "The objectives of this lab are to learn to:\n\nmake basic statistical graphics for visualizing frequency distributions\ncompute common measures of location and spread\ndiscern appropriate measures of location and spread based on presence of outliers and skewness\n\nWe’ll use the FAMuSS dataset, as in lecture.\n\nlibrary(tidyverse)\n\n# load famuss dataset \nload('data/famuss.RData')\n\n# inspect data frame\nhead(famuss)\n\n# A tibble: 6 × 9\n  ndrm.ch drm.ch sex      age race      height weight genotype   bmi\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;\n1      40     40 Female    27 Caucasian   65      199 CC        33.1\n2      25      0 Male      36 Caucasian   71.7    189 CT        25.8\n3      40      0 Female    24 Caucasian   65      134 CT        22.3\n4     125      0 Female    40 Caucasian   68      171 CT        26.0\n5      40     20 Female    32 Caucasian   61      118 CC        22.3\n6      75      0 Female    24 Hispanic    62.2    120 CT        21.8\n\n\nAs a quick refresher, you can extract a vector of the observations for any particular variable from the dataframe as follows: famuss$[variable name].\nWhile not strictly necessary, I recommend retrieving and storing the variable(s) you’ll use as separate objects, at least while you’re still a beginner. For example:\n\n# extract the age variable\nfamuss$age\n\n# store the age column as a vector\nage &lt;- famuss$age\n\n\nBasic statistical graphics\n\nCategorical variables\nFor categorical variables, as you saw in the last lab, table(...) will tabulate counts of the number of occurrences of each unique values of a categorical variable. The result can be passed to barplot(...) for a simple barplot to visualize the frequency distribution:\n\n# retrieve genotype\ngenotype &lt;- famuss$genotype\n\n# make a table, generate a barplot\ntable(genotype) |&gt; barplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nMake a barplot to visualize the frequency distribution of racial groups in the FAMuSS study.\n\n# retrieve race\n\n# make a table, generate a barplot\n\n\n\n\n\nNumerical variables\nIf a numeric variable is discrete without too many values, the frequency distribution could be visualized without any binning as a barplot. However, this is not recommended because it will result in a plot that is not scaled properly.\nInstead, it is better to make a histogram with one bin per unique possible value; this will scale the axis properly. However, it is also acceptable to make a histogram with binning that will result in aggregating some values. Both are shown below.\nThe approximate number of bins, and thus the amount of aggregation, is controlled by the argument breaks = ...:\n\n# retrieve age\nage &lt;- famuss$age\n\n# effectively, a bar plot of ages\nhist(age, breaks = 25)\n\n\n\n\n\n\n\n# fewer bins\nhist(age, breaks = 10)\n\n\n\n\n\n\n\n\nFor continuous variables, binning is a necessity. The second plot is better, because it shows the shape more clearly without obscuring too much detail.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a histogram of percent change in dominant arm strength. Experiment to see how the shape of the distribution appears at various binning resolutions; then pick a number of breaks that you feel reflects the data best.\n\n# retrieve dominant arm percent change\n\n# make a histogram; find a binning that captures the shape well\n\n\n\nTo store a graphic as a separate file for use in other documents, find the ‘export’ icon in the plot panel and select the ‘Save as image’ option; then follow prompts. Try this for the plot you just made.\n\n\n\nDescriptive statistics\nIn class we discussed several descriptive statistics for numeric variables. These statistics are so commonly used that they have their own functions in R.\n\nMeasures of location\nThe following functions return common measures of location for numeric variables:\n\nmean(...) returns an average\nmedian(...) returns a median\nquantile(...) returns a quantile\nmin(...) and max(...) return a minimum and a maximum, respectively\n\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age (middle value)\nmedian(age)\n\n[1] 22\n\n# 25th percentile of age (\"quantile\" is another term for percentile)\nquantile(age, probs = 0.25)\n\n25% \n 20 \n\n# 25th *and* 75th percentile of age\nquantile(age, probs = c(0.25, 0.75))\n\n25% 75% \n 20  27 \n\n# minimum age\nmin(age)\n\n[1] 17\n\n# maximum age\nmax(age)\n\n[1] 40\n\n\nNotice how the probs = ... argument to the quantile() function, which specifies which percentile R will calculate, can be used to calculate multiple percentiles at once.\nIf you want to inspect all of the location measures above (the five-number summary plus the mean) the summary(...) function will do just that.\n\n# all common location measures\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   17.0    20.0    22.0    24.4    27.0    40.0 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry computing the location measures above for percent change in dominant arm strength. Compare the mean and median. What does the comparison tell you about the skewness of this variable? Is this consistent with the histogram from the previous ‘your turn’?\n\n# compute the five-number summary for change in dominant arm strength\n\n\n\n\n\nMeasures of spread\nThe following functions return common measures of spread for numeric variables:\n\nrange(...) returns the range (min, max)\nIQR(...) returns the interquartile range (middle 50% of data)\nvar(...) returns the variance (average squared deviations from mean)\nsd(...) returns the standard deviation (variance, on original scale)\n\n\n# age range\nrange(age)\n\n[1] 17 40\n\n# interquartile range of ages (width of interval containing middle 50% of data)\nIQR(age)\n\n[1] 7\n\n# variance of age\nvar(age)\n\n[1] 33.79966\n\n# standard deviation of age\nsd(age)\n\n[1] 5.813748\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCompute the standard deviation of percent change in dominant arm strength.\n\n# standard deviation of percent change in dominant arm strength\n\nInterpret the value in context.\n\n\n\n\nRobustness\nWhen would you use median instead of mean? IQR instead of standard deviation? The answer has to do with robustness, which in statistics means sensitivity to outliers or extreme values.\nTo explore this idea, recall first the actual mean and median ages for the participants in the FAMuSS study as well as the age range:\n\n# average age\nmean(age)\n\n[1] 24.40168\n\n# median age\nmedian(age)\n\n[1] 22\n\n# range\nrange(age)\n\n[1] 17 40\n\n\nNow let’s add an artificial outlier – a few hypothetical participant who are in their 80’s and 90’s – and compute the measures of location again:\n\n# average age\nmean(c(age, 96, 92, 87, 91))\n\n[1] 24.84975\n\n# median age\nmedian(c(age, 96, 92, 87, 91))\n\n[1] 22\n\n\nThe mean increases while the median does not. More broadly, statistics based on percentiles are in general insensitive to outliers, unless there’s a large group of outlying observations. In this sense they are robust statistics.\nA similar difference can be observed between deviation-based measures and interquartile range. The original measures were:\n\n# variance of ages\nvar(age)\n\n[1] 33.79966\n\n# interquartile range of ages\nIQR(age)\n\n[1] 7\n\n\nNow adding in our artificial outliers:\n\n# age variance\nvar(c(age, 96, 92, 87, 91))\n\n[1] 63.55598\n\n# age iqr\nIQR(c(age, 96, 92, 87, 91))\n\n[1] 7\n\n\n\n\n\nGrouped summaries\nWhat if you wish to find the mean percent change in dominant arm strength separately for each genotype?\nThe tidyverse package loaded at the outset has a pair of functions, group_by and summarize, that allow you to do this efficiently. The steps are:\n\nStart with the data frame famuss\ngroup by the genotype variable\nsummarize\n\n\n# average dominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch))\n\n# A tibble: 3 × 2\n  genotype avg.drm.ch\n  &lt;fct&gt;         &lt;dbl&gt;\n1 CC            10.7 \n2 CT             8.49\n3 TT            13.0 \n\n\nThe summarize function can actually compute multiple summaries: each argument should specify a name for the summary and the calculation to perform in the format &lt;NAME&gt; = &lt;FUNCTION&gt;(&lt;COLUMN NAME&gt;):\n\n# average dominant and nondominant arm change by genotype\nfamuss |&gt;\n  group_by(genotype) |&gt;\n  summarize(avg.drm.ch = mean(drm.ch),\n            avg.ndrm.ch = mean(ndrm.ch))\n\n# A tibble: 3 × 3\n  genotype avg.drm.ch avg.ndrm.ch\n  &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 CC            10.7         48.9\n2 CT             8.49        53.2\n3 TT            13.0         58.1\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate the median percent change in dominant arm strength separately for each genotype by modifying the first example above.\n\n# median percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\nThese problems may look lengthy at face value, but the calculations are rather brief. A suggestion is: determine which calculations are required for each part and focus on doing those calculations first; then look back over your results to interpret them and answer the prompts.\n\n[L3] Use the census data again from the previous problem set and carry out the following descriptive summaries.\n\nMake a histogram of total personal income. Choose the binning so as to capture the shape well but not obscure too much detail. Are there outliers?\nCompute the mean and five-number summary of total personal income. Which measure of location is most appropriate and why?\nCompute the interquartile range and standard deviation of total personal income and interpret them in context. Which measure is more appropriate and why?\nCompute the median total personal income separately for men and women.\n\n\n\n[L3] Data from Chen, W., et al., Maternal investment increases with altitude in a frog on the Tibetan Plateau. Journal of Evolutionary Biology 26-12 (2013) includes measurements pertaining to egg clutches of several populations of frog at breeding ponds (sites) in the eastern Tibetan Plateau.\n\nHow many samples were collected at each site?\nCompute the frequency distribution of site altitudes among samples collected in the study.\nMake a barplot of the frequency distribution from (a). Are samples collected more or less uniformily across altitudes? If not, which altitudes are most represented in the sample?\nMake a histogram of clutch volumes. Describe the shape and number of modes.\nCalculate the mean and five-number summary of clutch volume.\nCalculate and interpret the standard deviation, variance, and interquartile range.\nCalculate the average clutch volume separately for each altitude. Does average clutch volume seem to differ by altitude?\n[optional] Devise a way to calculate the average absolute deviation."
  },
  {
    "objectID": "content/week2-descriptive.html",
    "href": "content/week2-descriptive.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "[lecture] frequency distributions; measures of spread and center\n[lab] descriptive statistics and simple graphics in R"
  },
  {
    "objectID": "content/week3-multivariate.html#todays-agenda",
    "href": "content/week3-multivariate.html#todays-agenda",
    "title": "Bivariate summaries",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\nLoose end: robustness\nBivariate numeric and graphical summaries\nLab: bivariate graphics in R"
  },
  {
    "objectID": "content/week3-multivariate.html#robustness",
    "href": "content/week3-multivariate.html#robustness",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\n\nPercentile-based measures of location and spread are less sensitive to outliers\n\nConsider adding an observation of 94 to our 12 ages from last time. This is an outlier.\n\n\n\n# append an outlier\nages_add &lt;- c(ages, 94)\n\n# means\nc(original = mean(ages), with.outlier = mean(ages_add))\n\n    original with.outlier \n    24.00000     29.38462 \n\n# medians\nc(original = median(ages), with.outlier = median(ages_add))\n\n    original with.outlier \n        23.5         25.0 \n\n# IQR\nc(original = IQR(ages), with.outlier = IQR(ages_add))\n\n    original with.outlier \n         8.5          9.0 \n\n# SD\nc(original = sd(ages), with.outlier = sd(ages_add))\n\n    original with.outlier \n    5.526794    20.122701 \n\n\n\nThe effect of this outlier on each statistic is:\n\nmean increases by 22.44%\nmedian increases by 6.38%\nIQR increases by 5.88%\nSD increases by 264.09%\n\nRobustness refers to sensitivity to outliers. Mean and SD are less robust than median and IQR."
  },
  {
    "objectID": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "href": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "title": "Bivariate summaries",
    "section": "Limitations of univariate summaries",
    "text": "Limitations of univariate summaries\n\nUnivariate summaries aim to capture the distribution of values of a single variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nboth unimodal, no obvious outliers\nheights symmetric\nweights right-skewed\nbut these observations actually come in pairs\n\n\n\nUnivariate summaries don’t reflect how the variables might be related."
  },
  {
    "objectID": "content/week3-multivariate.html#bivariate-summaries",
    "href": "content/week3-multivariate.html#bivariate-summaries",
    "title": "Bivariate summaries",
    "section": "Bivariate summaries",
    "text": "Bivariate summaries\n\nBivariate summaries aim to capture a relationship between two variables.\n\n\n\nA simple example is a scatterplot:\n\n\n\n\n\n\n\n\n\n\nEach point represents a pair of values \\((h, w)\\) for one study participant.\n\nReveals a relationship: taller participants tend to be heavier\nBut no longer shows individual distributions clearly\n\nNotice, though, that the marginal means (dashed red lines) still capture the center well."
  },
  {
    "objectID": "content/week3-multivariate.html#summary-types",
    "href": "content/week3-multivariate.html#summary-types",
    "title": "Bivariate summaries",
    "section": "Summary types",
    "text": "Summary types\n\nBivariate summary techniques differ depending on the data types of the variables.\n\n\n\n\n\n\n\n\nQuestion\nComparison type\n\n\n\n\nDid genotype frequencies differ by race or sex among study participants?\ncategorical/categorical\n\n\nWere differential changes in arm strength observed according to genotype?\nnumeric/categorical\n\n\nDid change in arm strength appear related in any way to body size among study participants?\nnumeric/numeric\n\n\nDid study participants experience similar or different changes in arm strength depending on arm dominance?\n??"
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical",
    "href": "content/week3-multivariate.html#categoricalcategorical",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\n\nA contingency table is a bivariate tabular summary of two categorical variables; it shows the frequency of each pair of values. Usually the marginal totals are also shown.\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n106\n149\n98\n353\n\n\nMale\n67\n112\n63\n242\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\nThere are multiple ways to convert to proportions by using different denominators, and these yield proportions with distinct interpretations:\n\ngrand total – frequency of genotype/sex combination\nrow total – genotype frequency by sex\ncolumn total – sex frequency by genotype"
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-1",
    "href": "content/week3-multivariate.html#categoricalcategorical-1",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid genotype frequencies differ by sex among study participants?\nFor this question, the row totals should be used to convert to proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n0.3003\n0.4221\n0.2776\n1\n\n\nMale\n0.2769\n0.4628\n0.2603\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are quite close, suggesting minimal sex differences."
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-2",
    "href": "content/week3-multivariate.html#categoricalcategorical-2",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid sex frequencies differ by genotype among study participants?\nFor this question, the column totals should be used to compute proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.6127\n0.5709\n0.6087\n\n\nMale\n0.3873\n0.4291\n0.3913\n\n\ntotal\n1\n1\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are close, suggesting minimal genotype differences."
  },
  {
    "objectID": "content/week3-multivariate.html#numericnumeric",
    "href": "content/week3-multivariate.html#numericnumeric",
    "title": "Bivariate summaries",
    "section": "Numeric/numeric",
    "text": "Numeric/numeric\nDid change in arm strength appear related in any way to body size among study participants?\n\nPairwise scatterplots indicate no apparent relationships."
  },
  {
    "objectID": "content/week3-multivariate.html#correlation",
    "href": "content/week3-multivariate.html#correlation",
    "title": "Bivariate summaries",
    "section": "Correlation",
    "text": "Correlation\nIn addition to graphical techniques, for numeric/numeric comparisons, there are also quantiative measures of relationship.\n\n\n\n\nCorrelation measures the strength of linear relationship, and is defined as: \\[r_{xy} = \\frac{1}{n - 1}\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\\]\n\n\\(r \\rightarrow 1\\): positive relationship\n\\(r \\rightarrow -1\\): negative relationship\n\\(r \\rightarrow 0\\): no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "href": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "title": "Bivariate summaries",
    "section": "Uncorrelated \\(\\neq\\) no relationship",
    "text": "Uncorrelated \\(\\neq\\) no relationship\nCorrelation only captures linear relationships. Always do a graphical check.\n\nCommon misconceptions:\n\nstronger correlation \\(\\longleftrightarrow\\) greater slope\nweaker correlation \\(\\longleftrightarrow\\) no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-correlations",
    "href": "content/week3-multivariate.html#interpreting-correlations",
    "title": "Bivariate summaries",
    "section": "Interpreting correlations",
    "text": "Interpreting correlations\nDid change in arm strength appear related in any way to body size among study participants?\nHere are the correlations corresponding to the plots we checked earlier.\n\n\n\n\n\n\n\n\n\n\n\n \nheight\nweight\nbmi\n\n\n\n\ndrm.ch\n-0.1104\n-0.1159\n-0.07267\n\n\nndrm.ch\n-0.265\n-0.2529\n-0.1436\n\n\n\n\n\nSo there aren’t any linear relationships here. A rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\\(|r| = 1\\): either a mistake or not real data"
  },
  {
    "objectID": "content/week3-multivariate.html#numericcategorical",
    "href": "content/week3-multivariate.html#numericcategorical",
    "title": "Bivariate summaries",
    "section": "Numeric/categorical",
    "text": "Numeric/categorical\nSide-by-side boxplots are usually a good option. Avoid stacked histograms.\nWere differential changes in arm strength observed according to genotype?\n\n\n\n\n\n\n\n\n\n\n\n\nLook for differences:\n\nlocation shift\nspread\ncenter\n\nWhat do you think? Any notable relationships?"
  },
  {
    "objectID": "content/week3-multivariate.html#robustness-1",
    "href": "content/week3-multivariate.html#robustness-1",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\nThe effect of the outlier on each measure is captured by the ratio \\(\\frac{\\text{measure with outlier}}{\\text{measure without outlier}}\\), which shows:\n\nthe IQR increases by 5.88%\nthe standard deviation increases by 264%"
  },
  {
    "objectID": "content/week3-multivariate.html#choosing-appropriate-measures-of-spread",
    "href": "content/week3-multivariate.html#choosing-appropriate-measures-of-spread",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures of spread",
    "text": "Choosing appropriate measures of spread"
  },
  {
    "objectID": "content/week3-multivariate.html#choosing-appropriate-measures",
    "href": "content/week3-multivariate.html#choosing-appropriate-measures",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures",
    "text": "Choosing appropriate measures\n\nWhen outliers are present, use percentile-based measures; otherwise, use mean and standard deviation or variance\n\n\nCheck your understanding: which measures are most appropriate for each variable above?"
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-scatterplots",
    "href": "content/week3-multivariate.html#interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots\n\nScatterplots show the presence or absence of an association.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf there is an association (i.e., discernible pattern), it can be:\n\nlinear or nonlinear\n\nlinear if scatter roughly follows a straight line, nonlinear otherwise\n\npositive or negative\n\npositive if scatter is increasing from left to right, negative otherwise\n\n\nThe plot at left is an example of a positive and (slightly) nonlinear relationship."
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-scatterplots-1",
    "href": "content/week3-multivariate.html#interpreting-scatterplots-1",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots"
  },
  {
    "objectID": "content/week3-multivariate.html#practice-interpreting-scatterplots",
    "href": "content/week3-multivariate.html#practice-interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Practice interpreting scatterplots",
    "text": "Practice interpreting scatterplots"
  },
  {
    "objectID": "content/week3-multivariate.html#data-transformations",
    "href": "content/week3-multivariate.html#data-transformations",
    "title": "Bivariate summaries",
    "section": "Data transformations",
    "text": "Data transformations\nSometimes a simple transformation can reveal a linear relationship on an alternate scale.\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, weight)\n\n[1] 0.5308787\n\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356"
  },
  {
    "objectID": "content/lab3-multivariate.html",
    "href": "content/lab3-multivariate.html",
    "title": "Lab 3: Bivariate summaries",
    "section": "",
    "text": "This lab covers descriptive summaries and graphics for two variables. The goal of the activity is to learn to produce joint summaries of two variables for identifying relationships, in particular:\n\ncontingency tables\nproportional stacked bar plots\nscatterplots\nside-by-side boxplots\n\nWe will use the FAMuSS dataset again.\n\nlibrary(tidyverse)\nload('data/famuss.RData')\n\n\nContingency tables\n\nCounts\nA contingency table is a summary of two categorical variables. Specifically, it is a two-way table in which:\n\nrows correspond to the values of one variable\ncolumns correspond to the values of the other variable\nentries are counts of the numbers of observations for each combination of values\n\nIn fact, a contingency table is a frequency distribution for two variables. It can be constructed in R by retrieving the two variables of interest and providing them as arguments to table().\n\n# retrieve variables of interest\ngenotype &lt;- famuss$genotype\nsex &lt;- famuss$sex\n\n# make a contingency table\ntable(sex, genotype)\n\n        genotype\nsex       CC  CT  TT\n  Female 106 149  98\n  Male    67 112  63\n\n\nThe order of the arguments determines the row/column orientation of the table.\nThe values show the observation counts for each combination, so, for example, the number 106 in the upper-leftmost cell indicates that 106 participants were women with genotype CC.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a contingency table of race and genotype, with race as the row variable and genotype as the column variable.\n\n# retrieve variables of interest\n\n# make a contingency table\n\n\n\n\n\nProportions\nTo convert a contingency table to proportions, one can use either the grand total, row totals, or column totals. The resulting proportions have different meanings:\n\ngrand total – combination frequencies\nrow totals – column variable frequencies by row variable value\ncolumn totals – row variable frequencies by column variable value\n\nIn the sex/genotype contingency table, these proportions would be computed and interpreted as follows:\n\n# combination frequencies (using grand total as denominator)\ntable(sex, genotype) |&gt; proportions(margin = NULL)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.1781513 0.2504202 0.1647059\n  Male   0.1126050 0.1882353 0.1058824\n\n# genotype frequencies by sex (using row margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 1)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.3002833 0.4220963 0.2776204\n  Male   0.2768595 0.4628099 0.2603306\n\n# sex frequencies by genotype (using column margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 2)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.6127168 0.5708812 0.6086957\n  Male   0.3872832 0.4291188 0.3913043\n\n\nNotice that the margin = ... argument controls which totals are used as denominators in computing proportions. The syntax is:\n\nmargin = NULL specifies grand total\nmargin = 1 uses row totals\nmargin = 2 uses column totals\n\nThe interpretation of entries depends on which type of proportion is comupted. For instance, looking at only the upper-leftmost entry in each of the three tables above:\n\n[first table] 17.82% of participants were women with genotype CC\n[second table] 30.03% of female participants had genotype CC\n[third table] 61.27% of participants with genotype CC were women\n\nIf you lose track of which margin was which, you can always check yourself by looking at which values sum to one: if all the values add up to one, grand totals were used; if rows sum to one, row totals were used; and if columns add up to one, column totals were used.\n\n\n\n\n\n\nYour turn\n\n\n\nConvert your contingency table of genotype and race from the previous ‘your turn’ to a table showing genotype frequencies by race.\n\n# genotype frequencies by race\n\nThen, interpret the entry for Hispanic/CT in context.\n\n\n\n\nGraphics\nThe latter two proportion tables – those constructed using row or column totals – can be visualized graphically using barplot(...):\n\n# stacked bar plot showing genotype frequencies by sex\ntable(genotype, sex) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nThis function is a little finnicky about the table orientation, so the most straightforward way to approach making the plot is to adhere to this rule of thumb:\n\nalways use margin = 2\nspecify the order of the variables as they are input to table(...) so that the grouping variable appears second\n\nSo, for instance, to visualize the sex frequencies by genotype, we would do the following:\n\n# stacked bar plot showing sex frequencies by genotype\ntable(sex, genotype) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nIf you see uneven bar heights, that means you have not configured the table correctly before passing it to barplot(...).\n\n\n\n\n\n\nYour turn\n\n\n\nMake a plot showing the genotype frequencies by race.\n\n# stacked bar plot showing genotype frequencies by race\n\nCheck your understanding:\n\nFor which group is the CC frequency highest?\nFor which group is the CC frequency lowest?\n\n\n\n\n\n\nScatterplots and correlation\nTaller people tend to be heavier. We should expect a relationship between weight and height. The example below shows a scatterplot of the two variables, and computes the correlation (measure of linear relationship).\n\n# retrieve height and weight columns\nheight &lt;- famuss$height\nweight &lt;- famuss$weight\n\n# basic scatterplot\nplot(height, weight)\n\n\n\n\n\n\n\n# correlation\ncor(weight, height)\n\n[1] 0.5308787\n\n\nA rough rule of thumb for interpreting correlations is as follows:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case, the correlation of 0.53 indicates a moderate positive relationship.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there a relationship between nondominant and dominant percent change in arm strength? Make a scatterplot and compute the correlation.\n\n# retrieve the percent change variables\n\n# construct a scatterplot\n\n# compute the correlation\n\n\n\n\n\nSide-by-side boxplots\nConsider one of the main questions for the study: were differences on the ACTN gene region associated with differential change in arm strength after resistance training?\nThis is a comparison between a categorical variable (genotype) and numeric variable (percent change in arm strength). For this type of comparison, a set of side-by-side boxplots is best:\n\n# side-by-side boxplots for non-dominant arm\nboxplot(ndrm.ch ~ genotype, data = famuss)\n\n\n\n\n\n\n\n# change the orientation \nboxplot(ndrm.ch ~ genotype, data = famuss, horizontal = T)\n\n\n\n\n\n\n\n\nThis graphics summarizes the frequency distribution of percent change in non-dominant arm strength separately for each genotype. It is essentially a grouped summary.\n\n\n\n\n\n\nYour turn\n\n\n\nConstruct side-by-side boxplots of percent change in dominant arm strength by genotype.\n\n# make side-by-side boxplots of percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\n\nTrait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease (CHD); 12,986 participants were recruited for a study examining this hypothesis and followed for five years. The anger dataset includes data for the 8557 participants identified as having normal blood pressure (normotensives) and records, for each participant, whether they are classified as having high, moderate, or low trait anger, and whether they experienced a CHD event during the study period.\n\nMake a contingency table showing trait anger and CHD event occurrence.\nFind the proportion of participants who experienced a CHD event in each trait anger group.\nConstruct a stacked bar plot to visualize the frequencies you found in (b).\nFind the ratio \\(\\frac{\\text{CHD frequency in the high anger group}}{\\text{CHD frequency in the low anger group}}\\). This is called the “relative risk” of a CHD event. (You can calculate this quantity by hand based on your result in (b).)\n\n\n\nUse the nhanes data from earlier to answer the following questions.\n\nMake a scatterplot of systolic blood pressure and diastolic blood pressure. Put systolic blood pressure on the y axis. Characterize the association, if any, as positive/negative/non-associated and linear/nonlinear.\nCompute and interpret the correlation between systolic blood pressure and diastolic blood pressure.\nDoes there appear to be a substantial difference in the distribution of total cholesterol by sex? Construct side-by-side barplots to answer this question."
  },
  {
    "objectID": "content/week3-multivariate.html#interpretation",
    "href": "content/week3-multivariate.html#interpretation",
    "title": "Bivariate summaries",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356\n\n\n\nApplying these rules of thumb:\n\n\\(|r| &lt; 0.3\\): minimal association\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong\n\nAnd:\n\n\\(r &gt; 0\\): positive association\n\\(r &lt; 0\\): negative association\n\nThe interpretation is:\nThere is a moderately strong positive linear relationship between height and log weight.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week3-bivariate.html#todays-agenda",
    "href": "content/week3-bivariate.html#todays-agenda",
    "title": "Bivariate summaries",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\nLoose end: robustness\nBivariate numeric and graphical summaries\nLab: bivariate graphics in R"
  },
  {
    "objectID": "content/week3-bivariate.html#robustness",
    "href": "content/week3-bivariate.html#robustness",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\n\nPercentile-based measures of location and spread are less sensitive to outliers\n\nConsider adding an observation of 94 to our 12 ages from last time. This is an outlier.\n\n\n\n# append an outlier\nages_add &lt;- c(ages, 94)\n\n# means\nc(original = mean(ages), with.outlier = mean(ages_add))\n\n    original with.outlier \n    24.00000     29.38462 \n\n# medians\nc(original = median(ages), with.outlier = median(ages_add))\n\n    original with.outlier \n        23.5         25.0 \n\n# IQR\nc(original = IQR(ages), with.outlier = IQR(ages_add))\n\n    original with.outlier \n         8.5          9.0 \n\n# SD\nc(original = sd(ages), with.outlier = sd(ages_add))\n\n    original with.outlier \n    5.526794    20.122701 \n\n\n\nThe effect of this outlier on each statistic is:\n\nmean increases by 22.44%\nmedian increases by 6.38%\nIQR increases by 5.88%\nSD increases by 264.09%\n\nRobustness refers to sensitivity to outliers. Mean and SD are less robust than median and IQR."
  },
  {
    "objectID": "content/week3-bivariate.html#choosing-appropriate-measures",
    "href": "content/week3-bivariate.html#choosing-appropriate-measures",
    "title": "Bivariate summaries",
    "section": "Choosing appropriate measures",
    "text": "Choosing appropriate measures\n\nWhen outliers are present, use percentile-based measures; otherwise, use mean and standard deviation or variance\n\n\nCheck your understanding: which measures are most appropriate for each variable above?"
  },
  {
    "objectID": "content/week3-bivariate.html#limitations-of-univariate-summaries",
    "href": "content/week3-bivariate.html#limitations-of-univariate-summaries",
    "title": "Bivariate summaries",
    "section": "Limitations of univariate summaries",
    "text": "Limitations of univariate summaries\n\nUnivariate summaries aim to capture the distribution of values of a single variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nboth unimodal, no obvious outliers\nheights symmetric\nweights right-skewed\nbut these observations actually come in pairs\n\n\n\nUnivariate summaries don’t reflect how the variables might be related."
  },
  {
    "objectID": "content/week3-bivariate.html#bivariate-summaries",
    "href": "content/week3-bivariate.html#bivariate-summaries",
    "title": "Bivariate summaries",
    "section": "Bivariate summaries",
    "text": "Bivariate summaries\n\nBivariate summaries aim to capture a relationship between two variables.\n\n\n\nA simple example is a scatterplot:\n\n\n\n\n\n\n\n\n\n\nEach point represents a pair of values \\((h, w)\\) for one study participant.\n\nReveals a relationship: taller participants tend to be heavier\nBut no longer shows individual distributions clearly\n\nNotice, though, that the marginal means (dashed red lines) still capture the center well."
  },
  {
    "objectID": "content/week3-bivariate.html#summary-types",
    "href": "content/week3-bivariate.html#summary-types",
    "title": "Bivariate summaries",
    "section": "Summary types",
    "text": "Summary types\n\nBivariate summary techniques differ depending on the data types of the variables.\n\n\n\n\n\n\n\n\nQuestion\nComparison type\n\n\n\n\nDid genotype frequencies differ by race or sex among study participants?\ncategorical/categorical\n\n\nWere differential changes in arm strength observed according to genotype?\nnumeric/categorical\n\n\nDid change in arm strength appear related in any way to body size among study participants?\nnumeric/numeric\n\n\nDid study participants experience similar or different changes in arm strength depending on arm dominance?\n??"
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical",
    "href": "content/week3-bivariate.html#categoricalcategorical",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\n\nA contingency table is a bivariate tabular summary of two categorical variables; it shows the frequency of each pair of values. Usually the marginal totals are also shown.\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n106\n149\n98\n353\n\n\nMale\n67\n112\n63\n242\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\nThere are multiple ways to convert to proportions by using different denominators, and these yield proportions with distinct interpretations:\n\ngrand total – frequency of genotype/sex combination\nrow total – genotype frequency by sex\ncolumn total – sex frequency by genotype"
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical-1",
    "href": "content/week3-bivariate.html#categoricalcategorical-1",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid genotype frequencies differ by sex among study participants?\nFor this question, the row totals should be used to convert to proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n0.3003\n0.4221\n0.2776\n1\n\n\nMale\n0.2769\n0.4628\n0.2603\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are quite close, suggesting minimal sex differences."
  },
  {
    "objectID": "content/week3-bivariate.html#categoricalcategorical-2",
    "href": "content/week3-bivariate.html#categoricalcategorical-2",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid sex frequencies differ by genotype among study participants?\nFor this question, the column totals should be used to compute proportions.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.6127\n0.5709\n0.6087\n\n\nMale\n0.3873\n0.4291\n0.3913\n\n\ntotal\n1\n1\n1\n\n\n\n\n\n\nAs a stacked bar plot:\n\n\n\n\n\n\n\n\n\n\n\nThe proportions are close, suggesting minimal genotype differences."
  },
  {
    "objectID": "content/week3-bivariate.html#numericcategorical",
    "href": "content/week3-bivariate.html#numericcategorical",
    "title": "Bivariate summaries",
    "section": "Numeric/categorical",
    "text": "Numeric/categorical\nSide-by-side boxplots are usually a good option. Avoid stacked histograms.\nWere differential changes in arm strength observed according to genotype?\n\n\n\n\n\n\n\n\n\n\n\n\nLook for differences:\n\nlocation shift\nspread\ncenter\n\nWhat do you think? Any notable relationships?"
  },
  {
    "objectID": "content/week3-bivariate.html#numericnumeric",
    "href": "content/week3-bivariate.html#numericnumeric",
    "title": "Bivariate summaries",
    "section": "Numeric/numeric",
    "text": "Numeric/numeric\nDid change in arm strength appear related in any way to body size among study participants?\n\nPairwise scatterplots indicate no apparent relationships."
  },
  {
    "objectID": "content/week3-bivariate.html#interpreting-scatterplots",
    "href": "content/week3-bivariate.html#interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Interpreting scatterplots",
    "text": "Interpreting scatterplots\n\nScatterplots show the presence or absence of an association.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf there is an association (i.e., discernible pattern), it can be:\n\nlinear or nonlinear\n\nlinear if scatter roughly follows a straight line, nonlinear otherwise\n\npositive or negative\n\npositive if scatter is increasing from left to right, negative otherwise\n\n\nThe plot at left is an example of a positive and (slightly) nonlinear relationship."
  },
  {
    "objectID": "content/week3-bivariate.html#practice-interpreting-scatterplots",
    "href": "content/week3-bivariate.html#practice-interpreting-scatterplots",
    "title": "Bivariate summaries",
    "section": "Practice interpreting scatterplots",
    "text": "Practice interpreting scatterplots"
  },
  {
    "objectID": "content/week3-bivariate.html#correlation",
    "href": "content/week3-bivariate.html#correlation",
    "title": "Bivariate summaries",
    "section": "Correlation",
    "text": "Correlation\nIn addition to graphical techniques, for numeric/numeric comparisons, there are also quantiative measures of relationship.\n\n\n\n\nCorrelation measures the strength of linear relationship, and is defined as: \\[r_{xy} = \\frac{1}{n - 1}\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\\]\n\n\\(r \\rightarrow 1\\): positive relationship\n\\(r \\rightarrow -1\\): negative relationship\n\\(r \\rightarrow 0\\): no relationship"
  },
  {
    "objectID": "content/week3-bivariate.html#interpreting-correlations",
    "href": "content/week3-bivariate.html#interpreting-correlations",
    "title": "Bivariate summaries",
    "section": "Interpreting correlations",
    "text": "Interpreting correlations\nDid change in arm strength appear related in any way to body size among study participants?\nHere are the correlations corresponding to the plots we checked earlier.\n\n\n\n\n\n\n\n\n\n\n\n \nheight\nweight\nbmi\n\n\n\n\ndrm.ch\n-0.1104\n-0.1159\n-0.07267\n\n\nndrm.ch\n-0.265\n-0.2529\n-0.1436\n\n\n\n\n\nSo there aren’t any linear relationships here. A rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\\(|r| = 1\\): either a mistake or not real data"
  },
  {
    "objectID": "content/week3-bivariate.html#data-transformations",
    "href": "content/week3-bivariate.html#data-transformations",
    "title": "Bivariate summaries",
    "section": "Data transformations",
    "text": "Data transformations\nSometimes a simple transformation can reveal a linear relationship on an alternate scale.\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, weight)\n\n[1] 0.5308787\n\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356"
  },
  {
    "objectID": "content/week3-bivariate.html#interpretation",
    "href": "content/week3-bivariate.html#interpretation",
    "title": "Bivariate summaries",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n# correlation coefficient\ncor(height, log(weight))\n\n[1] 0.5609356\n\n\n\nApplying these rules of thumb:\n\n\\(|r| &lt; 0.3\\): minimal association\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong\n\nAnd:\n\n\\(r &gt; 0\\): positive association\n\\(r &lt; 0\\): negative association\n\nThe interpretation is:\nThere is a moderately strong positive linear relationship between height and log weight.\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/lab3-bivariate.html",
    "href": "content/lab3-bivariate.html",
    "title": "Lab 3: Bivariate summaries",
    "section": "",
    "text": "This lab covers descriptive summaries and graphics for two variables. The goal of the activity is to learn to produce joint summaries of two variables for identifying relationships, in particular:\n\ncontingency tables\nproportional stacked bar plots\nscatterplots\nside-by-side boxplots\n\nWe will use the FAMuSS dataset again.\n\nlibrary(tidyverse)\nload('data/famuss.RData')\n\n\nContingency tables\n\nCounts\nA contingency table is a summary of two categorical variables. Specifically, it is a two-way table in which:\n\nrows correspond to the values of one variable\ncolumns correspond to the values of the other variable\nentries are counts of the numbers of observations for each combination of values\n\nIn fact, a contingency table is a frequency distribution for two variables. It can be constructed in R by retrieving the two variables of interest and providing them as arguments to table().\n\n# retrieve variables of interest\ngenotype &lt;- famuss$genotype\nsex &lt;- famuss$sex\n\n# make a contingency table\ntable(sex, genotype)\n\n        genotype\nsex       CC  CT  TT\n  Female 106 149  98\n  Male    67 112  63\n\n\nThe order of the arguments determines the row/column orientation of the table.\nThe values show the observation counts for each combination, so, for example, the number 106 in the upper-leftmost cell indicates that 106 participants were women with genotype CC.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a contingency table of race and genotype, with race as the row variable and genotype as the column variable.\n\n# retrieve variables of interest\n\n# make a contingency table\n\n\n\n\n\nProportions\nTo convert a contingency table to proportions, one can use either the grand total, row totals, or column totals. The resulting proportions have different meanings:\n\ngrand total – combination frequencies\nrow totals – column variable frequencies by row variable value\ncolumn totals – row variable frequencies by column variable value\n\nIn the sex/genotype contingency table, these proportions would be computed and interpreted as follows:\n\n# combination frequencies (using grand total as denominator)\ntable(sex, genotype) |&gt; proportions(margin = NULL)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.1781513 0.2504202 0.1647059\n  Male   0.1126050 0.1882353 0.1058824\n\n# genotype frequencies by sex (using row margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 1)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.3002833 0.4220963 0.2776204\n  Male   0.2768595 0.4628099 0.2603306\n\n# sex frequencies by genotype (using column margins as denominators)\ntable(sex, genotype) |&gt; proportions(margin = 2)\n\n        genotype\nsex             CC        CT        TT\n  Female 0.6127168 0.5708812 0.6086957\n  Male   0.3872832 0.4291188 0.3913043\n\n\nNotice that the margin = ... argument controls which totals are used as denominators in computing proportions. The syntax is:\n\nmargin = NULL specifies grand total\nmargin = 1 uses row totals\nmargin = 2 uses column totals\n\nThe interpretation of entries depends on which type of proportion is comupted. For instance, looking at only the upper-leftmost entry in each of the three tables above:\n\n[first table] 17.82% of participants were women with genotype CC\n[second table] 30.03% of female participants had genotype CC\n[third table] 61.27% of participants with genotype CC were women\n\nIf you lose track of which margin was which, you can always check yourself by looking at which values sum to one: if all the values add up to one, grand totals were used; if rows sum to one, row totals were used; and if columns add up to one, column totals were used.\n\n\n\n\n\n\nYour turn\n\n\n\nConvert your contingency table of genotype and race from the previous ‘your turn’ to a table showing genotype frequencies by race.\n\n# genotype frequencies by race\n\nThen, interpret the entry for Hispanic/CT in context.\n\n\n\n\nGraphics\nThe latter two proportion tables – those constructed using row or column totals – can be visualized graphically using barplot(...):\n\n# stacked bar plot showing genotype frequencies by sex\ntable(genotype, sex) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nThis function is a little finnicky about the table orientation, so the most straightforward way to approach making the plot is to adhere to this rule of thumb:\n\nalways use margin = 2\nspecify the order of the variables as they are input to table(...) so that the grouping variable appears second\n\nSo, for instance, to visualize the sex frequencies by genotype, we would do the following:\n\n# stacked bar plot showing sex frequencies by genotype\ntable(sex, genotype) |&gt; \n  proportions(margin = 2) |&gt; \n  barplot(legend = T)\n\n\n\n\n\n\n\n\nIf you see uneven bar heights, that means you have not configured the table correctly before passing it to barplot(...).\n\n\n\n\n\n\nYour turn\n\n\n\nMake a plot showing the genotype frequencies by race.\n\n# stacked bar plot showing genotype frequencies by race\n\nCheck your understanding:\n\nFor which group is the CC frequency highest?\nFor which group is the CC frequency lowest?\n\n\n\n\n\n\nScatterplots and correlation\nTaller people tend to be heavier. We should expect a relationship between weight and height. The example below shows a scatterplot of the two variables, and computes the correlation (measure of linear relationship).\n\n# retrieve height and weight columns\nheight &lt;- famuss$height\nweight &lt;- famuss$weight\n\n# basic scatterplot\nplot(height, weight)\n\n\n\n\n\n\n\n# correlation\ncor(weight, height)\n\n[1] 0.5308787\n\n\nA rough rule of thumb for interpreting correlations is as follows:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case, the correlation of 0.53 indicates a moderate positive relationship.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there a relationship between nondominant and dominant percent change in arm strength? Make a scatterplot and compute the correlation.\n\n# retrieve the percent change variables\n\n# construct a scatterplot\n\n# compute the correlation\n\n\n\n\n\nSide-by-side boxplots\nConsider one of the main questions for the study: were differences on the ACTN gene region associated with differential change in arm strength after resistance training?\nThis is a comparison between a categorical variable (genotype) and numeric variable (percent change in arm strength). For this type of comparison, a set of side-by-side boxplots is best:\n\n# side-by-side boxplots for non-dominant arm\nboxplot(ndrm.ch ~ genotype, data = famuss)\n\n\n\n\n\n\n\n# change the orientation \nboxplot(ndrm.ch ~ genotype, data = famuss, horizontal = T)\n\n\n\n\n\n\n\n\nThis graphics summarizes the frequency distribution of percent change in non-dominant arm strength separately for each genotype. It is essentially a grouped summary.\n\n\n\n\n\n\nYour turn\n\n\n\nConstruct side-by-side boxplots of percent change in dominant arm strength by genotype.\n\n# make side-by-side boxplots of percent change in dominant arm strength by genotype\n\n\n\n\n\nPractice problems\n\nTrait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease (CHD); 12,986 participants were recruited for a study examining this hypothesis and followed for five years. The anger dataset includes data for the 8557 participants identified as having normal blood pressure (normotensives) and records, for each participant, whether they are classified as having high, moderate, or low trait anger, and whether they experienced a CHD event during the study period.\n\nMake a contingency table showing trait anger and CHD event occurrence.\nFind the proportion of participants who experienced a CHD event in each trait anger group.\nConstruct a stacked bar plot to visualize the frequencies you found in (b).\nFind the ratio \\(\\frac{\\text{CHD frequency in the high anger group}}{\\text{CHD frequency in the low anger group}}\\). This is called the “relative risk” of a CHD event. (You can calculate this quantity by hand based on your result in (b).)\n\n\n\nUse the nhanes data from earlier to answer the following questions.\n\nMake a scatterplot of systolic blood pressure and diastolic blood pressure. Put systolic blood pressure on the y axis. Characterize the association, if any, as positive/negative/non-associated and linear/nonlinear.\nCompute and interpret the correlation between systolic blood pressure and diastolic blood pressure.\nDoes there appear to be a substantial difference in the distribution of total cholesterol by sex? Construct side-by-side boxplots to answer this question."
  },
  {
    "objectID": "content/r-cheatsheet.html",
    "href": "content/r-cheatsheet.html",
    "title": "R Cheatsheet",
    "section": "",
    "text": "Note\n\n\n\nThis document is a work in progress and will be updated prior to each test.\n\n\n\nBasics\nLoading data:\n\n[.RData file] load('&lt;FILEPATH&gt;')\n[from package] data(&lt;DATASET NAME&gt;, package = '&lt;PACKAGE NAME&gt;')\n[reading a CSV file] read.csv('&lt;FILEPATH&gt;')\n\nViewing dataframes:\n\n[preview] head(&lt;NAME&gt;)\n[data viewer] view(&lt;NAME&gt;)\n[check structure] str(&lt;NAME&gt;)\n\nExtracting a variable from a dataframe:\n\nDATAFRAME$VARIABLE\n\n\n\nSummary statistics\nIf x is a vector of values of a numeric variable…\n\nmean(x) computes the average\nmedian(x) computes the median\nmin(x) and max(x) compute the minimum and maximum\nquantile(x, probs = &lt;PERCENTILE&gt;) computes the percentile\nsummary(x) computes the five-number summary, plus the mean\nrange(x) computes the range (min, max)\nIQR(x) computes the interquartile range\nvar(x) computes the variance\nsd(x) computes the standard deviation\n\nIf df is a dataframe with a numeric variable y and a categorical variable x…\n\ndf |&gt; group_by(x) |&gt; summarize(&lt;OUTPUT.NAME&gt; = &lt;FUNCTION&gt;(y)) computes the statistic specified by &lt;FUNCTION&gt; separately for each category of the variable x (requires tidyverse package)\n\nSee especially Lab 2: Descriptive statistics.\n\n\nTables\nIf x and y are a vectors of values of two categorical variables…\n\ntable(x) computes the frequency distribution (counts)\ntable(x) |&gt; proportions() computes the frequency distribution (proportions)\ntable(x, y) computes a contingency table\ntable(x, y) |&gt; proportions(margin = NULL) computes proportions using grand total\ntable(x, y) |&gt; proportions(margin = 1) computes proportions using row total (group by x)\ntable(x, y) |&gt; proportions(margin = 2) computes proportions using column total (group by y)\n\nSee especially Lab 1: R basics and Lab 3: Bivariate summaries.\n\n\nGraphics\nIf x and y are vectors of values of two numeric variables…\n\nhist(x, breaks = &lt;NUMBER OF BINS&gt;) generates a histogram\nboxplot(x) generates a boxplot\nplot(x, y) generates a scatterplot\n\nIf x and y are vectors of values of two categorical variables…\n\ntable(x) |&gt; barplot() generates a bar plot (counts)\ntable(x) |&gt; proportions() |&gt; barplot() generates a bar plot (proportions)\ntable(x, y) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by y\ntable(y, x) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by x\n\nIf x is a vector of values of a categorical variable and y is a vector of values of a numeric variable…\n\nboxplot(y ~ x) generates a boxplot with x on the x axis (vertical orientation)\nboxplot(y ~ x, horizontal = T) generates a boxplot with y on the x axis (horizontal orientation)\n\nSee especially Lab 2: Descriptive statistics and Lab 3: Bivariate summaries."
  },
  {
    "objectID": "content/week3-review.html#basics",
    "href": "content/week3-review.html#basics",
    "title": "Review session 1",
    "section": "Basics",
    "text": "Basics\nLoading data:\n\n[.RData file] load('&lt;FILEPATH&gt;')\n[from package] data(&lt;DATASET NAME&gt;, package = '&lt;PACKAGE NAME&gt;')\n[reading a CSV file] read.csv('&lt;FILEPATH&gt;')\n\nViewing dataframes:\n\n[preview] head(&lt;NAME&gt;)\n[data viewer] view(&lt;NAME&gt;)\n[check structure] str(&lt;NAME&gt;)\n\nExtracting a variable from a dataframe:\n\nDATAFRAME$VARIABLE"
  },
  {
    "objectID": "content/week3-review.html#summary-statistics",
    "href": "content/week3-review.html#summary-statistics",
    "title": "Review session 1",
    "section": "Summary statistics",
    "text": "Summary statistics\nIf x is a vector of values of a numeric variable…\n\n\n\nmean(x) computes the average\nmedian(x) computes the median\nmin(x) and max(x) compute the minimum and maximum\nquantile(x, probs = &lt;PERCENTILE&gt;) computes the percentile\nsummary(x) computes the five-number summary, plus the mean\n\n\n\nrange(x) computes the range (min, max)\nIQR(x) computes the interquartile range\nvar(x) computes the variance\nsd(x) computes the standard deviation\n\n\n\nSee especially Lab 2: Descriptive statistics."
  },
  {
    "objectID": "content/week3-review.html#tables",
    "href": "content/week3-review.html#tables",
    "title": "Review session 1",
    "section": "Tables",
    "text": "Tables\nIf x and y are a vectors of values of two categorical variables…\n\ntable(x) computes the frequency distribution (counts)\ntable(x) |&gt; proportions() computes the frequency distribution (proportions)\ntable(x, y) computes a contingency table\ntable(x, y) |&gt; proportions(margin = NULL) computes proportions using grand total\ntable(x, y) |&gt; proportions(margin = 1) computes proportions using row total (group by x)\ntable(x, y) |&gt; proportions(margin = 2) computes proportions using column total (group by y)\n\nSee especially Lab 3: Bivariate summaries."
  },
  {
    "objectID": "content/week3-review.html#graphics",
    "href": "content/week3-review.html#graphics",
    "title": "Review session 1",
    "section": "Graphics",
    "text": "Graphics\n\n\nIf x and y are vectors of values of two numeric variables…\n\nhist(x, breaks = &lt;NUMBER OF BINS&gt;) generates a histogram\nboxplot(x) generates a boxplot\nplot(x, y) generates a scatterplot\n\n\nIf x and y are vectors of values of two categorical variables…\n\ntable(x) |&gt; barplot() generates a bar plot (counts)\ntable(x) |&gt; proportions() |&gt; barplot() generates a bar plot (proportions)\ntable(x, y) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by y\ntable(y, x) |&gt; proportions(margin = 2) |&gt; barplot(legend = T) generates a stacked bar plot grouped by x"
  },
  {
    "objectID": "content/test1.html",
    "href": "content/test1.html",
    "title": "Test 1",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 1 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 4/19. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/week3-review.html#study-design",
    "href": "content/week3-review.html#study-design",
    "title": "Review session 1",
    "section": "Study design",
    "text": "Study design"
  },
  {
    "objectID": "content/week3-review.html#data-types",
    "href": "content/week3-review.html#data-types",
    "title": "Review session 1",
    "section": "Data types",
    "text": "Data types"
  },
  {
    "objectID": "content/week3-review.html#descriptive-statistics",
    "href": "content/week3-review.html#descriptive-statistics",
    "title": "Review session 1",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics"
  },
  {
    "objectID": "content/test1.html#instructions",
    "href": "content/test1.html#instructions",
    "title": "Test 1",
    "section": "",
    "text": "You have 48 hours from the release of this assignment to complete and submit your work. You may refer to all class materials, notes, and textbooks, but must complete this assignment on your own. By submitting your work, you are affirming that your work is your own and you have not consulted with anyone else in preparing your answers or generated your answers or analyses using AI. Failure to adhere to this expectation will be considered an act of academic dishonesty and result in loss of credit.\nYou will find a project with a mostly empty script in the class Posit cloud workspace; use this to complete your analyses where required. Note that not all parts require you to perform any calculations; some questions are purely qualitative. Use the prompts as your guide, not the script.\nOnce you have completed your analyses for the portions requiring use of statistical software, submit your work by filling out the [test 1 form] (also posted on the course website). The form will automatically save your work, so you can return to it over the course of the 48-hour test window.\nThe form will stop accepting responses at the deadline, so make sure you submit by 5pm on Friday 4/19. Lastly, keep in mind that you will be given the opportunity to revise problems that you miss the first time around to earn back credit."
  },
  {
    "objectID": "content/test1.html#question-prompts",
    "href": "content/test1.html#question-prompts",
    "title": "Test 1",
    "section": "Question prompts",
    "text": "Question prompts\n\n[L2, L3] The yrbss dataset contains measurements on a small collection of variables from 10,587 survey responses collected as part of the CDC’s Youth Risk Behavior Surveillance System (YRBSS) from 1991-2013. The objective of the survey program is to track behaviors with potential negative physical and mental health impacts among adolescents. In this problem you’ll explore the amount of sleep that respondents get on school nights and the number of days per week respondents are physically active.\n\n[L2] Read briefly about the YRBSS on the CDC website: https://www.cdc.gov/healthyyouth/data/yrbs/overview.htm. Based on the overview and the description above, are the data observational or experimental?\n[L1] Based on the overview (link in part (a)), identify the study population.\n[LX] Load the dataset and identify the type of each variable. What kind of variable is sleep.hours?\n[LX] Examine the frequency distributions of age and grade level. Do any grades or ages seem over-represented in the sample?\n[L3] Make a bar plot showing the frequency distribution of hours of sleep on school nights. Based on the summary, what is the typical amount of sleep respondents get on school nights?\n[L3] Make a stacked bar plot showing levels of sleep by grade. Do older students sleep more on school nights than younger students?\n[L3] Visualize the frequency distribution of the number of days per week that survey participants are physically active. Describe the distribution and interpret any patterns.\n\n\n\n[L2, L3] Diet restriction and longevity. The longevity dataset contains data from a study in which 237 mice were randomly allocated to one of four diets at different levels of restriction: no restriction (NP), normal 85kCal diet before and after weaning (N/N85), normal diet before weaning and restricted 50kCal diet after weaning (N/R50), and normal diet before weaning and restricted 40kCal diet after weaning (N/R40). Researchers recorded the lifetime in months of each mouse in the study.\n\n[L2] Was this an experiment or observational study and why?\n[L3] Find the average lifetime of mice in each diet group.\n[L3] Find the standard deviation of lifetimes in each diet group.\n[L3] Make a plot comparing lifetimes by diet group.\n[L2] Write a short summary of the study results based on your work in (b)-(d). Indicate specifically whether there appears to be a relationship between dietary caloric intake and lifetime.\n\n\n\n[L3] Brain and body size. The mammals dataset contains average body weights (kg) and average brain weights (g) for 62 common species of mammal, as well as log-transformed versions of those weights.\n\n[L3] Make a scatterplot of log brain weights against log body weights and describe the apparent relationship, if any.\n[L3] Compute and interpret the correlation between log brain weight and log body weight.\n[L3] Make a histogram of brain weights (not on the log scale) with an appropriate number of bins. Describe the distribution. Are there outliers?\n[L3] Based on (c), compute an appropriate measure of center and spread for brain weight.\n[L3] Which species of mammal has the largest average brain weight? Inspect the data directly using view(...) to answer this question.\n[L3] Which species of mammal has the smallest \\(\\frac{\\text{brain weight}}{\\text{body weight}}\\) ratio? The largest? Inspect the data directly using view(...) to answer this question.\n\n\n\n[L1, L2] The following is an excerpt from the abstract of the study that reported the results of the Moderna Covid vaccine phase three clinical trial1: “Vaccines are needed to prevent coronavirus disease 2019 (Covid-19) and to protect persons who are at high risk for complications. The mRNA-1273 vaccine is a lipid nanoparticle–encapsulated mRNA-based vaccine that encodes the prefusion stabilized full-length spike protein of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes Covid-19. This [study] was conducted at 99 centers across the United States … The trial enrolled 30,420 [adult volunteers with no known history of SARS-CoV-2 infection and no circumstances that put them at high risk of infection or severe Covid-19 or both,] who were randomly assigned in a 1:1 ratio to receive either vaccine or placebo (15,210 participants in each group) … Symptomatic Covid-19 illness was confirmed in 185 participants in the placebo group and in 11 participants in the mRNA-1273 group; vaccine efficacy was 94.1%.”\n\n[L2] Is this an experiment or observational study? Explain.\n[L1] Identify the study population.\n[L1] Describe the study sample.\n[L1] What outcome(s) were measured in the study?\n[L3] The moderna dataset contains simulated observations according to the study description. Make a contingency table and use it to construct a table showing the proportions of volunteers infected and not infected in each group.\n[L3] Optional. Find the relative risk of illness in the vaccine group compared with the placebo group. Can you determine how efficacy is defined?\n\n\n\n[L1, L2, L3] The temps dataset contains physical data collected on a number of individuals. Explore the dataset and write a brief summary of your descriptive analysis. While open-ended, your analysis should include descriptions of the variables and their statistical properties, and descriptions of relationships between the variables. Include at least one graphic related to your summary. Your summary need not be exhaustive – in fact, it is better to pick 1-2 interesting findings and report those, rather than describe everything you tried."
  },
  {
    "objectID": "content/test1-practice.html",
    "href": "content/test1-practice.html",
    "title": "Extra practice problems",
    "section": "",
    "text": "The test will consist of four questions with multiple parts, and one less structured question in which your task is to explore and summarize a dataset based on descriptive statistics. You’ll have 48 hours to work on the test, and will submit your answers via a fillable form exactly as in the homework assignment. You can use any class resources (notes, textbook, lecture slides, past homeworks) but are required to work alone. You’ll be provided with a Posit cloud project in which to carry out your analyses, and will be expected to upload your script as a supporting document when you fill out the submission form. The practice problems below provide a sample of questions that approximate the test problems (but are slightly shorter)."
  },
  {
    "objectID": "content/test1-practice.html#test-information",
    "href": "content/test1-practice.html#test-information",
    "title": "Extra practice problems",
    "section": "",
    "text": "The test will consist of four questions with multiple parts, and one less structured question in which your task is to explore and summarize a dataset based on descriptive statistics. You’ll have 48 hours to work on the test, and will submit your answers via a fillable form exactly as in the homework assignment. You can use any class resources (notes, textbook, lecture slides, past homeworks) but are required to work alone. You’ll be provided with a Posit cloud project in which to carry out your analyses, and will be expected to upload your script as a supporting document when you fill out the submission form. The practice problems below provide a sample of questions that approximate the test problems (but are slightly shorter)."
  },
  {
    "objectID": "content/test1-practice.html#practice-problems",
    "href": "content/test1-practice.html#practice-problems",
    "title": "Extra practice problems",
    "section": "Practice problems",
    "text": "Practice problems\n\nAnother version of the frog dataset from earlier also includes measurements of egg size and body size. Use this dataset to practice visualizing and describing distributions of numeric variables.\n\nMake histograms of each of the four numeric variables, with appropriate numbers of bins, and describe the shape and number of modes.\nFor each variable, suggest an appropriate measure of center and measure spread and identify any measures that would not be appropriate.\nMake pairwise scatterplots of each of the four numeric variables and describe the association, if any. (Hint: try pairs(frog) for a more efficient way to generate these plots.)\nFor linear associations in (c), compute and interpret the correlation.\n\n\n\n# load and inspect data\nload('data/frog.RData')\nhead(frog)\n\n  site altitude clutch.size clutch.volume egg.size body.size\n1  040    3,462    181.9701      177.8279 1.949845  3.630781\n2  040    3,462    269.1535      257.0396 1.949845  3.630781\n3  040    3,462    158.4893      151.3561 1.949845  3.715352\n4  040    3,462    234.4229      223.8721 1.949845  3.801894\n5  040    3,462    245.4709      234.4229 1.949845  3.890451\n6  040    3,462    301.9952      288.4032 1.949845  3.890451\n\n# part a: histograms of each numeric variable; describe shape and modes\npar(mfrow = c(2, 2), mar = c(4, 4, 4, 1))\nhist(frog$clutch.size)\nhist(frog$clutch.volume)\nhist(frog$egg.size)\nhist(frog$body.size)\n\n\n\n\n\n\n\n# part c: pairwise scatterplots of clutch volume, egg size, body size, clutch size\npairs(frog)\n\n\n\n\n\n\n\n# part d: correlations for linear associations\ncor(frog$clutch.size, frog$clutch.volume)\n\n[1] 0.8077344\n\ncor(frog$clutch.volume, frog$egg.size)\n\n[1] 0.6462605\n\ncor(frog$body.size, frog$clutch.volume, use = 'complete.obs')\n\n[1] 0.6755435\n\ncor(frog$body.size, frog$clutch.size, use = 'complete.obs')\n\n[1] 0.6147564\n\n\n\nThe chick data data come from a study investigating the early growth of chicks on different diets. In the study, 47 chicks were randomly assigned one of four diets at birth and researchers measured body weight in grams daily. The data below show body weights at 18 days since birth for each chick. The question of interest is: which diet is best?\n\nIs this observational or experimental data? Explain your reasoning.\nProduce a visualization that compares body weight distributions by diet. For which diet have chicks grown the most? The least? Explain the statistic(s) or features of the distribution you used to make this determination.\nBased on your plot in (b), suggest a measure of center and measure of spread that would be appropriate for summarizing the data.\nCalculate the measures you suggested in (c) separately for each diet group.\nAssume that in the previous question you found that chicks on diet 3 grew the most, regardless of your actual answer. Can you conclude that diet 3 caused the fastest growth? Explain why or why not.\n\n\n\n# load and inspect data\nload('data/chick.RData')\nhead(chick)\n\n# A tibble: 6 × 3\n  chick.id weight diet  \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n1        1    171 diet 1\n2        2    187 diet 1\n3        3    187 diet 1\n4        4    154 diet 1\n5        5    199 diet 1\n6        6    160 diet 1\n\n# part b: visualize body weights by diet\nboxplot(weight ~ diet, data = chick)\n\n\n\n\n\n\n\n# part c-d: determine and compute appropriate measures of spread and center\nchick |&gt;\n  group_by(diet) |&gt;\n  summarize(avg.weight = mean(weight),\n            sd.weight = sd(weight))\n\n# A tibble: 4 × 3\n  diet   avg.weight sd.weight\n  &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 diet 1       159.      49.2\n2 diet 2       188.      63.3\n3 diet 3       233.      57.6\n4 diet 4       203.      33.6\n\n\n\nThe gss dataset contains observations for 500 respondents in the General Social Survey on a small number of demographic categorical variables. Use this to practice tabular and graphical summaries for categorical variables.\n\nFor each variable, determine whether the variable is nominal or ordinal.\nMake a contingency table of age bracket and whether participants have obtained a college degree.\nVisualize the relationship between age and having obtained a college degree.\nDoes the proportion of respondents with a college degree differ by sex?\nBy political party?\nBy socioeconomic class?\nMake one additional comparison of your choice and interpret the result.\n\n\n\n# load and inspect data\nload('data/gss.RData')\nhead(gss)\n\n# A tibble: 6 × 5\n  age     sex    college.degree political.party class        \n  &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;          &lt;fct&gt;           &lt;fct&gt;        \n1 (29,38] male   degree         ind             middle class \n2 (29,38] female no degree      rep             working class\n3 [18,29] male   degree         ind             working class\n4 (38,50] male   no degree      ind             working class\n5 (29,38] male   degree         rep             middle class \n6 (29,38] female no degree      rep             middle class \n\n# part b: contingency table of age and college degree\ntable(gss$college.degree, gss$age)\n\n           \n            [18,29] (29,38] (38,50] (50,87]\n  degree         36      44      60      34\n  no degree      99      74      64      89\n\n# part c: visualize relationship between age and college degree\ntable(gss$college.degree, gss$age) |&gt; \n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part d: does the proportion of respondents with a degree differ by sex?\ntable(gss$college.degree, gss$sex) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part e: by political party?\ntable(gss$college.degree, gss$political.party) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part f: by class?\ntable(gss$college.degree, gss$class) |&gt;\n  proportions(margin = 2) |&gt;\n  barplot(legend = T)\n\n\n\n\n\n\n\n# part g: one additional comparison of your choosing\n\n\nLong COVID is a multi-systemic and often debilitating condition that develops in at least 10% of patients following a COVID infection. The following is an excerpt of the abstract from a recent study seeking to identify symptoms and risk factors associated with long COVID and published in Nature Medicine1: “We undertook a … study using a UK-based primary care database, Clinical Practice Research Datalink Aurum, to determine symptoms that are associated with confirmed SARS-CoV-2 infection beyond 12 weeks in non-hospitalized adults and the risk factors associated with developing persistent symptoms. We selected 486,149 adults with confirmed SARS-CoV-2 infection … Outcomes included 115 individual symptoms, as well as long COVID, defined as a composite outcome of 33 symptoms by the World Health Organization clinical case definition … Among the patients infected with SARS-CoV-2, risk factors for long COVID included female sex, belonging to an ethnic minority, socioeconomic deprivation, smoking, obesity and a wide range of comorbidities. The risk of developing long COVID was also found to be increased along a gradient of decreasing age.”\n\nIdentify the type of study.\nIdentify the study population.\nDescribe the sample.\nList the study outcomes of interest.\nIdentify any non-outcome variables."
  },
  {
    "objectID": "content/test1-practice.html#footnotes",
    "href": "content/test1-practice.html#footnotes",
    "title": "Extra practice problems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSubramanian et al. (2022). Symptoms and risk factors for long COVID in non-hospitalized adults. Nature medicine, 28(8), 1706-1714.↩︎"
  },
  {
    "objectID": "content/test1.html#footnotes",
    "href": "content/test1.html#footnotes",
    "title": "Test 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBaden, L. R., et al. (2021). Efficacy and safety of the mRNA-1273 SARS-CoV-2 vaccine. New England journal of medicine, 384(5), 403-416.↩︎"
  },
  {
    "objectID": "content/week3-review.html",
    "href": "content/week3-review.html",
    "title": "Review session 1",
    "section": "",
    "text": "[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques"
  },
  {
    "objectID": "content/week3-review.html#outcomes",
    "href": "content/week3-review.html#outcomes",
    "title": "Review session 1",
    "section": "Outcomes",
    "text": "Outcomes\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques"
  },
  {
    "objectID": "content/week3-review.html#lecturelab-recap",
    "href": "content/week3-review.html#lecturelab-recap",
    "title": "Review session 1",
    "section": "Lecture/lab recap",
    "text": "Lecture/lab recap\n\n\n\n\n\n\n\n\nWeek/day\nLecture\nLab\n\n\n\n\n1/W\nExperiments and observational studies\nReading abstracts\n\n\n2/M\nData vocabulary, proportions and means, common notation\nLoading data, extracting variables, computing means and proportions\n\n\n2/W\nMeasures of location/center and spread, simple graphics\nHistograms, barplots, summary statistics\n\n\n3/M\nGraphics and tables for two variables, interpreting relationships, correlation\nStacked bar plots, side-by-side boxplots, scatterplots"
  },
  {
    "objectID": "content/week3-review.html#assignment-recap",
    "href": "content/week3-review.html#assignment-recap",
    "title": "Review session 1",
    "section": "Assignment recap",
    "text": "Assignment recap\n\n\n\n\n\n\n\nProblem set\nTopics\n\n\n\n\nPS1\nInterpreting study descriptions\n\n\nPS2\nSummary statistics (mean, proportion) for one variable; identifying variable types\n\n\nPS3\nMeasures of location and spread for one variable; visualizing frequency distributions\n\n\nPS4\nVisualizing relationships between two variables (C/C, C/N, N/N)"
  },
  {
    "objectID": "content/week3-review.html#question-types-l2-study-design",
    "href": "content/week3-review.html#question-types-l2-study-design",
    "title": "Review session 1",
    "section": "Question types: [L2] study design",
    "text": "Question types: [L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#question-types-l3-descriptive-statistics",
    "href": "content/week3-review.html#question-types-l3-descriptive-statistics",
    "title": "Review session 1",
    "section": "Question types: [L3] descriptive statistics",
    "text": "Question types: [L3] descriptive statistics\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week3-review.html#questions-l2-study-design",
    "href": "content/week3-review.html#questions-l2-study-design",
    "title": "Review session 1",
    "section": "Questions: [L2] study design",
    "text": "Questions: [L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#questions-l3-descriptive-statistics",
    "href": "content/week3-review.html#questions-l3-descriptive-statistics",
    "title": "Review session 1",
    "section": "Questions: [L3] descriptive statistics",
    "text": "Questions: [L3] descriptive statistics\n\nCompute/interpret mean/median/percentiles/IQR/variance/SD\nCompute/interpret a grouped summary with one or more of the above measures\nDetermine appropriate measures of location/center/spread\nMake/interpret a table of proportions for values of a categorical variable\nMake/interpret a contingency table\nMake/interpret a histogram\nMake/interpret two-way tables of proportions\nMake/interpret stacked bar plots representing proportions\nMake/interpret side-by-side boxplots\nMake/interpret scatterplots\nCompute/interpret correlations"
  },
  {
    "objectID": "content/week3-review.html#questions-l1-l2-study-design",
    "href": "content/week3-review.html#questions-l1-l2-study-design",
    "title": "Review session 1",
    "section": "Questions: [L1, L2] study design",
    "text": "Questions: [L1, L2] study design\n\nExperiment or observational study?\nDescribe the population\nDescribe the sample\nIdentify the outcomes\nIdentify variable types"
  },
  {
    "objectID": "content/week3-review.html#commonly-missed-question-types",
    "href": "content/week3-review.html#commonly-missed-question-types",
    "title": "Review session 1",
    "section": "Commonly missed question types",
    "text": "Commonly missed question types\nIdentifying appropriate measures of center/spread (i.e., understanding robustness) Grouped summaries (getting syntax right) Determining"
  },
  {
    "objectID": "content/week3-review.html#key-concepts",
    "href": "content/week3-review.html#key-concepts",
    "title": "Review session 1",
    "section": "Key concepts",
    "text": "Key concepts\n\n[L1, L2] Experiments and observational studies\n[L1, L2] Samples and populations\n[L2] Variable types: categorical (nominal/ordinal) and numeric (discrete/continuous)\n[L3] Summary statistics: mean, median, percentile, IQR, SD, variance, correlation\n[L3] Distribution properties: skewness, outliers, modes\n[L3] Robustness of common summary statistics\n[L3] Relationships: positive/negative; linear/nonlinear"
  },
  {
    "objectID": "content/week3-review.html#about-the-test",
    "href": "content/week3-review.html#about-the-test",
    "title": "Review session 1",
    "section": "About the test",
    "text": "About the test\n\n4-5 questions\nExpect one challenge part, but otherwise very similar to homework problems and practice questions\n48 hours, open book, open note\nPosit cloud project + fillable form\nRevisions will be allowed to earn back credit\nNo collaboration\nDo your own analysis/writing (no AI plagiarism)\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#todays-agenda",
    "href": "content/week4-sampling.html#todays-agenda",
    "title": "Introduction to inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nTest 1 discussion\n[lecture] Inference vs. description, point estimation, interval estimation\n[lab] Exploring interval coverage"
  },
  {
    "objectID": "content/week4-sampling.html#inferential-statistics",
    "href": "content/week4-sampling.html#inferential-statistics",
    "title": "Introduction to inference",
    "section": "Inferential statistics",
    "text": "Inferential statistics\nDescriptive findings are statements about sample statistics. For instance:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nBy contrast, statistical inferences are statements about population statistics. For example:\n\nIndividuals with genotype TT exhibit the largest median percent change in strength."
  },
  {
    "objectID": "content/week4-sampling.html#sampling-variability",
    "href": "content/week4-sampling.html#sampling-variability",
    "title": "Introduction to inference",
    "section": "Sampling variability",
    "text": "Sampling variability\nA population model represents the distribution of values you’d see if you measured every individual in your population (a census).\nBut if you don’t measure every unit, results are subject to sampling variation: sample statistics change depending on which individuals you measure."
  },
  {
    "objectID": "content/week4-sampling.html#random-sampling",
    "href": "content/week4-sampling.html#random-sampling",
    "title": "Introduction to inference",
    "section": "Random sampling",
    "text": "Random sampling\n\nSampling establishes the link (or lack thereof) between a sample and a population.\n\n\n\n\n\nIn a simple random sample, units are chosen in such a way that every individual in the population has an equal probability of inclusion. For the SRS:\n\nsample statistics mirror population statistics (sample is representative)\nsampling variability depends only on population variability and sample size"
  },
  {
    "objectID": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "href": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "title": "Introduction to inference",
    "section": "A pretend population: NHANES data",
    "text": "A pretend population: NHANES data\nThe National Health and Nutrition Esamination Survey (NHANES) is an annual CDC program to collect health and nutrition data on the non-institutionalized civilian resident population of the United States. Here are a few variables:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubj.id\ngender\nage\npoverty\npulse\nbpsys1\nbpdia1\ntotchol\nsleephrsnight\n\n\n\n\n1\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n2\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n3\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n5\nfemale\n49\n1.91\n86\n118\n82\n6.7\n8\n\n\n\n\n\nI’ve selected 3,179 responses from the 2009-2010 survey; let’s pretend the corresponding individuals form a population of interest."
  },
  {
    "objectID": "content/week4-sampling.html#population-distribution-of-a-variable",
    "href": "content/week4-sampling.html#population-distribution-of-a-variable",
    "title": "Introduction to inference",
    "section": "Population distribution of a variable",
    "text": "Population distribution of a variable\nConsider the totchol variable: total HDL cholesterol in mmol/L. It has a certain frequency distribution among the population that we’ll call its population distribution.\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we draw a random sample of 50 individuals…\n\nhow closely will the sample align with the population distribution?\nhow much will alignment change if we select a new sample?"
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability",
    "href": "content/week4-sampling.html#simulating-sampling-variability",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\n\n\n\n\n\n\n\n\n\n\n\n\nThese are 20 random samples with the sample mean indicated by the dashed line and the population distribution and mean overlaid in red.\n\nsample size \\(n = 20\\)\nfrequency distributions differ a lot\nsample means differ some\n\nWe can actually measure this variability!"
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability-1",
    "href": "content/week4-sampling.html#simulating-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\nIf we had means calculated from a much larger number of samples, we could make a frequency distribution for the values of the sample mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample\n1\n2\n\\(\\cdots\\)\n10,000\n\n\nmean\n4.957\n5.039\n\\(\\cdots\\)\n5.24\n\n\n\n\nWe could then use the usual measures of center and spread to characterize the distribution of sample means.\n\nmean of \\(\\bar{x}\\): 5.0373228\nstandard deviation of \\(\\bar{x}\\): 0.2387869\n\n\nAcross 10,000 random samples of size 20, the typical sample mean was 5.04 and the root average squared distance of the sample mean from its typical value was 0.239."
  },
  {
    "objectID": "content/week4-sampling.html#point-estimation",
    "href": "content/week4-sampling.html#point-estimation",
    "title": "Introduction to inference",
    "section": "Point estimation",
    "text": "Point estimation\nSample statistics, viewed as guesses of population statistics, are called ‘point estimates’.\n\n\n\nParameter name\nParameter notation\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)\n\n\n\nNow we can more formally describe statistical inference:\n\na population parameter is any numeric characteristic of a population distribution\nan inference is a conclusion about the value of a population parameter based on point estimates and their sampling variability\n\nWe will focus initially on inferences about the mean \\(\\mu\\) based on the point estimate \\(\\bar{x}\\)."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability",
    "href": "content/week4-sampling.html#measuring-sampling-variability",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nIn practice \\(\\sigma\\) is not known so we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared error of the sample mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability-1",
    "href": "content/week4-sampling.html#measuring-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nTheory indicates the standard deviation of the sample mean under random sampling is: \\[\nSD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{population SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor totchol, the theoretical standard deviation is \\(SD(\\bar{x}) = \\frac{1.0747}{\\sqrt{50}} =\\) 0.1519822.\nWe can estimate this quantity by replacing \\(\\sigma\\) with the point estimate \\(s_x\\), resulting in a standard error (estimated standard deviation): \\[SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\\]"
  },
  {
    "objectID": "content/week4-sampling.html#example-with-one-sample",
    "href": "content/week4-sampling.html#example-with-one-sample",
    "title": "Introduction to inference",
    "section": "Example with one sample",
    "text": "Example with one sample\nThe simulations we’ve done so far have been a means of understanding just what a standard error is meant to capture; these are not a practicable method for measuring sampling variation.\nIn practice we’d simply compute a point estimate and standard error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.031\n0.1396\n\n\n\n\n\n\nThe estimated mean total HDL cholesterol among the population is 5.031 mmol/L.\nThe point estimate is expected to deviate by 0.1396 mmol/L on average from the population mean."
  },
  {
    "objectID": "content/week4-sampling.html#effect-of-sample-size",
    "href": "content/week4-sampling.html#effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\) is inversely proportional to sample size.\n\n\n\n\n\n\n\n\n\n\n\n\nAs sample size increases…\n\naccuracy remains the same\nestimates get more precise\nskewness vanishes"
  },
  {
    "objectID": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "href": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Visualizing effect of sample size",
    "text": "Visualizing effect of sample size\nGenerating a large number (10K) of samples allows us to approximate the sampling distribution for a variety of sample sizes.\n\n\nspread diminishes with sample size\nless variability among estimates from larger samples\n\nSo estimates are more accurate with more data, assuming data are from a random sample."
  },
  {
    "objectID": "content/week4-sampling.html#normal-model",
    "href": "content/week4-sampling.html#normal-model",
    "title": "Introduction to inference",
    "section": "Normal model",
    "text": "Normal model\nNotice that each simulated sampling distribution has produced a unimodal, symmetric, bell-shaped histogram.\n\n\nThe normal model is a theoretical frequency distribution characterized by two parameters:\n\na mean (center)\na standard deviation (spread)\n\nTheory dictates that the sampling distribution of the sample mean is well-approximated by a normal model under simple random sampling.\n\n\n\n\n\n\n\n\n\n\n\n\nBased on discussion thus far, what do you think the model parameters might be?\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#description-vs.-inference",
    "href": "content/week4-sampling.html#description-vs.-inference",
    "title": "Introduction to inference",
    "section": "Description vs. inference",
    "text": "Description vs. inference\n\nStatistical inferences are statements about population statistics based on samples.\n\nTo appreciate the meaning of this, consider the contrast with descriptive findings, which are statements about sample statistics:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nA descriptive finding is:\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nThe corresponding inference would be:\nThe median percent change in strength is highest among adults with genotype TT."
  },
  {
    "objectID": "content/week4-sampling.html#a-simpler-question",
    "href": "content/week4-sampling.html#a-simpler-question",
    "title": "Introduction to inference",
    "section": "A simpler question",
    "text": "A simpler question\n\n\nConsider total HDL cholesterol in mmol/L. We already know how to summarize the distribution of sample values:\n\n\n\n\n\n\n\n\n\nSample mean\nSample SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.043\n1.117"
  },
  {
    "objectID": "content/week4-sampling.html#population-model",
    "href": "content/week4-sampling.html#population-model",
    "title": "Introduction to inference",
    "section": "Population model",
    "text": "Population model\n\n\nConsider total HDL cholesterol in mmol/L. We already know how to summarize the distribution of sample values:\n\n\n\n\n\n\n\n\n\nSample mean\nSample SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.067\n1.126"
  },
  {
    "objectID": "content/week4-sampling.html#population-models",
    "href": "content/week4-sampling.html#population-models",
    "title": "Introduction to inference",
    "section": "Population models",
    "text": "Population models\n\nInference consists in using statistics of a random sample to ascertain population statistics under a population model.\n\nA population model represents the distribution of values you’d see if you measured every individual in the study population. We think of the sample values as a random draw.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Density is an alternative scale to frequency that is independent of population size.)"
  },
  {
    "objectID": "content/week4-sampling.html#sampling-variation",
    "href": "content/week4-sampling.html#sampling-variation",
    "title": "Introduction to inference",
    "section": "Sampling variation",
    "text": "Sampling variation\nA population model represents the distribution of values you’d see if you measured every individual in the study population (a census).\nThink of a sample as a random draw from the population:\n\ndifferent samples comprise different sets of individuals\nsample statistics depend on which individuals you measure"
  },
  {
    "objectID": "content/week4-sampling.html#sampling-distributions",
    "href": "content/week4-sampling.html#sampling-distributions",
    "title": "Introduction to inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nWhat we are simulating is known as a sampling distribution: the frequency of values of a statistic across all possible random samples.\n\n\n\n\n\n\n\n\n\n\n\n\nProvided data are from a random sample, the sample mean \\(\\bar{x}\\) has a sampling distribution with\n\nmean \\(\\color{red}{\\mu}\\) (population mean)\nstandard deviation \\(\\color{red}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\nregardless of its exact form.\n\n\nIn other words, across all random samples of a fixed size…\n\nThe average value of the sample mean is the population mean.\nThe average squared error (sample mean - population mean)\\(^2\\) is \\(\\frac{\\sigma^2}{n}\\)"
  },
  {
    "objectID": "content/week4-sampling.html#stuff",
    "href": "content/week4-sampling.html#stuff",
    "title": "Introduction to inference",
    "section": "Stuff",
    "text": "Stuff\nGenerating a large number (10K) of samples allows us to approximate the sampling distribution for a variety of sample sizes.\n\n\n\n\n\n\n\n\n\n\nspread diminishes with sample size\nless variability among estimates from larger samples\n\nSo estimates are more precise with more data.\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#standard-error",
    "href": "content/week4-sampling.html#standard-error",
    "title": "Introduction to inference",
    "section": "Standard error",
    "text": "Standard error\nIn practice we only have one sample so instead of a direct measure we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared distance of the sample mean from the population mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-sampling.html#point-estimates",
    "href": "content/week4-sampling.html#point-estimates",
    "title": "Introduction to inference",
    "section": "Point estimates",
    "text": "Point estimates\n\nSample statistics, viewed as guesses for the values of population statistics, are called ‘point estimates’.\n\nWe’ll focus on inferences involving the following:\n\n\n\nPopulation statistic\nParameter\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)"
  },
  {
    "objectID": "content/week4-sampling.html#standard-errors",
    "href": "content/week4-sampling.html#standard-errors",
    "title": "Introduction to inference",
    "section": "Standard errors",
    "text": "Standard errors\nIn practice we only have one sample so instead of a direct measure we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\n\n\n\n\n5.381\n1.073\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]"
  },
  {
    "objectID": "content/week4-sampling.html#reporting-point-estimates",
    "href": "content/week4-sampling.html#reporting-point-estimates",
    "title": "Introduction to inference",
    "section": "Reporting point estimates",
    "text": "Reporting point estimates\nIt is common style to report the value of a point estimate with a standard error given parenthetically.\n\n\nStatistics from full NHANES sample:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\n\n\n\n\n5.043\n1.075\n3179\n\n\n\n\n\n\n\nThe mean total cholesterol among the population is estimated to be 5.043 mmol/L (SE 0.019)\n\n\n\nThis style of report communicates:\n\nparameter of interest\nvalue of point estimate\nerror/variability of point estimate"
  },
  {
    "objectID": "content/week4-sampling.html#interval-estimation",
    "href": "content/week4-sampling.html#interval-estimation",
    "title": "Introduction to inference",
    "section": "Interval estimation",
    "text": "Interval estimation\n\nAn interval estimate is a range of plausible values for a population parameter.\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.043 \\pm 2\\times 0.0191 = (5.005, 5.081)\\]\n\nIn R:\n\navg.totchol &lt;- mean(totchol)\nse.totchol &lt;- sd(totchol)/sqrt(length(totchol))\navg.totchol + c(-2, 2)*se.totchol\n\n[1] 5.004817 5.081059\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#interval-construction",
    "href": "content/week4-sampling.html#interval-construction",
    "title": "Introduction to inference",
    "section": "Interval construction",
    "text": "Interval construction\nIn general, an interval estimate is constructed from two main ingredients:\n\npoint estimate\nstandard error\n\nAnd one secret ingredient:\n\na model for the sampling distribution of the point estimate\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]"
  },
  {
    "objectID": "content/week4-sampling.html#precision-and-coverage",
    "href": "content/week4-sampling.html#precision-and-coverage",
    "title": "Introduction to inference",
    "section": "Precision and coverage",
    "text": "Precision and coverage\n\n\nIntervals have two main and attributes:\n\nprecision refers to how wide or narrow the interval is\ncoverage refers to how often, under random sampling, the interval captures the parameter of interest\n\n\nThese properties are inversely related:\n\nif I say mean cholesterol is between 0 and 50 I’m almost certainly right, but the estimate is useless\nif I say mean cholesterol is between 5.0299 and 5.0301, I’ve made a very precise guess, but I’m likely wrong (think about sampling variability)\n\n\n\n\nAn accurate interval should maintain high coverage while achieving practically useful precision. This isn’t always possible!\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-sampling.html#inference-or-description",
    "href": "content/week4-sampling.html#inference-or-description",
    "title": "Introduction to inference",
    "section": "Inference or description?",
    "text": "Inference or description?\nSee if you can tell the difference:\n\nThe proportion of children who developed a peanut allergy was 0.133 in the avoidance group.\nThe proportion of children who develop peanut allergies is estimated to be 0.133 when peanut protein is avoided in infancy.\nThe average lifetime of mice on normal 85kCal diets is estimated to be 32.7 months.\nThe 57 mice on the normal 85kCal diet lived 32.7 months on average.\nThe relative risk of a CHD event in the high trait anger group compared with the low trait anger group was 2.5.\nThe relative risk of a CHD event among adults with high trait anger compared with adults with low trait anger is estimated to be 2.5."
  },
  {
    "objectID": "content/week4-sampling.html#a-difficulty",
    "href": "content/week4-sampling.html#a-difficulty",
    "title": "Introduction to inference",
    "section": "A difficulty",
    "text": "A difficulty\n\nDifferent samples yield different estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample means:\n\n\n\n\n\n\n\n\n\nsample.1\nsample.2\n\n\n\n\n5.093\n5.136\n\n\n\n\n\n\nestimates are close but not identical\nthe population mean can’t be both 5.093 and 5.136\nprobably neither estimate is exactly correct\n\nEstimation error and sample-to-sample variability are inherent to point estimation."
  },
  {
    "objectID": "content/week4-sampling.html#interpreting-standard-errors",
    "href": "content/week4-sampling.html#interpreting-standard-errors",
    "title": "Introduction to inference",
    "section": "Interpreting standard errors",
    "text": "Interpreting standard errors\nThe standard error is a point estimate of the (population) standard deviation of sample means across all possible random samples:\n\\[\nSE(\\bar{x}) \\text{ estimates } \\sqrt{\\text{average value of } (\\bar{x} - \\mu)^2}\n\\] Two phrasings for an interpretation:\n\nEstimated root average squared deviation of the sample mean from the population mean.\nEstimated root mean square error."
  },
  {
    "objectID": "content/lab4-sampling.html",
    "href": "content/lab4-sampling.html",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "",
    "text": "library(tidyverse)\nload('data/nhanes.RData')\nload('data/temps.RData')\nThe goal of this lab is to learn to compute a point estimate, standard error, and interval estimate for a population mean “by hand” by performing the arithmetic directly in R. You will have an opportunity to practice interpreting these quantities as you go.\nWe will also use this lab for a short class activity to explore how often the interval estimate we introduced is “correct”."
  },
  {
    "objectID": "content/lab4-sampling.html#your-turn-1",
    "href": "content/lab4-sampling.html#your-turn-1",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "Your turn",
    "text": "Your turn\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error"
  },
  {
    "objectID": "content/lab4-estimation.html",
    "href": "content/lab4-estimation.html",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "",
    "text": "library(tidyverse)\nload('data/nhanes.RData')\nload('data/temps.RData')\n\nThe goal of this lab is to learn to compute a point estimate, standard error, and interval estimate for a population mean “by hand” by performing the arithmetic directly in R. You will have an opportunity to practice interpreting these quantities as you go.\nWe will also use this lab for a short class activity to explore how often the interval estimate we introduced is “correct”.\n\nPoint estimation\n\nEstimate for the population mean\nSince the point estimate for the population mean of a numeric variable is the sample mean, you already know how to perform the calculation in R. We’ll store this for later use:\n\n# retrieve total cholesterol variable\ntotchol &lt;- nhanes$totchol\n\n# compute and store sample mean\ntotchol.mean &lt;- mean(totchol)\ntotchol.mean\n\n[1] 5.042938\n\n\nThe only novelty here is that we now interpret this as a point estimate of the population mean total cholesterol:\n\nThe mean total cholesterol of U.S. adults is estimated to be 5.043 mmol/L.\n\nThis is in contrast to the interpretation as a descriptive summary:\n\nThe average total cholesterol among the respondents in the NHANES survey was 5.043 mmol/L.\n\nBoth interpretations are valid; just different. By interpreting the sample mean as a point estimate, we are implicitly assuming that the data are a random sample from the U.S. adult population.\n\n\n\n\n\n\nYour turn\n\n\n\nUse the temps data to estimate mean body temperature.\n\n# retrieve variable of interest\n\n# compute and store sample mean\n\nCheck your understanding:\n\ninterpret the result as a descriptive summary\ninterpret the result as a point estimate\n\n\n\n\n\nStandard error for the sample mean\nA standard error is a measure of the sampling variability of a point estimate. Technically, it’s an estimate of the point estimate’s standard deviation across all possible random samples of a fixed size.\nThe standard error for the sample mean is calculated according to the formula: \\[SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\] Where:\n\n\\(s_x\\) is the sample standard deviation\n\\(n\\) is the sample size\n\nTo calculate this in R, we perform the arithmetic by hand (for now):\n\n# store sample sd and sample size\ntotchol.sd &lt;- sd(totchol)\ntotchol.n &lt;- length(totchol)\n\n# compute standard error\ntotchol.se &lt;- totchol.sd/sqrt(totchol.n)\ntotchol.se\n\n[1] 0.01906042\n\n\nThis result is interpreted as follows:\n\nThe root average deviation of the sample mean from the population mean is estimated to be 0.0191 mmol/L.\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error\n\n\n\n\n\n\nInterval estimation\n\nInterval estimate for the mean\nA common interval for the population mean is:\n\\[\\bar{x} \\pm \\underbrace{2\\times SE(\\bar{x})}_{\\text{margin of error}}\\]\nFor now, we’ll calculate this by directly performing the arithmetic. Later, you’ll use commands that return interval estimates by default.\n\n# add/subtract two standard errors from the mean\ntotchol.mean + c(-2, 2)*totchol.se\n\n[1] 5.004817 5.081059\n\n\nWe’ll talk more about the exact interpretation later; for now, you should think of this as a range of plausible values for the population mean.\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate an interval estimate for the mean body temperature using the body temperature data.\n\n# interval estimate for mean body temp\n\n\n\n\n\nHow often is the interval correct?\nAn interval “covers” the population mean if the true value is between the interval endpoints.\nWe can explore how often the interval covers the parameter by having everyone in the class simulate their own sample from a population with a known mean and check whether the interval they obtain from the sample covers the population mean or not.\nThe commands below simulate a sample and then compute an interval.\n\n# function to simulate body temp data from a population with mean 98.6\nsample.bodytemps &lt;- function(n){\n  rnorm(n, mean = 98.6, sd = 1)\n}\n\n# simulate a sample of body temperatures\nbodytemp &lt;- sample.bodytemps(n = 150)\n\n# compute interval 'ingredients'\nbodytemp.mean &lt;- mean(bodytemp)\nbodytemp.sd &lt;- sd(bodytemp)\nbodytemp.n &lt;- length(bodytemp)\nbodytemp.se &lt;- bodytemp.sd/sqrt(bodytemp.n)\n\n# compute interval estimate\nbodytemp.mean + c(-2, 2)*bodytemp.se\n\n[1] 98.41325 98.73135\n\n# margin of error\n2*bodytemp.se\n\n[1] 0.1590505\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nUse the example above to generate a sample of size 20 and compute an interval estimate for the mean body temperature.\nThen:\n\nDetermine whether your interval covers the population mean.\nCompute the margin of error used in your interval (\\(2\\times SE(\\bar{x})\\)).\n\nRepeat with \\(n = 150\\). Then fill out this form.\n\n\n\n\n\nPractice problems\n\nVu and Harrington exercise 4.1. Additionally:\n\nCompute an interval estimate for the mean BGC of nests.\nSupposing a sample of 30 nests returned exactly the same summary statistics, recompute your interval in (e). Is the margin of error smaller or larger?\n\n\n\nThe brfss dataset contains a measurement of body weight, weight, as well as a variable, wtdesire, that is the desired weight reported by respondents.\n\nEstimate the mean difference between actual and desired weight. Report the point estimate and standard error.\nDoes the point estimate suggest that the average U.S. adult would prefer to lose or gain weight?\nCompute an interval estimate for the mean difference between actual and desired weight."
  },
  {
    "objectID": "content/lab4-estimation.html#your-turn-1",
    "href": "content/lab4-estimation.html#your-turn-1",
    "title": "Lab 4: Point and interval estimation for a population mean",
    "section": "Your turn",
    "text": "Your turn\nCalculate and interpret the standard error for the sample mean of the body temperature variable.\n\n# store sample sd and sample size\n\n# compute standard error"
  },
  {
    "objectID": "content/week4-intervals.html",
    "href": "content/week4-intervals.html",
    "title": "Interval estimation",
    "section": "",
    "text": "HW2 remarks/discussion\n[Lecture] A basic interval estimate for the mean\n[Lecture/lab] Exploring interval coverage\n[Lecture/lab] Comparing normal and \\(t\\) models"
  },
  {
    "objectID": "content/week4-intervals.html#todays-agenda",
    "href": "content/week4-intervals.html#todays-agenda",
    "title": "Confidence intervals",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nHW2 remarks/discussion\n[Lecture] A basic interval estimate for the mean\n[Lecture/lab] Exploring interval coverage\n[Lecture/lab] Comparing normal and \\(t\\) models"
  },
  {
    "objectID": "content/week4-intervals.html#from-last-time",
    "href": "content/week4-intervals.html#from-last-time",
    "title": "Confidence intervals",
    "section": "From last time",
    "text": "From last time\n\n\nUnder simple random sampling:\n\nthe sample mean \\(\\bar{x}\\) provides a good point estimate of the population mean \\(\\mu\\)\nits estimated sampling variability is given by the standard error \\(SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}} = \\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\nse\n\n\n\n\n5.043\n1.075\n3179\n0.01906\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean total HDL cholesterol among the U.S. adult population is estimated to be 5.043 mmol/L (SE 0.0191)."
  },
  {
    "objectID": "content/week4-intervals.html#interval-estimation",
    "href": "content/week4-intervals.html#interval-estimation",
    "title": "Confidence intervals",
    "section": "Interval estimation",
    "text": "Interval estimation\nA common interval estimate for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\nA range of plausible values for the mean total cholesterol among U.S. adults is 5.005 to 5.081 mmol/L.\n\nTwo related questions:\n\nWhat do we mean by “plausible”?\nWhere did the number 2 come from?"
  },
  {
    "objectID": "content/week4-intervals.html#precision-and-coverage",
    "href": "content/week4-intervals.html#precision-and-coverage",
    "title": "Confidence intervals",
    "section": "Precision and coverage",
    "text": "Precision and coverage\n\n\nIntervals have two main and attributes:\n\nprecision refers to how wide or narrow the interval is\ncoverage refers to how often, under random sampling, the interval captures the parameter of interest\n\n\nThese properties are inversely related:\n\nif I say mean cholesterol is between 0 and 50 I’m almost certainly right, but the estimate is useless\nif I say mean cholesterol is between 5.0299 and 5.0301, I’ve made a very precise guess, but I’m likely wrong (think about sampling variability)\n\n\n\n\nAn accurate interval should maintain high coverage while achieving practically useful precision. This isn’t always possible!"
  },
  {
    "objectID": "content/week4-intervals.html#an-interval-for-the-mean",
    "href": "content/week4-intervals.html#an-interval-for-the-mean",
    "title": "Confidence intervals",
    "section": "An interval for the mean",
    "text": "An interval for the mean\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.031 \\pm 2\\times 0.1396 = (4.75, 5.31)\\]\n\nIn R:\n\nc(lwr = mean(cholesterol) - 2*sd(cholesterol)/sqrt(50), \n  upr = mean(cholesterol) + 2*sd(cholesterol)/sqrt(50))\n\n     lwr      upr \n4.738974 5.346902 \n\n\n\n\nThe precision is evident from the interval width (0.5611). But what about coverage?"
  },
  {
    "objectID": "content/week4-intervals.html#exploring-interval-coverage",
    "href": "content/week4-intervals.html#exploring-interval-coverage",
    "title": "Confidence intervals",
    "section": "Exploring interval coverage",
    "text": "Exploring interval coverage\nLet’s carry on pretending that the NHANES data comprise a population.\nThe first section of lab5-intervals contains some simple commands to draw a sample and calculate an interval estimate.\n\nEach of you will generate an interval based on a different sample\nWe’ll tally how many of you obtained intervals capturing the population mean\n\nOur tally will give an approximate idea of the coverage."
  },
  {
    "objectID": "content/week4-intervals.html#more-simulation",
    "href": "content/week4-intervals.html#more-simulation",
    "title": "Confidence intervals",
    "section": "More simulation",
    "text": "More simulation\n\n\nArtificially simulating a larger number of intervals provides a slightly better approximation of coverage.\n\nat right, 100 intervals\n97% cover the population mean (vertical dashed line)\n\nWhat do you expect would happen to coverage if, for the same samples…\n\na wider margin of error (say, \\(3\\times SE\\)) were used?\na narrower margin of error (say, \\(1\\times SE\\)) were used?"
  },
  {
    "objectID": "content/week4-intervals.html#so-why-2-standard-errors",
    "href": "content/week4-intervals.html#so-why-2-standard-errors",
    "title": "Confidence intervals",
    "section": "So why 2 standard errors?",
    "text": "So why 2 standard errors?\n\n\nThe margin of error of \\(2\\times SE\\) comes from the so-called “empirical rule”.\n\nunder the normal model, 95% of values are within 2SD of center\nso for 95% of samples, the sample mean is within 2SD of the population mean\n\nSo in theory, according to the normal model, \\(\\bar{x} \\pm 2\\times SD\\) achieves 95% coverage.\n\n\n\n\nBut we are using standard error (SE), not standard deviation (SD). Do we still get the same coverage using the normal model?"
  },
  {
    "objectID": "content/week4-intervals.html#normal-model-coverage",
    "href": "content/week4-intervals.html#normal-model-coverage",
    "title": "Confidence intervals",
    "section": "Normal model coverage",
    "text": "Normal model coverage\n\n\nAt right, the misses are compared between intervals calculated with SD (left) and SE (right) using the multiplier from the normal model on the same 10,000 simulated datasets with sample size \\(n = 15\\).\n\nSE misses more often\nso the normal model produces under-coverage\n\n\n\n\n\n\n\n\n\n\ntype\ncoverage\n\n\n\n\nsd\n0.954\n\n\nse\n0.9294\n\n\n\n\n\nWhat do you think: the multiplier should be [smaller/larger] to ensure 95% coverage."
  },
  {
    "objectID": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "href": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "title": "Confidence intervals",
    "section": "A closer look at the normal model",
    "text": "A closer look at the normal model\nAn alternate but equivalent way to understand the normal model for the sampling distribution of \\(\\bar{x}\\) is in terms of deviations. The following are equivalent:\n\nThe expression \\(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\\) measures the number of standard deviations from center."
  },
  {
    "objectID": "content/week4-intervals.html#simulating-deviations",
    "href": "content/week4-intervals.html#simulating-deviations",
    "title": "Confidence intervals",
    "section": "Simulating deviations",
    "text": "Simulating deviations\nAnother way to check normal model coverage is to use deviations:\n\nSimulate many samples\nCompute scaled deviations\nTally how many scaled deviations are between -2 and 2\n\nThe proportion of samples for which the scaled deviation is between -2 and 2 approximates the coverage.\nWe’ll try it in the next part of the lab5-intervals. Hypotheses:\n\ndeviations scaled by SD should be between -2 and 2 95% of the time\ndeviations scaled by SE should be between -2 and 2 [more/less] than 95% of the time"
  },
  {
    "objectID": "content/week4-intervals.html#the-t-model",
    "href": "content/week4-intervals.html#the-t-model",
    "title": "Confidence intervals",
    "section": "The \\(t\\) model",
    "text": "The \\(t\\) model\n\n\nConsider the statistic:\n\\[\nT = \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n\\]\nThe sampling distribution of \\(T\\) is well-approximated by a \\(t_{n - 1}\\) model whenever either:\n\nthe population model is symmetric and unimodal\n\nOR\n\nthe sample size is not too small"
  },
  {
    "objectID": "content/week4-intervals.html#model-specification",
    "href": "content/week4-intervals.html#model-specification",
    "title": "Confidence intervals",
    "section": "Model specification",
    "text": "Model specification\nThe \\(t\\) model is characterized by its degrees of freedom.\n\nfor interval estimates for the mean, \\(n - 1\\) is used\ndepending on the degrees of freedom (i.e., sample size), a different multiplier is applied to the standard error to obtain the margin of error\n\nThe multiplier is called a critical value, and can be found in R via:\n\n# pseudo code -- replace coverage with desired level, e.g., 0.95\nqt((1 - coverage)/2, df = (n - 1), lower.tail = F)\n\n\nchosen to ensure a specified nominal coverage level (usually 95%)\nhigher nominal coverage levels utilize larger critical values, producing wider intervals"
  },
  {
    "objectID": "content/week4-intervals.html#model-validation",
    "href": "content/week4-intervals.html#model-validation",
    "title": "Confidence intervals",
    "section": "Model validation",
    "text": "Model validation\nUsing the \\(t\\) model should produce coverage closer to the nominal level compared with the normal model. Let’s check through simulation.\n\n\nAt right, misses are compared between intervals using SE and critical values from the normal model (left) and \\(t\\) model (right) constructed on the same 10,000 simulated datasets with sample size \\(n = 10\\).\n\n\n\n\n\n\n\n\n\nmodel\ncoverage\n\n\n\n\nnormal\n0.9219\n\n\nt\n0.9461\n\n\n\n\n\nThe \\(t\\) model produces coverage much closer to the nominal level."
  },
  {
    "objectID": "content/week4-intervals.html#calculations",
    "href": "content/week4-intervals.html#calculations",
    "title": "Confidence intervals",
    "section": "Calculations",
    "text": "Calculations\nSo, to sum up, the general formula for an interval for a population mean is: \\[\\bar{x} \\pm c \\times SE(\\bar{x}) \\quad\\text{where}\\quad SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\]\n\n\nRules of thumb:\n\nfor moderate to large samples, use the normal model\n\n\\(c = 1\\) for 68% coverage\n\\(c = 2\\) for 95% coverage\n\\(c = 3\\) for 99.7% coverage\n\nfor small sample sizes, use the \\(t\\) model\nwhen in doubt, use the \\(t\\) model\n\n\nExact critical values in R:\n\n# normal critical value\nc &lt;- qnorm((1 - coverage)/2, lower.tail = F)\n\n# t critical value\nc &lt;- qt((1 - coverage)/2, df = n - 1, lower.tail = F)\n\nInterval calculation:\n\n# pseudo code\nmean(data_vec) + c(-1, 1)*c*sd(data_vec)/sqrt(n)"
  },
  {
    "objectID": "content/week4-intervals.html#interpretation",
    "href": "content/week4-intervals.html#interpretation",
    "title": "Confidence intervals",
    "section": "Interpretation",
    "text": "Interpretation\nAs we’ve seen, coverage pertains to how often an interval of a particular form captures the population parameter of interest across samples of a fixed size. Loosely speaking, this represents how often you’d be right if you were to fully replicate your study ad infinitum.\nThis leads to the following interpretation:\n\nWith [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units].\n\nFor this reason, statisticians call interval estimates confidence intervals."
  },
  {
    "objectID": "content/week4-intervals.html#revisiting-initial-example",
    "href": "content/week4-intervals.html#revisiting-initial-example",
    "title": "Confidence intervals",
    "section": "Revisiting initial example",
    "text": "Revisiting initial example\n\n\nSo in the example we began with:\n\n# calculate 95% interval\nmean(cholesterol) + c(-1, 1)*2*sd(cholesterol)/sqrt(50)\n\n[1] 4.738974 5.346902\n\n\nWith 95% confidence, the mean total HDL cholesterol is estimated to be between 4.739 and 5.347 mmol/L.\nRemember, “95% confidence” refers to coverage under sampling variation.\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.043\n0.01906\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-inference.html#todays-agenda",
    "href": "content/week4-inference.html#todays-agenda",
    "title": "Introduction to inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nTest 1 discussion\n[lecture] Inference vs. description, point estimation, interval estimation\n[lab] Exploring interval coverage"
  },
  {
    "objectID": "content/week4-inference.html#description-vs.-inference",
    "href": "content/week4-inference.html#description-vs.-inference",
    "title": "Introduction to inference",
    "section": "Description vs. inference",
    "text": "Description vs. inference\n\nStatistical inferences are statements about population statistics based on samples.\n\nTo appreciate the meaning of this, consider the contrast with descriptive findings, which are statements about sample statistics:\n\n\n\n\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nA descriptive finding is:\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nThe corresponding inference would be:\nThe median percent change in strength is highest among adults with genotype TT."
  },
  {
    "objectID": "content/week4-inference.html#inference-or-description",
    "href": "content/week4-inference.html#inference-or-description",
    "title": "Introduction to inference",
    "section": "Inference or description?",
    "text": "Inference or description?\nSee if you can tell the difference:\n\nThe proportion of children who developed a peanut allergy was 0.133 in the avoidance group.\nThe proportion of children who develop peanut allergies is estimated to be 0.133 when peanut protein is avoided in infancy.\nThe average lifetime of mice on normal 85kCal diets is estimated to be 32.7 months.\nThe 57 mice on the normal 85kCal diet lived 32.7 months on average.\nThe relative risk of a CHD event in the high trait anger group compared with the low trait anger group was 2.5.\nThe relative risk of a CHD event among adults with high trait anger compared with adults with low trait anger is estimated to be 2.5."
  },
  {
    "objectID": "content/week4-inference.html#random-sampling",
    "href": "content/week4-inference.html#random-sampling",
    "title": "Introduction to inference",
    "section": "Random sampling",
    "text": "Random sampling\n\nSampling establishes the link (or lack thereof) between a sample and a population.\n\n\n\n\n\nIn a simple random sample, units are chosen in such a way that every individual in the population has an equal probability of inclusion. For the SRS:\n\nsample statistics mirror population statistics (sample is representative)\nsampling variability depends only on population variability and sample size"
  },
  {
    "objectID": "content/week4-inference.html#population-models",
    "href": "content/week4-inference.html#population-models",
    "title": "Introduction to inference",
    "section": "Population models",
    "text": "Population models\n\nInference consists in using statistics of a random sample to ascertain population statistics under a population model.\n\nA population model represents the distribution of values you’d see if you measured every individual in the study population. We think of the sample values as a random draw.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Density is an alternative scale to frequency that is independent of population size.)"
  },
  {
    "objectID": "content/week4-inference.html#point-estimates",
    "href": "content/week4-inference.html#point-estimates",
    "title": "Introduction to inference",
    "section": "Point estimates",
    "text": "Point estimates\n\nSample statistics, viewed as guesses for the values of population statistics, are called ‘point estimates’.\n\nWe’ll focus on inferences involving the following:\n\n\n\nPopulation statistic\nParameter\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)"
  },
  {
    "objectID": "content/week4-inference.html#a-difficulty",
    "href": "content/week4-inference.html#a-difficulty",
    "title": "Introduction to inference",
    "section": "A difficulty",
    "text": "A difficulty\n\nDifferent samples yield different estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample means:\n\n\n\n\n\n\n\n\n\nsample.1\nsample.2\n\n\n\n\n5.093\n5.136\n\n\n\n\n\n\nestimates are close but not identical\nthe population mean can’t be both 5.093 and 5.136\nprobably neither estimate is exactly correct\n\nEstimation error and sample-to-sample variability are inherent to point estimation."
  },
  {
    "objectID": "content/week4-inference.html#simulating-sampling-variability",
    "href": "content/week4-inference.html#simulating-sampling-variability",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\n\n\n\n\n\n\n\n\n\n\n\n\nThese are 20 random samples with the sample mean indicated by the dashed line and the population distribution and mean overlaid in red.\n\nsample size \\(n = 20\\)\nfrequency distributions differ a lot\nsample means differ some\n\nWe can actually measure this variability!"
  },
  {
    "objectID": "content/week4-inference.html#simulating-sampling-variability-1",
    "href": "content/week4-inference.html#simulating-sampling-variability-1",
    "title": "Introduction to inference",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\nIf we had means calculated from a much larger number of samples, we could make a frequency distribution for the values of the sample mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample\n1\n2\n\\(\\cdots\\)\n10,000\n\n\nmean\n4.957\n5.039\n\\(\\cdots\\)\n5.24\n\n\n\n\nWe could then use the usual measures of center and spread to characterize the distribution of sample means.\n\nmean of \\(\\bar{x}\\): 5.0373228\nstandard deviation of \\(\\bar{x}\\): 0.2387869\n\n\nAcross 10,000 random samples of size 20, the typical sample mean was 5.04 and the root average squared distance of the sample mean from its typical value was 0.239."
  },
  {
    "objectID": "content/week4-inference.html#sampling-distributions",
    "href": "content/week4-inference.html#sampling-distributions",
    "title": "Introduction to inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nWhat we are simulating is known as a sampling distribution: the frequency of values of a statistic across all possible random samples.\n\n\n\n\n\n\n\n\n\n\n\n\nProvided data are from a random sample, the sample mean \\(\\bar{x}\\) has a sampling distribution with\n\nmean \\(\\color{red}{\\mu}\\) (population mean)\nstandard deviation \\(\\color{red}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\nregardless of its exact form.\n\n\nIn other words, across all random samples of a fixed size…\n\nThe average value of the sample mean is the population mean.\nThe average squared error (sample mean - population mean)\\(^2\\) is \\(\\frac{\\sigma^2}{n}\\)"
  },
  {
    "objectID": "content/week4-inference.html#effect-of-sample-size",
    "href": "content/week4-inference.html#effect-of-sample-size",
    "title": "Introduction to inference",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nThe standard deviation of the sampling distribution of \\(\\bar{x}\\) is inversely proportional to sample size.\n\n\n\n\n\n\n\n\n\n\n\n\nAs sample size increases…\n\naccuracy remains the same\nestimates get more precise\nskewness vanishes"
  },
  {
    "objectID": "content/week4-inference.html#measuring-sampling-variability",
    "href": "content/week4-inference.html#measuring-sampling-variability",
    "title": "Introduction to inference",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nIn practice \\(\\sigma\\) is not known so we use an estimate of sampling variability known as a standard error: \\[\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n\\]\n\nThe root average squared error of the sample mean is estimated to be 0.240 mmol/L."
  },
  {
    "objectID": "content/week4-inference.html#reporting-point-estimates",
    "href": "content/week4-inference.html#reporting-point-estimates",
    "title": "Introduction to inference",
    "section": "Reporting point estimates",
    "text": "Reporting point estimates\nIt is common style to report the value of a point estimate with a standard error given parenthetically.\n\n\nStatistics from full NHANES sample:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nn\n\n\n\n\n5.043\n1.075\n3179\n\n\n\n\n\n\n\nThe mean total cholesterol among the population is estimated to be 5.043 mmol/L (SE 0.019)\n\n\n\nThis style of report communicates:\n\nparameter of interest\nvalue of point estimate\nerror/variability of point estimate"
  },
  {
    "objectID": "content/week4-inference.html#interval-estimation",
    "href": "content/week4-inference.html#interval-estimation",
    "title": "Introduction to inference",
    "section": "Interval estimation",
    "text": "Interval estimation\n\nAn interval estimate is a range of plausible values for a population parameter.\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.043 \\pm 2\\times 0.0191 = (5.005, 5.081)\\]\n\nIn R:\n\navg.totchol &lt;- mean(totchol)\nse.totchol &lt;- sd(totchol)/sqrt(length(totchol))\navg.totchol + c(-2, 2)*se.totchol\n\n[1] 5.004817 5.081059\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week4-intervals.html#the-t-model-1",
    "href": "content/week4-intervals.html#the-t-model-1",
    "title": "Confidence intervals",
    "section": "The \\(t\\) model",
    "text": "The \\(t\\) model\nWe’re actually using \\(\\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\\) to construct intervals, because we don’t know \\(\\sigma\\).\nThese deviations are better approximated by a \\(t\\) model, which adjusts the normal model for the extra uncertainty that comes from estimating the standard deviation.\n\n\nThe difference between models depends mainly on sample size:\n\nbehaves almost exactly the same for moderate to large samples\nlarger deviations from center for small samples\nleads to larger multipliers for computing margin of error\n\n\n\n\n\nComparison of \\(t\\) model with normal model for various degrees of freedom."
  },
  {
    "objectID": "content/week4-intervals.html#model-interpretation",
    "href": "content/week4-intervals.html#model-interpretation",
    "title": "Confidence intervals",
    "section": "Model interpretation",
    "text": "Model interpretation\n\nThe area under the density curve between any two values \\((a, b)\\) gives the proportion of random samples for which \\(a &lt; T &lt; b\\).\n\n\\[(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a &lt; T &lt; b)\\]\n\n\n\n50% of samples give negative values of \\(T\\)\n\n\npt(0, df = 20 - 1) # area less than 0\n\n[1] 0.5\n\n\n\n83.5% of samples give values of \\(T\\) under 1\n\n\npt(1, df = 20 - 1) # area less than 1\n\n[1] 0.8350616\n\n\n\n33.5% of samples give values of \\(T\\) between 0 and 1"
  }
]