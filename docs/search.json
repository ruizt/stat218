[
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "Refer to this page for a detailed schedule of coursework and links to all materials. This information is organized by week, and you should check this page on a weekly basis for updates as the quarter progresses.\nI will aim to post/update outlines for class meetings at the beginning of each week (Monday), and update with links to relevant materials as the week progresses. So, you should look at the beginning of the week to see what’s ahead, and check back for newly posted materials before and after class meetings.\n\nPreparing for class meetings\n\nComplete readings in advance of the class meetings for which they are listed. Take notes or make annotations, and bring your notes to class.\nDownload and/or print copies of posted slides and handouts and bring these to class.\nIf a lab is indicated, it is recommended that you bring your laptop/tablet. While these are group activities and it is an option to share, you will not have direct access to your groupmates’ work after the class meeting.\n\n\n\nWeek 1 (1/8/24)\nIntroduction to statistical thinking and study designs\nTuesday class meeting [slides]\n\n[lecture/discussion] course introduction and syllabus review\n[activity] working groups and icebreakers\n[activity] orientation to posit.cloud\n\nThursday class meeting: [slides] [handout]\n\n[reading] van Belle et al. 2.1 - 2.5; Vu and Harrington 1.1\n[lecture/discussion] study designs\n[activity] distinguishing types of studies\n[case study] preventing peanut allergies\n\nReading quiz responses: 2pm section 4pm section\nAssignment: [HW1] due Thursday 1/18 11:59pm PST [L1, L2]\n\n\nWeek 2 (1/15/24)\nAcademic holiday 1/15/24 and Monday class schedule on 1/16/24\nIntroduction to data and data types\nThursday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 1.2; Douglas et al. 3.1 - 3.2\n[lecture/discussion] data types and data structures\n[activity] distinguishing data types in practice\n[lab] R basics and data in R\n\nReading quiz responses: 2pm section 4pm section\nNo assignment this week\n\n\nWeek 3 (1/22/24)\nDescriptive statistics and graphical summaries\nTuesday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 1.4 - 1.5; van Belle et al. 3.4\n[lecture/discussion] descriptive statistics for numerical and categorical data\n[lab] descriptive statistics in R; robustness\n[case study] functional SNPs associated with muscle strength and size (FAMuSS)\n\nThursday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 1.6\n[lecture/discussion] descriptive statistics for relationships between two variables\n[lab] bivariate summaries in R\n[case study] FAMUSS\n\nReading quiz responses: 2pm section 4pm section\nAssignment: extended to next week\n\n\nWeek 4 (1/29/24)\nFoundations for inference\nTuesday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 4.1\n[lecture/discussion] sampling and sampling distributions\n[lab] exploring sampling variability\n[case study] National health and nutrition examination survey (NHANES)\n\nReading quiz responses: 2pm section 4pm section\nThursday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 3.3.1, 3.3.2, and 3.3.3; and 4.2\n[lecture/discussion] interval estimation for a population mean\n[lab] computing confidence intervals in R\n[case study] NHANES\n\nAssignment: [HW2] due Thursday 2/8 11:59pm PST [L3, L4]\n\n\nWeek 5 (2/5/24)\nOne- and two-sample inference for numerical data\nTuesday class meeting [slides] [lab]\n\n[reading] Vu and Harrington 4.3\n[lecture/discussion] hypothesis tests for a population mean\n[lab] one-sample \\(t\\) tests \n[case study] DDT in kale \n\nReading quiz responses: 2pm section 4pm section\nThursday:\n\n[reading] Vu and Harrington 5.2 and 5.3\n[lecture/discussion] two-sample t tests and intervals for paired and independent data\n[lab] two-sample t tests \n[case study] evolution of Darwin’s finches \n\nAssignment: Test 1 released Friday 2/9 and due Saturday 2/10 11:59pm PST [L1, L2, L3]\n\n\nWeek 6 (2/12/24)\nNonparametric inference\n\n\n\n\nAssignment: HW3 due Thursday 2/22 11:59pm PST [L4, L5]\n\n\nWeek 7 (2/19/24)\nAnalysis of variance\nAcademic holiday 2/19/24\nAssignment: HW4 due Thursday 2/29 11:59pm PST [L9]\n\n\n\nWeek 8 (2/26/24)\nInference for categorical data\nAssignment: Test 2 due Friday 3/1 11:59pm PST [L4, L5, L9]\n\n\n\nWeek 9 (3/4/24)\nSimple linear regression\n\nAssignment: HW5 due Thursday 3/14 11:59pm PST [L6, L7, L8]\n\n\nWeek 10 (3/11/24)\nSimple linear regression\n\nAssignment: Test 3 due Friday 3/15 11:59pm PST [L6, L7, L8, L10]\n\n\nFinals week (3/18/24)\nOral exams to be held during scheduled exam time"
  },
  {
    "objectID": "content/week1-welcome.html#about-me",
    "href": "content/week1-welcome.html#about-me",
    "title": "Welcome to STAT218",
    "section": "About me",
    "text": "About me\nHi I’m Trevor. You can call me Trevor, or, if you prefer, Dr. Ruiz.\n\nBorn in Oregon, grew up in Maryland\nBA Philosophy, MS & PhD Statistics\nResearch interests: methods for large and complex scientific data\nHobbies: juggling, dance, classical guitar\nHave a cat named Mona"
  },
  {
    "objectID": "content/week1-welcome.html#why-am-i-a-statistician",
    "href": "content/week1-welcome.html#why-am-i-a-statistician",
    "title": "Welcome to STAT218",
    "section": "Why am I a statistician?",
    "text": "Why am I a statistician?\nShort answer: I like variety.\nLong answer:\n\nas a philosophy student I was interested in how we support claims of empirical fact through scientific investigation\nusually involves data as quantitative evidence\nstatistics is the science of evaluating quantitative evidence\n\nSilly answer: I couldn’t come up with anything better to do."
  },
  {
    "objectID": "content/week1-welcome.html#about-you",
    "href": "content/week1-welcome.html#about-you",
    "title": "Welcome to STAT218",
    "section": "About you",
    "text": "About you\nBy the numbers…"
  },
  {
    "objectID": "content/week1-welcome.html#about-you-1",
    "href": "content/week1-welcome.html#about-you-1",
    "title": "Welcome to STAT218",
    "section": "About you",
    "text": "About you\nBy show of hands…\n\n\nFirst statistics class ever?\nLast statistics class you expect to take?\nYes to 1 and 2?\nExpect to use statistics for your degree coursework?\nExpect to use statistics for research or senior project?\nRequired for your major?\nConsidering a statistics or data science minor?"
  },
  {
    "objectID": "content/week1-welcome.html#uncertainty",
    "href": "content/week1-welcome.html#uncertainty",
    "title": "Welcome to STAT218",
    "section": "Uncertainty",
    "text": "Uncertainty\nLife is full of uncertainty, and this can make a lot of questions hard to answer:\n\nImagine you’re a medical care provider. Which therapy should you prescribe for a patient given their prognosis? You don’t know precisely how each one will turn out, so how do you decide?\nFlu shots don’t always prevent people from getting the flu; many people who are innoculated still become ill, sometimes fatally. Your doctor asks if you want the shot. Should you get it?\nYou’re a trainer devising a fitness regimen for a client. You’ve decided on the basic elements, but what order should you recommend? Will it make any difference with respect to their goals? How will you know?\nYou’re considering trying a nutritional supplement for weight loss. Some people that try it experience severe side effects, but not too many. So, is it safe? It works sometimes, but not all the time. So, is it effective?"
  },
  {
    "objectID": "content/week1-welcome.html#statistics-and-uncertainty",
    "href": "content/week1-welcome.html#statistics-and-uncertainty",
    "title": "Welcome to STAT218",
    "section": "Statistics and uncertainty",
    "text": "Statistics and uncertainty\nKey problem: similar situations do not always result in the same outcome.\nStatistical thinking: uncertainty is measurable.\nWhat statistics can offer:\n\nprinciples for designing studies and collecting data in order to capture outcome variability\ndata analytic tools to distinguish random from systematic variability\nheuristics to make inferences that account for uncertainty"
  },
  {
    "objectID": "content/week1-welcome.html#course-goal-and-scope",
    "href": "content/week1-welcome.html#course-goal-and-scope",
    "title": "Welcome to STAT218",
    "section": "Course goal and scope",
    "text": "Course goal and scope\nThe overarching goal of 218 is to introduce you to statistics in a hands-on way that is relevant to your major.\nSo we will focus on:\n\nstatistical thinking, study design, and data analysis\nclassical methods, mostly developed 1900-1940\ncase studies from life sciences\napplication, not theory"
  },
  {
    "objectID": "content/week1-welcome.html#materials",
    "href": "content/week1-welcome.html#materials",
    "title": "Welcome to STAT218",
    "section": "Materials",
    "text": "Materials\nWeb. All materials are hosted/linked on the course website. I won’t be using Canvas.\nBooks. Readings will be assigned from three textbooks, all available at no cost:\n\nVu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences.\nVan Belle, Fisher, Heagerty, and Lumley (2004). Biostatistics: a methodology for the health sciences.\nDouglas et al. (2023). An Introduction to R.\n\nComputing. Hosted online via posit.cloud; more details in a bit.\nOther. You’ll need a laptop, tablet with keyboard, or access to a computer. MS Word strongly recommended but not strictly necessary."
  },
  {
    "objectID": "content/week1-welcome.html#printing-slides",
    "href": "content/week1-welcome.html#printing-slides",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nOpen menu from lower left"
  },
  {
    "objectID": "content/week1-welcome.html#printing-slides-1",
    "href": "content/week1-welcome.html#printing-slides-1",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nNavigate to tools"
  },
  {
    "objectID": "content/week1-welcome.html#printing-slides-2",
    "href": "content/week1-welcome.html#printing-slides-2",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n\nSelect PDF export mode"
  },
  {
    "objectID": "content/week1-welcome.html#printing-slides-3",
    "href": "content/week1-welcome.html#printing-slides-3",
    "title": "Welcome to STAT218",
    "section": "Printing slides",
    "text": "Printing slides\n - I suggest landscape layout and either 1 slide per page or per page"
  },
  {
    "objectID": "content/week1-welcome.html#preparing-for-class",
    "href": "content/week1-welcome.html#preparing-for-class",
    "title": "Welcome to STAT218",
    "section": "Preparing for class",
    "text": "Preparing for class\nEvery class meeting you should arrive having:\n\nprepared reading notes or annotations of assigned reading\nobtained a copy of the slides for taking notes\nobtained a copy of activity or lab handouts\n\nLet’s do this now for next class."
  },
  {
    "objectID": "content/week1-welcome.html#patterns",
    "href": "content/week1-welcome.html#patterns",
    "title": "Welcome to STAT218",
    "section": "Patterns",
    "text": "Patterns\nClass meeting pattern:\n\nreading quiz (not for credit)\nshort lecture, with discussion\n5min break\nlab activity in groups\ncase study presentation, with discussion\n\nWeekly pattern:\n\nclass meeting agendas updated Mondays\nassignments distributed Thursdays\n\nQuarterly pattern:\n\n[two homeworks, one test] x 3"
  },
  {
    "objectID": "content/week1-welcome.html#tentative-schedule",
    "href": "content/week1-welcome.html#tentative-schedule",
    "title": "Welcome to STAT218",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\n\n\n\n\n\nWeek\nTopics\nAssignments\n\n\n\n\n1 (1/8/24)\nIntroduction to statistical thinking and study design\nHW1\n\n\n2 (1/15/24)\nData, data types, and data collection\nNone\n\n\n3 (1/22/24)\nFoundations for inference\nHW2\n\n\n4 (1/29/24)\nInterval estimation and hypothesis tests\nTest 1\n\n\n5 (2/5/24)\nTwo-sample inference; nonparametric alternatives\nHW3\n\n\n6 (2/12/24)\nComparing means with analysis of variance\nHW4\n\n\n7 (2/19/24)\nInference for categorical data: proportions\nTest 2\n\n\n8 (2/26/24)\nInference for categorical data: chi-square tests\nHW5\n\n\n9 (3/4/24)\nSimple linear regression: model framework and estimation\nHW6\n\n\n10 (3/11/24)\nSimple linear regression: inference\nTest 3\n\n\nFinals (3/18/24)\nN/A\nOral exam"
  },
  {
    "objectID": "content/week1-welcome.html#assessments",
    "href": "content/week1-welcome.html#assessments",
    "title": "Welcome to STAT218",
    "section": "Assessments",
    "text": "Assessments\nOn the syllabus, you’ll find 11 learning outcomes labelled [L1] — [L11].\n\nGraded questions on the homeworks are matched to these outcomes.\nTest questions are also matched to outcomes.\n\nGraded questions receive a binary assessment.\n\n(S) satisfactory: outcome attained\n(NI) needs improvement: outcome not yet attained\n\nYou can submit revisions to any assignment or test for reassessment at least once.\n\nTimelines and number of attempts differ by assignment type (homework or test)."
  },
  {
    "objectID": "content/week1-welcome.html#final-oral-exam",
    "href": "content/week1-welcome.html#final-oral-exam",
    "title": "Welcome to STAT218",
    "section": "Final oral exam",
    "text": "Final oral exam\nYou’ll be responsible for identifying a case study in your field of study that uses methods from the class:\n\nin pairs, otherwise private\n5min presentation\nshort Q&A\nscheduled during final exam period\n\nYour assessment for this will pertain only to [L11]: applying methods from the course to your major field of study."
  },
  {
    "objectID": "content/week1-welcome.html#grades",
    "href": "content/week1-welcome.html#grades",
    "title": "Welcome to STAT218",
    "section": "Grades",
    "text": "Grades\nAt the end of the quarter, you will receive for each outcome L1 — L11:\n\na score: proportion of questions that received a satisfactory assessment\nan evaluation: whether that outcome was fully met, partly met, or not met\n\nnot met: under 50%\npartly met: 50% — 80%\nfully met: 80% or better\n\n\nYour letter grade will be based on the tally of fully met and partly met outcomes."
  },
  {
    "objectID": "content/week1-welcome.html#letter-grades",
    "href": "content/week1-welcome.html#letter-grades",
    "title": "Welcome to STAT218",
    "section": "Letter grades",
    "text": "Letter grades\nDetails are in the syllabus, but:\n\nMust at partly or fully meet at least…\n\n6/11 to pass\n8/11 for B or better\n10/11 for A or better\n\nSubject to those thresholds, letter grade is determined by the number of outcomes fully met"
  },
  {
    "objectID": "content/week1-welcome.html#grading-example",
    "href": "content/week1-welcome.html#grading-example",
    "title": "Welcome to STAT218",
    "section": "Grading example",
    "text": "Grading example\n\n\n\n\n\nLearning outcome\nScore\nEvaluation\n\n\n\n\nL1\n83%\nFully met\n\n\nL2\n87%\nFully met\n\n\nL3\n100%\nFully met\n\n\nL4\n64%\nPartly met\n\n\nL6\n86%\nFully met\n\n\nL7\n91%\nFully met\n\n\nL8\n89%\nFully met\n\n\nL9\n57%\nPartly met\n\n\nL10\n41%\nNot met\n\n\nL11\n100%\nFully met\n\n\n\n\nYour grade might look something like this:\n[10 partly or fully met] + [7 fully met] ➞ B+"
  },
  {
    "objectID": "content/week1-welcome.html#select-policies",
    "href": "content/week1-welcome.html#select-policies",
    "title": "Welcome to STAT218",
    "section": "Select policies",
    "text": "Select policies\n\ncollaboration encouraged on homeworks; not allowed on tests\nattendance expected; more than two unexcused absences may negatively impact grade\nexpect to allocate 12-16hr/wk, including class meetings\ntwo free late assignment submissions, anytime and without notice, up to one week from deadline"
  },
  {
    "objectID": "content/week1-welcome.html#office-hours-email",
    "href": "content/week1-welcome.html#office-hours-email",
    "title": "Welcome to STAT218",
    "section": "Office hours & email",
    "text": "Office hours & email\nOffice hours are 12-2 M-W in 25-236\n\nSchedule 10min time slots via Calendly (see syllabus for link)\n\nUsually I get to email quickly, but allow at least 48 (weekday) hours\n\nIf you haven’t heard back in a week, that means I missed it — send me a reminder!"
  },
  {
    "objectID": "content/week1-welcome.html#working-groups",
    "href": "content/week1-welcome.html#working-groups",
    "title": "Welcome to STAT218",
    "section": "Working groups",
    "text": "Working groups\nFor labs and in class activities, you’ll work in groups. Let’s form those now.\nIcebreakers:\n\nName, major, class standing\nHometown\nOne personal interest"
  },
  {
    "objectID": "content/week1-welcome.html#introduction-to-posit.cloud",
    "href": "content/week1-welcome.html#introduction-to-posit.cloud",
    "title": "Welcome to STAT218",
    "section": "Introduction to posit.cloud",
    "text": "Introduction to posit.cloud\nYou’ll be learning some basics in R in this class. It’s not a programming class, so the goal is to learn simply to implement the methods we’re covering.\nWhy R?\n\nFree\nOpen source — rapid development for new methods\nWidely used\n\nLet’s take some time to introduce this environment.\nGo to: course webpage &gt; syllabus &gt; materials. Then look for:\n\nTake a moment to create an account."
  },
  {
    "objectID": "content/week1-welcome.html#an-example-project",
    "href": "content/week1-welcome.html#an-example-project",
    "title": "Welcome to STAT218",
    "section": "An example project",
    "text": "An example project\n\n\nMake sure the class workspace “stat218” is highlighted at left. If “Your Workspace” is highlighted, you won’t see the example assignment.\n\n\n\nClick on the example assignment, then wait.\n\nOnce everyone is ready, we’ll have a look at the example files together."
  },
  {
    "objectID": "content/week1-welcome.html#orientation-checklist",
    "href": "content/week1-welcome.html#orientation-checklist",
    "title": "Welcome to STAT218",
    "section": "Orientation checklist",
    "text": "Orientation checklist\n\nOverall layout of RStudio: file navigator, console, file editor.\nREADME.md — keep this open for reference as we go.\n\nAs an aside: whenever you see a file called README, you should read it.\n\nexample-script.R is an R script\n\nhow to execute lines of code\ndata import on L18\nmake and save changes\n\nexample-doc.qmd is a quarto doc\n\nYAML header\nhow to render\nmake and save changes\n\nFiles you should ignore"
  },
  {
    "objectID": "content/week1-welcome.html#for-next-time",
    "href": "content/week1-welcome.html#for-next-time",
    "title": "Welcome to STAT218",
    "section": "For next time",
    "text": "For next time\nYou’ll read about study designs. As you’re reading…\nConsider: there is broad agreement that smoking causes cancer, but not all smokers develop cancer.\n\nWhat then do we mean by “cause” when we say that smoking causes cancer?\nWhat do you imagine the evidence might be for this claim?\nHow would you design a study to test the hypothesis that smoking causes cancer?\n\nWhich of the designs mentioned in the reading seem potentially applicable?\nPick one design that seems best suited to the question. If you were to carry out a study according to that design, what would constitute evidence for or against the hypothesis?\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/syllabus.html",
    "href": "content/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Statistics plays a crucial role in the sciences: statistical techniques provide a means of weighing quantitative evidence derived from observation and experimentation in the face of uncertainty. Statistical thinking and data analysis also facilitate discovery, exploration, and hypothesis generation. Likewise, the sciences play a crucial role in statistics: technological and knowledge innovations in methods of scientific investigation motivate the development of new statistical methods for data analysis.\nThis class aims to provide a hands-on introduction to common statistical methods used almost universally across the sciences — descriptive and graphical techniques, inferential methods for comparing population means, analysis of categorical data and contingency tables, and linear regression — while drawing on examples from the life sciences to help illuminate the potential for application in students’ chosen field(s) of study and providing basic training in the use of statistical software. The class also creates a unique opportunity for students to interact broadly and make connections across majors and class standing.\n\nCourse information\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 05/8273] 2:10pm — 4:00pm TR Construction Innovations Center Room C202\n[Section 06/8274] 4:10pm — 6:00pm TR Business Room 113\n\nClass meetings will comprise a mixture of lecture, lab activities, class activities, and discussion.\nOffice hours: 12:10pm — 2:00pm MW 25-236 or Zoom [by appointment].\nThese times are partitioned into 10 minute intervals that you can schedule via the appointment link above; this system is intended to minimize waiting times and guarantee one-on-one availability. Slots can be scheduled anywhere from 7 calendar days to 10 minutes in advance. While drop-ins are welcome, I can’t guarantee availability outside of scheduled times.\n\n\nCatalog Description\nData collection and experimental design, descriptive statistics, confidence intervals, parametric and non parametric one and two-sample hypothesis tests, analysis of variance, correlation, simple linear regression, chi-square tests. Applications of statistics to the life sciences. Substantial use of statistical software.\nPrerequisite: MATH 96; or MATH 115; or appropriate Math Placement Level.\nFulfills GE Area B4 (GE Area B1 for students on the 2019-20 or earlier catalogs); a grade of C- or better is required in one course in this GE area.\n\n\nMaterials\nTo access course materials, engage in class, and complete assignments, you’ll need an internet-connected (a) laptop or (b) tablet with a keyboard — the keyboard is necessary since we will do some web-hosted computation and you will be expected to type assignments. I’ll let you know in advance when you need to bring a laptop/tablet to class. Besides your personal computer, all materials are free.\nTextbooks:\n\nVu and Harrington (2020). Introdutory Statistics for the Life and Biomedical Sciences, First edition. A PDF and tablet-friendly version are available for free online at the link above. This will be our primary reference and we will cover chapters 1 – 2, 4 – 6, and 8.\nVan Belle, Fisher, Heagerty, and Lumley (2004). Biostatistics: a methodology for the health sciences. Wiley. A PDF can be obtained through the Kennedy Library via the link above. This text provides a thorough introduction to biostatistics (statistics for life sciences) and is an excellent reference for more depth of coverage. Select readings will be assigned from this book.\nDouglas et al. (2023). An Introduction to R. This online book covers a variety of introductory topics pertaining to R/RStudio: installation, packages, files and directories, objects, functions, data types, data structures, graphics, basic statistics, markdown, and version control. Select readings will be assigned from this book.\n\nComputing: computing will be hosted online via a [posit.cloud workspace]. To access the workspace, you’ll need to create a (free) posit.cloud account — open the invitation by clicking the link above and follow prompts. Activities and assignments will be distributed via this workspace. Please be aware that any files you create on the workspace will be visible to admins.\nCourse notes: notes to supplement readings will be posted as needed on the course website.\n\n\nLearning outcomes\nThe course aims to enable you to demonstrate the following abilities.\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each\n[L3] summarize data using graphical and numerical techniques\n[L4] construct and interpret confidence intervals for means and differences between means for independent and paired samples\n[L5] conduct parametric and non-parametric two-sample hypothesis tests for means\n[L6] construct and interpret a confidence interval for a single proportion\n[L7] conduct Chi-square goodness-of-fit tests and tests for independence\n[L8] distinguish between case-control and cohort studies and compute relative-risk and odds in the appropriate settings\n[L9] perform analysis of variance tests and post-hoc comparisons for completely randomized designs\n[L10] use simple linear regression to describe relationships between variables\n[L11] apply one or more methods from the course to your major field of study\n\nIn addition, you will learn to perform simple analyses and computations in R and can expect to attain a basic level of proficiency with the software; however, as this is not a programming class, emphasis will be placed on obtaining and interpreting relevant outputs in the context of the analyses indicated above.\n\n\nAssessments\nAttainment of learning outcomes will be measured by performance on homework assignments, tests, and a final oral exam.\nHomeworks are your opportunity to practice using course concepts and methods covered in class and comprise both graded and ungraded questions, marked as such. Graded questions will be largely data analytic and will correspond to specific marked learning outcomes; responses will be assessed as satisfactory (S) or needing improvement (NI) according to whether they are fully correct, and qualitative feedback will be offered if an assessment of NI is given. Ungraded questions are entirely for your benefit as extra practice. You can submit revisions to any graded responses needing improvement for reassessment.\nTests are your opportunity to demonstrate that you’ve synthesized course material and achieved learning outcomes. Tests will comprise sets of questions, some conceptual and some data analytic, corresponding to specific marked learning outcomes. Responses will be assessed as satisfactory (S) or needing improvement (NI) according to whether they are fully correct, and qualitative feedback will be offered if an assessment of NI is given. You will be given a 24 hour window to complete each test, and you can submit revisions to responses needing improvement.\nThe final oral exam will require you to find an application of course material in the field of your major and present it as a case study in 5 minutes. You will be expected to describe the research question, study design, analysis, and conclusion, and answer 1-2 questions. These will be carried out in private and scheduled during the final exam time. This assignment applies to learning outcome L11 only, and you will receive an assessment of ‘fully met’, ‘partly met’, or ‘unmet’ for that outcome depending on your presentation and ability to answer questions. A rubric will be provided in advance to clearly define the expectations associated with each possible assessment.\n\n\nLetter grades\nStudents will receive a score for each learning outcome representing the weighted proportion of questions matched with that outcome that received a satisfactory assessment across all assignments, with relatively more weight given to questions from tests. The outcome will be determined as ‘fully met’ if the weighted proportion is at least 0.8, ‘partly met’ if the weighted proportion is between 0.5 and 0.8, and ‘unmet’ otherwise. To receive a passing grade in the class, at least six outcomes must be partly met. Letter grades are then defined as follows:\n\n\n\n\n\n\n\n\nGrade\nMinimum number of partly + fully met outcomes\nMinimum number of fully met outcomes\n\n\n\n\nA\n10\n9\n\n\nA-\n10\n8\n\n\nB+\n8\n7\n\n\nB\n8\n6\n\n\nB-\n8\n5\n\n\nC+\n6\n5\n\n\nC\n6\n4\n\n\nC-\n6\n3\n\n\nD+\n6\n2\n\n\nD\n6\n1\n\n\nD-\n6\n0\n\n\n\nPlease note that these definitions are tentative and potentially subject to change. Please also note that failure to adhere to course policies — particularly collaboration, academic integrity, and attendance policies — may result in a lower letter grade than would otherwise be assigned.\n\n\nTentative schedule\nSubject to change.\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings (V&H)\nAssignments\n\n\n\n\n1 (1/8/24)\nIntroduction to statistical thinking and study design\n1.1\nHW1\n\n\n2 (1/15/24)\nData, data types, and data collection\n1.2\nNone\n\n\n3 (1/22/24)\nDescriptive statistics and graphical summaries\n1.4 – 1.6\nHW2\n\n\n4 (1/29/24)\nFoundations for inference\n4.1 – 4.2\nHW2\n\n\n5 (2/5/24)\nOne- and two-sample inference for numerical data\n4.3; 5.1 — 5.3\nTest 1\n\n\n6 (2/12/24)\nNonparametric inference\nTBD\nHW3\n\n\n7 (2/19/24)\nComparing many means with analysis of variance\n5.5\nHW4\n\n\n8 (2/26/24)\nInference for categorical data\n8.1 — 8.4\nTest 2\n\n\n9 (3/4/24)\nSimple linear regression: model framework and estimation\n6.1 — 6.3\nHW5\n\n\n10 (3/11/24)\nSimple linear regression: inference\n6.4 — 6.5\nTest 3\n\n\nFinals (3/18/24)\nN/A\nN/A\nOral exam\n\n\n\n\n\nCourse policies\n\nCollaboration\nCollaboration within the class and across class sections is allowed and encouraged on homework assignments, subject to certain conditions outlined in the paragraph below. Collaborations should not include individuals outside of the class. Students collaborating with a group are expected to prepare their own assignment submissions, in their own words and writing, and should to indicate their collaborators in writing on their submission. This is limited to peers that were consulted closely in completing the assignment; brief or passing interactions are not considered collaborations.\nA collaboration is a shared effort. Students that choose to work together on homework assignments are expected to make material contributions towards producing one or more shared answers or solutions. Material contributions might include participation in discussions, critique of a proposed solution, or presentation of a problem approach. In the absence of such contributions, submitting solutions prepared in a group is not appropriate. The best way to adhere to this policy is to attempt problems individually before consulting others and exchanging work.\n\n\nAttendance\nRegular attendance is essential for success in the course and required per University policy. Each student may miss two class meetings without notice but additional absences should be excusable and students should notify the instructor. Unexcused absences may negatively impact course grades.\n\n\nCommunication and email\nStudents are encouraged to use face-to-face means of communication (office hours, class meetings, appointments) when possible. Email may be used on a secondary basis or when a written record of communication is needed. Every effort is made to respond to email within 48 weekday hours — so a message sent Thursday or Friday may not receive a reply until Monday or Tuesday. Time-sensitive messages should be identified as such in the subject line. For non-time-sensitive messages, please wait one week before sending a reminder.\n\n\nTime commitment\nSTAT218 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including lectures, reading, assignments, and study time. Some variability in workload by week should be expected, and most students will need to budget a few extra hours each week. However, students can expect to be able to meet course expectations with a time commitment of 12-16 hours per week. Considering that lecture accounts for four hours per week, students should anticipate devoting 8-12 hours outside of class.\n\n\nAssignment scores and final grades\nEvery effort will be made to provide consistent, fair, and accurate evaluation of student work. Students should notify the instructor of any suspected errors or discrepancies in evaluation promptly on an assignment-by-assignment basis (i.e., not at the end of the quarter) to guarantee consideration. Final (letter) grades will only be changed in the case of clerical errors. Attempting to negotiate scores or final grades is not appropriate.\nPer University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. If any student feels their grade is unfairly assigned at the end of the course, they have the right to appeal it according to the procedure outlined here.\n\n\nDeadlines and late work\nEach student may turn in two homework assignments up to one week late without penalty at any time during the quarter and without notice. Subsequently, late work will incur a penalty in final grade calculations unless an extension is granted in advance. As a general policy, late work will be not accepted beyond one week after the original due date. Deadlines for tests are strict.\nThese policies are intended to provide you with some flexibility to work around unforeseen circumstances while maintaining accountability for completing coursework in a timely manner. That said, if any circumstances arise that the policies do not accommodate well, please let me know and I will do my best to work with you to keep you on track in the course. Exceptions may be granted for significant and unforeseen challenges (medical absences, family emergencies, and the like).\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nConduct and Academic Integrity\nStudents are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR).\nInstances of academic dishonesty will be reported to OSRR and will result in penalty ranging from credit reduction to grade reduction to course failure, depending on the severity of the situation.\n\n\nCopyright and distribution of course materials\nAll course materials, including handouts, homework assignments, lab assignments, study guides, course notes, exams, and solutions are subject to copyright; students are not permitted to share or distribute any course materials without the written consent of the instructor. This includes, in particular, uploading materials or prepared solutions to online services and sharing materials or prepared solutions with students who may take the course in a future term. Transgressions of this policy compromise the effectiveness of instruction and assessment and do a disservice to current and future students."
  },
  {
    "objectID": "content/week3-multivariate.html#todays-agenda",
    "href": "content/week3-multivariate.html#todays-agenda",
    "title": "Bivariate summaries",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nHW discussion\nReading quiz [2pm section] [4pm section]\n(Univariate) Measures of spread\nGraphical summmaries for two variables\n\nnumeric/numeric\nnumeric/categorical\ncategorial/categorical\n\nLab: bivariate graphics in R"
  },
  {
    "objectID": "content/week3-multivariate.html#measures-of-spread",
    "href": "content/week3-multivariate.html#measures-of-spread",
    "title": "Bivariate summaries",
    "section": "Measures of spread",
    "text": "Measures of spread\nThe spread of observations refers to how concentrated or diffuse the values are.\n\nTwo ways to understand and measure spread:\n\nranges of values capturing much of the distribution\ndeviations of values from a central value"
  },
  {
    "objectID": "content/week3-multivariate.html#range-based-measures",
    "href": "content/week3-multivariate.html#range-based-measures",
    "title": "Bivariate summaries",
    "section": "Range-based measures",
    "text": "Range-based measures\nA simple way to understand and measure spread is based on ranges. Consider more ages, sorted and ranked:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\nrank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\nThe range is the difference [maximum value] - [minimum value] \\[\\text{range} = 34 - 16 = 18\\]\nThe interquartile range (IQR) is the difference [75th percentile] - [25th percentile] \\[\\text{IQR} = 29 - 19 = 10\\] When might you prefer IQR to range? Can you think of an example?"
  },
  {
    "objectID": "content/week3-multivariate.html#deviation-based-measures",
    "href": "content/week3-multivariate.html#deviation-based-measures",
    "title": "Bivariate summaries",
    "section": "Deviation-based measures",
    "text": "Deviation-based measures\nAnother way is based on deviations from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n16\n18\n19\n20\n21\n22\n25\n26\n28\n29\n30\n34\n\n\ndeviation\n-8\n-6\n-5\n-4\n-3\n-2\n1\n2\n4\n5\n6\n10\n\n\n\n\n\nThe average deviation is defined as the average of the absolute values of the deviations from the mean: \\[\\frac{8 + 6 + 5 + 4 + 3 + 2 + 1 + 2 + 4 + 5 + 6}{12}\\]\nThe standard deviation is defined in terms of the squared deviations from the mean: \\[\\sqrt{\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2}{12 - 1}}\\]"
  },
  {
    "objectID": "content/week3-multivariate.html#mathematical-notations",
    "href": "content/week3-multivariate.html#mathematical-notations",
    "title": "Bivariate summaries",
    "section": "Mathematical notations",
    "text": "Mathematical notations\n\n\nDenote \\(n\\) observations of a variable \\(x\\) by \\(x_1, \\dots, x_n\\), so that \\(x_i\\) indicates the value of the \\(i\\)th observation. Our ages:\n\n\n16, 18, 19, 20, 21, 22, 25, 26, 28, 29, 30 and 34\n\n\nApplying the notation at right:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(x_i\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_i - \\bar{x}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\((x_i - \\bar{x})^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean of the observations is written: \\[\\bar{x} = \\frac{1}{n}\\sum_i x_i\\]\nThe standard deviation is: \\[s_x = \\sqrt{\\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2}\\]\n\n\nHow would you write the formula for calculating average deviation using this notation?"
  },
  {
    "objectID": "content/week3-multivariate.html#interpretations",
    "href": "content/week3-multivariate.html#interpretations",
    "title": "Bivariate summaries",
    "section": "Interpretations",
    "text": "Interpretations\nListed from largest to smallest, here are each of the measures of spread for the 12 ages:\n\n\n\n\n\n\n\n\n\n\n\nrange\niqr\nst.dev\navg.dev\n\n\n\n\n18\n8.5\n5.527\n4.667\n\n\n\n\n\nThe interpretations differ between these statistics:\n\n[range] all of the data lies on an interval of 18 years\n[IQR] the middle half of the data lies on an interval of 8.5 years\n[average deviation] the average distance from the mean is 4.67 years\n[standard deviation] the average squared distance from the mean, rescaled to years, is 5.53 years"
  },
  {
    "objectID": "content/week3-multivariate.html#robustness",
    "href": "content/week3-multivariate.html#robustness",
    "title": "Bivariate summaries",
    "section": "Robustness",
    "text": "Robustness\nThe IQR is more robust than any of the other measures, because outliers only affect extreme percentiles.\n\n\nConsider adding an observation of 94 to our 12 ages:\n\n# initial range\nrange(ages)\n\n[1] 16 34\n\n# append an outlier\nages_add &lt;- c(ages, 94)\n\n# relative change in IQR\n(IQR(ages_add) - IQR(ages))/IQR(ages)\n\n[1] 0.05882353\n\n# relative change in SD\n(sd(ages_add) - sd(ages))/sd(ages)\n\n[1] 2.640935\n\n\n\nThe effect of the outlier on each measure is captured by the ratio \\(\\frac{\\text{measure with outlier}}{\\text{measure without outlier}}\\), which shows:\n\nthe IQR increases by 5.88%\nthe standard deviation increases by 264%"
  },
  {
    "objectID": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "href": "content/week3-multivariate.html#limitations-of-univariate-summaries",
    "title": "Bivariate summaries",
    "section": "Limitations of univariate summaries",
    "text": "Limitations of univariate summaries\nSo far we have discussed univariate descriptive techniques — those that pertain to one variable at a time. Consider, for example, the height and weight of participants in the FAMuSS study:\n\n\n\n\n\n\n\n\n\nboth unimodal, no obvious outliers\nheights symmetric\nweights right-skewed\nbut these observations actually come in pairs\n\n\n\nThe summaries we know how to make don’t reflect how the variables might be related."
  },
  {
    "objectID": "content/week3-multivariate.html#bivariate-summaries",
    "href": "content/week3-multivariate.html#bivariate-summaries",
    "title": "Bivariate summaries",
    "section": "Bivariate summaries",
    "text": "Bivariate summaries\nBivariate (and by extension multivariate) summaries are graphical or numerical descriptions that represent two (or more) variables jointly.\n\n\nA simple example is a scatterplot:\n\n\n\n\n\n\nEach point represents a pair of values \\((h, w)\\) for one study participant.\n\nReveals a relationship: taller participants tend to be heavier\nBut no longer shows individual distributions clearly\n\nNotice, though, that the marginal means (dashed red lines) still capture the center well."
  },
  {
    "objectID": "content/week3-multivariate.html#summary-types",
    "href": "content/week3-multivariate.html#summary-types",
    "title": "Bivariate summaries",
    "section": "Summary types",
    "text": "Summary types\nBivariate summary techniques differ depending on the data types of the variables being compared. Some examples the context of the FAMuSS study:\n\n\n\n\n\n\n\nQuestion\nComparison\n\n\n\n\nDid genotype frequencies differ by race or sex among study participants?\ncategorical/categorical\n\n\nWere differential changes in arm strength observed according to genotype?\nnumeric/categorical\n\n\nDid change in arm strength appear related in any way to body size among study participants?\nnumeric/numeric\n\n\nDid study participants experience similar or different changes in arm strength depending on arm dominance?\n??"
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical",
    "href": "content/week3-multivariate.html#categoricalcategorical",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\n\nA contingency table is a bivariate tabular summary of two categorical variables; it shows the frequency of each pair of values. Usually the marginal totals are also shown.\n\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\ntotal\n\n\n\n\nFemale\n106\n149\n98\n353\n\n\nMale\n67\n112\n63\n242\n\n\ntotal\n173\n261\n161\n595\n\n\n\n\n\nThere are multiple ways to convert to proportions by using different denominators:\n\ngrand total\nrow total\ncolumn total\n\nEach has a different interpretation and should be chosen according to the question of interest."
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-1",
    "href": "content/week3-multivariate.html#categoricalcategorical-1",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid genotype frequencies differ by sex among study participants?\nFor this question, the row totals should be used to see the genotype composition of each sex.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.3003\n0.4221\n0.2776\n\n\nMale\n0.2769\n0.4628\n0.2603\n\n\n\n\n\n\nAs a graphic:\n\n\n\n\n\n\n\nThe proportions are quite close, suggesting minimal sex differences."
  },
  {
    "objectID": "content/week3-multivariate.html#categoricalcategorical-2",
    "href": "content/week3-multivariate.html#categoricalcategorical-2",
    "title": "Bivariate summaries",
    "section": "Categorical/categorical",
    "text": "Categorical/categorical\nDid sex frequencies differ by genotype among study participants?\nFor this question, the column totals should be used to see the sex composition of each genotype.\n\n\nAs a table:\n\n\n\n\n\n\n\n\n\n\n\n \nCC\nCT\nTT\n\n\n\n\nFemale\n0.6127\n0.5709\n0.6087\n\n\nMale\n0.3873\n0.4291\n0.3913\n\n\n\n\n\n\nAs a graphic:\n\n\n\n\n\n\n\nThe proportions are close, suggesting minimal genotype differences."
  },
  {
    "objectID": "content/week3-multivariate.html#numericnumeric",
    "href": "content/week3-multivariate.html#numericnumeric",
    "title": "Bivariate summaries",
    "section": "Numeric/numeric",
    "text": "Numeric/numeric\nDid change in arm strength appear related in any way to body size among study participants?\nComparing numeric variables is easiest accomplished by scatterplots."
  },
  {
    "objectID": "content/week3-multivariate.html#correlation",
    "href": "content/week3-multivariate.html#correlation",
    "title": "Bivariate summaries",
    "section": "Correlation",
    "text": "Correlation\nIn addition to graphical techniques, for numeric/numeric comparisons, there are also quantiative measures of relationship.\n\n\n\n\nCorrelation measures the strength of linear relationship, and is defined as: \\[r_{xy} = \\frac{1}{n - 1}\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\\]\n\n\\(r \\rightarrow 1\\): positive relationship\n\\(r \\rightarrow -1\\): negative relationship\n\\(r \\rightarrow 0\\): no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "href": "content/week3-multivariate.html#uncorrelated-neq-no-relationship",
    "title": "Bivariate summaries",
    "section": "Uncorrelated \\(\\neq\\) no relationship",
    "text": "Uncorrelated \\(\\neq\\) no relationship\nCorrelation only captures linear relationships. Always do a graphical check.\n\nCommon misconceptions:\n\nstronger correlation \\(\\longleftrightarrow\\) greater slope\nweaker correlation \\(\\longleftrightarrow\\) no relationship"
  },
  {
    "objectID": "content/week3-multivariate.html#interpreting-correlations",
    "href": "content/week3-multivariate.html#interpreting-correlations",
    "title": "Bivariate summaries",
    "section": "Interpreting correlations",
    "text": "Interpreting correlations\nDid change in arm strength appear related in any way to body size among study participants?\nHere are the correlations corresponding to the plots we checked earlier.\n\n\n\n\n\n\n\n\n\n\n\n \nheight\nweight\nbmi\n\n\n\n\ndrm.ch\n-0.1104\n-0.1159\n-0.07267\n\n\nndrm.ch\n-0.265\n-0.2529\n-0.1436\n\n\n\n\n\nSo there aren’t any linear relationships here. A rule of thumb:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\\(|r| = 1\\): either a mistake or not real data"
  },
  {
    "objectID": "content/week3-multivariate.html#numericcategorical",
    "href": "content/week3-multivariate.html#numericcategorical",
    "title": "Bivariate summaries",
    "section": "Numeric/categorical",
    "text": "Numeric/categorical\nSide-by-side boxplots are usually a good option. Avoid stacked histograms.\nWere differential changes in arm strength observed according to genotype?\n\n\n\n\n\n\n\n\nLook for differences:\n\nlocation shift\nspread\ncenter\n\nWhat do you think? Any notable relationships?\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week1-studydesigns.html#what-is-a-study",
    "href": "content/week1-studydesigns.html#what-is-a-study",
    "title": "Study designs",
    "section": "What is a study?",
    "text": "What is a study?\nA study is an effort to collect data in order to answer one or more research questions.\nUltimately, you’re in this class to learn how to answer questions (or evaluate the answers of others) in a sound way, so we will start with the basics of study design because:\n\nstudies must be well-matched to research questions to provide good answers\nhow data are obtained is just as important as how the resulting data are analyzed\nno analysis, no matter how sophisticated will rescue a poorly conceived study\n\n[Terminology] A study unit is the smallest object or entity that is measured in a study; also called experimental unit or observational unit."
  },
  {
    "objectID": "content/week1-studydesigns.html#two-types-of-studies",
    "href": "content/week1-studydesigns.html#two-types-of-studies",
    "title": "Study designs",
    "section": "Two types of studies",
    "text": "Two types of studies\nObservational studies collect data from an existing situation without intervention.\n\nAim is to detect associations and patterns\nCan’t be used to establish causal links (more on this later)\n\nExperiments collect data from a situation in which one or more interventions have been introduced by the investigator.\n\nAim is to draw conclusions about the effect of one or more interventions\nStronger form of scientific evidence than an observational study\n\nOften, observational studies are used to explore/generate hypotheses prior to designing an experiment."
  },
  {
    "objectID": "content/week1-studydesigns.html#comparing-study-types",
    "href": "content/week1-studydesigns.html#comparing-study-types",
    "title": "Study designs",
    "section": "Comparing study types",
    "text": "Comparing study types\nEither type of study can be used to address a question.\n\n\n\n\n\n\n\n\nQuestion\nObservational study\nExperiment\n\n\n\n\nAre diet and mood related?\nConduct surveys on diet, lifestyle, and affect\nRecruit study participants, assign diets, measure affect\n\n\nIs vaping safer than smoking?\nFollow groups of vapers and smokers over time and record health outcomes\nAmong a group of smokers, assign some to switch to vaping; compare health outcomes over time\n\n\nDo insecticide applications affect soil microbes?\nAnalyze soil samples from farms using different insecticides\n[Your turn]\n\n\n\nIn each example, which approach would you choose if you were the researcher? What are the pros and cons?"
  },
  {
    "objectID": "content/week1-studydesigns.html#why-does-intervention-matter",
    "href": "content/week1-studydesigns.html#why-does-intervention-matter",
    "title": "Study designs",
    "section": "Why does intervention matter?",
    "text": "Why does intervention matter?\nIn an experiment, the researcher intentionally intervenes in a way that they expect to change outcomes.\n\nintervention confers a degree of control over study conditions\n\n\nIn an observational study, there is no (intentional) intervention.\n\nwithout intervention the researcher has no control over study conditions\n\n\nControl over conditions allows a researcher to study causal effects resulting from interventions. This can best be understood in terms of a phenomenon known as confounding."
  },
  {
    "objectID": "content/week1-studydesigns.html#confounding",
    "href": "content/week1-studydesigns.html#confounding",
    "title": "Study designs",
    "section": "Confounding",
    "text": "Confounding\nConfounding occurs when two conditions cannot be differentiated.\nThis happens when either:\n\n[omission] one study condition is not measured\n\na researcher finds that dog owners live longer, but failed to measure how much exercise each subject gets\nis it the dog, or the daily walks?\n\n[conflation] two study conditions are always observed simultaneously\n\na researcher finds that people on diet 1 tend to be overweight, but all study participants on diet 1 are from the U.S., and all study participants on diet 2 are from Canada\nis it the diet, or the country?"
  },
  {
    "objectID": "content/week1-studydesigns.html#consequences-of-confounding",
    "href": "content/week1-studydesigns.html#consequences-of-confounding",
    "title": "Study designs",
    "section": "Consequences of confounding",
    "text": "Consequences of confounding\nUsually, researchers are concerned with confounding by omission.\n\n\nWhen an unobserved condition is associated with both the observed condition and the outcome one may observe:\n\nan artificial association between the study condition and outcome\na distorted association between the study condition and outcome\n\n\n\n\n\n\nflowchart TD\n  A(unobserved condition) --- B(study condition) & C(outcome) \n\n\n\n\n\n\n\nThis is very common in observational studies, because you can’t measure every study condition."
  },
  {
    "objectID": "content/week1-studydesigns.html#example-of-confounding",
    "href": "content/week1-studydesigns.html#example-of-confounding",
    "title": "Study designs",
    "section": "Example of confounding",
    "text": "Example of confounding\n\n\n[left] Higher state educational expenditure looks associated with lower SAT scores\n[center] But increased expenditure looks associated with more test takers\n[right] And higher percentages of test takers are associated with lower averages\n\nIf you ignored how many students take the test, you’d conclude that expenditure is counterproductive"
  },
  {
    "objectID": "content/week1-studydesigns.html#antidote-randomization",
    "href": "content/week1-studydesigns.html#antidote-randomization",
    "title": "Study designs",
    "section": "Antidote: randomization",
    "text": "Antidote: randomization\nThe ability to control study conditions allows researchers to randomize interventions among study subjects:\n\nallocated in such a way that all subjects are equally likely to receive any intervention\n\n\nRandomization eliminates confounding by isolating the condition(s) of interest:\n\ninterventions are independent of extraneous conditions ⟹ no association possible\nif outcomes differ systematically according to the intervention, you can be certain that it is not an artifact\n\n\n\n\n\n\nflowchart TD\n  A(unobserved condition) x-.-x B(study condition)\n  A --- C(outcome)\n\n\n\n\n\n\n\nExtraneous conditions may still be associated with outcomes and add noise — well-thought-out experiments find ways to reduce this."
  },
  {
    "objectID": "content/week1-studydesigns.html#practical-consequences",
    "href": "content/week1-studydesigns.html#practical-consequences",
    "title": "Study designs",
    "section": "Practical consequences",
    "text": "Practical consequences\nThe ability to randomize interventions in experiments means:\n\nobserved associations are independent of extraneous factors\nresults can support causal inferences\n\nThe absence of randomization in observational studies means:\n\nconfounding is always possible\nresults may be misleading\n\nUsually, observational studies are considered weaker forms of scientific evidence than experiments, with some caveats:\n\na large volume of independent observational studies pointing to the same conclusions may be more convincing than a few narrow experiments\nobservational studies may provide better evidence under real world conditions than available experimental data"
  },
  {
    "objectID": "content/week1-studydesigns.html#common-study-designs",
    "href": "content/week1-studydesigns.html#common-study-designs",
    "title": "Study designs",
    "section": "Common study designs",
    "text": "Common study designs\nStudies can be designed to investigate outcomes in a variety of ways, regardless of whether study conditions include intervention. Many study designs originate in clinical medicine — the following are among the most common.\n\n\nIn prospective studies participants are recruited before developing a condition\n\n\nIn retrospective studies participants are recruited after developing a condition\n\n\n\nThese terms usually refer to observational studies; experiments are typically prospective in nature, since one recruits participants before administering an intervention."
  },
  {
    "objectID": "content/week1-studydesigns.html#example-study",
    "href": "content/week1-studydesigns.html#example-study",
    "title": "Study designs",
    "section": "Example study",
    "text": "Example study\nImagine following this year’s Cal Poly freshmen (a “cohort”) and comparing four-year graduation (outcome) rates between those that participated in a new orientation activity and those that didn’t. This would be:\n\nprospective if an observational study\n\n\n\nan experiment if students were assigned to participate or not\n\nCan you change the example so that it’s a retrospective study?"
  },
  {
    "objectID": "content/week1-studydesigns.html#experimental-designs",
    "href": "content/week1-studydesigns.html#experimental-designs",
    "title": "Study designs",
    "section": "Experimental designs",
    "text": "Experimental designs\nA treatment is an experimental intervention; the design of an experiment refers to how treatments are allocated to study units.\nThe most basic design is:\n\n[balanced] each treatment is replicated an equal number of times\n[randomized] treatments are allocated completely at random to study units\n[no crossover] each study unit receives exactly one treatment\n\nWe’ll call this a completely randomized design. It’s the only kind of experimental design we’re going to consider in STAT218.\nThere are many other designs that we won’t discuss (but see STAT313); these are all about improving experimental efficiency by controlling extraneous variation."
  },
  {
    "objectID": "content/week1-studydesigns.html#blinding",
    "href": "content/week1-studydesigns.html#blinding",
    "title": "Study designs",
    "section": "Blinding",
    "text": "Blinding\nThe terms single-blind and double-blind refer to experiments with human subjects, typically in clinical studies.\nIn a single-blind study, participants are unaware of which treatment they receive.\n\nExample: in a case-control study, one group of patients is given a new medicine and the other is given a saline solution administered under identical conditions\nIdea: controls for placebo effect\n\nIn a double-blind study, both participants and investigators are unaware of which treatments are administered.\n\nExample: same as above, but now the person administering the doses is unaware of which are medicine and which are saline\nIdea: controls for unconscious influence on outcome by researchers (e.g., attend more carefully and compassionately to patients receiving medicine out of concern for side-effects)"
  },
  {
    "objectID": "content/week1-studydesigns.html#data-collection",
    "href": "content/week1-studydesigns.html#data-collection",
    "title": "Study designs",
    "section": "Data collection",
    "text": "Data collection\nWe will discuss data collection in more depth later, but the way that study units are obtained, independently of the type of study or design, determines the breadth of conclusions supported by the results, depending on whether study units chosen are representative of a larger collection.\nThe gold standard is a simple random sample of units selected from a larger collection:\n\nevery unit in the larger collection has an equal chance of being selected\neasier said than done — failure to obtain a random sample is one of the most common study flaws\n\nFor now we will set the issue of sampling aside."
  },
  {
    "objectID": "content/week1-studydesigns.html#considerations-in-creating-a-study",
    "href": "content/week1-studydesigns.html#considerations-in-creating-a-study",
    "title": "Study designs",
    "section": "Considerations in creating a study",
    "text": "Considerations in creating a study\n\nThe research question should be motivated by science, not statistics.\nYou may have to work within limitations:\n\nThe design that best aligns with the question may not be possible or practical.\nAn experiment might give the strongest evidence but not be feasible or ethical.\n\nA plan should be in place for all of the following:\n\nhow to obtain/recruit study units/participants\nhow many to obtain/recruit and how to ensure that target is met\nprotocol for allocating treatment\ncollecting data and measuring outcomes\ncontingencies for dropout/failure to ensure the study yields results\ndata handling and data analysis\nhow results will be used"
  },
  {
    "objectID": "content/week1-studydesigns.html#study-design-in-stat218",
    "href": "content/week1-studydesigns.html#study-design-in-stat218",
    "title": "Study designs",
    "section": "Study design in STAT218",
    "text": "Study design in STAT218\nIn STAT218 you are expected to be fluent with study design concepts, but of course won’t actually carry out an end-to-end study.\n\n[L1] design a data collection scheme based on simple random sampling or simple experimental designs\n[L2] distinguish between observational studies and experiments and understand the limitations (practical and consequential) of each"
  },
  {
    "objectID": "content/week1-studydesigns.html#case-study-leap",
    "href": "content/week1-studydesigns.html#case-study-leap",
    "title": "Study designs",
    "section": "Case study: LEAP",
    "text": "Case study: LEAP\nLearning early about peanut allergy (LEAP) study:\n\n640 infants in UK with eczema, egg allergy, or both enrolled; 530 passed a skin test showing no peanut allergy\neach infant randomly assigned to peanut consumption and peanut avoidance groups\n\npeanut consumption: fed 6g peanut protein daily until 5 years old\npeanut avoidance: no peanut consumption until 5 years old\n\nat 5 years old, oral food challenge (OFC) allergy test administered\n\nPASS: no allergy detected\nFAIL: allergy detected\n\n\nWhat kind of study is this? Experiment or observational study? Retrospective, prospective, or neither?"
  },
  {
    "objectID": "content/week1-studydesigns.html#case-study-leap-1",
    "href": "content/week1-studydesigns.html#case-study-leap-1",
    "title": "Study designs",
    "section": "Case study: LEAP",
    "text": "Case study: LEAP\n\n\n\n\n\nTabular summary of LEAP results.\n\n\n\n\n\n\nGraphical summary of LEAP results.\n\n\n\n\n\nDoes peanut avoidance/consumption appear associated with developing an allergy?\nIf so, does the study provide evidence that early exposure likely reduces the chance of developing an allergy by 5 years of age?\nWhat calculation could you perform to summarize the difference between study groups?\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week3-descriptive.html#last-time",
    "href": "content/week3-descriptive.html#last-time",
    "title": "Descriptive statistics",
    "section": "Last time",
    "text": "Last time\n\n\n\nData semantics\n\n\ncategorical data: ordinal (ordered) or nominal (unordered)\nnumeric data: continuous (no ‘gaps’) or discrete (‘gaps’)\n\n\nData types and data structures in R\n\n\nbasic types: numeric, character, logical, integer\na vector is a collection of values of one type\na data frame is a type-heterogeneous list of vectors of equal length\n\n\nVectors can store observations of one variable:\n\n# 4 observations of age\nages &lt;- c(18, 22, 18, 12)\nages\n\n[1] 18 22 18 12\n\n\nData frames can store observations of many variables:\n\n# 3 observations of 2 variables\ndata.frame(subject.id = c(11, 2, 31),\n           age = c(24, 31, 17),\n           sex = c('m', 'm', 'f'))\n\n  subject.id age sex\n1         11  24   m\n2          2  31   m\n3         31  17   f\n\n\n\n\nTechniques for summarizing data depend on the data type"
  },
  {
    "objectID": "content/week3-descriptive.html#todays-agenda",
    "href": "content/week3-descriptive.html#todays-agenda",
    "title": "Descriptive statistics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nGraphical and tabular summaries for numeric and categorical data\n\nfrequency distributions\nbarplots\nhistograms\n\nQuantitative summaries for numeric data\n\npercentiles\nmeasures of center\nmeasures of spread\n\nLab exploring descriptive statistics and robustness"
  },
  {
    "objectID": "content/week3-descriptive.html#what-are-descriptive-statistics",
    "href": "content/week3-descriptive.html#what-are-descriptive-statistics",
    "title": "Descriptive statistics",
    "section": "What are descriptive statistics?",
    "text": "What are descriptive statistics?\nDescriptive statistics are quantitative summaries of the observations of one or more variables. They usually serve one of three aims:\n\nIdentify typical or “central” values\nCharacterize the variability or “spread” of values\nCharacterize relationships between variables\n\nDescriptive statistics are often accompanied by graphical summaries that aid in visualizing these same characteristics."
  },
  {
    "objectID": "content/week3-descriptive.html#todays-example-data-famuss",
    "href": "content/week3-descriptive.html#todays-example-data-famuss",
    "title": "Descriptive statistics",
    "section": "Today’s example data: FAMuSS",
    "text": "Today’s example data: FAMuSS\nObservational study of 595 individuals comparing change in arm strength before and after resistance training between genotypes for a region of interest on the ACTN3 gene.\n\nPescatello, L. S., et al. (2013). Highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or FAMuSS study. BioMed research international, 2013.\n\n\n\n\nExample data rows\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n40\n40\nFemale\n27\nCaucasian\n65\n199\nCC\n33.11\n\n\n25\n0\nMale\n36\nCaucasian\n71.7\n189\nCT\n25.84\n\n\n40\n0\nFemale\n24\nCaucasian\n65\n134\nCT\n22.3\n\n\n125\n0\nFemale\n40\nCaucasian\n68\n171\nCT\n26"
  },
  {
    "objectID": "content/week3-descriptive.html#categorical-frequency-distributions",
    "href": "content/week3-descriptive.html#categorical-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Categorical frequency distributions",
    "text": "Categorical frequency distributions\nFor categorical variables, the frequency distribution is simply an observation count by category. For example:\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\ngenotype\n\n\n\n\n494\nTT\n\n\n510\nTT\n\n\n216\nCT\n\n\n19\nTT\n\n\n278\nCT\n\n\n86\nTT\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n173\n261\n161"
  },
  {
    "objectID": "content/week3-descriptive.html#numeric-frequency-distributions",
    "href": "content/week3-descriptive.html#numeric-frequency-distributions",
    "title": "Descriptive statistics",
    "section": "Numeric frequency distributions",
    "text": "Numeric frequency distributions\nFrequency distributions of numeric variables are observation counts by range.\n\n\n\n\n\nData table\n\n\n\n\n\n\nparticipant.id\nbmi\n\n\n\n\n194\n22.3\n\n\n141\n20.76\n\n\n313\n23.48\n\n\n522\n29.29\n\n\n504\n42.28\n\n\n273\n20.34\n\n\n\n\n\n\n\n\n\nFrequency distribution\n\n\n\n\n\n\n\n\n(10,20]\n(20,30]\n(30,40]\n(40,50]\n\n\n\n\n69\n461\n58\n7\n\n\n\n\n\n\n\n\n\n\nThe operation of dividing a numeric variable into interval ranges is called binning."
  },
  {
    "objectID": "content/week3-descriptive.html#histograms",
    "href": "content/week3-descriptive.html#histograms",
    "title": "Descriptive statistics",
    "section": "Histograms",
    "text": "Histograms\nThe graphical display of a frequency distribution for a numeric variable is called a histogram. Binning has a big effect on the visual impression."
  },
  {
    "objectID": "content/week3-descriptive.html#shapes",
    "href": "content/week3-descriptive.html#shapes",
    "title": "Descriptive statistics",
    "section": "Shapes",
    "text": "Shapes\nFor numeric variables, the histogram reveals the shape of the distribution:\n\nsymmetric if it shows left-right symmetry about a central value\nskewed if it stretches farther in one direction from a central value"
  },
  {
    "objectID": "content/week3-descriptive.html#modes",
    "href": "content/week3-descriptive.html#modes",
    "title": "Descriptive statistics",
    "section": "Modes",
    "text": "Modes\nHistograms also reveal the number of modes or local peaks of frequency distributions.\n\nuniform if there are zero peaks\nunimodal if there is one peak\nbimodal if there are two peaks\nmultimodal if there are two or more peaks"
  },
  {
    "objectID": "content/week3-descriptive.html#your-turn-characterizing-distributions",
    "href": "content/week3-descriptive.html#your-turn-characterizing-distributions",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nConsider four variables from the FAMuSS study. Describe the shape and modality."
  },
  {
    "objectID": "content/week3-descriptive.html#your-turn-characterizing-distributions-1",
    "href": "content/week3-descriptive.html#your-turn-characterizing-distributions-1",
    "title": "Descriptive statistics",
    "section": "Your turn: characterizing distributions",
    "text": "Your turn: characterizing distributions\nHere are some made-up data. Describe the shape and modality."
  },
  {
    "objectID": "content/week3-descriptive.html#descriptive-measures",
    "href": "content/week3-descriptive.html#descriptive-measures",
    "title": "Descriptive statistics",
    "section": "Descriptive measures",
    "text": "Descriptive measures\nFrequency distributions are great for many purposes but they have limitations:\n\nminimal “data reduction”, especially for many bins/categories\nsensitive to choice of binning\nperception of pattern is subjective\n\nDescriptive measures, by contrast, reduce all observations of a variable down to just one number. There are two common types of measures:\n\nmeasures of center: mean, median, mode\nmeasures of spread: absolute deviation, standard deviation, interquartile range, range"
  },
  {
    "objectID": "content/week3-descriptive.html#measures-of-center",
    "href": "content/week3-descriptive.html#measures-of-center",
    "title": "Descriptive statistics",
    "section": "Measures of center",
    "text": "Measures of center\nA measure of center is a statistic that reflects the typical value of one or more variables.\n\n\nThere are three common measures of center, each of which corresponds to a slightly different meaning of “typical”:\n\n\n\nMeasure\nDefinition\n\n\n\n\nMode\nMost frequent value\n\n\nMean\nAverage value\n\n\nMedian\nMiddle value\n\n\n\n\nSuppose your data consisted of the following observations of age in years:\n\n\n19, 19, 21, 25 and 31\n\n\n\nthe mode or most frequent value is 19\nthe median or middle value is 21\nthe mean or average value is \\(\\frac{19 + 19 + 21 + 25 + 31}{5}\\) = 23\n\n\n\nThese measures are only used with numeric variables."
  },
  {
    "objectID": "content/week3-descriptive.html#your-turn",
    "href": "content/week3-descriptive.html#your-turn",
    "title": "Descriptive statistics",
    "section": "Your turn",
    "text": "Your turn\nConsider the first 8 observations of change in nondominant arm strength from the FAMuSS study data:\n\n\n40, 25, 40, 125, 40, 75, 100 and 57.1\n\n\nCompute the mean, median, and mode."
  },
  {
    "objectID": "content/week3-descriptive.html#comparing-measures-of-center",
    "href": "content/week3-descriptive.html#comparing-measures-of-center",
    "title": "Descriptive statistics",
    "section": "Comparing measures of center",
    "text": "Comparing measures of center\nEach statistic is a little different, but often they roughly agree; for example, all are between 20 and 25, which seems to capture the typical age well enough.\n\nHow do you think the frequency distribution affects which one is “best”?"
  },
  {
    "objectID": "content/week3-descriptive.html#percentiles",
    "href": "content/week3-descriptive.html#percentiles",
    "title": "Descriptive statistics",
    "section": "Percentiles",
    "text": "Percentiles\nThe median is an example of a percentile: a value with specified proportions of data lying both above and below. For example, the 20th percentile is the value with 20% of observations below and 80% of observations above.\nRanking observations helps to find this number. Suppose we have 5 observations:\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\n19\n20\n21\n25\n31\n\n\nrank\n1\n2\n3\n4\n5\n\n\n\n\n\nThe 20th percentile is 20 since it is ranked second when observations are listed in order:\n\n20% below (19, 20)\n80% above (20, 21, 25, 31)\n\n\nSoftware implementations have a variety of ways for calculating percentiles when an exact solution isn’t available due to ties (repeated values) or sample size."
  },
  {
    "objectID": "content/week3-descriptive.html#cumulative-frequency-distribution",
    "href": "content/week3-descriptive.html#cumulative-frequency-distribution",
    "title": "Descriptive statistics",
    "section": "Cumulative frequency distribution",
    "text": "Cumulative frequency distribution\nThe cumulative frequency distribution is a function showing all percentiles with an exact solution. Think of it as percentile (y) against value (x).\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of some specific values:\n\nabout 40% of the subjects are 20 or younger\nabout 80% of the subjects are 24 or younger\n\nYour turn:\n\nRoughly what percentage of subjects are 22 or younger?\nAbout what age is the 10th percentile?"
  },
  {
    "objectID": "content/week3-descriptive.html#common-percentiles",
    "href": "content/week3-descriptive.html#common-percentiles",
    "title": "Descriptive statistics",
    "section": "Common percentiles",
    "text": "Common percentiles\n\n\nThe five-number summary is a collection of five percentiles that succinctly describe the frequency distribution:\n\n\n\nStatistic name\nMeaning\n\n\n\n\nminimum\n0th percentile\n\n\nfirst quartile\n25th percentile\n\n\nmedian\n50th percentile\n\n\nthird quartile\n75th percentile\n\n\nmaximum\n100th percentile\n\n\n\n\nBoxplots provide a graphical display of the five-number summary."
  },
  {
    "objectID": "content/week3-descriptive.html#boxplots-vs.-histograms",
    "href": "content/week3-descriptive.html#boxplots-vs.-histograms",
    "title": "Descriptive statistics",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\nNotice how the two displays align, and also how they differ. The histogram shows shape in greater detail, but the boxplot is much more compact."
  },
  {
    "objectID": "content/week3-descriptive.html#boxplots-vs.-histograms-1",
    "href": "content/week3-descriptive.html#boxplots-vs.-histograms-1",
    "title": "Descriptive statistics",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\nSuppose we wanted to compare the change in dominant arm strength with the change in nondominant arm strength in the FAMuSS (sports gene) study.\nThe boxplot is a cleaner display due to its compactness."
  },
  {
    "objectID": "content/week3-descriptive.html#lab-robustness",
    "href": "content/week3-descriptive.html#lab-robustness",
    "title": "Descriptive statistics",
    "section": "Lab: robustness",
    "text": "Lab: robustness\nFor this lab we’ll continue to work with the FAMuSS data as we have throughout lecture.\nThis lab has two objectives:\n\nTeach you to compute descriptive statistics and prepare graphical summaries for a single variable in R\nLearn when and why to use certain descriptive statistics in place of others"
  },
  {
    "objectID": "content/week3-descriptive.html#up-next-multivariate-summaries",
    "href": "content/week3-descriptive.html#up-next-multivariate-summaries",
    "title": "Descriptive statistics",
    "section": "Up next: multivariate summaries",
    "text": "Up next: multivariate summaries\nToday we discussed numeric and graphical summaries for a single variable. These are univariate techniques.\n\nWill reveal basic statistical properties (shape, skew, outliers)\nWon’t reveal relationships\n\nWhat if you wish to understand relationships? The fact is, most data are multivariate because several variables are measured together.\nNext time we’ll discuss\n\nMeasures of spread/variability\nBivariate descriptive and graphical techniques\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/labs/lab3-multivariate.html",
    "href": "content/labs/lab3-multivariate.html",
    "title": "Lab 3: More descriptive statistics",
    "section": "",
    "text": "This lab covers two separate topics: measures of spread and bivariate graphical summaries. There are two goals for the activity:\n\nlearn to calculate measures of spread (IQR and standard deviation) and explore their robustness to outliers\nlearn to produce joint summaries of two variables for identifying relationships\n\ncontingency tables\nproportional barplots\nscatterplots\nside-by-side boxplots\n\n\nWe will use the FAMuSS dataset again.\n\n# openintro biostat package\nlibrary(oibiostat)\n\n# famuss data\ndata(famuss)\n\n\nRobustness and measures of spread\nLet’s explore how some of the other descriptive statistics we’ve discussed behave in response to outliers. Specifically, measures of spread: standard deviation and IQR.\n\n# extract dominant arm percent change in strength\ndrm &lt;- famuss$drm.ch\n\n# calculate standard deviation\nsd(drm)\n\n# interquartile range\nIQR(drm)\n\n# average deviation\nmean(abs(drm - mean(drm)))\n\n# range\nmax(drm) - min(drm)\n\n# range endpoints\nrange(drm)\n\nThe variable you just looked at — dominant arm percent change in strength — has a group of observations over 60%.\n\n# boxplot of percent change in dominant arm strength\nboxplot(drm, horizontal = T, range = 2)\n\nIf these are removed, the standard deviation increases by 24%, but the IQR only increases by 18%.\n\n# drop the observations over 60%\ndrm.drop &lt;- drm[drm &lt; 60]\n\n# compute the numeric summary with and without outliers\nsummary(drm)\nsummary(drm.drop)\n\n# compare standard deviations\nsd(drm)/sd(drm.drop)\n\n# compare interquartile ranges\nIQR(drm)/IQR(drm.drop)\n\nThis may not seem very notable, so let’s make up an example that’s a bit more extreme: let’s add a very large positive observation, say, 1000. Then, the IQR does not change at all, but the standard deviation more than doubles!\n\n# add a large observation\ndrm.add &lt;- c(drm, 1000)\n\n# compare IQR with and without\nIQR(drm.add)/IQR(drm)\n\n# compare SD with and without\nsd(drm.add)/sd(drm)\n\nThe differences in robustness between IQR and standard deviation, and between mean and median, are largely why both the five-number summary and the mean and standard deviation are reported. When these statistics differ dramatically, it is most likely due to the presence of outliers!\n\n\n\n\n\n\nYour turn\n\n\n\nCompute the numeric summary for a variable from a different dataset and based on this alone attempt a guess at whether there are outliers. If so, are they more likely outliers to the left or right?\n\n# load a new dataset (census)\ndata(census.2010)\n\n# number of doctors per state (thousands)\ndoctors &lt;- census.2010$doctors\n\n# compute numeric summary -- guess whether there are outliers?\n\n# make a histogram or boxplot to confirm your guess\n\n# bonus: can you figure out which state??\n\n\n\n\n\nBivariate graphics\nThis part of the lab is organized according to which types of variables are being compared as potentially related. In each sub-part, you’ll see a series of examples that illustrate how to produce a given graphic or other summary, and then you’ll have an opportunity to try it with a different pair of variables from the FAMuSS study data.\n\nCategorical/categorical\nConsider: is there differential expression of the ACTN gene region of interest between sexes?\nThis can be answered by comparing the proportions of study participants of each genotype by sex. The steps are:\n\nStart by making a contingency table\nConvert to proportions using the appropriate row/column sums\nVisualize and compare genotype composition by group\n\nThe examples below illustrate how to perform these steps. As you’re walking through them, consider which summary answers the question — do you want to compute proportions using the genotype totals or the sex totals?\n\n# retrieve the genotype and sex columns\ngenotype &lt;- famuss$actn3.r577x\nsex &lt;- famuss$sex\n\n# construct a contingency table\ntable(genotype, sex)\n\n# stacked bar plots -- automatically groups by column\ntbl &lt;- table(famuss$actn3.r577x, famuss$sex)\nbarplot(tbl, legend = T)\n\n# turn the table on its side with t() to group by row\nbarplot(t(tbl), legend = T)\n\n# row and column margins\nrowSums(tbl)\ncolSums(tbl)\n\n# proportions, grouping by row\ntbl_row &lt;- tbl/rowSums(tbl)\ntbl_row\n\n# proportional stacked bar plot, grouped by row\nbarplot(t(tbl_row), legend = T)\n\n# proportions, grouping by column (a little trickier)\ntbl_col &lt;- t(t(tbl)/colSums(tbl))\ntbl_col\n\n# proportional stacked bar plot, groupde by column\nbarplot(tbl_col, legend = T, horiz = T)\n\n\n\n\n\n\n\nYour turn\n\n\n\nIs there differential expression of the ACTN gene region by racial group?\nFollow the examples above to make a contingency table and bar plot of genotype composition for each racial group. Do you see differences?\n\n# retrieve the genotype and race columns\n\n# make a contingency table of genotype and race\n\n# are there apparent genotype differences by race? make an appropriate bar plot\n\n\n\n\n\nNumeric/numeric\nTaller people tend to be heavier. We should expect a relationship between weight and height. The example below shows a scatterplot of the two variables, and computes the correlation (measure of linear relationship).\n\n# retrieve height and weight columns\nheight &lt;- famuss$height\nweight &lt;- famuss$weight\n\n# basic scatterplot\nplot(height, weight)\n\n# correlation\ncor(weight, height)\n\nA rough rule of thumb for interpreting correlations is as follows:\n\n\\(|r| &lt; 0.3\\): no relationship\n\\(0.3 \\leq |r| &lt; 0.6\\): weak to moderate relationship\n\\(0.6 \\leq |r| &lt; 1\\): moderate to strong relationship\n\nIn this case, the correlation of 0.53 indicates a moderate positive relationship.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there a relationship between nondominant and dominant percent change in arm strength? Make a scatterplot and compute the correlation.\n\n# retrieve the percent change variables\n\n# construct a scatterplot\n\n# compute the correlation\n\n\n\n\n\nCategorical/numeric\nConsider one of the main questions for the study:\n\nWere differences on the ACTN gene region associated with differential change in arm strength after resistance training?\n\nThis is a comparison between a categorical variable (genotype) and numeric variable (percent change in arm strength). Of course, we have measurements for both dominant and non-dominant arms; while there are other ways of handling this, we’ll just make comparisons separately for each arm.\nThe examples below produce boxplots for a quick comparison of the summary statistics of percent change in arm strength between genotypes. Recall that the summary statistics are summarizing the frequency distribution.\n\n# side-by-side boxplots for non-dominant arm\nboxplot(ndrm.ch ~ actn3.r577x, data = famuss)\n\n# change the orientation \nboxplot(ndrm.ch ~ actn3.r577x, data = famuss, horizontal = T)\n\n# change the whisker length (range = multiples of IQR)\nboxplot(ndrm.ch ~ actn3.r577x, data = famuss, horizontal = T, range = 2)\n\n# side-by-side boxplots for dominant arm\nboxplot(drm.ch ~ actn3.r577x, data = famuss, horizontal = T)\n\nThere are some slight observed differences for the non-dominant arm, but it’s unclear whether they are meaningful. We’ll return to that later, but for now, try the graphical technique with a different set of variables.\n\n\n\n\n\n\nYour turn\n\n\n\nInvestigate whether BMI seems to differ by racial group among the FAMuSS study participants.\n\n# make side-by-side boxplots of BMI by race"
  },
  {
    "objectID": "content/labs/lab4-sampling.html",
    "href": "content/labs/lab4-sampling.html",
    "title": "Lab 4: Sampling variability",
    "section": "",
    "text": "In this lab you’ll explore sampling variation. There are two learning goals:\nWe’ll use 3,179 responses to the 2009-2010 NHANES and consider specifically the total cholesterol variable. We will treat these observations as a population, and simulate the effect of sampling on summary statistics by generating small to moderate subcollections of observations drawn at random without replacement.\nThe dataset has been stored as a separate file nhanes.RData. Load the dataset, extract the variable of interest, and preview the first handful of observations using the commands below.\n# load nhanes dataset\nload('data/nhanes.RData')\n\n# extract total cholesterol column\ncholesterol &lt;- nhanes$TotChol\n\n# view summary\nstr(cholesterol)\n\n num [1:3179] 3.49 3.49 3.49 6.7 5.82 5.82 5.82 4.99 4.24 6.41 ...\nSince we are pretending that these 3,179 respondents form a population, we can examine the population distribution of the cholesterol variable and also calculate population statistics.\n# numeric summary\nsummary(cholesterol)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.330   4.240   4.970   5.043   5.690  13.650 \n\n# compute population mean and sd\npop_mean &lt;- mean(cholesterol)\npop_sd &lt;- sd(cholesterol)\n\n# construct histogram\nhist(cholesterol, breaks = 30)\nabline(v = pop_mean, col = 4, lty = 2)"
  },
  {
    "objectID": "content/labs/lab4-sampling.html#exploring-sampling-variability",
    "href": "content/labs/lab4-sampling.html#exploring-sampling-variability",
    "title": "Lab 4: Sampling variability",
    "section": "Exploring sampling variability",
    "text": "Exploring sampling variability\nThe commands below will draw a sample, compute the mean and standard deviation, and compare them with the corresponding population values.\n\ndoes the histogram look similar to the population distribution?\ndo the sample statistics closely align with the population values?\n\n\n# draw a sample -- try running this line a few times\nsample(cholesterol, size = 50, replace = F)\n\n# now store a sample\nsamp &lt;- sample(cholesterol, size = 50)\n\n# calculate mean and sd\nsamp_mean &lt;- mean(samp)\nsamp_sd &lt;- sd(samp)\nsamp_mean\nsamp_sd\n\n# estimation error\nsamp_mean - pop_mean\nsamp_sd - pop_sd\n\n# make a histogram\nhist(samp, breaks = 10)\n\n# add lines at sample mean and population mean\nabline(v = samp_mean, col = 2) # red line\nabline(v = pop_mean, col = 4, lty = 2) # blue line\n\nBecause you are drawing a random sample, results will differ each time you run the above commands. Likewise, results will differ from your groupmates.\n\n\n\n\n\n\nYour turn\n\n\n\nRun the lines above and compare results with your group. Manually enter your means and standard deviations in a vector. Repeat and add entries until you have at least 6 sample means and 6 standard deviations. Discuss:\n\nDoes it appear that the sample statistics tend to be close to the population values?\nDo the sample statistics vary much across samples? How might you measure this using your simulated means and standard deviations?\n\n\n# make vectors of 6 or more means and standard deviations\nsamp_means &lt;- ...\nsamp_sds &lt;- ...\n\n# calculate errors\nsamp_means - pop_mean\nsamp_sds - pop_sd\n\n# how would you measure variability?"
  },
  {
    "objectID": "content/labs/lab4-sampling.html#effect-of-sample-size",
    "href": "content/labs/lab4-sampling.html#effect-of-sample-size",
    "title": "Lab 4: Sampling variability",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nNow let’s explore how sample size affects sampling variation of summary statistics.\nFirst let’s examine how the frequency distribution of a single sample changes when we increase the sample size. Notice that the histogram of the larger sample more closely matches the population distribution.\n\n# population distribution\nhist(cholesterol, breaks = 30)\n\n\n\n# small sample size (try running a few times)\nhist(sample(cholesterol, size = 50), breaks = 15)\n\n\n\n# large sample size (try running a few times)\nhist(sample(cholesterol, size = 1000), breaks = 20)\n\n\n\n\nWe should therefore expect that summary statistics more closely approximate population values for larger samples. Our measure of sampling variability reflects this expectation. Recall that the theoretical standard deviation of the sample mean is: \\[ SD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\n\\quad\\left(\\frac{\\text{population SD}}{\\sqrt{\\text{sample size}}}\\right)\\]\nThis will diminish as \\(n\\) increases, indicating less sampling variation for larger samples. Let’s explore that empirically.\nThe commands below simulate nsim samples of size n and calculate the mean of each sample. Don’t worry about understanding the code that creates samp_means; your job is to use the results to measure sampling variability. First run this without any changes.\n\n# number of samples to simulate\nnsim &lt;- 1000\n\n# this generates nsim sample means from samples of size 10\nsamp_means_10 &lt;- sapply(1:nsim,\n                     function(i){\n                       mean(sample(cholesterol, size = 10))\n                     })\n\n# repeat, but for samples of size 100\nsamp_means_100 &lt;- sapply(1:nsim,\n                     function(i){\n                       mean(sample(cholesterol, size = 100))\n                     })\n\n\n# inspect\nstr(samp_means_10)\n\n num [1:1000] 4.56 5.12 4.88 5.47 5.06 ...\n\nstr(samp_means_100)\n\n num [1:1000] 5.13 4.89 5.07 5.04 5.1 ...\n\n\nEffectively, you’ve just simulated a sampling distribution for the mean cholesterol of a sample of size 10 by generating means from lots of random samples. Notice already the additional variability for the smaller sample size — the means seem to deviate more often by a larger amount from the population value.\n\n\n\n\n\n\nYour turn\n\n\n\nNow try using your simulated means to measure the sampling variability of the mean; compare this with the theoretical standard deviation.\nDo this for both the \\(n = 10\\) and \\(n = 100\\) cases. What changes?\n\n# calculate standard deviation of simulated means\nsim_sd_10 &lt;- ...\n\n# calculate theoretical standard deviation\ntheory_sd_10 &lt;- ...\n\n# calculate average error\navg_error_10 &lt;- ...\n\n# calculate standard deviation of simulated means\nsim_sd_100 &lt;- ...\n\n# calculate theoretical standard deviation\ntheory_sd_100 &lt;- ...\n\n# calculate average error\navg_error_100 &lt;- ...\n\n\n\nTake note of the fact that in practice, it would not be possible to simulate sampling distributions in this way, because you’d lack data for a complete population. In practice, you’ll only have one sample, and will need to use this to estimate the sampling variability based on theory.\nThe simulations above don’t provide an actionable method for estimating sampling variability, but are rather an exercise to aid in understanding exactly what theoretical estimates of sampling variability are designed to measure."
  },
  {
    "objectID": "content/labs/lab1-rbasics.html",
    "href": "content/labs/lab1-rbasics.html",
    "title": "Lab 1: R Basics",
    "section": "",
    "text": "This lab is intended to introduce you to the nuts and bolts of R: arithmetic operations, data types, vectors, and other data structures. While you won’t be expected to do much data manipulation or manual calculation for this class, it will be helpful for you to have some understanding of this material going forward, as most R objects are arranged as data structures comprising one or more data types.\nThis lab covers a lot of ground; consider it a reference you can return to later as needed.\n\nArithmetic operations\nWhile R does a lot more than function as a calculator, it can be used to perform basic arithmetic. All of the usual operations — addition, subtraction, multiplication, division, exponentiation — can be performed as shown below.\n\n# addition\n2 + 1 \n\n# subtraction\n2 - 1\n\n# multiplication\n2*3\n\n# division\n2/3\n\n# exponentiation\n3^2\n\n# parentheses for order of operations: compare\n(2 + 1)/4\n2 + 1/4\n\nBasic functions can also be evaluated. To name a few:\n\n# square root\nsqrt(9)\n\n# exponential function e^x\nexp(1)\n\n# logarithm: compare, default base is e (i.e., natural log)\nlog(2, base = exp(1))\nlog(2)\n\nThe result of each of these calculations is a value. Values are the most elementary objects in the R environment. Values can be assigned names using the assignment operator &lt;-:\n\n# store a value\nx &lt;- log(2)\n\n# print value in console\nx\n\nNames can consist of any combination of letters, numbers, and the delimiters _ and .; names cannot start with numbers (try it and see what happens).\nIf a name is used multiple times, the most recently assigned value will be stored. However, ‘most recent’ in this context means ‘most recently executed’: R will not pay attention to the order of lines in your script, but the order in which you run them. To see this, try redefining x to be a new value. if you then go back and run the previous line that displays the value, you’ll notice you get the new value, not the old one, even though the line appears before the new assignment.\n\n# assign a new value the name 'x': this overwrites the first value\nx &lt;- 4\n\n\n\n\n\n\n\nYour turn\n\n\n\nWrite one line of code each to perform the following calculations:\n\nCalculate the sum of the ages of each member of your group, in years.\nHow about in months? Multiply by 12.\nCalculate the average number of siblings among the members of your group.\n\n\n# sum of ages of group members\n\n# convert to months: multiply by 12\n\n# average number of siblings among group members\n\n\n\n\n\nData types\nValues may be of different types. The function class(...) will return the “class” of object it is given; for values, the object class is simply the data type.\nThere are four main data types in R.\n\n# numeric\nclass(12)\n\n# character\nclass('text')\n\n# logical\nclass(TRUE)\n\n# integer\nclass(12L)\n\nArithmetic only works with logical, integer, and numeric values:\n\n# valid operations\n12*12\n12L*12\n12L*12L\n12*TRUE\n12*FALSE\n12L*TRUE\n12L*FALSE\n\nIt will not work with character values. Notice the text of the error message:\n\n# invalid operation\n'12'*'12'\n\nError in \"12\" * \"12\": non-numeric argument to binary operator\n\n\nThis is a very common error message — when you see it, you’ll know that most likely, somewhere R attempted to perform arithmetic with character values, so it’s probably a data type issue.\n\n\n\n\n\n\nYour turn\n\n\n\n\nCalculate the product of TRUE and FALSE. Which data type results from this operation?\nWhat do you think will happen if you add a logical and numeric value? Discuss first, then verify.\nWhat kind of value does each of the above operations return? Discuss first, then verify.\n\n\n# product of true and false\n\n# add a logical and numeric value\n\n# check data type of the above\n\n\n\n\n\nVectors\nIn practice, data usually consists of multiple values, not just one; in R, the most basic type of collection of values is called a vector. Technically, a vector is a “concatenation” of values of the same data type. Vectors can be formed using the concatenation function c(...):\n\n# concatenate values with c(...) to form a vector\nc(1, 2, 3)\n\n# store it\nx &lt;- c(1, 2, 3)\n\n# data type\nclass(x)\n\nIf values of different types are concatenated, they are coerced to the same data type; vectors cannot contain values of mixed type. The lines below demonstrate this behavior:\n\n# integer and logical -&gt; integer\nclass(c(2L, TRUE))\n\n# integer and numeric -&gt; numeric\nclass(c(2L, 12))\n\n# numeric and logical -&gt; numeric\nclass(c(12, TRUE))\n\n# character and anything -&gt; character\nclass(c('text', TRUE))\nclass(c('text', 2L))\nclass(c('text', 12))\n\n\nVectorization\nArithmetic between vectors is carried out elementwise; arithmetic operations between vectors and values are ‘vectorized’, meaning operations are applied over each element. This makes certain calculations quite efficient. The following examples illustrate vectorized arithmetic operations in two verbose ways: first as an elementwise operation between two vectors; then as a concatenation of the results of each elementwise operation.\n\n# equivalent\n2*c(1, 2, 3) # vectorized\nc(2, 2, 2)*c(1, 2, 3) # verbose\nc(2*1, 2*2, 2*3) # verbose\n\n# equivalent\nc(1, 2, 3) + 1 # vectorized\nc(1, 2, 3) + c(1, 1, 1) # verbose\nc(1 + 1, 2 + 1, 3 + 1) # verbose\n\n# equivalent\nc(1, 2, 3)/3 # vectorized\nc(1, 2, 3)/c(3, 3, 3) # verbose\nc(1/3, 2/3, 3/3) # verbose\n\n# equivalent\nc(1, 2, 3)^2 # vectorized\nc(1, 2, 3)^c(2, 2, 2) # verbose\nc(1^2, 2^2, 3^2) # verbose\n\nIf an operation is carried out between vectors of different lengths, a warning is printed indicating as much.\n\n# can you figure out what calculation was performed?\nc(1, 2, 3)*c(4, 5)\n\nWarning in c(1, 2, 3) * c(4, 5): longer object length is not a multiple of\nshorter object length\n\n\n[1]  4 10 12\n\n\nMany functions, including those mentioned at the outset, are also vectorized (calculations are performed elementwise).\n\nsqrt(c(1, 4, 9))\nlog(c(1, 10, 100), base = 10)\nexp(c(0, 1, 2))\n\n\n\nIndexing\nElements of vectors are indexed by consecutive integers; elements can be retrieved by specifying the indices in square brackets after the object name.\n\n# define a vector\nx &lt;- c(10, 20, 30, 40)\n\n# second element\nx[2]\n\n# first and third elements\nx[c(1, 3)]\n\n# second through fourth elements\nx[2:4]\n\n\n\nMissing values\nMany datasets have missing values that occur for various reasons: equipment failure, participant dropout, survey nonresponse, and so on. These can be represented in R as well. This is useful as a means of retaining the information that there is an observation that was somehow lost. Missing values are displayed as the special character NA in R.\n\n# vector with a missing third element\nc(1, 2, NA, 4)\n\n# note: still numeric\nclass(c(1, 2, NA, 4))\n\n\n\n\n\n\n\nYour turn\n\n\n\n\nCreate a vector with the ages of each person in your group (in years). Call it ages.\nConvert to months using vectorized arithmetic.\nNow pretend one of you is absent; repeat (1) but replace their age by a missing value. call the vector ages_incomplete.\nConvert to months again using the same operation as in (2); how is the missing value handled?\n\n\n# vector of ages in years\n\n# convert to months with vectorized arithmetic\n\n# pretend one person is absent: input a missing value\n\n# repeat conversion to months: how is the NA handled?\n\n\n\n\n\nMatrices, arrays, lists, and data frames\nMatrices can be constructed from vectors by specifying row and column dimensions, and how to arrange the values – by row or by column.\n\nx &lt;- c(1, 2, 3, 4)\nmatrix(data = x, nrow = 2, ncol = 2, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nWe won’t really see arrays much, but since they are mentioned in the reading, here’s an example of how to construct one, and how it is displayed.\n\nx &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)\narray(data = x, dim = c(2, 2, 2))\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nA list is a highly general data structure; it’s just an indexed amalgamation of objects of any type.\n\n# make a list\nlist('anchor', c(22, 5), log)\n\n[[1]]\n[1] \"anchor\"\n\n[[2]]\n[1] 22  5\n\n[[3]]\nfunction (x, base = exp(1))  .Primitive(\"log\")\n\n# the list elements can be named, which makes for easy retrieval\nmy_list &lt;- list(words = 'anchor', numbers = c(22, 5), functions = log)\nmy_list$words\n\n[1] \"anchor\"\n\n# even so, indexing can still be used\nmy_list[[1]]\n\n[1] \"anchor\"\n\n\nData frames are like lists, except each element is a vector of the same length; they appear much like matrices, but behave differently in important ways.\n\n# make a data frame\ndata.frame(col1 = c(1, 2, 3, 4), \n           col2 = c(T, F, T, T), \n           col3 = c('red', 'blue', 'green', 'yellow'))\n\n  col1  col2   col3\n1    1  TRUE    red\n2    2 FALSE   blue\n3    3  TRUE  green\n4    4  TRUE yellow\n\n# unlike matrices, however, columns can be retrieved by name\nmy_df &lt;- data.frame(number = c(1, 2, 3, 4), \n                    truth = c(T, F, T, T), \n                    color = c('red', 'blue', 'green', 'yellow'))\n\n# the result is a vector containing the values of that column\nmy_df$color\n\n[1] \"red\"    \"blue\"   \"green\"  \"yellow\"\n\n\nData frames are the fundamental data structure on which a large portion of R software is built. When data files are read into R, as you saw very briefly earlier, they are read in by default as data frames. Many R functions used in performing data analyses require data to be supplied as a data frame.\n\n\n\n\n\n\nYour turn\n\n\n\nCreate a data frame in which each row corresponds to one member of your group and columns are the variables age, number of siblings, and favorite singer.\n\n# data frame with columns age, no. of siblings, and favorite singer; each row corresponds to one group member"
  },
  {
    "objectID": "content/labs/lab2-descriptive.html",
    "href": "content/labs/lab2-descriptive.html",
    "title": "Lab 2: Descriptive statistics",
    "section": "",
    "text": "By now you have seen basic descriptive and graphical summaries. But what determines when some descriptive statistics are “better” to use than others?\nThis lab has two objectives:\n\nTeach you to compute descriptive statistics and prepare graphical summaries for a single variable in R\nLearn when and why to use certain descriptive statistics in place of others\n\nWe will focus on how outliers, or observations far from center, affect different descriptive measures. In statistics, the term for this is robustness: a robust statistic is less affected by the presence of outliers.\n\nAccessing data\nFor this lab we’ll use the FAMuSS dataset, as in lecture. This dataset is stored in the oibiostat package, which is an R package companion to the Vu and Harrington textbook.\n\n# load openintro biostat package\nlibrary(oibiostat)\n\n# load famuss data\ndata(famuss)\n\n# open data documentation\n?famuss\n\nThe command above makes the dataset available in your environment as a data frame named famuss in which the columns represent variables and the rows represent observations. You can see the first few rows using head():\n\nhead(famuss)\n\n  ndrm.ch drm.ch    sex age      race height weight actn3.r577x    bmi\n1      40     40 Female  27 Caucasian   65.0    199          CC 33.112\n2      25      0   Male  36 Caucasian   71.7    189          CT 25.845\n3      40      0 Female  24 Caucasian   65.0    134          CT 22.296\n4     125      0 Female  40 Caucasian   68.0    171          CT 25.998\n5      40     20 Female  32 Caucasian   61.0    118          CC 22.293\n6      75      0 Female  24  Hispanic   62.2    120          CT 21.805\n\n\nYou can extract a vector of the observations for any particular variable from the dataframe as follows: famuss$[variable name]. We will be performing calculations one column at a time, so you’ll need to be able to extract and store a column as a new R object.\n\n# extract the age variable\nfamuss$age\n\n# extract the bmi variable\nfamuss$bmi\n\n# store the age column as a vector\nage &lt;- famuss$age\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n# try it yourself: pick a variable to extract and store\n\n\n\n\n\nGraphical and tabular summaries\nOur most basic summaries are frequency distributions. For categorical variables, these are simply observation counts by category; for numeric variables, values must be binned and then tabulated.\nFor categorical variables (i.e., character vectors or factors):\n\ntable() will tabulate the number of occurrences of unique values in a vector\nfor a categorical variable, plot() will create a barplot of the frequency distribution\n\nFor numeric variables:\n\nhist() will create a histogram\n\n\n## categorical summaries\nrace &lt;- famuss$race\n\n# frequency distribution\ntable(race)\n\n# barplot of frequency distribution\nplot(race)\n## quantitative summaries\nage &lt;- famuss$age\n\n# histogram; 'breaks = ...' controls the binning\nhist(age, breaks = 20)\n\nA quick word about functions. The inputs to functions are called arguments. The histogram function hist() requires a data argument, in this case age, to make the plot. However, the function also has several optional arguments that control things like labels, binning, axis limits, and so on. The breaks = 20 part of the last command is an example of an optional argument. Most functions in R have optional arguments that control their behavior.\n\n\n\n\n\n\nYour turn\n\n\n\nTry making a table, barplot, and histogram on your own with some of the other variables.\nWhen generating the histogram, adjust the number of bins using the breaks = ... argument. Experiment to see how the shape of the distribution appears at various binning resolutions; then pick a number of breaks that you feel reflects the data best.\n\n# make a table of the frequency distribution of genotypes\n\n# make a barplot of the frequency distribution of genotypes\n\n# make a histogram of dominant arm change; play with the binning!\n\n\n\n\n\nNumerical summaries\nIn class we discussed several descriptive statistics for numeric variables.\n\nmeasures of center: mean, median\npercentiles: quartiles, min, max\n\nThese statistics are so commonly used that they have their own functions in R.\n\n# the median gets its own function\nmedian(age)\n\n# ditto mean\nmean(age)\n\n# ... and minimum and maximum\nmin(age)\nmax(age)\n\n# 30th percentile of age (\"quantile\" is another term for percentile)\nquantile(age, probs = 0.3)\n\n# 30th *and* 60th percentile of age\nquantile(age, probs = c(0.3, 0.6))\n\nThe five-number summary is the collection of the percentiles that give the minimum, quartiles, and maximum.\n\n\n\n\n\n\nYour turn\n\n\n\nTry computing the five-number summary using a variable of your choice. Be sure you pick a numeric and not a categorical variable.\nPay attention to how the probs = ... argument to the quantile() function can be used to calculate multiple percentiles at once. Use this feature to calculate all five numbers in the five-number summary with one command.\n\n# choose a *quantiative* variable from the dataset\n\n# compute the five-number summary using quantile()\n\n# compare the mean and the median. are they close?\n\n\n\nAlternatively, the summary() command will do the work for you.\n\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   17.0    20.0    22.0    24.4    27.0    40.0 \n\n\n\n\nExploring robustness\nStatistics based on percentiles are in general insensitive to outliers, unless there’s a large group of outlying observations. In this sense they are robust statistics.\nAn easy way to see this is to consider the median (middle value or 50th percentile). The maximum observation could be arbitrarily large without changing the middle value, so the median will be the same whether the largest value is 10 or 10,000. The mean, by contrast, does not share this property.\n\n# make up some observations between 1 and 100\nx &lt;- sample(1:100, size = 20)\nx\n\n# median of made up observations, plus 101\nmedian(c(x, 101))\n\n# median of made up observations, plus 1M\nmedian(c(x, 1000000))\n\n# same comparison, but with mean\nmean(c(x, 101))\nmean(c(x, 1000000))\n\nSo in the presence of outliers, the median will capture the center of the distribution of values more accurately. In fact, even if the distribution is simply skewed, the mean will shift away from center.\n\n# distribution of ages is right-skewed\nhist(age)\n\n# plot the mean and median on top of the histogram\nhist(age)\nabline(v = mean(age), col = 2)\nabline(v = median(age), col = 4)\n\n\n\n\nThis difference is useful — the comparison of median and mean can indicate the direction and amount of skewness present.\n\n# mean &gt; median ---&gt; right-skewed\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   17.0    20.0    22.0    24.4    27.0    40.0 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\nCalculate the numeric summary for percent change in dominant arm strength and see if you can determine the direction and magnitude of skewness based on this summary only.\nThe amount of skewness is a bit subjective — so just discuss and make a determination about whether the difference between median and mean seems sizeable.\n\n# check the numeric summary for percent change in dominant arm strength\n# can you tell the direction of skewness?? does it seem very skewed??"
  },
  {
    "objectID": "content/week4-sampling.html#todays-agenda",
    "href": "content/week4-sampling.html#todays-agenda",
    "title": "Sampling variability",
    "section": "Today’s agenda",
    "text": "Today’s agenda\nToday we’ll focus on understanding sampling variability. This is a foundation for the development of inferential statistics.\n\nReading quiz [2pm section] [4pm section]\n[lecture/lab] Effect of sampling variability on summary statistics\n[lecture/lab] Effect of sample size on sampling variability of summary statistics\nThe normal model"
  },
  {
    "objectID": "content/week4-sampling.html#inferential-statistics",
    "href": "content/week4-sampling.html#inferential-statistics",
    "title": "Sampling variability",
    "section": "Inferential statistics",
    "text": "Inferential statistics\nConsider this descriptive finding from the FAMuSS study:\n\n\n\n\n\n\n\n\nMedian percent change by genotype:\n\n\n\n\n\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\n42.9\n45.5\n50\n\n\n\n\n\nSubjects with genotype TT exhibited the largest median percent change in strength\n\n\nInferential statistics will allow us to address questions like:\n\nShould we expect these differences to persist among the general population?\n\nTo do so we will need to articulate how samples relate to populations."
  },
  {
    "objectID": "content/week4-sampling.html#sampling-variability",
    "href": "content/week4-sampling.html#sampling-variability",
    "title": "Sampling variability",
    "section": "Sampling variability",
    "text": "Sampling variability\nDifferent samples inevitably produce different outcomes. This is sampling variation:\n\nIf we re-run the study with new participants we’ll get different results\n\nThe basis for statistical inference is distinguishing sampling variation from systematic variation.\nWhat we really want to know in the FAMuSS study:\n\nIs the CC/TT/CT difference systematic in the population or an artefact of the sample?\n\nIn other words…\n\nwas the result due to chance?\nor was it genuine?"
  },
  {
    "objectID": "content/week4-sampling.html#random-sampling",
    "href": "content/week4-sampling.html#random-sampling",
    "title": "Sampling variability",
    "section": "Random sampling",
    "text": "Random sampling\nIf we assume study units are sampled at random from a broader population, we can quantify how much summary statistics are expected to change from sample to sample.\n\n\n\n\nA study population is a collection of all study units of interest.\nA sample is a subcollection from a population:\n\nrandom if study units have a known chance of inclusion in the sample\nnonrandom or convenience otherwise\n\n\n\nIn STAT218 we’ll limit attention to simple random samples: each study unit in the population has an equal chance of inclusion in the sample."
  },
  {
    "objectID": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "href": "content/week4-sampling.html#a-pretend-population-nhanes-data",
    "title": "Sampling variability",
    "section": "A pretend population: NHANES data",
    "text": "A pretend population: NHANES data\nThe National Health and Nutrition Esamination Survey (NHANES) is an annual CDC program to collect health and nutrition data on the non-institutionalized civilian resident population of the United States. Here are a few variables:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubj.id\nGender\nAge\nPoverty\nPulse\nBPSys1\nBPDia1\nTotChol\nSleepHrsNight\n\n\n\n\n1\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n2\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n3\nmale\n34\n1.36\n70\n114\n88\n3.49\n4\n\n\n5\nfemale\n49\n1.91\n86\n118\n82\n6.7\n8\n\n\n\n\n\nI’ve selected 3,179 responses from the 2009-2010 survey; let’s pretend the corresponding individuals form a population of interest."
  },
  {
    "objectID": "content/week4-sampling.html#population-distribution-of-a-variable",
    "href": "content/week4-sampling.html#population-distribution-of-a-variable",
    "title": "Sampling variability",
    "section": "Population distribution of a variable",
    "text": "Population distribution of a variable\nConsider the TotChol variable: total HDL cholesterol in mmol/L. It has a certain frequency distribution among the population that we’ll call its population distribution.\n\n\n\n\n\n\n\n\n\n\n\nPopulation mean\nPopulation SD\n\n\n\n\n5.043\n1.075\n\n\n\n\n\n\n\n\n\nIf we draw a random sample of 50 individuals…\n\nhow closely will the sample align with the population distribution?\nhow much will alignment change if we select a new sample?"
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability",
    "href": "content/week4-sampling.html#simulating-sampling-variability",
    "title": "Sampling variability",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\nOpen the class activity lab4-sampling. The first part of this lab will load the NHANES data and provide you with a command for extracting a sample.\nYour task:\n\nHave each person in your group extract a sample.\nCalculate the mean and standard deviation.\nMake a histogram.\nCompare your results to the population.\nCompare your results to each other.\n\nAfter you’ve had a chance to try in groups, we’ll compare across the class."
  },
  {
    "objectID": "content/week4-sampling.html#simulating-sampling-variability-1",
    "href": "content/week4-sampling.html#simulating-sampling-variability-1",
    "title": "Sampling variability",
    "section": "Simulating sampling variability",
    "text": "Simulating sampling variability\n\n\n\n\n\n\n\n\nYou should have observed something a bit like this.\nThese are 16 random samples with the sample mean indicated by the blue dashed line and the population mean indicated by the red solid line.\n\nfrequency distributions differ a lot\nsample means vary a little\nmost means are close to 5\nmost standard deviations are near 1"
  },
  {
    "objectID": "content/week4-sampling.html#point-estimation",
    "href": "content/week4-sampling.html#point-estimation",
    "title": "Sampling variability",
    "section": "Point estimation",
    "text": "Point estimation\nIt should seem plausible that the sample mean and standard deviation provide good estimates of the corresponding population quantities.\nWe call them point estimates of population parameters.\n\n\n\nParameter name\nParameter notation\nPoint estimate\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s_x\\)\n\n\n\nNow we can more formally describe statistical inference:\n\na population parameter is any numeric characteristic of a population distribution\nan inference is a conclusion about the value of a population parameter based on point estimates and their sampling variability\n\nWe will focus initially on inferences about the mean \\(\\mu\\) based on the point estimate \\(\\bar{x}\\)."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability",
    "href": "content/week4-sampling.html#measuring-sampling-variability",
    "title": "Sampling variability",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nIf we had means calculated from a large number of samples, we could make a frequency distribution for the values of the sample mean. This is called a sampling distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample\n1\n2\n3\n4\n5\n\n\nmean\n4.957\n5.039\n5.24\n5.24\n4.864\n\n\n\n\n\nCould measure the sampling variability using any measure of spread.\n\nstandard deviation: 0.1513356\n\nOn average, the sample mean varies about the population mean by 0.15 mmol/L across simple random samples."
  },
  {
    "objectID": "content/week4-sampling.html#measuring-sampling-variability-1",
    "href": "content/week4-sampling.html#measuring-sampling-variability-1",
    "title": "Sampling variability",
    "section": "Measuring sampling variability",
    "text": "Measuring sampling variability\nTheory indicates the standard deviation of the sample mean under random sampling is: \\[\nSD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{population SD}}{\\sqrt{\\text{sample size}}}\\right)\n\\]\nFor TotChol, the theoretical standard deviation is \\(SD(\\bar{x}) = \\frac{1.0747}{\\sqrt{50}} =\\) 0.1519822.\nWe can estimate this quantity by replacing \\(\\sigma\\) with the point estimate \\(s_x\\), resulting in a standard error (estimated standard deviation): \\[SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\\]"
  },
  {
    "objectID": "content/week4-sampling.html#example-with-one-sample",
    "href": "content/week4-sampling.html#example-with-one-sample",
    "title": "Sampling variability",
    "section": "Example with one sample",
    "text": "Example with one sample\nThe simulations we’ve done so far have been a means of understanding just what a standard error is meant to capture; these are not a practicable method for measuring sampling variation.\nIn practice we’d simply compute a point estimate and standard error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.031\n0.1396\n\n\n\n\n\n\nThe estimated mean total HDL cholesterol among the population is 5.031 mmol/L.\nThe point estimate is expected to deviate by 0.1396 mmol/L on average from the population mean."
  },
  {
    "objectID": "content/week4-sampling.html#effect-of-sample-size",
    "href": "content/week4-sampling.html#effect-of-sample-size",
    "title": "Sampling variability",
    "section": "Effect of sample size",
    "text": "Effect of sample size\nThe formula for the theoretical standard deviation of \\(\\bar{x}\\) suggests that sampling variability diminishes with sample size. For example:\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\n10\n50\n100\n1000\n10000\n\n\nSD\n0.3398\n0.152\n0.1075\n0.03398\n0.01075\n\n\n\n\n\nThe second section of lab4-sampling explores this. With your group:\n\nStart with a sample size of 10.\nHave each person draw a sample and compute the mean (or draw a series of samples and compute the mean each time).\nCompare and observe how big the differences between your means are.\nRepeat with a sample size of 1000.\n\nYou should see much less sampling variability after increasing \\(n\\)."
  },
  {
    "objectID": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "href": "content/week4-sampling.html#visualizing-effect-of-sample-size",
    "title": "Sampling variability",
    "section": "Visualizing effect of sample size",
    "text": "Visualizing effect of sample size\nGenerating a large number (10K) of samples allows us to approximate the sampling distribution for a variety of sample sizes.\n\n\nspread diminishes with sample size\nless variability among estimates from larger samples\n\nSo estimates are more accurate with more data, assuming data are from a random sample."
  },
  {
    "objectID": "content/week4-sampling.html#normal-model",
    "href": "content/week4-sampling.html#normal-model",
    "title": "Sampling variability",
    "section": "Normal model",
    "text": "Normal model\nNotice that each simulated sampling distribution has produced a unimodal, symmetric, bell-shaped histogram.\n\n\nThe normal model is a theoretical frequency distribution characterized by two parameters:\n\na mean (center)\na standard deviation (spread)\n\nTheory dictates that the sampling distribution of the sample mean is well-approximated by a normal model under simple random sampling.\n\n\n\n\n\n\n\n\nBased on discussion thus far, what do you think the model parameters might be?\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/hw/hw1.html",
    "href": "content/hw/hw1.html",
    "title": "Homework 1: Study designs",
    "section": "",
    "text": "Instructions: type up your answers and submit your work electronically. Questions with a learning outcome indicated in brackets will be evaluated for credit; other questions are provided for additional practice. You are expected to answer all questions."
  },
  {
    "objectID": "content/hw/hw1.html#education-and-injury-prevention",
    "href": "content/hw/hw1.html#education-and-injury-prevention",
    "title": "Homework 1: Study designs",
    "section": "Education and injury prevention",
    "text": "Education and injury prevention\nSuppose you wish to study the efficacy of physical activity and exercise education as a means of sports injury prevention. Your research question is, “does knowledge about injury prevention during physical activity and exercise reduce the risk of injury?”. Imagine that ASI offers a short injury prevention training.\nYour initial study proposal is as follows:\n\n150 Cal Poly students will be selected for the study: 75 from among those who completed the injury prevention training voluntarily, and 75 from among those who did not. Participants will be followed for a year to determine how many in each group experience an injury related to physical activity at any point during the study period. At the end of the study, injury rates will be compared between those that participated in the injury prevention training and those that did not.\n\n\n[L2] Is this an observational study or an experiment? Explain your answer.\n[L1] How would you go about selecting study participants at random? Propose a specific means of identifying and contacting students to participate in the study.\n[L2] Imagine you found that those who completed the training are less likely to experience injury. Are these results potentially subject to confounding? If you answer yes, give a hypothetical example of a confounding factor; if you answer no, explain why confounding is not a concern.\n[L1] If you answered that it is an observational study, propose an experiment that would address the same question. If you answered that it is an experiment, propose an observational study that would address the same question.\n[L2] Suppose the alternate study you proposed in (4) indicated that those who completed the training are less likely to experience injury. Are these results potentially subject to confounding? If so, give a hypothetical example of a confounding factor; if not, explain why confounding is not a concern.\nIs the original proposal a retrospective or prospective study? Explain your answer.\nIf you answered that it is retrospective, determine an alternate study to investigate the same research question that is instead prospective; if you answered that it is prospective, determine an alternate study that is retrospective. Write a short proposal similar to the above for your alternate study."
  },
  {
    "objectID": "content/hw/hw1.html#peanut-allergies",
    "href": "content/hw/hw1.html#peanut-allergies",
    "title": "Homework 1: Study designs",
    "section": "Peanut allergies",
    "text": "Peanut allergies\nConsider the Learning Early About Peanut allergy (LEAP) study discussed in class and in your reading:\n\nFor the LEAP study, 640 infants in the United Kingdom were enrolled with risk factors for peanut allergies (eczema or egg allergy); 530 passed a skin test at the start of the study showing no peanut allergy. Each infant was randomly assigned to peanut consumption (6g peanut protein per day) or peanut avoidance (no peanut consumption) groups. At 5 years of age, an allergy test was administered to each study participant; the rates of peanut allergy were compared between the two groups.\n\n\n[L1] Propose a retrospective observational study to investigate the research question, “is early peanut exposure associated with a lower risk of developing peanut allergies?” Include a specific description of how you might enroll participants."
  },
  {
    "objectID": "content/week2-datatypes.html#todays-agenda",
    "href": "content/week2-datatypes.html#todays-agenda",
    "title": "Introduction to data",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz\nLoose ends from last time: LEAP case study; experimental design basics\nData basics: data semantics and data types\nLab: data types and data structures in R"
  },
  {
    "objectID": "content/week2-datatypes.html#case-study-leap",
    "href": "content/week2-datatypes.html#case-study-leap",
    "title": "Introduction to data",
    "section": "Case study: LEAP",
    "text": "Case study: LEAP\nLearning early about peanut allergy (LEAP) study:\n\n640 infants in UK with eczema, egg allergy, or both enrolled; 530 passed a skin test showing no peanut allergy\neach infant randomly assigned to peanut consumption and peanut avoidance groups\n\npeanut consumption: fed 6g peanut protein daily until 5 years old\npeanut avoidance: no peanut consumption until 5 years old\n\nat 5 years old, oral food challenge (OFC) allergy test administered\n\nPASS: no allergy detected\nFAIL: allergy detected\n\n\nReview questions from last time:\n\nExperiment or observational study?\nRetrospective, prospective, or neither?"
  },
  {
    "objectID": "content/week2-datatypes.html#case-study-leap-1",
    "href": "content/week2-datatypes.html#case-study-leap-1",
    "title": "Introduction to data",
    "section": "Case study: LEAP",
    "text": "Case study: LEAP\n \nA greater proportion of children in the avoidance group developed an allergy (11.8% specifically). Given the type of study, which is a more accurate interpretation of this result?\n\nCompared with consumption, peanut avoidance early in life was associated with more frequent occurrence of peanut allergies among children in the study.\nCompared with consumption, peanut avoidance early in life led to more frequent occurrence of peanut allergies among children in the study."
  },
  {
    "objectID": "content/week2-datatypes.html#experimental-design",
    "href": "content/week2-datatypes.html#experimental-design",
    "title": "Introduction to data",
    "section": "Experimental design",
    "text": "Experimental design\nThe LEAP study exemplifies the simple kind of design we will consider for much of STAT218:\n\ntwo “treatments” (peanut consumption and peanut avoidance)\ntreatments are allocated completely at random among all study units\n\nThe design of an experiment refers to how treatments (experimental interventions) are allocated to study units.\n\nthe number of replicates of a treatment refers to the number of study units receiving that treatment\na design is balanced if all treatments have equal numbers of replicates\ntreatment allocation should always be randomized; different designs use different randomization schemes\n\nCheck your understanding: is the experimental design of the LEAP study balanced?"
  },
  {
    "objectID": "content/week2-datatypes.html#data-semantics",
    "href": "content/week2-datatypes.html#data-semantics",
    "title": "Introduction to data",
    "section": "Data semantics",
    "text": "Data semantics\n\nData are a set of measurements.\nA variable is any measured attribute of study units.\nAn observation is a measurement of one or more variables taken on one particular study unit.\n\nIt is usually expedient to arrange data values in a table in which each row is an observation and each column is a variable:"
  },
  {
    "objectID": "content/week2-datatypes.html#leap-example",
    "href": "content/week2-datatypes.html#leap-example",
    "title": "Introduction to data",
    "section": "LEAP example",
    "text": "LEAP example\nA table showing the observations and variables for the LEAP study would look like this:\n\n\n\n\n\n\n\n\n\n\nparticipant.ID\ntreatment.group\nofc.test.result\n\n\n\n\nLEAP_100522\nPeanut Consumption\nPASS OFC\n\n\nLEAP_103358\nPeanut Consumption\nPASS OFC\n\n\nLEAP_105069\nPeanut Avoidance\nPASS OFC\n\n\nLEAP_105328\nPeanut Consumption\nPASS OFC\n\n\n\n\n\nThe table you saw earlier was a summary of the data (not the data itself):\n\n\n\n\n\n\n\n\n\n\n \nFAIL OFC\nPASS OFC\n\n\n\n\nPeanut Avoidance\n36\n227\n\n\nPeanut Consumption\n5\n262"
  },
  {
    "objectID": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "href": "content/week2-datatypes.html#numeric-and-categorical-variables",
    "title": "Introduction to data",
    "section": "Numeric and categorical variables",
    "text": "Numeric and categorical variables\nVariables are classified according to their values. The broadest distinction made is between numeric and categorical variables.\n\nA variable is numeric if its value is a number\nA variable is categorical if its value is a category, usually recorded as a name or label\n\nFor example:\n\nthe value of sex can be male or female, so it is categorical\nwhereas age (in years) can be any positive integer, so it is numeric\n\nCheck your understanding: in the LEAP study…\n\ntreatment group is [numeric/categorical]\nOFC test result is [numeric/categorical]"
  },
  {
    "objectID": "content/week2-datatypes.html#variable-subtypes",
    "href": "content/week2-datatypes.html#variable-subtypes",
    "title": "Introduction to data",
    "section": "Variable subtypes",
    "text": "Variable subtypes\nFurther distinctions are made based on the type of number or type of category used to measure an attribute. Can you match the subtypes to the variables at right?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nhispanic\ngrade\nweight\n\n\n\n\n15\nnot\n10\n78.02\n\n\n18\nhispanic\n12\n78.47\n\n\n17\nnot\n11\n95.26\n\n\n18\nnot\n12\n95.26\n\n\n\n\n\n\n\n\na numerical variable is discrete if there are ‘gaps’ between its possible values\na numerical variable is continuous if there are no such gaps\na categorical variable is nominal if its levels are not ordered\na categorical variable is ordinal if its levels are ordered"
  },
  {
    "objectID": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "href": "content/week2-datatypes.html#many-ways-to-measure-attributes",
    "title": "Introduction to data",
    "section": "Many ways to measure attributes",
    "text": "Many ways to measure attributes\nVariable type (or subtype) is not an inherent quality — attributes can often be measured in many different ways.\nFor instance, age might be measured as either a discrete, continuous, or ordinal variable, depending on the situation:\n\n\n\nAge (years)\nAge (minutes)\nAge (brackets)\n\n\n\n\n12\n6307518.45\n10-18\n\n\n8\n4209187.18\n5-10\n\n\n21\n11258103.08\n18-30\n\n\n\n\nNumeric variables can always be represented as categorical, but not the other way around."
  },
  {
    "objectID": "content/week2-datatypes.html#your-turn-identifying-data-types-i",
    "href": "content/week2-datatypes.html#your-turn-identifying-data-types-i",
    "title": "Introduction to data",
    "section": "Your turn: identifying data types I",
    "text": "Your turn: identifying data types I\nClassify each variable as nominal, ordinal, discrete, or continuous:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nndrm.ch\ndrm.ch\nsex\nage\nrace\nheight\nweight\nactn3.r577x\nbmi\n\n\n\n\n33.3\n0\nFemale\n19\nCaucasian\n65.7\n129\nCT\n21.01\n\n\n71.4\n0\nFemale\n18\nOther\n67\n148\nCT\n23.18\n\n\n37.5\n0\nFemale\n21\nCaucasian\n66.7\n183\nCC\n28.92\n\n\n50\n0\nFemale\n28\nAsian\n61\n112\nCC\n21.16\n\n\n\n\n\nData are from an observational study investigating demographic, physiological, and genetic characteristics associated with muscle strength.\n\nndrm.ch and drm.ch are change in strength in nondominant and dominant arms before and after training\nactn3.r577x gives genotype at a particular location within the ACTN3 gene (so-called “sports gene”)"
  },
  {
    "objectID": "content/week2-datatypes.html#your-turn-identifying-data-types-ii",
    "href": "content/week2-datatypes.html#your-turn-identifying-data-types-ii",
    "title": "Introduction to data",
    "section": "Your turn: identifying data types II",
    "text": "Your turn: identifying data types II\nRecall the kimchi/prediabetes study from last time:\n\nA total of 21 participants with prediabetes were enrolled. During the first 8 weeks, they consumed either fresh (1-day-old) or fermented (10-day-old) kimchi. After a 4-week washout period, they switched to the other type of kimchi for the next 8 weeks. Consumption of both types of kimchi significantly decreased body weight, body mass index, and waist circumference. Fermented kimchi decreased insulin resistance, and increased insulin sensitivity … Systolic and diastolic blood pressure (BP) decreased significantly in the fermented kimchi group. The percentages of participants who showed improved glucose tolerance were 9.5 and 33.3% in the fresh and fermented kimchi groups, respectively.\n\n\nlist the minimal collection of variables measured\ndetermine a likely type for each variable\ninvent a few example observations to sketch what the data table might look like"
  },
  {
    "objectID": "content/week2-datatypes.html#describing-studies",
    "href": "content/week2-datatypes.html#describing-studies",
    "title": "Introduction to data",
    "section": "Describing studies",
    "text": "Describing studies\nLet’s put some pieces together. The “essential information” to report when explaining/presenting a study comprises:\n\nResearch question or hypothesis\nType of study\nNumber and type of study units\nIf applicable, experimental design\nObservations and variables measured"
  },
  {
    "objectID": "content/week2-datatypes.html#lab-data-types-and-structures-in-r",
    "href": "content/week2-datatypes.html#lab-data-types-and-structures-in-r",
    "title": "Introduction to data",
    "section": "Lab: data types and structures in R",
    "text": "Lab: data types and structures in R\nThe simplest data structure in R is a vector: a collection of values of a single “type”. This lab will introduce you to data types, vectors, and a few other common data structures.\n\n\n\n\nThe variable types we just discussed map pretty neatly (but not perfectly) onto the main “data types” in R:\n\nnumeric ➜ integer, numeric\ncategorical ➜ character, logical\n\nYour objectives in this lab are:\n\nlearn to perform simple calculations\nlearn to recognize data types and data structures"
  },
  {
    "objectID": "content/week2-datatypes.html#up-next-descriptive-statistics",
    "href": "content/week2-datatypes.html#up-next-descriptive-statistics",
    "title": "Introduction to data",
    "section": "Up next: descriptive statistics",
    "text": "Up next: descriptive statistics\nSo far we’ve discussed study designs and the basics of data.\nDescriptive statistics refers to techniques for summarizing data:\n\nnumerical summaries\ntables\ngraphical summaries\n\nSuch summaries are essential first steps in any data analysis, as they convey what was directly observed in the study.\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week1-handout-designs.html",
    "href": "content/week1-handout-designs.html",
    "title": "Distinguishing study designs",
    "section": "",
    "text": "Recall that the difference between an observational study and an experiment hinges on whether researchers intentionally intervene on the system of study (experiment) or passively record outcomes (observational study). Additionally, the following common study designs were discussed:\n\nIn prospective (“looking ahead”) studies participants are recruited before developing a condition based on exposure; results look ahead to the proportions in each exposure group that develop the condition\nIn retrospective (“looking back”) studies participants are recruited after developing a condition; results look back on the proportions in each group that met certain criteria\n\nIn this activity you’ll read abstracts from a few published studies and determine, in consultation with your group, what kind of study is described in the abstract. You do not need to consider the examples in order — start with the ones that look most interesting.\n\nExample 1: long COVID\nLong COVID is a multi-systemic and often debilitating condition that develops in at least 10% of patients following a COVID infection. The following is an excerpt of the abstract from a recent study seeking to identify symptoms and risk factors associated with long COVID and published in Nature Medicine:\n\nWe undertook a … study using a UK-based primary care database, Clinical Practice Research Datalink Aurum, to determine symptoms that are associated with confirmed SARS-CoV-2 infection beyond 12 weeks in non-hospitalized adults and the risk factors associated with developing persistent symptoms. We selected 486,149 adults with confirmed SARS-CoV-2 infection … Outcomes included 115 individual symptoms, as well as long COVID, defined as a composite outcome of 33 symptoms by the World Health Organization clinical case definition … Among the patients infected with SARS-CoV-2, risk factors for long COVID included female sex, belonging to an ethnic minority, socioeconomic deprivation, smoking, obesity and a wide range of comorbidities. The risk of developing long COVID was also found to be increased along a gradient of decreasing age.\n\nSubramanian et al. (2022). Symptoms and risk factors for long COVID in non-hospitalized adults. Nature medicine, 28(8), 1706-1714.\nDiscuss the following questions with your group:\n\nIs this an observational study or an experiment?\nIs this study retrospective, prospective, or neither?\n\n\n\nExample 2: selenium exposure and Mediterranean diet\nThe following is from the abstract of a study investigating dietary mitigation of selenium exposure:\n\nSelenium is a trace element found in many chemical forms. Selenium and its species have nutritional and toxicologic properties, some of which may play a role in the etiology of neurological disease. We hypothesized that adherence to the Mediterranean-Dietary Approach to Stop Hypertension Intervention for Neurodegenerative Delay (MIND) diet could influence intake and endogenous concentrations of selenium and selenium species, thus contributing to the beneficial effects of this dietary pattern. We carried out a cross-sectional study of 137 non-smoking blood donors (75 females and 62 males) from the Reggio Emilia province, Northern Italy. We assessed MIND diet adherence using a semiquantitative food frequency questionnaire. We assessed selenium exposure through dietary intake and measurement of urinary and serum concentrations, including speciation of selenium compound in serum … Adherence to the MIND diet was positively associated with dietary selenium intake and urinary selenium excretion, whereas it was inversely associated with serum concentrations of overall selenium and organic selenium … Our results suggest that greater adherence to the MIND diet is non-linearly associated with lower circulating concentrations of selenium and of 2 potentially neurotoxic species of this element, selenoprotein P and selenate. This may explain why adherence to the MIND dietary pattern may reduce cognitive decline.\n\nUrbano, T., et al. (2023). Adherence to the Mediterranean-DASH Intervention for Neurodegenerative Delay (MIND) diet and exposure to selenium species: A cross-sectional study. Nutrition Research.\nDiscuss the following questions with your group:\n\nDoes the abstract describe an observational study or an experiment?\nIs this study prospective, retrospective, or neither?\nConsider the finding that MIND adherence is associated with lower circulating concentrations of selenium. Does this provide evidence that adoption of the MIND diet is likely to reduce selenium concentrations? Why or why not?\n\n\n\nExample 3: fermented kimchi and glucose metabolism\nThe following is from an abstract of a study investigating possible benefits of kimchi consumption among prediabetic individuals:\n\nWith the increased incidence of diabetes mellitus, the importance of early intervention in prediabetes has been emphasized … We hypothesized that kimchi and its fermented form would have beneficial effects on glucose metabolism in patients with prediabetes. A total of 21 participants with prediabetes were enrolled. During the first 8 weeks, they consumed either fresh (1-day-old) or fermented (10-day-old) kimchi. After a 4-week washout period, they switched to the other type of kimchi for the next 8 weeks. Consumption of both types of kimchi significantly decreased body weight, body mass index, and waist circumference. Fermented kimchi decreased insulin resistance, and increased insulin sensitivity … Systolic and diastolic blood pressure (BP) decreased significantly in the fermented kimchi group. The percentages of participants who showed improved glucose tolerance were 9.5 and 33.3% in the fresh and fermented kimchi groups, respectively.\n\nAn, S. Y., et al. (2013). Beneficial effects of fresh and fermented kimchi in prediabetic individuals. Annals of Nutrition and Metabolism, 63(1-2), 111-119.\nDiscuss the following questions with your group:\n\nDoes the abstract describe an observational study or an experiment?\nIs this study retrospective, prospective, or neither?\nConsider the finding that a third of participants consuming fermented kimchi showed improved glucose tolerance. Does this provide evidence that consuming fermented kimchi is likely to improve glucose tolerance? Would you answer differently if the study had found that number were 8 in 10?\n\n\n\nExample 4: Moderna vaccine efficacy\nFrom the journal publication reporting clinical trial results for the Moderna COVID vaccine:\n\nVaccines are needed to prevent coronavirus disease 2019 (Covid-19) and to protect persons who are at high risk for complications. The mRNA-1273 vaccine is a lipid nanoparticle–encapsulated mRNA-based vaccine that encodes the prefusion stabilized full-length spike protein of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes Covid-19. This [study] was conducted at 99 centers across the United States … The trial enrolled 30,420 volunteers who were randomly assigned in a 1:1 ratio to receive either vaccine or placebo (15,210 participants in each group) … Symptomatic Covid-19 illness was confirmed in 185 participants in the placebo group and in 11 participants in the mRNA-1273 group; vaccine efficacy was 94.1%. \n\nBaden, L. R., et al. (2021). Efficacy and safety of the mRNA-1273 SARS-CoV-2 vaccine. New England journal of medicine, 384(5), 403-416.\nDiscuss the following questions with your group:\n\nIs this an observational study or an experiment?\nIs this study prospective, retrospective, or neither?\nConsider the finding that 11 symptomatic illnesses were observed in the vaccine group and 185 were observed in the placebo group. Does this provide evidence that the vaccine reduces the likelihood of symptomatic illness?"
  },
  {
    "objectID": "content/week4-intervals.html#todays-agenda",
    "href": "content/week4-intervals.html#todays-agenda",
    "title": "Interval estimation",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nHW2 remarks/discussion\n[Lecture] A basic interval estimate for the mean\n[Lecture/lab] Exploring interval coverage\n[Lecture/lab] Comparing normal and \\(t\\) models"
  },
  {
    "objectID": "content/week4-intervals.html#from-last-time",
    "href": "content/week4-intervals.html#from-last-time",
    "title": "Interval estimation",
    "section": "From last time",
    "text": "From last time\n\n\nUnder simple random sampling:\n\nthe sample mean provides a good point estimate of the population mean\nits theoretical sampling variability is given by the standard deviation \\(\\frac{\\sigma}{\\sqrt{n}} = \\frac{\\text{population SD}}{\\sqrt{\\text{sample size}}}\\)\nits estimated sampling variability is given by the standard error \\(\\frac{s_x}{\\sqrt{n}} = \\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n \nmean\nsd\n\n\n\n\nsample\n5.031\n0.9873\n\n\npopulation\n5.043\n1.075\n\n\n\n\n\n\n\n\n\n\nSo in the example: the estimated mean total HDL cholesterol among the population is 5.031 mmol/L and this point estimate is expected to deviate by 0.1396 mmol/L from the population mean on average across samples."
  },
  {
    "objectID": "content/week4-intervals.html#interval-estimation",
    "href": "content/week4-intervals.html#interval-estimation",
    "title": "Interval estimation",
    "section": "Interval estimation",
    "text": "Interval estimation\nA point estimate for the mean provides a guess at the exact value of the parameter; an interval estimate is a range of plausible values.\nIn general, an interval estimate is constructed from two main ingredients:\n\npoint estimate\nstandard error\n\nAnd one secret ingredient:\n\na model for the sampling distribution of the point estimate\n\nThe general form of an interval estimate is: \\[\\text{point estimate} \\pm \\text{margin of error}\\]"
  },
  {
    "objectID": "content/week4-intervals.html#precision-and-coverage",
    "href": "content/week4-intervals.html#precision-and-coverage",
    "title": "Interval estimation",
    "section": "Precision and coverage",
    "text": "Precision and coverage\n\n\nIntervals have two main and attributes:\n\nprecision refers to how wide or narrow the interval is\ncoverage refers to how often, under random sampling, the interval captures the parameter of interest\n\n\nThese properties are inversely related:\n\nif I say mean cholesterol is between 0 and 50 I’m almost certainly right, but the estimate is useless\nif I say mean cholesterol is between 5.0299 and 5.0301, I’ve made a very precise guess, but I’m likely wrong (think about sampling variability)\n\n\n\n\nAn accurate interval should maintain high coverage while achieving practically useful precision. This isn’t always possible!"
  },
  {
    "objectID": "content/week4-intervals.html#an-interval-for-the-mean",
    "href": "content/week4-intervals.html#an-interval-for-the-mean",
    "title": "Interval estimation",
    "section": "An interval for the mean",
    "text": "An interval for the mean\nA common interval for the population mean is: \\[\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)\\]\n\n\nBy hand: \\[5.031 \\pm 2\\times 0.1396 = (4.75, 5.31)\\]\n\nIn R:\n\nc(lwr = mean(samp) - 2*sd(samp)/sqrt(50), \n  upr = mean(samp) + 2*sd(samp)/sqrt(50))\n\n     lwr      upr \n4.751348 5.309852 \n\n\n\n\nThe precision is evident from the interval width (0.5611). But what about coverage?"
  },
  {
    "objectID": "content/week4-intervals.html#exploring-interval-coverage",
    "href": "content/week4-intervals.html#exploring-interval-coverage",
    "title": "Interval estimation",
    "section": "Exploring interval coverage",
    "text": "Exploring interval coverage\nLet’s carry on pretending that the NHANES data comprise a population.\nThe first section of lab5-intervals contains some simple commands to draw a sample and calculate an interval estimate.\n\nEach of you will generate an interval based on a different sample\nWe’ll tally how many of you obtained intervals capturing the population mean\n\nOur tally will give an approximate idea of the coverage."
  },
  {
    "objectID": "content/week4-intervals.html#more-simulation",
    "href": "content/week4-intervals.html#more-simulation",
    "title": "Interval estimation",
    "section": "More simulation",
    "text": "More simulation\n\n\nArtificially simulating a larger number of intervals provides a slightly better approximation of coverage.\n\nat right, 100 intervals\n97% cover the population mean (vertical dashed line)\n\nWhat do you expect would happen to coverage if, for the same samples…\n\na wider margin of error (say, \\(3\\times SE\\)) were used?\na narrower margin of error (say, \\(1\\times SE\\)) were used?"
  },
  {
    "objectID": "content/week4-intervals.html#so-why-2-standard-errors",
    "href": "content/week4-intervals.html#so-why-2-standard-errors",
    "title": "Interval estimation",
    "section": "So why 2 standard errors?",
    "text": "So why 2 standard errors?\n\n\nThe margin of error of \\(2\\times SE\\) comes from the so-called “empirical rule”.\n\nunder the normal model, 95% of values are within 2SD of center\nso for 95% of samples, the sample mean is within 2SD of the population mean\n\nSo in theory, according to the normal model, \\(\\bar{x} \\pm 2\\times SD\\) achieves 95% coverage.\n\n\n\n\nBut we are using standard error (SE), not standard deviation (SD). Do we still get the same coverage using the normal model?"
  },
  {
    "objectID": "content/week4-intervals.html#normal-model-coverage",
    "href": "content/week4-intervals.html#normal-model-coverage",
    "title": "Interval estimation",
    "section": "Normal model coverage",
    "text": "Normal model coverage\n\n\nAt right, the misses are compared between intervals calculated with SD (left) and SE (right) using the multiplier from the normal model on the same 10,000 simulated datasets with sample size \\(n = 15\\).\n\nSE misses more often\nso the normal model produces under-coverage\n\n\n\n\n\n\n\n\n\n\ntype\ncoverage\n\n\n\n\nsd\n0.954\n\n\nse\n0.9294\n\n\n\n\n\nWhat do you think: the multiplier should be [smaller/larger] to ensure 95% coverage."
  },
  {
    "objectID": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "href": "content/week4-intervals.html#a-closer-look-at-the-normal-model",
    "title": "Interval estimation",
    "section": "A closer look at the normal model",
    "text": "A closer look at the normal model\nAn alternate but equivalent way to understand the normal model for the sampling distribution of \\(\\bar{x}\\) is in terms of deviations. The following are equivalent:\n\nThe expression \\(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\\) measures the number of standard deviations from center."
  },
  {
    "objectID": "content/week4-intervals.html#simulating-deviations",
    "href": "content/week4-intervals.html#simulating-deviations",
    "title": "Interval estimation",
    "section": "Simulating deviations",
    "text": "Simulating deviations\nAnother way to check normal model coverage is to use deviations:\n\nSimulate many samples\nCompute scaled deviations\nTally how many scaled deviations are between -2 and 2\n\nThe proportion of samples for which the scaled deviation is between -2 and 2 approximates the coverage.\nWe’ll try it in the next part of the lab5-intervals. Hypotheses:\n\ndeviations scaled by SD should be between -2 and 2 95% of the time\ndeviations scaled by SE should be between -2 and 2 [more/less] than 95% of the time"
  },
  {
    "objectID": "content/week4-intervals.html#the-t-model",
    "href": "content/week4-intervals.html#the-t-model",
    "title": "Interval estimation",
    "section": "The \\(t\\) model",
    "text": "The \\(t\\) model\nWe’re actually using \\(\\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\\) to construct intervals, because we don’t know \\(\\sigma\\).\nThese deviations are better approximated by a \\(t\\) model, which adjusts the normal model for the extra uncertainty that comes from estimating the standard deviation.\n\n\nThe difference between models depends mainly on sample size:\n\nbehaves almost exactly the same for moderate to large samples\nlarger deviations from center for small samples\nleads to larger multipliers for computing margin of error\n\n\n\n\n\nComparison of \\(t\\) model with normal model for various degrees of freedom."
  },
  {
    "objectID": "content/week4-intervals.html#model-specification",
    "href": "content/week4-intervals.html#model-specification",
    "title": "Interval estimation",
    "section": "Model specification",
    "text": "Model specification\nThe \\(t\\) model is characterized by its degrees of freedom.\n\nfor interval estimates for the mean, \\(n - 1\\) is used\ndepending on the degrees of freedom (i.e., sample size), a different multiplier is applied to the standard error to obtain the margin of error\n\nThe multiplier is called a critical value, and can be found in R via:\n\n# pseudo code -- replace coverage with desired level, e.g., 0.95\nqt((1 - coverage)/2, df = (n - 1), lower.tail = F)\n\n\nchosen to ensure a specified nominal coverage level (usually 95%)\nhigher nominal coverage levels utilize larger critical values, producing wider intervals"
  },
  {
    "objectID": "content/week4-intervals.html#model-validation",
    "href": "content/week4-intervals.html#model-validation",
    "title": "Interval estimation",
    "section": "Model validation",
    "text": "Model validation\nUsing the \\(t\\) model should produce coverage closer to the nominal level compared with the normal model. Let’s check through simulation.\n\n\nAt right, misses are compared between intervals using SE and critical values from the normal model (left) and \\(t\\) model (right) constructed on the same 10,000 simulated datasets with sample size \\(n = 10\\).\n\n\n\n\n\n\n\n\n\nmodel\ncoverage\n\n\n\n\nnormal\n0.9219\n\n\nt\n0.9461\n\n\n\n\n\nThe \\(t\\) model produces coverage much closer to the nominal level."
  },
  {
    "objectID": "content/week4-intervals.html#calculations",
    "href": "content/week4-intervals.html#calculations",
    "title": "Interval estimation",
    "section": "Calculations",
    "text": "Calculations\nSo, to sum up, the general formula for an interval for a population mean is: \\[\\bar{x} \\pm c \\times SE(\\bar{x}) \\quad\\text{where}\\quad SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\]\n\n\nRules of thumb:\n\nfor moderate to large samples, use the normal model\n\n\\(c = 1\\) for 68% coverage\n\\(c = 2\\) for 95% coverage\n\\(c = 3\\) for 99.7% coverage\n\nfor small sample sizes, use the \\(t\\) model\nwhen in doubt, use the \\(t\\) model\n\n\nExact critical values in R:\n\n# normal critical value\nc &lt;- qnorm((1 - coverage)/2, lower.tail = F)\n\n# t critical value\nc &lt;- qt((1 - coverage)/2, df = n - 1, lower.tail = F)\n\nInterval calculation:\n\n# pseudo code\nmean(data_vec) + c(-1, 1)*c*sd(data_vec)/sqrt(n)"
  },
  {
    "objectID": "content/week4-intervals.html#interpretation",
    "href": "content/week4-intervals.html#interpretation",
    "title": "Interval estimation",
    "section": "Interpretation",
    "text": "Interpretation\nAs we’ve seen, coverage pertains to how often an interval of a particular form captures the population parameter of interest across samples of a fixed size. Loosely speaking, this represents how often you’d be right if you were to fully replicate your study ad infinitum.\nThis leads to the following interpretation:\n\nWith [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units].\n\nFor this reason, statisticians call interval estimates confidence intervals."
  },
  {
    "objectID": "content/week4-intervals.html#revisiting-initial-example",
    "href": "content/week4-intervals.html#revisiting-initial-example",
    "title": "Interval estimation",
    "section": "Revisiting initial example",
    "text": "Revisiting initial example\n\n\nSo in the example we began with:\n\n# calculate 95% interval\nmean(samp) + c(-1, 1)*2*sd(samp)/sqrt(50)\n\n[1] 4.751348 5.309852\n\n\nWith 95% confidence, the mean total HDL cholesterol is estimated to be between 4.751 and 5.31 mmol/L.\nRemember, “95% confidence” refers to coverage under sampling variation.\n\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n5.031\n0.1396\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics for Life Sciences",
    "section": "",
    "text": "Announcements\n\n\n\n\nOutlines updated for week 5\nHW2 due Thursday 2/8 11:59pm PST\nUpcoming test: Friday 2/9 (take-home)\nReading quiz responses posted (see materials page)\n\n\n\nThis is the class website for Applied Statistics for Life Sciences (STAT 218) offered at Cal Poly in Winter 2024 for sections 05-8273 (TR 2:10pm – 4:00pm) and 06-8274 (TR 4:10pm – 6:00pm). Please note that this site does not pertain to other sections of STAT 218 (of which there are several).\nMaterial posted on this site is intended for use by currently enrolled students in the above-mentioned sections for educational purposes only, and is subject to copyright. Please do not reproduce any material on this site without permission."
  },
  {
    "objectID": "content/hw/hw2.html",
    "href": "content/hw/hw2.html",
    "title": "Homework 2: Descriptive statistics",
    "section": "",
    "text": "Instructions: type up your answers and submit your work electronically via Gradescope. Questions with a learning outcome indicated in brackets will be evaluated for credit; other questions are provided for additional practice. You are expected to answer all questions. Note that an R project with datasets and prompts is provided on the class posit.cloud workspace. Please do not submit R codes; show only output or graphics relevant to answering the question.\n\n[L3] The oibiostat::frog dataset contains measurements on samples of frog egg clutches collected at various study sites in early 2013 to investigate the effect of altitude on relative investment in egg size versus clutch size (number of eggs). Visualize the frequency distributions of clutch volume, egg size, and clutch size. For each variable, describe the shape and modality of the distribution and calculate appropriate measures of spread and center.\n\n\nlibrary(oibiostat)\ndata(frog)\n?oibiostat::frog\n\n\nVu and Harrington exercise 1.8. Below are some observations from a study collecting data to analyze smoking habits of UK residents.\n\nWhat does each row of this table represent?\nHow many participants were included in the study?\nFor each variable, indicate whether it is numerical or categorical. If numerical, indicate whether it is continuous or discrete; if categorical, indicate if it is nominal or ordinal.\n\n\n\n\n[L2] Vu and Harrington exercise 1.17. The following scatterplot shows life expectancies and percentages of internet users for 208 countries.\n\nDescribe the relationship between life expectancy and percentage of internet users. Specifically: is there an apparent association, and if so, is it positive or negative and linear or nonlinear?\nState a possible confounding variable that might explain this relationship and describe how the confounder might relate to both the percentage of internet users and the life expectancy of a country.\nAre these data experimental or observational? And if you had to guess, were they obtained by a random or nonrandom sample, and why?\n\n\n\n\nVu and Harrington exercise 1.28. For each of the distributions (a), (b), and (c), describe the shape and modality and identify the matching boxplot from (1), (2), and (3).\n\n\n\nVu and Harrington exercise 1.36.\n\n\n\n[L3] The oibiostat::yrbss dataset contains measurements on a small collection of variables from 13,583 survey responses collected as part of the CDC’s Youth Risk Behavior Surveillance System (YRBSS) from 1991-2013. The objective of the survey program is to track behaviors with potential negative physical and mental health impacts among adolescents.\n\nSummarize the racial composition of survey respondents by academic grade. Produce both a contingency table and a proportional bar plot. Make sure to choose the correct (row or column) normalization for your bar plot so that it shows the racial composition by grade (not the grade composition by race). Are there apparent differences in racial composition across grades?\nMake a bar plot showing the frequency distribution of hours of sleep on school nights. Based on the summary, what is the typical amount of sleep respondents get on school nights?\nProduce a tabular or graphical summary that addresses the question: do older students sleep more on school nights than younger students?\nVisualize the frequency distribution of the number of days per week that survey participants are physically active. Describe the distribution and indicate whether the variable is discrete or continuous. Is the mean an appropriate measure of center for this data? Why or why not?\nProduce side-by-side boxplots visualizing the number of days per week that survey participants are physically active by grade. Based on the graphical summary, do there appear to be differences in physical activity by grade? Explain.\n\n\n\nlibrary(oibiostat)\ndata(yrbss)\n?oibiostat::yrbss\n\n\n[L3] Vu and Harrington exercise 1.39. Trait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. People with high trait anger have rage and fury more often, more intensely, and with long-laster episodes than people with low trait anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart disease; 12,986 participants were recruited for a study examining this hypothesis. Participants were followed for five years. The following table shows data for the participants identified as having normal blood pressure (normotensives).\n\nWhat percentage of participants have moderate anger scores?\nWhat percentage of participants who experienced a CHD event have moderate anger scores?\nWhat percentage of participants with high trait anger scores experienced a CHD event?\nWhat percentage of participants with low trait anger scores experienced a CHD event?\nWhat is the ratio of the percentages in (c) and (d)? (This is called a “relative risk” of CHD events.) Based on this, does it appear that the risk of a CHD event is higher in the high trait anger group?\nProduce a proportional bar plot that substantiates your answer in (e)."
  },
  {
    "objectID": "content/labs/lab5-intervals.html",
    "href": "content/labs/lab5-intervals.html",
    "title": "Lab 5: Confidence intervals",
    "section": "",
    "text": "The focus of this lab is on confidence intervals, with a particular emphasis on understanding the meaning of coverage (i.e., confidence level) and exploring this concept through simulation. There are three main learning objectives:\nWe will use the same total cholesterol variable from the NHANES data and continue to pretend that these 3,179 observations form a population for which we have complete data. You’ll need to load the dataset and extract the cholesterol variable.\n# load dataset\nload('data/nhanes.RData')\n\n# extract cholesterol variable\ncholesterol &lt;- nhanes$TotChol\nRemember, we’re pretending that cholesterol contains all population values."
  },
  {
    "objectID": "content/labs/lab5-intervals.html#exploring-interval-coverage",
    "href": "content/labs/lab5-intervals.html#exploring-interval-coverage",
    "title": "Lab 5: Confidence intervals",
    "section": "Exploring interval coverage",
    "text": "Exploring interval coverage\nFirst you’ll draw a single sample and calculate an interval estimate for the mean. The commands below will do this for you with no changes. Your goal will be to obtain an interval using these commands and compare your result with those of your groupmates. Since each of you is drawing a sample at random, your results will differ slightly.\n\n# fix sample size\nn &lt;- 35\n\n# draw a sample of size 35\nmy_samp &lt;- sample(cholesterol, size = n)\n\n# calculate mean and standard error\nxbar &lt;- mean(my_samp)\nxbar.se &lt;- sd(my_samp)/sqrt(n)\n\n# calculate the interval\nxbar.interval &lt;- xbar + c(-1, 1)*2*xbar.se\n\n# display\nxbar.interval\n\n[1] 4.763290 5.471567\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nDetermine whether your interval captured the parameter of interest. Compute the population mean and compare with the interval estimate.\n\n# does the interval contain the population mean?\n\n\n\nNow imagine your group was really big, say, 10,000 students, and you all generated one interval each. The proportion of you who captured the population mean measures the coverage of the interval. We will simulate this in class to determine the coverage of the interval you just calculated."
  },
  {
    "objectID": "content/labs/lab5-intervals.html#simulating-deviations",
    "href": "content/labs/lab5-intervals.html#simulating-deviations",
    "title": "Lab 5: Confidence intervals",
    "section": "Simulating deviations",
    "text": "Simulating deviations\nThe multiplier \\(2\\times SE\\) comes from the normal model for the sampling distribution of the point estimate \\(\\bar{x}\\). This particular multiplier is intended to achieve 95% coverage — so in theory, 95 out of every 100 intervals will capture the population mean. The normal model can be expressed as a model for the scaled deviations \\(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\\), where the scaling factor uses the population standard deviation \\(\\sigma/\\sqrt{n}\\). A nominal 95% coverage arises from the assumption that 95% of these deviations are between -2 and 2. But when we calculate an interval, we’re actually applying the normal model to \\(\\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\\), where the deviations have been scaled by the standard error \\(s_x/\\sqrt{n}\\). Are 95% of these deviations between -2 and 2?\nThe commands below allow you to simulate scaled deviations of sample means from a large number of samples, using both (a) population standard deviation and (b) standard error as scaling factors. Don’t worry about understanding the codes too much — as long as you understand the output, that is sufficient. Your task is to run these commands and compare the coverage of the normal model when SE is used in place of SD.\n\n# fix sample size \nn &lt;- 5\n\n# function to simulate scaled deviation of one sample mean\nsim_dev &lt;- function(n, data){\n  samp &lt;- sample(data, size = n)\n  xbar &lt;- mean(samp)\n  xbar.se &lt;- sd(samp)/sqrt(n)\n  xbar.sd &lt;- sd(data)/sqrt(n)\n  mu &lt;- mean(data)\n  dev &lt;- c(se = (xbar - mu)/xbar.se, \n           sd = (xbar - mu)/xbar.sd)\n  return(dev)\n}\n\n# repeat many simulations\nnsim &lt;- 1000\nsim.devs &lt;- sapply(1:nsim, function(i){sim_dev(n, data = cholesterol)}) |&gt; \n  t() |&gt; \n  as.data.frame()\n\n# how many are between -2 and 2 using SD?\ncoverage.sd &lt;- sum(abs(sim.devs$sd) &lt; 2)/nsim\ncoverage.sd\n\n[1] 0.955\n\n# using SE?\ncoverage.se &lt;- sum(abs(sim.devs$se) &lt; 2)/nsim\ncoverage.se\n\n[1] 0.879\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nRun the codes above and discuss with your group:\n\nWhat did these commands do? Affirm your understanding of this simulation by listing the steps performed.\nDo either, none, or both scaled deviations achieve the nominal coverage of 95%?\nIf you change the sample size, do you get a different answer?"
  },
  {
    "objectID": "content/labs/lab5-intervals.html#calculating-an-interval",
    "href": "content/labs/lab5-intervals.html#calculating-an-interval",
    "title": "Lab 5: Confidence intervals",
    "section": "Calculating an interval",
    "text": "Calculating an interval\nIn light of the under-coverage resulting from the normal model for small sample sizes, the \\(t\\) model provides a better interval in practice.\n\\[\\bar{x} \\pm c \\times SE(\\bar{x})\\]\nIn the expression above, \\(c\\) is a “critical value” obtained from the \\(t\\) model with \\(n - 1\\) degrees of freedom; its exact value depends on the desired coverage. The commands below show you how to calculate each piece and form an interval.\n\n# fix sample size and desired coverage\nn &lt;- 10\ncoverage &lt;- 0.95\n\n# draw one sample\nsamp &lt;- sample(cholesterol, size = n)\n\n# interval ingredients\nxbar &lt;- mean(samp)\nxbar.se &lt;- sd(samp)/sqrt(n)\nc.val &lt;- qt((1 - coverage)/2, df = n - 1, lower.tail = F)\n\n# calculate interval\nxbar + c(-1, 1)*c.val*xbar.se\n\n[1] 4.639147 6.136853\n\n\nYour answer will differ slightly from this result, since you will draw a different sample when you run these commands.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the same sample above, compute a 90% confidence interval for mean total HDL cholesterol. Discuss with your group and interpret the interval in context.\n\n# adjust coverage and calculate a new critical value\n\n# calculate the interval"
  },
  {
    "objectID": "content/week5-hypothesis.html#todays-agenda",
    "href": "content/week5-hypothesis.html#todays-agenda",
    "title": "Hypothesis testing",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz [2pm section] [4pm section]\nHypothesis tests for a population mean\nLab: \\(t\\)-tests in R\n(If time) Exploring decision errors"
  },
  {
    "objectID": "content/week5-hypothesis.html#ddt-data",
    "href": "content/week5-hypothesis.html#ddt-data",
    "title": "Hypothesis testing",
    "section": "DDT data",
    "text": "DDT data\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm). Each measurement was taken by a different laboratory.\n\n\n2.79, 2.93, 3.22, 3.78, 3.22, 3.38, 3.18, 3.33, 3.34, 3.06, 3.07, 3.56, 3.08, 4.64 and 3.34\n\n\n\nC. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits."
  },
  {
    "objectID": "content/week5-hypothesis.html#hypothesis-testing",
    "href": "content/week5-hypothesis.html#hypothesis-testing",
    "title": "Hypothesis testing",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nThis is an example of a hypothesis testing problem: we want to test the hypothesis that mean DDT in kale is within safe limits. Hypothesis testing is another form of statistical inference.\nThe general pattern for performing a hypothesis test is:\n\nFormulate the hypothesis to test in terms of the values of a population parameter.\nAssess the likelihood of the data under the hypothesis through use of a “test statistic”.\nConclude whether the data provide evidence favoring an alternative.\n\nToday we’ll cover each step in turn in the context of tests for a population mean."
  },
  {
    "objectID": "content/week5-hypothesis.html#components-of-a-test",
    "href": "content/week5-hypothesis.html#components-of-a-test",
    "title": "Hypothesis testing",
    "section": "Components of a test",
    "text": "Components of a test\n\n\n\n\n\n\n\n\nComponent\nExplanation\nDDT example\n\n\n\n\nPopulation parameter\nThe quantity of interest\nMean DDT \\(\\mu\\)\n\n\nNull hypothesis\nThe claim to be tested\n\\(\\mu \\leq 3\\)\n\n\nAlternative hypothesis\nThe alternative claim\n\\(\\mu &gt; 3\\)\n\n\nTest statistic\nA function of the sample data and the null value of the population parameter\n\\(T = \\frac{\\bar{x} - \\mu_0}{s_x/\\sqrt{n}} = 2.91\\)\n\n\nModel\nSampling distribution of the test statistic under \\(H_0\\)\n\\(t_{df = 14}\\) model\n\n\n\\(p\\)-value\nProbability under \\(H_0\\) of obtaining a result at least as favorable to \\(H_A\\)\n0.575% of samples are more favorable to \\(H_A\\)\n\n\nDecision\nReject or fail to reject \\(H_0\\)\nReject at \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#formulating-hypotheses",
    "href": "content/week5-hypothesis.html#formulating-hypotheses",
    "title": "Hypothesis testing",
    "section": "1. Formulating hypotheses",
    "text": "1. Formulating hypotheses\nHypotheses cannot be tested in isolation, but must be considered relative to a specified alternative.\nTo articulate the hypotheses for a test, we need:\n\npopulation parameter of interest\nnull hypothesis \\(H_0\\): possible value(s) under the claim to be tested\nalternative hypothesis \\(H_A\\): possible value(s) if the claim is found to be false\n\nIn the context of the DDT example…\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\]"
  },
  {
    "objectID": "content/week5-hypothesis.html#test-statistic",
    "href": "content/week5-hypothesis.html#test-statistic",
    "title": "Hypothesis testing",
    "section": "2. Test statistic",
    "text": "2. Test statistic\nTest statistics are data summaries that:\n\ndepend on the null value of the population parameter\nhave a known sampling distribution\n\nFor a population mean, we use: \\[T = \\frac{\\bar{x} - \\mu_0}{s_x/\\sqrt{n}}\\]\nThis is well-described by a \\(t_{n - 1}\\) model when \\(\\mu = \\mu_0\\). It is useful for the test because:\n\nlarge (absolute) values of \\(T\\) are unlikely if \\(\\mu = \\mu_0\\)\nsmall (absolute) values of \\(T\\) are expected if \\(\\mu = \\mu_0\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#drawing-a-conclusion",
    "href": "content/week5-hypothesis.html#drawing-a-conclusion",
    "title": "Hypothesis testing",
    "section": "3. Drawing a conclusion",
    "text": "3. Drawing a conclusion\n\n\nIn the DDT example, \\(T\\) = 2.906. This favors \\(H_A\\), but by how much?\nAccording to the \\(t\\) model, less than 1% of samples would produce a result more favorable to \\(H_A\\).\n\n\n\n\n\n\nPoint estimate:\n\n\n\n\n\n\n\n\n\nmean\nse\n\n\n\n\n3.328\n0.1129\n\n\n\n\n\nTest statistic:\n\\[T = \\frac{\\bar{x} - \\mu_0}{SE(\\bar{x})} = \\]\n\nThis is strong evidence against the claim that the DDT level is 3ppm or less and in favor of the claim that the DDT level exceeds 3ppm."
  },
  {
    "objectID": "content/week5-hypothesis.html#p-values",
    "href": "content/week5-hypothesis.html#p-values",
    "title": "Hypothesis testing",
    "section": "\\(p\\)-values",
    "text": "\\(p\\)-values"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-logic-of-a-test",
    "href": "content/week5-hypothesis.html#the-logic-of-a-test",
    "title": "Hypothesis testing",
    "section": "The logic of a test",
    "text": "The logic of a test\nWhy did we find evidence against \\(H_0\\) in favor of \\(H_A\\)? Let’s consider…\n\nThe test statistic is based on a value of the parameter under \\(H_0\\)\nLarger values favor \\(H_A\\), smaller values favor \\(H_0\\)\nFor our sample, \\(\\bar{x} &gt; \\mu_0\\), which favors \\(H_A\\)\nAssuming \\(H_0\\) is true, XX% of samples would be at least as favorable to \\(H_A\\)\nIf this proportion is small, our results are not likely under \\(H_0\\)"
  },
  {
    "objectID": "content/week5-hypothesis.html#choosing-a-null-value",
    "href": "content/week5-hypothesis.html#choosing-a-null-value",
    "title": "Hypothesis testing",
    "section": "Choosing a null value",
    "text": "Choosing a null value\nWhy did we choose \\(\\mu_0 = 3\\) for the test statistic if \\(H_0\\) includes any and all values less than or equal to 3?\nAny null value farther from the alternative will produce stronger results.\n\n\n\n\n\n\n\n\nNull value\nTest statistic numerator\nProportion of samples more favorable to \\(H_A\\)\n\n\n\n\n\\(\\mu_0 = 3\\)\n0.328\n0.00575\n\n\n\\(\\mu_0 = 2.99\\)\n0.338\n0.00483\n\n\n\\(\\mu_0 = 2.95\\)\n0.378\n0.00239"
  },
  {
    "objectID": "content/week5-hypothesis.html#the-logic-of-hypothesis-testing",
    "href": "content/week5-hypothesis.html#the-logic-of-hypothesis-testing",
    "title": "Hypothesis testing",
    "section": "The logic of hypothesis testing",
    "text": "The logic of hypothesis testing"
  },
  {
    "objectID": "content/week5-hypothesis.html#strength-of-evidence",
    "href": "content/week5-hypothesis.html#strength-of-evidence",
    "title": "Hypothesis testing",
    "section": "Strength of evidence",
    "text": "Strength of evidence\nThe result that 0.575% of samples would produce a test statistic more strongly favoring the alternative hypothesis is an example of a p-value:\n\nthe probability under \\(H_0\\) of obtaining a sample for which the test statistic is at least as favorable to \\(H_A\\) as the value actually observed\n\nIn other words, \\(p\\)-values assume the null hypothesis is true, and then ask, “what is the chance I’d obtain data at least as suggestive as what I have that the alternative is more likely than the null?”\n\nsmaller \\(p\\)-values: if \\(H_0\\) is true, equally or more favorable results are not expected often by chance\nlarger \\(p\\)-values: if \\(H_0\\) is true, equally or more favorable results are expected often by chance"
  },
  {
    "objectID": "content/week5-hypothesis.html#composite-hypotheses",
    "href": "content/week5-hypothesis.html#composite-hypotheses",
    "title": "Hypothesis testing",
    "section": "Composite hypotheses",
    "text": "Composite hypotheses\nThe null hypothesis is a composite of values, so why did we choose just one (\\(\\mu_0 = 3\\)) to perform the test?\nAny null value farther from the alternative will produce stronger results.\n\n\n\n\n\n\n\n\nNull value\nTest statistic numerator\nProportion of samples more favorable to \\(H_A\\)\n\n\n\n\n\\(\\mu_0 = 3\\)\n0.328\n0.00575\n\n\n\\(\\mu_0 = 2.99\\)\n0.338\n0.00483\n\n\n\\(\\mu_0 = 2.95\\)\n0.378\n0.00239\n\n\n\nSo by using \\(\\mu_0 = 3\\), we are choosing the most conservative null value for the test."
  },
  {
    "objectID": "content/week5-hypothesis.html#interpreting-results",
    "href": "content/week5-hypothesis.html#interpreting-results",
    "title": "Hypothesis testing",
    "section": "Interpreting results",
    "text": "Interpreting results\nBecause of the anatomy of hypothesis tests, there are two possible findings:\n\n[above evidence threshold] reject \\(H_0\\) in favor of \\(H_A\\)\n[below evidence threshold] fail to reject \\(H_0\\)\n\n\nSince the null is assumed to be true to perform the test, the test can only result in (a) evidence against this assumption or (b) no evidence against this assumption. But because it’s an assumption, we don’t affirm it if the test fails.\n\nIn the DDT example: the data provide sufficiently strong evidence (p = 0.00575) to reject the hypothesis that mean DDT in kale is at most 3ppm in favor of the hypothesis that mean DDT in kale exceeds 3ppm."
  },
  {
    "objectID": "content/week5-hypothesis.html#your-turn-expressing-hypotheses",
    "href": "content/week5-hypothesis.html#your-turn-expressing-hypotheses",
    "title": "Hypothesis testing",
    "section": "Your turn: expressing hypotheses",
    "text": "Your turn: expressing hypotheses"
  },
  {
    "objectID": "content/week5-hypothesis.html#errors-and-evidence-thresholds",
    "href": "content/week5-hypothesis.html#errors-and-evidence-thresholds",
    "title": "Hypothesis testing",
    "section": "Errors and evidence thresholds",
    "text": "Errors and evidence thresholds\nThe end result of a test should be a decision about the hypotheses in question. So, it remains to define an evidence threshold above which we decide to reject \\(H_0\\).\nConsider:\n\nif indeed mean DDT in kale is 3ppm, then 0.575% of DDT samples produce test statistics at least as favorable to the alternative as what we saw in the study\nso if we set the evidence threshold for rejecting \\(H_0\\) exactly here, we’ll be wrong 0.575% of the time"
  },
  {
    "objectID": "content/week5-hypothesis.html#evidence-thresholds",
    "href": "content/week5-hypothesis.html#evidence-thresholds",
    "title": "Hypothesis testing",
    "section": "Evidence thresholds",
    "text": "Evidence thresholds\nIt remains to define an evidence threshold above which we decide to reject \\(H_0\\).\nA heuristic is to fix a significance level \\(\\alpha\\) and reject \\(H_0\\) whenever \\(p &lt; \\alpha\\).\n\nrepresents an evidence threshold\nconventionally, \\(\\alpha = 0.05\\)\ncontrols error rates\n\nImagine that indeed mean DDT in kale is 3ppm. Then 0.575% of samples produce test statistics at least as favorable to the alternative as what we saw in the study.\n\nso if we set the evidence threshold for rejecting \\(H_0\\) exactly here (\\(\\alpha = 0.00575\\)) we’ll be wrong 0.575% of the time\nif we set the evidence threshold lower (say \\(\\alpha = 0.01\\)) we’ll be wrong more than 0.575% of the time (in fact 1% of the time)"
  },
  {
    "objectID": "content/week5-hypothesis.html#error-types",
    "href": "content/week5-hypothesis.html#error-types",
    "title": "Hypothesis testing",
    "section": "Error types",
    "text": "Error types"
  },
  {
    "objectID": "content/week5-hypothesis.html#evidence-thresholds-and-errors",
    "href": "content/week5-hypothesis.html#evidence-thresholds-and-errors",
    "title": "Hypothesis testing",
    "section": "Evidence thresholds and errors",
    "text": "Evidence thresholds and errors\nThe end result of a test should be a decision about the hypotheses in question. So, it remains to define an evidence threshold above which we decide to reject \\(H_0\\).\nImagine that indeed mean DDT in kale is 3ppm. Then:\n\n0.575% of samples produce test statistics at least as favorable to the alternative as what we saw in the study\nso if we set the evidence threshold for rejecting \\(H_0\\) exactly here, we’ll be wrong 0.575% of the time\nif we set the evidence threshold lower, we’ll be wrong more than 0.575% of the time\n\nA heuristic is to fix a significance level \\(\\alpha\\) and reject \\(H_0\\) whenever \\(p &lt; \\alpha\\). This controls the false rejection rate, also known as a type I error."
  },
  {
    "objectID": "content/week5-hypothesis.html#decision-rules",
    "href": "content/week5-hypothesis.html#decision-rules",
    "title": "Hypothesis testing",
    "section": "Decision rules",
    "text": "Decision rules\nThe end result of a test should be a decision about the hypotheses in question. So, it remains to define an evidence threshold above which we decide to reject \\(H_0\\).\nA heuristic is to fix a significance level \\(\\alpha\\) and reject \\(H_0\\) whenever \\(p &lt; \\alpha\\)."
  },
  {
    "objectID": "content/week5-hypothesis.html#errors",
    "href": "content/week5-hypothesis.html#errors",
    "title": "Hypothesis testing",
    "section": "Errors",
    "text": "Errors\nImagine that indeed mean DDT in kale is 3ppm. Then:\n\n0.575% of samples produce test statistics at least as favorable to the alternative as what we saw in the study\nso if we set the evidence threshold for rejecting \\(H_0\\) exactly here, we’ll be wrong 0.575% of the time\nif we set the evidence threshold lower, we’ll be wrong more than 0.575% of the time\n\nThese are errors of the first type. Our decision heuristic controls these errors."
  },
  {
    "objectID": "content/week5-hypothesis.html#directional-hypotheses",
    "href": "content/week5-hypothesis.html#directional-hypotheses",
    "title": "Hypothesis testing",
    "section": "Directional hypotheses",
    "text": "Directional hypotheses\nTests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.\n\n\n\n\n\n\n\n\n\nTest type\nNull\nAlternative\nFavors alternative\n\n\n\n\nUpper-sided\n\\(\\mu \\leq \\mu_0\\)\n\\(\\mu &gt; \\mu_0\\)\npositive \\(T\\)\n\n\nLower-sided\n\\(\\mu \\geq \\mu_0\\)\n\\(\\mu &lt; \\mu_0\\)\nnegative \\(T\\)\n\n\nTwo-sided\n\\(\\mu = \\mu_0\\)\n\\(\\mu \\neq \\mu_0\\)\nlarge \\(|T|\\)\n\n\n\nThe direction of the test affects the \\(p\\)-value calculation (and thus decision), but won’t change the test statistic.\n\n# upper-sided\nt.test(ddt, mu = mu_0, alternative = 'greater')\n\n# lower-sided\nt.test(ddt, mu = mu_0, alternative = 'less')\n\n# two-sided (default)\nt.test(ddt, mu = mu_0, alternative = 'two.sided')"
  },
  {
    "objectID": "content/week5-hypothesis.html#decision-errors",
    "href": "content/week5-hypothesis.html#decision-errors",
    "title": "Hypothesis testing",
    "section": "Decision errors",
    "text": "Decision errors\nThere are two ways to make mistakes, and two ways to get the answer right.\n\nYou can’t optimize both error rates simultaneously.\n\nThe test heuristics we’ve outlined control type I error rates at \\(\\alpha\\)\nThese tests have the lowest type II error rates subject to a limit on type I error"
  },
  {
    "objectID": "content/week5-hypothesis.html#decision-errors-1",
    "href": "content/week5-hypothesis.html#decision-errors-1",
    "title": "Hypothesis testing",
    "section": "Decision errors",
    "text": "Decision errors\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-hypothesis.html#lab-t-tests-in-r",
    "href": "content/week5-hypothesis.html#lab-t-tests-in-r",
    "title": "Hypothesis testing",
    "section": "Lab: \\(t\\)-tests in R",
    "text": "Lab: \\(t\\)-tests in R\nOpen up lab6-hypotesting in the class workspace. The goals for this lab are:\n\nLearn how to implement \\(t\\) tests in R and interpret output\nPractice formulating and testing hypotheses from simple research questions\n(If time) Explore decision errors\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-hypothesis.html#another-example-weight-loss-goals",
    "href": "content/week5-hypothesis.html#another-example-weight-loss-goals",
    "title": "Hypothesis testing",
    "section": "Another example: weight loss goals",
    "text": "Another example: weight loss goals\n\nDoes the average U.S. adult sleep at least 7 hours per night?\n\n\n\nData are reported average hours of sleep per night from 135 NHANES respondents.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n6.896\n1.394\n0.12\n\n\n\n\n\n\\(t_{134}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#your-turn-body-temperatures",
    "href": "content/week5-hypothesis.html#your-turn-body-temperatures",
    "title": "Hypothesis testing",
    "section": "Your turn: body temperatures",
    "text": "Your turn: body temperatures\n\nIs mean body temperature actually 98.6 °F, or is it lower?\n\n\n\nData are 130 observations of body temperature (°F) derived from a JAMA study.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n98.25\n0.7332\n0.0643\n\n\n\n\n\n\\(t_{129}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#another-example-sleep",
    "href": "content/week5-hypothesis.html#another-example-sleep",
    "title": "Hypothesis testing",
    "section": "Another example: sleep",
    "text": "Another example: sleep\n\nDoes the average U.S. adult sleep at least 7 hours per night?\n\n\n\nData are reported average hours of sleep per night from 135 NHANES respondents.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n6.896\n1.394\n0.12\n\n\n\n\n\n\\(t_{134}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#recap",
    "href": "content/week5-hypothesis.html#recap",
    "title": "Hypothesis testing",
    "section": "Recap",
    "text": "Recap\n\nIs the mean DDT level in kale 3ppm or less?\n\n\n\nData are measurements of DDT levels in ppm from 15 labs.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n3.328\n0.4372\n0.1129\n\n\n\n\n\n\\(t_{14}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#recap-of-ddt-example",
    "href": "content/week5-hypothesis.html#recap-of-ddt-example",
    "title": "Hypothesis testing",
    "section": "Recap of DDT example",
    "text": "Recap of DDT example\n\nIs the mean DDT level in kale 3ppm or less?\n\n\n\nData are measurements of DDT levels in ppm from 15 labs.\nPopulation parameter:\n\\[H_0:\\hspace{15cm}\\] \\[H_A:\\hspace{15cm}\\] \\[\\mu_0 =\\hspace{15cm}\\] \\[T=\\hspace{15cm}\\] % of samples more favorable to \\(H_A \\approx\\)\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nse\n\n\n\n\n3.328\n0.4372\n0.1129\n\n\n\n\n\n\\(t_{14}\\) model:"
  },
  {
    "objectID": "content/week5-hypothesis.html#performing-tests-in-r",
    "href": "content/week5-hypothesis.html#performing-tests-in-r",
    "title": "Hypothesis testing",
    "section": "Performing tests in R",
    "text": "Performing tests in R\n\n\nInputs:\n\ndata vector\nnull value of parameter\nalternative hypothesis\n\nOutputs:\n\ntest statistic\ndegrees of freedom for \\(t\\) model\n\\(p\\)-value\nconfidence interval\npoint estimate\n\n\nt.test performs all calculations. Locate each input (1-3) and output (4-7) below:\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#performing-tests-in-r-t.test",
    "href": "content/week5-hypothesis.html#performing-tests-in-r-t.test",
    "title": "Hypothesis testing",
    "section": "Performing tests in R: t.test",
    "text": "Performing tests in R: t.test\n\n\nInputs:\n\ndata vector\nnull value of parameter\nalternative hypothesis\n\nOutputs:\n\ntest statistic\ndegrees of freedom for \\(t\\) model\n\\(p\\)-value\nconfidence interval\npoint estimate\n\n\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328"
  },
  {
    "objectID": "content/labs/lab6-hypotesting.html",
    "href": "content/labs/lab6-hypotesting.html",
    "title": "Lab 6: Hypothesis testing",
    "section": "",
    "text": "The objective of this lab is to learn how to perform \\(t\\)-tests for a population mean in R. We will cover:"
  },
  {
    "objectID": "content/labs/lab6-hypotesting.html#datasets",
    "href": "content/labs/lab6-hypotesting.html#datasets",
    "title": "Lab 6: Hypothesis testing",
    "section": "Datasets",
    "text": "Datasets\nIn this lab we’ll use two datasets:\n\nddt are the 15 measurements from lecture of DDT level (ppm) in kale\nbody.temps are 130 observations of body temperature from a sample of US adults\n\n\nlibrary(oibiostat)\n\nddt &lt;- MASS::DDT\nstr(ddt)\n\n num [1:15] 2.79 2.93 3.22 3.78 3.22 3.38 3.18 3.33 3.34 3.06 ...\n\ndata(thermometry)\nbody.temps &lt;- thermometry$body.temp\nstr(body.temps)\n\n num [1:130] 96.3 96.7 96.9 97 97.1 97.1 97.1 97.2 97.3 97.4 ..."
  },
  {
    "objectID": "content/labs/lab6-hypotesting.html#performing-a-t-test-in-r",
    "href": "content/labs/lab6-hypotesting.html#performing-a-t-test-in-r",
    "title": "Lab 6: Hypothesis testing",
    "section": "Performing a \\(t\\) test in R",
    "text": "Performing a \\(t\\) test in R\nWe’ll illustrate the use of t.test with the ddt data. The parameter of interest here is:\n\\[\n\\mu = \\text{mean DDT in kale}\n\\]\n\nUpper-sided test\nConsider first testing the hypotheses:\n\\[\n\\begin{align*}\nH_0: \\mu \\leq 3 \\qquad(\\text{mean DDT in kale} \\leq 3) \\\\\nH_A: \\mu &gt; 3 \\qquad(\\text{mean DDT in kale} &gt; 3)\n\\end{align*}\n\\]\nThe null parameter value is the cutoff point \\(\\mu_0 = 3\\). This is an upper-sided test because the alternative specifies that the parameter exceeds the null value. We implement this test by providing as arguments to t.test:\n\ndata vector ddt\nnull parameter value mu = 3\nupper sided alternative alternative = 'greater'\n\n\n# upper sided test H0: mean ddt &lt;= 3 vs HA: mean ddt &gt; 3\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328 \n\n\nTake a moment to locate each of the following outputs:\n\ntest statistic value\ndegrees of freedom for the \\(t\\) model\n\\(p\\)-value\nconfidence interval for the mean\npoint estiamte for the mean\n\nSince \\(p &lt; 0.05\\), we reject \\(H_0\\) at significance level \\(\\alpha = 0.05\\). Interpret the result:\n\nThe data provide sufficiently strong evidence to reject the null hypothesis that mean DDT in kale is at most 3 ppm in favor of the alternative hypothesis that mean DDT in kale exceeds 3ppm (p = 0.00575).\n\n\n\nLower-sided test\nNow suppose you wish to test whether the mean DDT in kale is at least 3.5ppm.\n\\[\n\\begin{align*}\nH_0: \\mu \\geq 3.5 \\qquad(\\text{mean DDT in kale} \\geq 3.5) \\\\\nH_A: \\mu &lt; 3.5 \\qquad(\\text{mean DDT in kale} &lt; 3.5)\n\\end{align*}\n\\]\nThe null parameter value is the cutoff point \\(\\mu_0 = 3.5\\). This is a lower-sided test because the alternative specifies that the parameter is smaller than the null value. We implement this test by providing as arguments to t.test:\n\nnull parameter value mu = 3.5\nupper sided alternative alternative = 'less'\n\n\n# lower-sided test, H0: mean ddt &gt;= 3.5 vs HA: mean ddt &lt; 3.5\nt.test(ddt, mu = 3.5, alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = -1.5238, df = 14, p-value = 0.07491\nalternative hypothesis: true mean is less than 3.5\n95 percent confidence interval:\n     -Inf 3.526803\nsample estimates:\nmean of x \n    3.328 \n\n\nSince \\(p &gt; 0.05\\), we fail to reject \\(H_0\\) at significance level \\(\\alpha = 0.05\\). Interpret the result:\n\nThe data do not provide sufficiently strong evidence to reject the null hypothesis that mean DDT in kale is at least 3.5 ppm in favor of the alternative hypothesis that mean DDT in kale exceeds 3.5ppm (p = 0.075).\n\n\n\nTwo-sided test\nLastly, suppose we wish to test whether mean DDT is 3 or not.\n\\[\n\\begin{align*}\nH_0: \\mu = 3 \\qquad(\\text{mean DDT in kale} = 3) \\\\\nH_A: \\mu \\neq 3 \\qquad(\\text{mean DDT in kale} \\neq 3)\n\\end{align*}\n\\]\nThe null parameter value is the point \\(\\mu_0 = 3\\). This is a two-sided test because the alternative specifies that the parameter is either greater or smaller than the null value. We implement this test by providing as arguments to t.test:\n\nnull parameter value mu = 3\nupper sided alternative alternative = 'two.sided'\n\n\n# two-sided test, H0: mean ddt == 3 vs HA: mean ddt =!= 3\nt.test(ddt, mu = 3, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.01151\nalternative hypothesis: true mean is not equal to 3\n95 percent confidence interval:\n 3.085913 3.570087\nsample estimates:\nmean of x \n    3.328 \n\n\nSince \\(p &lt; 0.05\\), we reject \\(H_0\\) at significance level \\(\\alpha = 0.05\\). Interpret the result:\n\nThe data provide sufficiently strong evidence to reject the null hypothesis that mean DDT in kale is 3 ppm in favor of the alternative hypothesis that mean DDT in kale is not 3ppm (p = 0.012).\n\nNotice that, the evidence against the null is slightly weaker with respect to the two-sided alternative than with respect to the upper-sided alternative. This makes sense, because the alternative comprises a larger range of values, some of which are not very consistent with the data."
  },
  {
    "objectID": "content/labs/lab6-hypotesting.html#answering-questions-with-t-tests",
    "href": "content/labs/lab6-hypotesting.html#answering-questions-with-t-tests",
    "title": "Lab 6: Hypothesis testing",
    "section": "Answering questions with \\(t\\) tests",
    "text": "Answering questions with \\(t\\) tests\n\nBody temperatures\nAnswer the following questions with hypothesis tests using the body.temps data. Be sure to consider how to frame the hypotheses appropriately to answer the question.\n\nIs the mean body temperature different from 98.6 °F?\nIs the mean body temperature higher than 98 °F?\nIs the mean body temperature higher than 98.2 °F?\nIs the mean body temperature lower than 98.2 °F?\nIs the mean body temperature actually 98.2 °F?\nIs the mean body temperature actually 98.3 °F?\n\n\n\n\n\n\n\nYour turn\n\n\n\nWrite commands to perform tests to answer questions 1-6 above.\n\n# 1. Is the mean body temperature different from 98.6 °F?\n\n# 2. Is the mean body temperature higher than 98 °F?\n\n# 3. Is the mean body temperature higher than 98.2 °F?\n\n# 4. Is the mean body temperature lower than 98.2 °F?\n\n# 5. Is the mean body temperature actually 98.2 °F?\n\n# 6. Is the mean body temperature actually 98.3 °F?\n\nInterpret the result of each test in context. Use significance level \\(\\alpha = 0.05\\) to make a decision.\n\n\n\n\n\n\n\n\nRemark\n\n\n\nThe hypothesis tests suggest that in fact, mean body temperature is lower than commonly thought. However, a visual inspection of the data reveals an interesting twist: the body temperature data are actually bimodal!\n\n\n\n\n\nThis doesn’t invalidate any of our inferences, but suggests that we might be asking the wrong question by focusing on the population mean if, in fact, there is no one mean body temperature. We should perhaps ask instead, what feature of the population explains the bimodal distribution?"
  },
  {
    "objectID": "content/labs/lab6-hypotesting.html#exploring-decision-errors",
    "href": "content/labs/lab6-hypotesting.html#exploring-decision-errors",
    "title": "Lab 6: Hypothesis testing",
    "section": "Exploring decision errors",
    "text": "Exploring decision errors\nIf there is time in class, we’ll explore decision errors a little. If not, you can simulate this activity by repeatedly running the commands and making tallies on your own.\nThere are two ways to make an error in a hypothesis test.\n\n\nType I errors\nFirst we’ll all generate a sample from 3,179 observations of total HDL cholesterol, pretending that the full set of observations constitutes a population. Each of us will obtain a different sample. Using our respective samples, we’ll each test whether the population mean is 5.043 and see how many of us produce an erroneous conclusion.\n\\[H_0: \\mu = 5.043\\] \\[H_A: \\mu \\neq 5.043\\]\n5.043 is the true population mean, so in point of fact \\(H_0\\) is true and we should not reject; any rejections are therefore type I errors.\n\nload('data/nhanes.RData')\n\n# \"true\" population mean\npop_mean &lt;- mean(nhanes$TotChol)\n\n# draw a sample \nsamp &lt;- sample(nhanes$TotChol, size = 20)\n\n# test a true null\nt.test(samp, mu = pop_mean, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  samp\nt = 1.1134, df = 19, p-value = 0.2794\nalternative hypothesis: true mean is not equal to 5.042938\n95 percent confidence interval:\n 4.808389 5.810611\nsample estimates:\nmean of x \n   5.3095 \n\n\nRecall that the significance level (denoted \\(\\alpha\\)) determines the decision rule: we reject if \\(p &lt; \\alpha\\). Let’s tally errors as follows:\n\n\n\nSignificance level\nError frequency\n\n\n\n\n\\(\\alpha = 0.2\\)\n\n\n\n\\(\\alpha = 0.1\\)\n\n\n\n\\(\\alpha = 0.05\\)\n\n\n\n\\(\\alpha = 0.02\\)\n\n\n\n\nWe should see that the type I error rate is approximately equal to the significance level. The significance level, in fact, directly controls type I error.\n\n\nType II errors\nNow let’s test whether the population mean is some \\(\\mu_0 \\neq 5.043\\). In this case, \\(H_0\\) will be false, and a correct decision is to reject \\(H_0\\). Any failures to reject will be considered type II errors.\n\n# draw a sample \nsamp &lt;- sample(nhanes$TotChol, size = 20)\n\n# test a false null\nt.test(samp, \n       mu = 4.2, # change this for exercise\n       alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  samp\nt = 4.2466, df = 19, p-value = 0.0004364\nalternative hypothesis: true mean is not equal to 4.2\n95 percent confidence interval:\n 4.614833 5.421167\nsample estimates:\nmean of x \n    5.018 \n\n\nLet’s use significance level \\(\\alpha = 0.05\\) throughout and tally errors as follows:\n\n\n\nNull value \\(\\mu_0\\)\nError frequency\n\n\n\n\n4.2\n\n\n\n4.6\n\n\n\n4.9\n\n\n\n5.1\n\n\n\n5.4\n\n\n\n5.7\n\n\n\n\nNotice that the type II error is quite high for null values near the true mean; this is because the test prioritizes avoiding type I errors, and as a result has little power to detect such alternatives."
  },
  {
    "objectID": "content/week5-hypothesis.html#summarizing-test-results",
    "href": "content/week5-hypothesis.html#summarizing-test-results",
    "title": "Hypothesis testing",
    "section": "Summarizing test results",
    "text": "Summarizing test results\n\n\nTo summarize, you should:\n\nState the hypotheses tested\nInterpret the conclusion of the test in context\nProvide the \\(T\\) statistic, degrees of freedom, and \\(p\\)-value\nState and interpret the point estimate and interval for the mean\n\n\nt.test performs all calculations. Locate each input (1-3) and output (4-7) below:\n\nt.test(ddt, mu = 3, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328"
  },
  {
    "objectID": "content/week5-hypothesis.html#reporting-test-results",
    "href": "content/week5-hypothesis.html#reporting-test-results",
    "title": "Hypothesis testing",
    "section": "Reporting test results",
    "text": "Reporting test results\n\n\nTo report the result of a hypothesis test, you should:\n\nState the hypotheses tested\nInterpret the conclusion of the test in context\nProvide the \\(T\\) statistic, degrees of freedom, and \\(p\\)-value\nState and interpret the point estimate and interval for the mean\n\n\n\nWe tested the null hypothesis that mean DDT in kale is 3ppm or less against the alternative that mean DDT exceeds 3ppm. The data provide sufficiently strong evidence to reject the hypothesis that mean DDT in kale is 3ppm or less in favor of the alternative that mean DDT exceeds 3ppm (\\(T = 2.906\\) on \\(14\\) degrees of freedom, \\(p = 0.0058\\)). With 95% confidence, the mean DDT level is estimated to be at least 3.129 ppm, with a point estimate of 3.328 (\\(SE = 0.113\\))."
  },
  {
    "objectID": "content/week5-twosample.html#todays-agenda",
    "href": "content/week5-twosample.html#todays-agenda",
    "title": "Two-sample inference",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReading quiz [2pm section] [4pm section]\nMore one-sample inference: reporting test results; decision errors\nComparing two population means\nLab: more \\(t\\)-tests in R"
  },
  {
    "objectID": "content/week5-twosample.html#from-last-time",
    "href": "content/week5-twosample.html#from-last-time",
    "title": "Two-sample inference",
    "section": "From last time",
    "text": "From last time\n\n\n\nWhat hypotheses were tested? \\[  \\]\nWhat was the test statistic and p-value? \\[  \\]\nWhat was the sample size? \\[  \\]\nWhat is the conclusion of the test? \\[  \\]\nInterpret the confidence interval.\n\n\nInference for the body temperature data.\n\ndata(thermometry)\nbody_temps &lt;- thermometry$body.temp\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923"
  },
  {
    "objectID": "content/week5-twosample.html#reporting-test-results",
    "href": "content/week5-twosample.html#reporting-test-results",
    "title": "Two-sample inference",
    "section": "Reporting test results",
    "text": "Reporting test results\n\n\nTo report the result of a hypothesis test, you should:\n\nLead with your conclusion\nState the hypotheses tested\nInterpret the conclusion of the test in context\nProvide the \\(T\\) statistic, degrees of freedom, and \\(p\\)-value\nState and interpret the point estimate and interval for the mean\n\n\n\nOur results suggest mean body temperature is less than 98.6 °F. We tested the null hypothesis that mean body temperature is 98.6 °F or greater against the alternative that mean body temperature is less than 98.6 °F. The data provide sufficiently strong evidence against the hypothesis that mean body temperature is 98.6 °F or greater in favor of the alternative that mean body temperature is less than 98.6 °F (T = -5.4548 on 129 degrees of freedom, p-value = .0000001205). With 95% confidence, the mean nightly hours of sleep is estimated to be at most 98.36 °F, with a point estimate of 98.23 (SE = 0.0643)."
  },
  {
    "objectID": "content/week5-twosample.html#swimsuit-data",
    "href": "content/week5-twosample.html#swimsuit-data",
    "title": "Two-sample inference",
    "section": "Swimsuit data",
    "text": "Swimsuit data\n\nAre swimmers faster in bodysuits than in regular swimsuits?\n\nBelow are the first few observations of the average velocity of competitive swimmers in a 1500m; one measurement was taken in a swimsuit, the other in a bodysuit.\n\n\n\n\n\n\n\n\n\n\nswimmer.number\nwet.suit.velocity\nswim.suit.velocity\n\n\n\n\n1\n1.57\n1.49\n\n\n2\n1.47\n1.37\n\n\n3\n1.42\n1.35\n\n\n\n\n\nCan you formulate a pair of hypotheses to test to answer this question using methods from last time?"
  },
  {
    "objectID": "content/week5-twosample.html#inference-for-paired-data",
    "href": "content/week5-twosample.html#inference-for-paired-data",
    "title": "Two-sample inference",
    "section": "Inference for paired data",
    "text": "Inference for paired data\n\nCalculate paired differences.\n\n\n\n\n\n\n\n\n\n\n\n\nswimmer.number\nwet.suit.velocity\nswim.suit.velocity\nvelocity.diff\n\n\n\n\n1\n1.57\n1.49\n0.08\n\n\n2\n1.47\n1.37\n0.1\n\n\n3\n1.42\n1.35\n0.07\n\n\n\n\n\n\n\n\nPerform test as before.\n\n\n# test for paired difference\nt.test(diffs, mu = 0, alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  diffs\nt = 12.318, df = 11, p-value = 4.443e-08\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 0.06620114        Inf\nsample estimates:\nmean of x \n   0.0775 \n\n\n\n\nReport the result of the test. You try."
  },
  {
    "objectID": "content/week5-twosample.html#comparing-two-means",
    "href": "content/week5-twosample.html#comparing-two-means",
    "title": "Two-sample inference",
    "section": "Comparing two means",
    "text": "Comparing two means\nWe can formulate the question as a comparison of two means:\n\\[H_0: \\mu_\\text{bodysuit} \\leq \\mu_\\text{swimsuit}\\] \\[H_A: \\mu_\\text{bodysuit} &gt; \\mu_\\text{swimsuit}\\] It is common to express hypotheses of this form in terms of a difference in means: \\[H_0: \\underbrace{\\mu_\\text{bodysuit} - \\mu_\\text{swimsuit}}_\\delta \\leq 0\\] \\[H_A: \\underbrace{\\mu_\\text{bodysuit} - \\mu_\\text{swimsuit}}_\\delta &gt; 0\\]"
  },
  {
    "objectID": "content/week5-twosample.html#comparing-two-means-1",
    "href": "content/week5-twosample.html#comparing-two-means-1",
    "title": "Two-sample inference",
    "section": "Comparing two means",
    "text": "Comparing two means\n\n\n  Treatment  n\n1    Seeded 26\n2  Unseeded 26\n\n\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-twosample.html#pairing",
    "href": "content/week5-twosample.html#pairing",
    "title": "Two-sample inference",
    "section": "Pairing",
    "text": "Pairing\nThe velocities in the swimsuit dataset are paired because every swimmer is measured in both suits.\nData are paired just in case the measurements in each group are taken on exactly the same study units.\n\nif you can calculate a difference for each study unit, your data are paired; otherwise, your data are not paired\n\nThis is the easiest situation to handle, because it reduces to a one-sample problem with the observed differences."
  },
  {
    "objectID": "content/week5-twosample.html#your-turn-sleep-data",
    "href": "content/week5-twosample.html#your-turn-sleep-data",
    "title": "Two-sample inference",
    "section": "Your turn: sleep data",
    "text": "Your turn: sleep data\n\n\nWork with your group on just one of the questions below.\n\nDo US adults sleep 7.5 hours per night on average?\nDo US adults sleep less than 7.5 hours per night on average?\nDo US adults sleep more than 7.5 hours per night on average?\nDo US adults sleep more than 6.5 hours per night on average?\n\n\nYour task is to determine and carry out an appropriate test, and then write a complete report of the test outcome:\n\nanswer the question\nhypotheses tested\ntest conclusion, interpreted in context\ntest statistic, degrees of freedom, \\(p\\)-value\nconfidence interval and point estimate, interpreted in context\n\nThese elements should be summarized together in complete sentences."
  },
  {
    "objectID": "content/week5-twosample.html#test-interval-duality",
    "href": "content/week5-twosample.html#test-interval-duality",
    "title": "Two-sample inference",
    "section": "Test-interval duality",
    "text": "Test-interval duality"
  },
  {
    "objectID": "content/week5-twosample.html#decision-errors",
    "href": "content/week5-twosample.html#decision-errors",
    "title": "Two-sample inference",
    "section": "Decision errors",
    "text": "Decision errors"
  },
  {
    "objectID": "content/week5-twosample.html#cloud-seeding-data",
    "href": "content/week5-twosample.html#cloud-seeding-data",
    "title": "Two-sample inference",
    "section": "Cloud seeding data",
    "text": "Cloud seeding data\n\n\n  Treatment  n\n1    Seeded 26\n2  Unseeded 26"
  },
  {
    "objectID": "content/week5-twosample.html#inference-for-independent-data",
    "href": "content/week5-twosample.html#inference-for-independent-data",
    "title": "Two-sample inference",
    "section": "Inference for independent data",
    "text": "Inference for independent data\n\n\n\nSTAT218"
  },
  {
    "objectID": "content/week5-twosample.html#choosing-alternatives",
    "href": "content/week5-twosample.html#choosing-alternatives",
    "title": "Two-sample inference",
    "section": "Choosing alternatives",
    "text": "Choosing alternatives\n\n\n\nIs the mean body temp less than 98.6?\n\nWhich test should you use? Consider the interpretations:\n\n[lower] evidence favoring lower temp\n[upper] no evidence against lower temp\n\nThe conclusions are consistent but not equivalent – (a) is a better answer.\nYour alternative should be the claim you hope to support with evidence, and your null the claim you hope to refute.\n\n\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -5.4548, df = 129, p-value = 1.205e-07\nalternative hypothesis: true mean is less than 98.6\n95 percent confidence interval:\n     -Inf 98.35577\nsample estimates:\nmean of x \n 98.24923 \n\nt.test(body_temps, \n       mu = 98.6, \n       alternative = 'greater')\n\n\n    One Sample t-test\n\ndata:  body_temps\nt = -5.4548, df = 129, p-value = 1\nalternative hypothesis: true mean is greater than 98.6\n95 percent confidence interval:\n 98.14269      Inf\nsample estimates:\nmean of x \n 98.24923"
  },
  {
    "objectID": "content/week5-twosample.html#significance-levels",
    "href": "content/week5-twosample.html#significance-levels",
    "title": "Two-sample inference",
    "section": "Significance levels",
    "text": "Significance levels\nThe conventional significance level is \\(\\alpha = 0.05\\), which leads to the following decision rule:\n\n\\(p &lt; 0.05\\): reject \\(H_0\\)\n\\(p \\geq 0.05\\): fail to reject \\(H_0\\)\n\nAlternatively, the \\(p\\)-value is often interpreted as a measure of the weight of evidence:\n\n\\(p &lt; 0.01\\): strong evidence against \\(H_0\\)\n\\(0.01 \\leq p &lt; 0.05\\): moderate evidence against \\(H_0\\)\n\\(0.05 \\leq p &lt; 0.1\\): weak evidence against \\(H_0\\)\n\\(0.1 \\leq p\\): no evidence against \\(H_0\\)\n\nYou may use either convention to interpret test results."
  },
  {
    "objectID": "content/week5-twosample.html#significance-conventions",
    "href": "content/week5-twosample.html#significance-conventions",
    "title": "Two-sample inference",
    "section": "Significance conventions",
    "text": "Significance conventions\n\n\nConvention 1: statistical significance\n\n\\(p &lt; 0.05\\): reject \\(H_0\\)\n\\(p \\geq 0.05\\): fail to reject \\(H_0\\)\n\n\n“The data provide significant evidence at level \\(\\alpha\\) = 0.05 against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (T = -5.4548 on 129 degrees of freedom, p = .0000002411).”\n\n\nConvention 2: weight of evidence against \\(H_0\\)\n\n\\(p &lt; 0.01\\): strong evidence\n\\(0.01 \\leq p &lt; 0.05\\): moderate evidence\n\\(0.05 \\leq p &lt; 0.1\\): weak evidence\n\\(0.1 \\leq p\\): no evidence\n\n\n“The data provide strong evidence against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (T = -5.4548 on 129 degrees of freedom, p = .0000002411).”\n\n\n\nYou may use either convention to interpret test results."
  }
]