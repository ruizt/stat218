{
  "hash": "79c6f2c4a0b91b04879dd464dae6ec58",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture slides\"\nsubtitle: \"Applied Statistics for Life Sciences\"\nformat: \n  html:\n    toc: true\n  docx:\n    toc: false\nprefer-html: true\nembed-resources: true\nexecute: \n  echo: false\n  warning: false\n  message: false\n---\n\n\n\n# Lecture 1: Welcome to STAT218\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. Course logistics\n2. [lecture] Study designs\n3. [activity, if time] Distinguishing study types\n\n## Icebreakers\n\nBy show of hands...\n\n::: incremental\n1.  First statistics class ever?\n2.  Last statistics class ever?\n3.  Expect to take STAT313?\n4.  Expect to use statistics for your degree coursework or senior project?\n5.  Considering a statistics or data science minor?\n:::\n\n## Class composition\n\nBy the numbers...\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/class info-1.png)\n:::\n:::\n\n\n\n## Statistics and uncertainty\n\n> Life is full of uncertainty, and this can make a lot of questions hard to answer, because similar situations do not always result in the same outcome.\n\nStatistical thinking: **uncertainty is measurable**.\n\nWhat statistics can offer:\n\n-   principles for designing studies and collecting data in order to capture outcome variability\n-   data analytic tools to distinguish random from systematic variability\n-   heuristics to make inferences that account for uncertainty\n\n## Course goal and scope\n\nThe overarching goal of 218 is to introduce you to statistics in a hands-on way that is relevant to your major.\n\nSo we will focus on:\n\n-   statistical thinking, study design, and data analysis\n-   classical methods, mostly developed 1900-1940\n-   case studies from life sciences\n\n## Materials\n\n**Computer/tablet.** You'll need a laptop (preferred) or tablet with keyboard (workable).\n\n**Course website**. All materials are hosted/linked on the course website. I *won't* be using Canvas.\n\n**Textbook**. Vu and Harrington (2020). [Introdutory Statistics for the Life and Biomedical Sciences](https://www.openintro.org/book/biostat/). I suggest a $5-15 donation.\n\n**Statistical software**. R/RStudio hosted online via posit.cloud workspace. You will need to create an account and purchase a $5/month student subscription.\n\n## Class meetings\n\nClass meetings will usually consist of a reading quiz, a lecture, a break, and a lab.\n\n**Preparing for class meetings:**\n\n1.   Check the course website for posted reading, materials, and assignments.\n2.   Complete readings *in advance* of the class meetings for which they are listed. \n3. Write down one question you have about the reading and bring it to class.\n4.   Download and/or print a copy of the posted course notes (slides) for you to annotate and bring them to class.\n\n## Assignments\n\nYou will have three categories of assignments:\n\n- **homework** problems: two per class due by next class\n- **tests**: every 2-3 weeks, distributed Wednesday, due Friday\n- a **project**: find and present a case study\n\nDeadline policies:\n\n- one-hour grace period on all deadlines\n- four homework problem sets can be turned in up to 48 hours late without notice\n- besides free lates, extensions must be arranged 24 hours in advance of the deadline\n\n## Grades\n\nEvery graded question/problem is matched to one or more of the 11 course learning outcomes.\n\n- Questions/problems are evaluated as satisfactory (S), needs improvement (NI), or missing (M).\n\n- For each outcome, the percentage of questions/problems awarded a satisfactory mark is used to determine whether that outcome is fully met, partly met, or not met:\n\n    + fully met: 80% or more of matched questions satisfactory\n    + partly met: 50% -- 80% of matched questions satisfactory\n    + not met: less than 50% of matched questions satisfactory\n\nYour course grade is based on how many learning outcomes are fully met. To pass, you must partly or fully meet at least 6 outcomes; for a C-, you must fully meet at least 3 outcomes.\n\n## Important policies\n\n- extensions must be confirmed (not simply requested) 24 hours in advance\n- collaboration on homework is encouraged, but everyone involved needs to...\n  \n    + make a contribution\n    + write up their own work\n    \n- submitting AI-generated content in place of your own work is not acceptable\n\n    + responsible use is okay, but not recommended (GPT outputs are misleading)\n    + penalties for AI plagiarism depend on precedent and severity\n\n Minor offense | Major offense | Penalty\n---|---|---\nFirst |  | loss of credit and warning\nSecond | First | loss of credit and OSRR report\nThird | Second | course failure and second OSRR report\n\n## What is a study?\n\nA **study** is an effort to collect data in order to answer one or more research questions.\n\n-   studies must be well-matched to research questions to provide good answers\n\n-   how data are obtained is just as important as how the resulting data are analyzed\n\n-   no analysis, no matter how sophisticated will rescue a poorly conceived study\n\nA **study unit** is the smallest object or entity that is measured in a study; also called *experimental unit* or *observational unit.*\n\n## Two types of studies\n\n**Observational studies** collect data from an existing situation without intervention.\n\n-   Aim is to detect associations and patterns\n\n-   Can't be used to establish causal links\n\n**Experiments** collect data from a situation in which one or more interventions have been introduced by the investigator.\n\n-   Aim is to draw conclusions about the causal effect of interventions\n-   Stronger form of scientific evidence than an observational study\n\nOften, observational studies are used to explore/generate hypotheses prior to designing an experiment.\n\n## Comparing study types\n\nEither type of study can be used to address a question.\n\n| Question                                          | Observational study                                                      | Experiment                                                                                   |\n|---------------------------------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|\n| Are diet and mood related?                        | Conduct surveys on diet, lifestyle, and affect                           | Recruit study participants, assign diets, measure affect                                     |\n| Is vaping safer than smoking?                     | Follow groups of vapers and smokers over time and record health outcomes | Among a group of smokers, assign some to switch to vaping; compare health outcomes over time |\n| Do insecticide applications affect soil microbes? | Analyze soil samples from farms using different insecticides             | \\[**Your turn**\\]                                                                            |\n\nCan you think of pros and cons for each study type?\n\n## Why does intervention matter?\n\nControl over conditions allows a researcher to study causal effects resulting from interventions. This is not possible in observational studies due to the potential for **confounding**.\n\n::: columns\n::: {.column width=\"60%\"}\nConfounding: an unobserved condition is associated with both the study condition and the outcome.\n\n- Failure to measure and account for confounders potentially distorts observed associations\n- Example: a study finds that dog owners live longer, but doesn't measure exercise; so it might just be the daily walks.\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n```{mermaid}\nflowchart TD\n  A(unobserved variable) --- B(study variable) & C(study outcome) \n```\n\n\n:::\n:::\n\nThis is very common in observational studies, because you can't measure every study condition.\n\n## Antidote: randomization\n\nThe ability to control study conditions allows researchers to randomly allocate interventions among study subjects.\n\n::: columns\n**Randomization eliminates confounding** by isolating the condition(s) of interest:\n\n::: {.column width=\"60%\"}\n-   interventions are independent of extraneous conditions ⟹ no association possible\n\n-   if outcomes differ systematically according to the intervention, you can be certain that it is not an artifact\n:::\n\n::: {.column width=\"40%\"}\n\n\n```{mermaid}\nflowchart TD\n  A(unobserved condition) x-.-x B(study condition)\n  A --- C(outcome)\n```\n\n\n:::\n:::\n\n## Practical consequences\n\nThe ability to randomize interventions in experiments means:\n\n-   observed associations are independent of extraneous factors\n\n-   results can support causal inferences\n\nThe absence of randomization in observational studies means:\n\n-   confounding is always possible\n\n-   results may be misleading\n\n## Experimental designs\n\nA **treatment** is an experimental intervention; the **design** of an experiment refers to how treatments are allocated to study units.\n\nThe most basic design is:\n\n-   \\[balanced\\] each treatment is replicated an equal number of times\n\n-   \\[randomized\\] treatments are allocated completely at random to study units\n\n-   \\[no crossover\\] each study unit receives exactly one treatment\n\nWe'll call this a **completely randomized design**. It's the only kind of experimental design we're going to consider in STAT218.\n\nThere are many other designs that we won't discuss (but see STAT313); these are all about improving experimental efficiency by controlling extraneous variation.\n\n## Data collection\n\nStudy units should be chosen so as to represent a larger collection.\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/srs.png)\n:::\n\n::: {.column width=\"50%\"}\nA study **population** is a collection of all study units of interest.\n\nA **sample** is a subcollection from a population:\n\n-   *random* if study units have a known chance of inclusion in the sample\n-   *nonrandom* or *convenience* otherwise\n:::\n:::\n\nThe gold standard is the **simple random sample**: each study unit in the population has an equal chance of inclusion in the sample.\n\n## LEAP Study\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\nLearning early about peanut allergy (LEAP) study:\n\n-   640 infants in UK with eczema or egg allergy but no peanut allergy enrolled\n\n-   each infant randomly assigned to peanut consumption and peanut avoidance groups\n\n    -   peanut consumption: fed 6g peanut protein daily until 5 years old\n\n    -   peanut avoidance: no peanut consumption until 5 years old\n\n-   at 5 years old, oral food challenge (OFC) allergy test administered\n\n-   13.3% of the avoidance group developed allergies, compared with 1.9% of the consumption group\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.callout-note icon=false}\n## Study characteristics\n\n**Study type:** experiment\n\n**Study population:** UK infants with eczema or egg allergy but no peanut allergy\n\n**Sample:** 640 infants from population\n\n**Study design:** completely randomized design\n\n**Treatments:** peanut consumption; peanut avoidance\n\n**Study outcome:** development of peanut allergy by 5 years of age\n:::\n\n::: {.callout-tip icon=false}\n## Study results\nModerated peanut consumption causes a reduction in the likelihood of developing an allergy.\n:::\n\n:::\n\n:::\n\n\n## Checklist for next time\n\n1. Obtain a copy of the textbook.\n2. Create a posit.cloud account and purchase a student subscription. Ensure you can access the `stat218-s24` workspace.\n3. Complete practice problems and reading before class.\n4. Write down one question about the reading.\n5. Print a paper or virtual copy of the slides.\n    \n## Posit cloud account\n\nGo to: course webpage \\> syllabus \\> materials. Then look for the link to join the class workspace:\n\n![](img/posit-syllabus.png){fig-align=\"center\"}\n\n## Posit cloud account\n\nFollow prompts to create an account. Use your Cal Poly email.\n\n![](img/posit-signup.png){fig-align=\"center\"}\n\n## Posit cloud account\n\nOnce your email is verified, return to posit.cloud (or click the link in the syllabus again), and join the class workspace.\n\n![](img/posit-join.png){fig-align=\"center\"}\n\n## Posit cloud account\n\nUpgrade your account to the student plan. Input payment details.\n\n![](img/posit-upgrade.png){fig-align=\"center\"}\n\n## Printing slides\n\n![Open menu from lower left](img/printing-slides-1.png)\n\n## Printing slides\n\n![Navigate to tools](img/printing-slides-2.png)\n\n## Printing slides\n\n![Select PDF export mode](img/printing-slides-3.png)\n\n## Printing slides\n\n![Then print from browser to PDF](img/printing-slides-4.png){width=550 fig-align=\"center\"}\n\nI suggest landscape layout and either 1, 2 or 4 slides per page\n\n# Lecture 2: Data semantics and data types\n\n## Today's agenda\n\n1.  Reading quiz\n2.  [lecture] data semantics and data types\n3.  [lab] R basics\n\n\n## Data semantics\n\n-   **Data** are a set of measurements.\n\n-   A **variable** is any measured attribute of study units.\n\n-   An **observation** is a measurement of one or more variables taken on one particular study unit.\n\nIt is usually expedient to arrange data values in a table in which each row is an observation and each column is a variable:\n\n![](img/data-matrix.png)\n\n## LEAP example\n\nA table showing the observations and variables for the LEAP study would look like this:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-------------------------------------------------------\n participant.ID    treatment.group     ofc.test.result \n---------------- -------------------- -----------------\n  LEAP_100522     Peanut Consumption      PASS OFC     \n\n  LEAP_103358     Peanut Consumption      PASS OFC     \n\n  LEAP_105069      Peanut Avoidance       PASS OFC     \n\n  LEAP_105328     Peanut Consumption      PASS OFC     \n-------------------------------------------------------\n\n\n:::\n:::\n\n\n\nThe table you saw in the reading was a **summary** of the data (not the data itself):\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n----------------------------------------------\n         &nbsp;           FAIL OFC   PASS OFC \n------------------------ ---------- ----------\n  **Peanut Avoidance**       36        227    \n\n **Peanut Consumption**      5         262    \n----------------------------------------------\n\n\n:::\n:::\n\n\n\n## Numeric and categorical variables\n\nVariables are classified according to their values. Values can be one of two different types:\n\n-   A variable is **numeric** if its value is a number\n-   A variable is **categorical** if its value is a category, usually recorded as a name or label\n\nFor example:\n\n-   the value of `sex` can be male or female, so it is categorical\n-   whereas `age` (in years) can be any positive integer, so it is numeric\n\n<!-- *Check your understanding: in the LEAP study...* -->\n\n<!-- -   *treatment group is \\[numeric/categorical\\]* -->\n<!-- -   *OFC test result is \\[numeric/categorical\\]* -->\n\n## Variable subtypes\n\nFurther distinctions are made based on the type of number or type of category used to measure an attribute. *Can you match the subtypes to the variables at right?*\n\n::: columns\n::: {.column width=\"0.65\"}\n![](img/variable-types.png)\n:::\n\n::: {.column width=\"0.35\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------------------\n age   hispanic   grade   weight \n----- ---------- ------- --------\n 15      not       10     78.02  \n\n 18    hispanic    12     78.47  \n\n 17      not       11     95.26  \n\n 18      not       12     95.26  \n---------------------------------\n\n\n:::\n:::\n\n\n:::\n:::\n\n-   a numerical variable is **discrete** if there are 'gaps' between its *possible* values\n-   a numerical variable is **continuous** if there are no such gaps\n-   a categorical variable is **nominal** if its levels are not ordered\n-   a categorical variable is **ordinal** if its levels are ordered\n\n## Many ways to measure attributes\n\nVariable type (or subtype) is not an inherent quality --- attributes can often be measured in many different ways.\n\nFor instance, `age` might be measured as either a discrete, continuous, or ordinal variable, depending on the situation:\n\n| Age (years) | Age (minutes) | Age (brackets) |\n|-------------|---------------|----------------|\n| 12          | 6307518.45    | 10-18          |\n| 8           | 4209187.18    | 5-10           |\n| 21          | 11258103.08   | 18-30          |\n\n> Numeric variables can always be represented as categorical, *but not the other way around*.\n\n## Your turn\n\nClassify each variable as nominal, ordinal, discrete, or continuous:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-------------------------------------------------------\n ndrm.ch   genotype    sex     age     race       bmi  \n--------- ---------- -------- ----- ----------- -------\n  33.3        CT      Female   19    Caucasian   21.01 \n\n  71.4        CT      Female   18      Other     23.18 \n\n  37.5        CC      Female   21    Caucasian   28.92 \n\n   50         CC      Female   28      Asian     21.16 \n-------------------------------------------------------\n\n\n:::\n:::\n\n\n\nData are from an observational study investigating demographic, physiological, and genetic characteristics associated with muscle strength.\n\n-   `ndrm.ch` is change in strength in nondominant arm after resistance training\n-   `genotype` indicates genotype at a particular location within the ACTN3 gene\n\n## Common summary statistics\n\n> A **statistic** is a data summary: in mathematical terms, a function of several observations\n\n::: {.columns}\n\n::: {.column}\nFor numeric variables, the most common summary statistic is the **average value**:\n\n$$\\text{average} = \\frac{\\text{sum of values}}{\\text{# observations}}$$\n\nFor example, the average percent change in nondominant arm strength was 53.291%.\n:::\n\n::: {.column}\nFor categorical variables, the most common summary statistic is a **proportion**:\n\n$$\\text{proportion}_i = \\frac{\\text{# observations in category } i}{\\text{# observations}}$$\n\nFor example:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------\n   CC       CT       TT   \n-------- -------- --------\n 0.2908   0.4387   0.2706 \n--------------------------\n\nTable: Genotype proportions\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n## Descriptive analyses\n\nSometimes, a few clever summary statistics can be used to answer a research question.\n\n> How much does the average change in arm strength differ by genotype, if at all?\n\nComputing *per-genotype* averages provides an answer:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------------------\n genotype   avg.change   n.obs   prop.obs \n---------- ------------ ------- ----------\n    TT        58.08       161     0.2706  \n\n    CT        53.25       261     0.4387  \n\n    CC        48.89       173     0.2908  \n------------------------------------------\n\n\n:::\n:::\n\n\n\nNumber of observations and proportions are included because they provide information about genotype frequencies in the sample.\n\n- conveys how many individuals were measured\n- also provides an estimate of genotype frequencies in the population\n\n## Common mathematical notation\n\nWhile we won't use mathematical expressions too often in STAT218, it's useful to be aware of some common notations.\n\nTypically, a set of observations is written as:\n\n$$x_1, x_2, \\dots, x_n$$\n\n- $x$ represents the variable (*e.g.*, genotype, age, percent change, etc.)\n- subscript indexes observations: $x_i$ is the $i$th observation\n- $n$ is the total **n**umber of observations\n\nThe sum of the observations is written $\\sum_i x_i$, where the symbol $\\sum$ stands for 'summation'. This is useful for writing the formula for computing an average:\n\n$$\\bar{x} = \\frac{1}{n}\\sum_i x_i$$\n\n## Lab: data basics in R\n\nThe variable types we just discussed map pretty neatly (but not perfectly) onto the main \"data types\" in R:\n\n-   numeric ➜ integer, numeric\n-   categorical ➜ character, factor, logical\n\nThe primary way data are arranged in R is in a **data frame**. This lab will show you how to load, inspect, and use data frames.\n\nYour objectives in this lab are: \n\n1. learn to load and inspect datasets\n2. learn to recognize data types\n3. learn to perform simple calculations (averages, etc.)\n\n## Opening the lab activity\n\nNavigate to posit.cloud. Then:\n\n![](img/posit-landing.png){fig-align=\"center\"}\n\n1.  Make sure the class workspace \"stat218-s24\" is highlighted at left. If \"Your Workspace\" is highlighted, you won't see the example assignment.\n\n2.  Click on the `lab1-rbasics`, then wait.\n\nOnce everyone is ready, we'll have a look at the example files together.\n\n# Lecture 3: Descriptive statistics\n\n## Today's agenda\n\n1. \\[lecture\\] frequency distributions; measures of spread and center\n2. \\[lab\\] descriptive statistics and simple graphics in R\n\n## Last time\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\n\n1. Data semantics\n\n- **categorical** data: ordinal (ordered) or nominal (unordered)\n- **numeric** data: continuous (no 'gaps') or discrete ('gaps')\n\n2. Data types and data structures in R\n\n- basic types: numeric, character, logical, integer\n- a **vector** is a collection of values of one type\n- a **data frame** is a type-heterogeneous list of vectors of equal length\n\n:::\n\n::: {.column width=\"40%\"}\n\nVectors can store observations of one variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4 observations of age\nages <- c(18, 22, 18, 12)\nages\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18 22 18 12\n```\n\n\n:::\n:::\n\n\n\nData frames can store observations of many variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3 observations of 2 variables\ndata.frame(subject.id = c(11, 2, 31),\n           age = c(24, 31, 17),\n           sex = c('m', 'm', 'f'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  subject.id age sex\n1         11  24   m\n2          2  31   m\n3         31  17   f\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n*Techniques for summarizing data depend on the data type*\n\n\n## What are descriptive statistics?\n\nWe learned last time that a statistic is a data summary, *i.e.*, any function of a set of observations.\n\n**Descriptive statistics** refers to analysis of sample characteristics using summary statistics.\n\n- these are data analyses that uses statistics interpreted on face value\n- in contrast to **inferential statistics**, which uses statistics interpreted relative to a broader population\n\nDescriptive statistics can be either numerical or graphical; we'll discuss both.\n\n## Dataset: FAMuSS study\n\nObservational study of 595 individuals comparing change in arm strength before and after resistance training between genotypes for a region of interest on the ACTN3 gene.\n\n> Pescatello, L. S., *et al.* (2013). Highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or FAMuSS study. *BioMed research international.*\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-------------------------------------------------------------------------------------\n ndrm.ch   drm.ch    sex     age     race      height   weight   actn3.r577x    bmi  \n--------- -------- -------- ----- ----------- -------- -------- ------------- -------\n   40        40     Female   27    Caucasian     65      199         CC        33.11 \n\n   25        0       Male    36    Caucasian    71.7     189         CT        25.84 \n\n   40        0      Female   24    Caucasian     65      134         CT        22.3  \n\n   125       0      Female   40    Caucasian     68      171         CT         26   \n-------------------------------------------------------------------------------------\n\nTable: Example data rows\n\n\n:::\n:::\n\n\n\n## Categorical frequency distributions\n\nFor categorical variables, the frequency distribution is simply an observation count by category. For example:\n\n::: columns\n::: {.column width=\"0.2\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------------\n participant.id   genotype \n---------------- ----------\n      494            TT    \n\n      510            TT    \n\n      216            CT    \n\n       19            TT    \n\n      278            CT    \n\n       86            TT    \n---------------------------\n\nTable: Data table\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"0.8\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------\n CC    CT    TT  \n----- ----- -----\n 173   261   161 \n-----------------\n\nTable: Frequency distribution\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-15-1.png)\n:::\n:::\n\n\n:::\n:::\n\n## Numeric frequency distributions\n\nFrequency distributions of numeric variables are observation counts by *range*; a plot of a numeric frequency distribution is called a **histogram**.\n\n::: columns\n::: {.column width=\"0.3\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------\n participant.id    bmi  \n---------------- -------\n      194         22.3  \n\n      141         20.76 \n\n      313         23.48 \n\n      522         29.29 \n\n      504         42.28 \n\n      273         20.34 \n------------------------\n\nTable: Data table\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"0.7\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------------------------\n (10,20]   (20,30]   (30,40]   (40,50] \n--------- --------- --------- ---------\n   69        461       58         7    \n---------------------------------------\n\nTable: Frequency distribution\n\n\n:::\n\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-17-1.png)\n:::\n:::\n\n\n:::\n:::\n\nThe operation of dividing a numeric variable into interval ranges is called **binning**.\n\n## Histograms and binning\n\nBinning has a big effect on the visual impression. Which one captures the shape best?\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-18-1.png)\n:::\n:::\n\n\n\n## Shapes\n\nFor numeric variables, the histogram reveals the **shape** of the distribution:\n\n-   **symmetric** if it shows left-right symmetry about a central value\n-   **skewed** if it stretches farther in one direction from a central value\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-19-1.png)\n:::\n:::\n\n\n\n## Modes\n\nHistograms also reveal the number of **modes** or local peaks of frequency distributions.\n\n-   **uniform** if there are zero peaks\n-   **unimodal** if there is one peak\n-   **bimodal** if there are two peaks\n-   **multimodal** if there are two or more peaks\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-20-1.png)\n:::\n:::\n\n\n\n## Your turn: characterizing distributions\n\nConsider four variables from the FAMuSS study. Describe the shape and modality.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-21-1.png)\n:::\n:::\n\n\n\n## Your turn: characterizing distributions\n\nHere are some made-up data. Describe the shape and modality.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-22-1.png)\n:::\n:::\n\n\n\n\n## Descriptive measures\n\nA **descriptive measure** is a summary statistic that captures a particular feature of the frequency distribution of a numeric variable.\n\nCommonly, measures capture either **location** or **spread**.\n\n::: {.columns}\n\n::: {.column}\nMeasures of location:\n\n- mean\n- median\n- mode\n- percentiles/quantiles\n:::\n\n::: {.column}\nMeasures of spread:\n\n- range (min and max)\n- interquartile range\n- average deviation\n- variance\n- standard deviation\n:::\n\n:::\n\nIt is common practice to report multiple measures.\n\n## Measures of location\n\nOften location is specified by the \"center\" of a frequency distribution.\n\n::: columns\n::: {.column width=\"0.5\"}\nThere are three common measures of center, each of which corresponds to a slightly different meaning of \"typical\":\n\n| Measure | Definition          |\n|---------|---------------------|\n| Mode    | Most frequent value |\n| Mean    | Average value       |\n| Median  | Middle value        |\n:::\n\n::: {.column width=\"0.5\"}\nSuppose your data consisted of the following observations of age in years:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n_19_, _19_, _21_, _25_ and _31_\n:::\n:::\n\n\n\n-   the **mode** or most frequent value is 19\n-   the **median** or middle value is 21\n-   the **mean** or average value is $\\frac{19 + 19 + 21 + 25 + 31}{5}$ = 23\n:::\n:::\n\n## Quick example\n\nConsider the first 8 observations of change in nondominant arm strength from the FAMuSS study data:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n_40_, _25_, _40_, _125_, _40_, _75_, _100_ and _57.1_\n:::\n:::\n\n\n\nCompute the mean, median, and mode.\n\n## Comparing measures of center\n\nEach statistic is a little different, but often they roughly agree; for example, all are between 20 and 25, which seems to capture the typical BMI well enough.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-25-1.png)\n:::\n:::\n\n\n\n*How do you think the frequency distribution affects which one is \"best\"?*\n\n## Means, medians, and skewness\n\nThe mean and median both get 'pulled' in the direction of skewness, but the mean is more sensitive:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-26-1.png)\n:::\n:::\n\n\n\nComparing means and medians captures information about skewness present since:\n\n- mean $>$ median: right skew\n- mean $<$ median: left skew\n- mean $\\approx$ median: symmetric\n\n## When to use mode(s)\n\nMode is rarely used unless extreme skewness or multiple modes are present; below are two examples.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-27-1.png)\n:::\n:::\n\n\n\n## Percentiles\n\nA **percentile** is a value with specified proportions of data lying both above and below that value. \n\n- measure of location (but not center)\n- defined with reference to the percentage of data below\n\nFor example, the 20th percentile is the value with 20% of observations below and 80% of observations above. Suppose we have 5 observations:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------- ---- ---- ---- ---- ----\n **age**    19   20   21   25   31 \n\n **rank**   1    2    3    4    5  \n---------- ---- ---- ---- ---- ----\n\n\n:::\n:::\n\n\n\nThe 20th percentile is not unique! In fact *any* number between 19 and 20 is a 20th percentile since it would satisfy:\n\n-   20% below (19)\n-   80% above (20, 21, 25, 31)\n\n\n## Cumulative frequency distribution\n\nThe *cumulative frequency distribution* is a data summary showing percentiles. Think of it as percentile (y) against value (x).\n\n::: columns\n::: {.column width=\"0.6\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-29-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"0.4\"}\nInterpretation of some specific values:\n\n-   about 40% of the subjects are 20 or younger\n-   about 80% of the subjects are 24 or younger\n\n*Your turn*:\n\n1.  Roughly what percentage of subjects are 22 or younger?\n2.  About what age is the 10th percentile?\n:::\n:::\n\n\n## Common percentiles\n\n::: columns\n::: {.column width=\"50%\"}\nThe **five-number summary** is a collection of five percentiles that succinctly describe the frequency distribution:\n\n| Statistic name     | Meaning          |\n|--------------------|------------------|\n| **minimum**        | 0th percentile   |\n| **first quartile** | 25th percentile  |\n| **median**         | 50th percentile  |\n| **third quartile** | 75th percentile  |\n| **maximum**        | 100th percentile |\n:::\n\n::: {.column width=\"50%\"}\nBoxplots provide a graphical display of the five-number summary.\n\n![](img/boxplot-anatomy.png)\n:::\n\n:::\n\n## Boxplots vs. histograms\n\nNotice how the two displays align, and also how they differ. The histogram shows shape in greater detail, but the boxplot is much more compact.\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-30-1.png)\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-31-1.png)\n:::\n:::\n\n\n:::\n:::\n\n\n## Measures of spread\n\nThe *spread* of observations refers to how concentrated or diffuse the values are.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-32-1.png){fig-align='center'}\n:::\n:::\n\n\n\nTwo ways to understand and measure spread:\n\n-   *ranges* of values capturing much of the distribution\n-   *deviations* of values from a central value\n\n## Range-based measures\n\nA simple way to understand and measure spread is based on ranges. Consider more ages, sorted and ranked:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n **age**    16   18   19   20   21   22   25   26   28   29   30   34 \n\n **rank**   1    2    3    4    5    6    7    8    9    10   11   12 \n---------- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n\n\n:::\n:::\n\n\n\n-   The **range** is the minimum and maximum values: $$\\text{range} = (\\text{min}, \\text{max}) = (16, 34)$$\n\n-   The **interquartile range** (IQR) is the difference \\[75th percentile\\] - \\[25th percentile\\] $$\\text{IQR} = 29 - 19 = 10$$ *When might you prefer IQR to range? Can you think of an example?*\n\n## Deviation-based measures\n\nAnother way is based on *deviations* from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n    **age**      16   18   19   20   21   22   25   26   28   29   30   34 \n\n **deviation**   -8   -6   -5   -4   -3   -2   1    2    4    5    6    10 \n--------------- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n\n\n:::\n:::\n\n\n\nThe **average deviation** is defined as the average of the absolute values of the deviations from the mean: $$\\frac{8 + 6 + 5 + 4 + 3 + 2 + 1 + 2 + 4 + 5 + 6 + 10}{12}$$\n\n## Deviation-based measures\n\nAnother way is based on *deviations* from a central value. Continuing the example, the mean age is is 24. The deviations of each observation from the mean are:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n    **age**      16   18   19   20   21   22   25   26   28   29   30   34 \n\n **deviation**   -8   -6   -5   -4   -3   -2   1    2    4    5    6    10 \n--------------- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n\n\n:::\n:::\n\n\n\nThe **variance** is the average *squared* deviation from the mean (but divided by one less than the sample size): $$\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}$$\n\nThe **standard deviation** is the square root of the variance: $$\\sqrt{\\frac{(-8)^2 + (-6)^2 + (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (1)^2 + (2)^2 + (4)^2 + (5)^2 + (6)^2 + (10)^2}{12 - 1}}$$\n\n## Mathematical notations\n\nFollowing the convention from before, write a set of $n$ observations as $x_1, x_2, \\dots, x_n$.\n\n::: {.columns}\n\n::: {.column}\n\nThe **mean** of the observations is written: $$\\bar{x} = \\frac{1}{n}\\sum_i x_i$$\n\nThe **average deviation** is: $$\\frac{1}{n} \\sum_i |x_i - \\bar{x}|$$\n\n:::\n\n::: {.column}\n\nThe **variance** is: $$s_x^2 = \\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2$$\n\nThe **standard deviation** is: $$s_x = \\sqrt{\\frac{1}{n - 1}\\sum_i (x_i - \\bar{x})^2}$$\n\n:::\n\n:::\n## Interpretations\n\nListed from largest to smallest, here are each of the measures of spread for the 12 ages:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------------------------------------\n min   max   iqr   variance   st.dev   avg.dev \n----- ----- ----- ---------- -------- ---------\n 16    34    8.5    30.55     5.527     4.667  \n-----------------------------------------------\n\n\n:::\n:::\n\n\n\nThe interpretations differ between these statistics:\n\n-   \\[range\\] all of the data lies on an between 16 and 34 years old on an interval 18 years in width\n-   \\[IQR\\] the middle half of the data lies on an interval 8.5 years in width\n-   \\[average deviation\\] the average distance from the mean is 4.67 years\n- \\[variance\\] the average squared distance from the mean is 30.55 years$^2$\n-   \\[standard deviation\\] the average squared distance from the mean, rescaled to years, is 5.53 years\n\n# Lecture 4: Bivariate summaries\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. Reading quiz\n2. Loose end: robustness\n3. Bivariate numeric and graphical summaries\n4. Lab: bivariate graphics in R\n\n## Robustness\n\n> Percentile-based measures of location and spread are less sensitive to outliers\n\nConsider adding an observation of 94 to our 12 ages from last time. This is an outlier.\n\n::: columns\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# append an outlier\nages_add <- c(ages, 94)\n\n# means\nc(original = mean(ages), with.outlier = mean(ages_add))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    original with.outlier \n    24.00000     29.38462 \n```\n\n\n:::\n\n```{.r .cell-code}\n# medians\nc(original = median(ages), with.outlier = median(ages_add))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    original with.outlier \n        23.5         25.0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# IQR\nc(original = IQR(ages), with.outlier = IQR(ages_add))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    original with.outlier \n         8.5          9.0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# SD\nc(original = sd(ages), with.outlier = sd(ages_add))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    original with.outlier \n    5.526794    20.122701 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\nThe effect of this outlier on each statistic is:\n\n- mean increases by 22.44%\n- median increases by 6.38%\n- IQR increases by 5.88%\n- SD increases by 264.09%\n\n**Robustness** refers to sensitivity to outliers. Mean and SD are less robust than median and IQR.\n\n:::\n:::\n\n## Choosing appropriate measures\n\n> When outliers are present, use percentile-based measures; otherwise, use mean and standard deviation or variance\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-39-1.png)\n:::\n:::\n\n\n\n*Check your understanding: which measures are most appropriate for each variable above?*\n\n## Limitations of univariate summaries\n\n> Univariate summaries aim to capture the distribution of values of a single variable.\n\n::: columns\n::: {.column width=\"70%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-40-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"30%\"}\n-   both unimodal, no obvious outliers\n-   heights symmetric\n-   weights right-skewed\n-   but these observations actually come in *pairs*\n:::\n:::\n\nUnivariate summaries don't reflect how the variables might be related.\n\n## Bivariate summaries\n\n> Bivariate summaries aim to capture a relationship between two variables.\n\n::: columns\n::: {.column width=\"60%\"}\nA simple example is a scatterplot:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-41-1.png)\n:::\n:::\n\n\n\n# Lecture 5: Introduction to inference\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. Test 1 discussion\n2. [lecture] Inference vs. description, point estimation, interval estimation\n3. [lab] Exploring interval coverage\n\n## Description vs. inference\n\n> **Statistical inferences** are statements about population statistics based on samples. \n\nTo appreciate the meaning of this, consider the contrast with descriptive findings, which are statements about sample statistics:\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-43-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\nMedian percent change by genotype:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------\n  CC     CT    TT \n------ ------ ----\n 42.9   45.5   50 \n------------------\n\n\n:::\n:::\n\n\n\nA descriptive finding is:\n\n*Subjects with genotype TT exhibited the largest median percent change in strength*\n:::\n:::\n\nThe corresponding inference would be:\n\n*The median percent change in strength is highest among adults with genotype TT.*\n\n## Inference or description?\n\nSee if you can tell the difference:\n\n1. The proportion of children who developed a peanut allergy was 0.133 in the avoidance group.\n2. The proportion of children who develop peanut allergies is estimated to be 0.133 when peanut protein is avoided in infancy.\n3. The average lifetime of mice on normal 85kCal diets is estimated to be 32.7 months.\n4. The 57 mice on the normal 85kCal diet lived 32.7 months on average.\n5. The relative risk of a CHD event in the high trait anger group compared with the low trait anger group was 2.5.\n6. The relative risk of a CHD event among adults with high trait anger compared with adults with low trait anger is estimated to be 2.5.\n\n## Random sampling\n\n> Sampling establishes the link (or lack thereof) between a sample and a population.\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/srs.png)\n:::\n\n::: {.column width=\"50%\"}\nIn a **simple random sample**, units are chosen in such a way that every individual in the population has an equal probability of inclusion. For the SRS:\n\n- sample statistics mirror population statistics (sample is *representative*)\n- sampling variability depends only on population variability and sample size\n\n:::\n:::\n\n\n## Population models\n\n> Inference consists in using statistics of a random sample to ascertain population statistics under a population model.\n\nA population model represents the distribution of values you'd see if you measured every individual in the study population. We think of the sample values as a random draw.\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-45-1.png)\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-46-1.png)\n:::\n:::\n\n\n:::\n:::\n\n(Density is an alternative scale to frequency that is independent of population size.)\n\n## Point estimates\n\n> Sample statistics, viewed as guesses for the values of population statistics, are called 'point estimates'.\n\nWe'll focus on inferences involving the following:\n\n| Population statistic     | Parameter | Point estimate |\n|--------------------|--------------------|----------------|\n| Mean               | $\\mu$              | $\\bar{x}$      |\n| Standard deviation | $\\sigma$           | $s_x$          |\n\n## A difficulty\n\n> Different samples yield different estimates.\n\n::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-47-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"60%\"}\nSample means:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------\n sample.1   sample.2 \n---------- ----------\n  5.093      5.136   \n---------------------\n\n\n:::\n:::\n\n\n\n- estimates are close but not identical\n- the population mean can't be both 5.093 *and* 5.136\n- probably neither estimate is exactly correct\n\n*Estimation error and sample-to-sample variability are inherent to point estimation.*\n:::\n\n:::\n\n\n## Simulating sampling variability\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: columns\n::: {.column width=\"\\\"55%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-50-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"45%\"}\nThese are 20 random samples with the sample mean indicated by the dashed line and the population distribution and mean overlaid in red.\n\n-   sample size $n = 20$\n-   frequency distributions differ a lot\n-   sample means differ some\n\nWe can actually measure this variability!\n\n:::\n:::\n\n## Simulating sampling variability\n\nIf we had means calculated from a much larger number of samples, we could make a frequency distribution for the values of the sample mean.\n\n::: columns\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-51-1.png)\n:::\n:::\n\n\n\n------------ ------- ------- ---------- ---------\n **sample**     1       2     $\\cdots$   10,000   \n\n  **mean**    4.957   5.039   $\\cdots$   5.24 \n------------ ------- ------- ---------- ---------\n\n\n:::\n\n::: {.column width=\"55%\"}\nWe could then use the usual measures of center and spread to characterize the distribution of sample means.\n\n-   mean of $\\bar{x}$: 5.0373228\n-   standard deviation of $\\bar{x}$: 0.2387869\n\n> *Across 10,000 random samples of size 20, the typical sample mean was 5.04 and the root average squared distance of the sample mean from its typical value was 0.239.*\n:::\n:::\n\n\n## Sampling distributions\n\nWhat we are simulating is known as a **sampling distribution**: the frequency of values of a statistic across all possible random samples.\n\n::: columns\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-52-1.png)\n:::\n:::\n\n\n\n:::\n::: {.column width=\"55%\"}\n\n\n\nProvided data are from a random sample, the sample mean $\\bar{x}$ has a sampling distribution with\n\n- mean $\\color{red}{\\mu}$ (population mean)\n- standard deviation $\\color{red}{\\frac{\\sigma}{\\sqrt{n}}}$\n\nregardless of its exact form.\n\n:::\n\n:::\n\nIn other words, across all random samples of a fixed size...\n\n1. The average value of the sample mean is the population mean.\n2. The average squared error (sample mean - population mean)$^2$ is $\\frac{\\sigma^2}{n}$ \n\n## Effect of sample size\n\nThe standard deviation of the sampling distribution of $\\bar{x}$ is inversely proportional to sample size.\n\n::: {.columns}\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-53-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n\nAs sample size increases...\n\n- accuracy remains the same\n- estimates get more precise\n- skewness vanishes\n\n:::\n:::\n\n## Measuring sampling variability\n\nIn practice $\\sigma$ is not known so we use an estimate of sampling variability known as a **standard error**: \n$$\nSE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}} \n\\qquad \\left(\\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}\\right)\n$$\n\nFor example:\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-54-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n$$\nSE(\\bar{x}) = \\frac{1.073}{\\sqrt{20}} = 0.240\n$$\n\n> The root average squared error of the sample mean is estimated to be 0.240 mmol/L.\n\n:::\n:::\n\n<!-- ## Interpreting standard errors -->\n\n<!-- The standard error is a point estimate of the (population) standard deviation of sample means across all possible random samples: -->\n\n<!-- $$ -->\n<!-- SE(\\bar{x}) \\text{ estimates } \\sqrt{\\text{average value of } (\\bar{x} - \\mu)^2} -->\n<!-- $$ -->\n<!-- Two phrasings for an interpretation: -->\n\n<!-- 1. Estimated root average squared deviation of the sample mean from the population mean. -->\n<!-- 2. Estimated root mean square error. -->\n\n## Reporting point estimates\n\nIt is common style to report the value of a point estimate with a standard error given parenthetically.\n\n::: columns\n::: {.column width=\"50%\"}\nStatistics from full NHANES sample:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n----------------------\n mean     sd      n   \n------- ------- ------\n 5.043   1.075   3179 \n----------------------\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n> The mean total cholesterol among the population is estimated to be 5.043 mmol/L (SE 0.019)\n\n:::\n:::\n\nThis style of report communicates:\n\n- parameter of interest\n- value of point estimate\n- error/variability of point estimate\n\n## Interval estimation\n\n> An interval estimate is **a range of plausible values** for a population parameter.\n\nThe general form of an interval estimate is: $$\\text{point estimate} \\pm \\text{margin of error}$$\n\nA common interval for the population mean is:\n$$\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)$$\n\n::: {.columns}\n\n::: {.column}\nBy hand:\n$$5.043 \\pm 2\\times 0.0191 = (5.005, 5.081)$$\n:::\n\n::: {.column}\nIn R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg.totchol <- mean(totchol)\nse.totchol <- sd(totchol)/sqrt(length(totchol))\navg.totchol + c(-2, 2)*se.totchol\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.004817 5.081059\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n# Lecture 6: Confidence intervals\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n- [lecture] $t$ confidence intervals for the mean\n- [lab] computing and interpreting confidence intervals\n\n## From last time\n\n::: {.columns}\n\n::: {.column}\nUnder simple random sampling:\n\n- the sample mean $\\bar{x}$ provides a good point estimate of the population mean $\\mu$\n- its estimated sampling variability is given by the standard error $SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}} = \\frac{\\text{sample SD}}{\\sqrt{\\text{sample size}}}$\n<!-- - an interval estimate (range of plausible values) is $\\bar{x} \\pm 2\\times SE(\\bar{x})$ -->\n::: \n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------\n mean     sd      n       se    \n------- ------- ------ ---------\n 5.043   1.075   3179   0.01906 \n--------------------------------\n\n\n:::\n\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-58-1.png)\n:::\n:::\n\n\n:::\n\n:::\n\n> The mean total HDL cholesterol among the U.S. adult population is estimated to be 5.043 mmol/L (SE 0.0191).\n\n## Interval estimation\n\nA common interval estimate for the population mean is:\n$$\\bar{x} \\pm 2\\times SE(\\bar{x}) \\qquad\\text{where}\\quad SE(\\bar{x}) = \\left(\\frac{s_x}{\\sqrt{n}}\\right)$$\n\n> A range of plausible values for the mean total cholesterol among U.S. adults is 5.005 to 5.081 mmol/L.\n\nTwo related questions:\n\n1. What do we mean by \"plausible\"?\n2. Where did the number 2 come from?\n\n## The $t$ model\n\n::: {.columns}\n\n::: {.column}\nConsider the statistic:\n\n$$\nT = \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n$$ \n\nThe sampling distribution of $T$ is well-approximated by a $t_{n - 1}$ model whenever either:\n\n(a) the population model is symmetric and unimodal\n\nOR\n\n(b) the sample size is not too small\n    \n:::\n\n::: {.column}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-59-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n## $t$ model interpretation\n\n> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.\n\n$$(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a < T < b)$$\n\n::: {.columns}\n\n::: {.column}\nFor example:\n\n- for 50% of samples, $T < 0$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# area less than 0\npt(0, df = 20 - 1) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n```\n\n\n:::\n:::\n\n\n\n- written as $P(T < 0) = 0.5$\n\n:::\n\n::: {.column}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-61-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n## $t$ model interpretation\n\n> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.\n\n$$(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a < T < b)$$\n\n::: {.columns}\n\n::: {.column}\nFor example:\n\n- for 83.5% of samples, $T < 1$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# area less than 1\npt(1, df = 20 - 1) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8350616\n```\n\n\n:::\n:::\n\n\n\n- written as $P(T < 1) = 0.835$\n\n:::\n\n::: {.column}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-63-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n## $t$ model interpretation\n\n> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.\n\n$$(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a < T < b)$$\n\n::: {.columns}\n\n::: {.column}\nFor example:\n\n- for 97% of samples, $T < 2$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# area less than 2\npt(2, df = 20 - 1) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.969999\n```\n\n\n:::\n:::\n\n\n\n- written as $P(T < 2) = 0.97$\n\n:::\n\n::: {.column}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-65-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n## $t$ model interpretation\n\n> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.\n\n$$(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a < T < b)$$\n\n::: {.columns}\n\n::: {.column}\nFor example:\n\n- for 3% of samples, $T > 2$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# area greater than 2\npt(2, df = 20 - 1, lower.tail = F) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03000102\n```\n\n\n:::\n:::\n\n\n\n- notice: \n$$\n\\begin{align*}\nP(T > 2) &= 1 - P(T < 2) \\\\\n(0.03) &= 1 - (0.97)\n\\end{align*}\n$$\n:::\n\n::: {.column}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-67-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n## $t$ model interpretation\n\n> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.\n\n$$(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a < T < b)$$\n\n::: {.columns}\n\n::: {.column}\nFor example:\n\n- for 13.5% of samples, $1 < T < 2$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# area between 1 and 2\npt(2, df = 20 - 1) - pt(1, df = 20 - 1) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1349374\n```\n\n\n:::\n:::\n\n\n\n- notice:\n$$\n\\begin{align*}\nP(1 < T < 2) &= P(T < 2) - P(T < 1) \\\\\n(0.135) &= (0.97) - (0.835)\n\\end{align*}\n$$\n\n:::\n\n::: {.column}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-69-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n## $t$ model interpretation\n\n> The *area under the density curve* between any two values $(a, b)$ gives the proportion of random samples for which $a < T < b$.\n\n$$(\\text{proportion of area between } a, b) = (\\text{proportion of samples where } a < T < b)$$\n\n::: {.columns}\n\n::: {.column}\nFor example:\n\n- for 94% of samples, $-2 < T < 2$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# area between 1 and 2\npt(2, df = 20 - 1) - pt(-2, df = 20 - 1) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.939998\n```\n\n\n:::\n:::\n\n\n\n- written $P(-2 < T < 2) = 0.94$\n\n:::\n\n::: {.column}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-71-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n## A closer look at interval construction\n\nSo where did that 2 come from in the margin of error for our interval estimate?\n\n$$\n\\bar{x} \\pm \\color{blue}{2}\\times SE(\\bar{x})\n$$\n\nWell:\n\n::: {.columns}\n\n::: {.column width=\"70%\"}\n\n$$\n\\begin{align*}\n0.94 &= P(-\\color{blue}{2} < T < \\color{blue}{2}) \\\\\n&= P\\left(-\\color{blue}{2} < \\frac{\\bar{x} - \\mu}{s_x/\\sqrt{n}} < \\color{blue}{2}\\right) \\\\\n&= P(\\underbrace{\\bar{x} - \\color{blue}{2}\\times SE(\\bar{x}) < \\mu < \\bar{x} + \\color{blue}{2}\\times SE(\\bar{x})}_{\\text{interval covers population mean}})\n\\end{align*}\n$$\n:::\n\n::: {.column width=\"30%\"}\n> For 94% of all random samples, the interval covers the population mean.\n:::\n\n:::\n\nSo the number 2 determines the proportion of samples for which the interval covers the mean, known as its **coverage**.\n\n\n## Effect of sample size\n\n> The sample size determines the exact shape of the $t$ model through its 'degrees of freedom' $n - 1$. This changes the areas slightly.\n\nThe exact coverage quickly converges to just over 95% as the sample size increases.\n\n::: {.columns}\n\n::: {.column}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n----------------\n  n    coverage \n----- ----------\n  4     0.8607  \n\n  8     0.9144  \n\n 16     0.9361  \n\n 32     0.9457  \n\n 64     0.9502  \n\n 128    0.9524  \n\n 256    0.9534  \n----------------\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-73-1.png)\n:::\n:::\n\n\n:::\n\n:::\n\n## Changing the coverage\n\nConsider a slightly more general expression for an interval for the mean:\n\n$$\n\\bar{x} \\pm c\\times SE(\\bar{x})\n$$\n\nThe number $c$ is called a **critical value**. It determines the coverage.\n\n- larger $c$ $\\longrightarrow$ higher coverage\n- smaller $c$ $\\longrightarrow$ lower coverage\n\nThe so-called \"empirical rule\" is that:\n\n- $c = 1 \\longrightarrow$ approximately 68% coverage\n- $c = 2 \\longrightarrow$ approximately 95% coverage\n- $c = 3 \\longrightarrow$ approximately 99.7% coverage\n\n## Interpreting critical values\n\n::: {.columns}\n\n::: {.column}\n$$\nP(\\color{#FF6459}{-2 < T < 2}) = 1 - 2\\times P(\\color{blue}{T > 2})\n$$\n\nLook at how the areas add up so that:\n$$\nP(\\color{blue}{T > 2}) = 0.03\n$$\nMoreover:\n$$\nP(T < 2) = 1 - 0.03 = 0.97\n$$\n:::\n\n::: {.column}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-74-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n*So the critical value 2 is actually the 97th percentile of the sampling distribution of $T$.*\n\n- also called the 0.97 \"quantile\"\n- (percentiles expressed in proportions are called quantiles)\n\n## Exact coverage using $t$ quantiles\n\nTo engineer an interval with a specific coverage, use the $p$th quantile where:\n\n$$p = \\left[1 - \\left(\\frac{1 - \\text{coverage}}{2}\\right)\\right]$$\nIn R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# coverage 95% using t quantile\ncoverage <- 0.95\nq.val <- 1 - (1 - coverage)/2\ncrit.val <- qt(q.val, df = 20 - 1)\ncrit.val\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.093024\n```\n\n\n:::\n:::\n\n\n\nThe effect of increasing/decreasing coverage on the quantile is:\n\n- increase coverage $\\longrightarrow$ larger quantile $\\longrightarrow$ wider interval\n- decrease coverage $\\longrightarrow$ smaller quantile $\\longrightarrow$ narrower interval\n\n## Contrasting coverage with precision\n\n> **Precision** refers to how wide or narrow the interval is.\n\nPrecision depends on every component of the margin of error:\n\n- critical value used\n- sample size\n- variability of values\n\nBy contrast, coverage depends only on the critical value used.\n\n## Confidence intervals\n\nInterval estimates constructed to achieve a specified coverage are called \"confidence intervals\"; the coverage is interpreted and reported as a \"confidence level\".\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ingredients\ncholesterol.mean <- mean(cholesterol)\ncholesterol.sd <- sd(cholesterol)\ncholesterol.n <- length(cholesterol)\ncholesterol.se <- cholesterol.sd/sqrt(cholesterol.n)\ncrit.val <- qt(1 - (1 - 0.95)/2, df = cholesterol.n - 1)\n\n# interval\ncholesterol.mean + c(-1, 1)*crit.val*cholesterol.se\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.005566 5.080310\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n> With 95% confidence, the mean total cholesterol among U.S. adults is estimated to be between 5.0056 and 5.0803 mmol/L.\n\n:::\n\n:::\n\nThe general formula for a confidence interval for the population mean is\n\n$$\n\\bar{x} \\pm c\\times SE(\\bar{x})\n$$\n\nwhere $c$ is a critical value, obtained as a quantile of the $t_{n - 1}$ model and chosen to ensure a specific coverage.\n\n## Recap\n\nThe \"common\" interval estimate for the mean is actually an approximate 95% confidence interval:\n\n$$\n\\bar{x} \\pm 2 \\times SE(\\bar{x})\n$$\n\n- captures the population mean $\\mu$ for roughly 95% of random samples\n- replacing 2 with a $t_{n - 1}$ quantile allows the analyst to adjust coverage\n- the $t_{n - 1}$ model is an approximation for the sampling distribution of $\\frac{\\bar{x} - \\mu}{SE(\\bar{x})}$\n\n    + approximation improves with increasing sample size or symmetry\n    + usually good quality except in \"extreme\" situations\n\nInterval interpretation:\n\n> With [XX]% confidence, the mean [population parameter] is estimated to be between [lower bound] and [upper bound] [units].\n\n# Extras\n\n## Simulation of coverage\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.columns}\n\n::: {.column width=\"65%\"}\nArtificially simulating a large number of intervals provides an empirical approximation of coverage. \n\n- at right, 200 intervals\n- 94% cover the population mean (vertical dashed line)\n- pretty close to nominal coverage level 95%\n\nThis is also a handy way to remember the proper interpretation:\n\n> If I made a lot of intervals from independent samples, 95% of them would 'get it right'.\n\n:::\n\n::: {.column width=\"35%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-78-1.png)\n:::\n:::\n\n\n:::\n\n:::\n\n# Lecture 7: Hypothesis testing\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. Loose end: working backwards to determine interval coverage\n2. [lecture] the $t$-test for a population mean\n3. [lab] computing test statistics, critical values, and $p$-values\n\n## Body temperatures\n\n::: columns\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-80-1.png)\n:::\n\n::: {.cell-output-display}\n\n------------------------------\n mean      sd     n      se   \n------- -------- ---- --------\n 98.41   0.9162   39   0.1467 \n------------------------------\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"55%\"}\n*Is the true mean body temperature actually 98.6°F?*\n\nSeems plausible given our data.\n\nBut what if the sample mean were instead...\n\n| $\\bar{x}$ | consistent with $\\mu = 98.6$? |\n|-----------|-------------------------------|\n| 98.30     | probably still yes            |\n| 98.15     | maybe                         |\n| 98.00     | hesitating                    |\n| 97.85     | skeptical                     |\n| 97.40     | unlikely                      |\n:::\n:::\n\n> If the estimation error is \"big enough\" the hypothesis seems\n> implausible.\n\n## How much error is too much?\n\nConsider how many standard errors away from the hypothesized value we'd be:\n\n| $\\bar{x}$ | estimation error | no. SE's | interpretation           |\n|-----------|------------------|----------|--------------------------|\n| 98.30     | -0.3             | 2        | double the average error |\n| 98.15     | -0.45            | 3        | triple the average error |\n| 98.00     | -0.6             | 4        | quadruple                |\n| 97.85     | -0.75            | 5        | quintuple                |\n| 97.40     | -1.2             | 8        | octuple!                 |\n\nWe know from discussing confidence intervals that we'd estimate the mean\ntemperature to be within about 2SE of the sample mean, and from interval\ncoverage that:\n\n-   an error less than 2SE occurs for about 95% of samples\n-   an error greater than 2SE occurs for only about 5% of samples\n\n*Exactly how often would we see the error we did if the population mean\nis in fact 98.6°F?*\n\n## Applying the $t$ model\n\n::: columns\n::: column\n***If the population mean is in fact 98.6°F*** then $$\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n$$ has a sampling distribution that is well-approximated by a\n$t_{39 - 1}$ model.\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-81-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n## Applying the $t$ model\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: columns\n::: column\n***If the population mean is in fact 98.6°F*** then $$\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n$$ has a sampling distribution that is well-approximated by a\n$t_{39 - 1}$ model.\n\n-   actual summary statistics give $T$ = -1.328\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-83-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n## Applying the $t$ model\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: columns\n::: column\n***If the population mean is in fact 98.6°F*** then $$\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n$$ has a sampling distribution that is well-approximated by a\n$t_{39 - 1}$ model.\n\n-   actual summary statistics give $T$ = -1.328\n-   underestimate more in 9.6%\n    of samples\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-85-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n## Applying the $t$ model\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: columns\n::: column\n***If the population mean is in fact 98.6°F*** then $$\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n$$ has a sampling distribution that is well-approximated by a\n$t_{39 - 1}$ model.\n\n-   actual summary statistics give $T$ = -1.328\n-   underestimate more in 9.6%\n    of samples\n-   overestimate more in 9.6%\n    of samples\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-87-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n## Applying the $t$ model\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: columns\n::: column\n***If the population mean is in fact 98.6°F*** then $$\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n$$ has a sampling distribution that is well-approximated by a\n$t_{39 - 1}$ model.\n\n-   actual summary statistics give $T$ = -1.328\n-   underestimate more in 9.6%\n    of samples\n-   overestimate more in 9.6%\n    of samples\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-89-1.png){fig-align='center'}\n:::\n:::\n\n\n\n$$P(|T| > 1.328) = 0.192$$\n:::\n:::\n\n> We'd see at least as much (absolute) estimation error\n> 19.2% of the time, assuming\n> the hypothesis is true. So this amount of error isn't surprising.\n\n## A more extreme scenario\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: columns\n::: column\n\n***If the population mean is in fact 98.6°F*** then $$\nT = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}}\n\\qquad\\left(\\frac{\\text{estimation error}}{\\text{standard error}}\\right)\n$$ has a sampling distribution that is well-approximated by a\n$t_{39 - 1}$ model.\n\n- suppose instead $\\bar{x} = 98.2$ so $T$ = -2.726\n-   underestimate more in 0.48%\n    of samples\n-   overestimate more in 0.48%\n    of samples\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-91-1.png){fig-align='center'}\n:::\n:::\n\n\n\n$$P(|T| > 2.726) = 0.0096$$\n:::\n:::\n\n> We'd see at least as much estimation error only\n> 0.96% of the time. So if the\n> hypothesis were true, this sample would be really unusual.\n\n## Evaluating the hypothesis\n\nTo evaluate the hypothesis that $\\mu = 98.6$, we assume it is true and then consider whether the estimation error would be unusually large purely by chance according to the $t$ model:\n\n-   unusually large error $\\longrightarrow$ hypothesis is implausible\n    $\\longrightarrow$ can be rejected\n-   not unusually large error $\\longrightarrow$ hypothesis is plausible\n    $\\longrightarrow$ can't be rejected\n\nWe just made these assessments:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------------------------------------------\n sample.mean     se     t.stat   how.often   evaluation  \n------------- -------- -------- ----------- -------------\n    98.41      0.1467   -1.328     0.192     not unusual \n\n    98.2       0.1467   -2.726   0.009632      unusual   \n---------------------------------------------------------\n\n\n:::\n:::\n\n\n\n*Seems reasonable, but why exactly isn't 19.2% of the time 'unusual'?*\n\n## Decisions, decisions\n\n> What would happen if we decided that $T = -1.328$ was unusual?\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-------------------------------------------\n sample.mean     se     t.stat   how.often \n------------- -------- -------- -----------\n    98.41      0.1467   -1.328     0.192   \n\n    98.2       0.1467   -2.726   0.009632  \n-------------------------------------------\n\n\n:::\n:::\n\n\n\nSuppose we drew a line at 20%. Then if in fact $\\mu = 98.6$:\n\n- errors in the 'reject' regime occur by chance 20% of the time\n- so we'll reach the wrong conclusion for 1 in 5 samples\n\nThis error rate is too high.\n\n## Formalizing a test for the mean\n\nA **statistical hypothesis** is a statement about a population\nparameter. For every hypothesis there is an opposing or \"alternative\"\nhypothesis.\n\nA **hypothesis test** is a procedure for deciding between a hypothesis\nand its alternative.\n\n::: columns\n::: {.column width=\"60%\"}\nWe just tested the hypotheses: \n\n$$\n\\begin{cases}\nH_0: &\\mu = 98.6 \\quad(\\text{\"null\" hypothesis}) \\\\\nH_A: &\\mu \\neq 98.6 \\quad(\\text{\"alternative\" hypothesis})\n\\end{cases}\n$$\n\nOur decision was based on the \"test statistic\":\n\n$$\nT = \\frac{\\bar{x} - 98.6}{SE(\\bar{x})}\n$$\n\nIf $H_0$ is true, the sampling distribution of $T$ is well-approximated\nby a $t_{n-1}$ model.\n:::\n\n::: {.column width=\"40%\"}\nWe reject $H_0$ if it entails that the estimation error is unusually large relative to the standard error.\n\n- 'unusual' determined by considering error rate\n- two equivalent approaches: \n\n    1. critical values \n    2. $p$-values\n:::\n:::\n\n## The critical value approach\n\n> Reject $H_0$ if $|T|$ exceeds the $1 - \\frac{\\alpha}{2}$ quantile of the $t_{n - 1}$ model\n\n::: {.columns}\n::: {.column width=\"45%\"}\nSteps:\n\n1. Decide on an error tolerance $\\alpha$.\n2. Find the $1 - \\frac{\\alpha}{2}$ quantile $q$.\n3. Reject if $|T| > q$.\n\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute test statistic\ntstat <- (temp.mean - 98.6)/temp.mean.se\ntstat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.328265\n```\n\n\n:::\n\n```{.r .cell-code}\n# compute critical value for a 5% error tolerance\ncrit.val <- qt(p = 0.975, df = 38)\ncrit.val\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.024394\n```\n\n\n:::\n\n```{.r .cell-code}\n# compare\nabs(tstat) > crit.val \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\nRationale: if $H_0$ is true...\n\n- $|T|$ will be smaller than the quantile for $(1 - \\alpha)\\times 100$% of samples\n- so using this rule you'll only make a mistake $\\alpha\\times 100$% of the time\n\n\n## The $p$-value approach\n\n> Reject $H_0$ if $T$ exceeds the observed value for less than $\\alpha\\times 100$% of samples: $$2\\times P(T > |T_\\text{obs}|) < \\alpha$$\n\n::: {.columns}\n\n::: {.column width=\"45%\"}\nSteps:\n\n1. Decide on an error tolerance $\\alpha$.\n2. Compute the proportion $p$ of samples for which $T$ exceeds observed value.\n3. Reject if $p < \\alpha$.\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute test statistic\ntstat <- (temp.mean - 98.6)/temp.mean.se\ntstat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.328265\n```\n\n\n:::\n\n```{.r .cell-code}\n# proportion of samples where T exceeds observed value\np.val <- 2*pt(abs(tstat), df = 38, lower.tail = F)\np.val\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1920133\n```\n\n\n:::\n\n```{.r .cell-code}\n# decision with error rate controlled at 5%\np.val < 0.05\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\nRationale: \n\n- $p$-value conveys exactly how unusual the test statistic is\n- $p < \\alpha$ exactly when $|T| > q$, so this rule controls the error rate at $\\alpha$\n\n## Test outcomes\n\nThere are two possible findings for a test:\n\n-   \\[crosses decision threshold\\] reject $H_0$ in favor of $H_A$\n-   \\[doesn't cross decision threshold\\] fail to reject $H_0$ in favor of $H_A$\n\nA **reject** decision is interpreted as:\n\n> The data provide evidence that... \\[against $H_0$/favoring $H_A$\\]\n\nA **fail to reject** decision is interpreted as:\n\n> The data *do not* provide evidence that... \\[against $H_0$/favoring\n> $H_A$\\]\n\n## Interpreting results\n\n::: columns\n::: {.column width=\"55%\"}\nCalculations in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute test statistic\ntstat <- (temp.mean - 98.6)/temp.mean.se\ntstat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.328265\n```\n\n\n:::\n\n```{.r .cell-code}\n# compute critical value for a 5% error tolerance\ncrit.val <- qt(p = 0.975, df = 38)\ncrit.val\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.024394\n```\n\n\n:::\n\n```{.r .cell-code}\n# test decision\nabs(tstat) > crit.val\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\n# p-value\n2*pt(abs(tstat), df = 38, lower.tail = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1920133\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"45%\"}\nConventional narrative summary style:\n\n> The data do not provide evidence that the mean body temperature differs from 98.6°F (*T* = -1.328 on 38 degrees of freedom, *p* = 0.192).\n\nConveys a lot of info succinctly:\n\n- test conclusion\n- hypotheses tested\n- number of standard errors from hypothesized value ($T$)\n- sample size (degrees of freedom + 1)\n- strength of evidence ($p$-value)\n:::\n:::\n\n<!-- ## Significance conventions -->\n\n<!-- ::: {.columns} -->\n\n<!-- ::: {.column width=\"43%\"} -->\n\n<!-- **Convention 1:** statistical significance -->\n\n<!-- - $p < 0.05$: reject $H_0$ -->\n\n<!-- - $p \\geq 0.05$: fail to reject $H_0$ -->\n\n<!-- > \"The data provide **significant evidence at level $\\alpha$ = 0.05** against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (*T* = -5.4548 on 129 degrees of freedom, *p* = .0000002411).\" -->\n\n<!-- ::: -->\n\n<!-- ::: {.column width=\"57%\"} -->\n\n<!-- **Convention 2:** weight of evidence against $H_0$ -->\n\n<!-- - $p < 0.01$: strong evidence -->\n\n<!-- - $0.01 \\leq p < 0.05$: moderate evidence -->\n\n<!-- - $0.05 \\leq p < 0.1$: weak evidence  -->\n\n<!-- - $0.1 \\leq p$: no evidence -->\n\n<!-- > \"The data **provide strong evidence** against the hypothesis that mean body temperature is 98.6 °F in favor of the alternative that mean body temperature differs from 98.6 °F (*T* = -5.4548 on 129 degrees of freedom, *p* = .0000002411).\" -->\n\n<!-- ::: -->\n\n<!-- ::: -->\n\n<!-- You may use either convention to interpret test results. -->\n\n## Components of a test\n\n| Component              | Explanation                                                                  | Example                                                     |\n|----------------|--------------------------------|-------------------------|\n| Population parameter   | The quantity of interest                                                     | Mean body temp $\\mu$                                        |\n| Null hypothesis        | The claim to be tested                                                       | $\\mu = 98.6$                                                |\n| Alternative hypothesis | The alternative claim                                                        | $\\mu \\neq 98.6$                                             |\n| Test statistic         | A function of the sample data and the hypothetical parameter value | $T = \\frac{\\bar{x} - 98.6}{s_x/\\sqrt{n}} = -1.328$          |\n| Model                  | Sampling distribution of the test statistic under $H_0$                      | $t_{38}$ model                                              |\n| $p$-value              | Probability under $H_0$ of obtaining a result at least as favorable to $H_A$ | 19.2% of samples produce a test statistic at least as large |\n| Decision               | Reject or fail to reject $H_0$ in favor of $H_A$                             | Fail to reject                                              |\n\n# Lecture 8: Directional alternatives\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. Quick review of decision criteria for the $t$ test\n2. [lecture] test-interval relationship; directional $t$ tests\n3. [lab] upper-sided, lower-sided, and two-sided tests for the population mean\n\n## Recap: decision criteria\n\n> A hypothesis test boils down to deciding whether your estimate is too far from a hypothetical value for that hypothesis to be plausible.\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\nTo test the hypotheses:\n\n$$\n\\begin{cases}\nH_0: &\\mu = \\mu_0 \\\\\nH_A: &\\mu \\neq \\mu_0\n\\end{cases}\n$$\n\nWe use the test statistic:\n\n$$\nT = \\frac{\\bar{x} - \\mu_0}{SE(\\bar{x})}\n\\quad\\left(\\frac{\\text{estimation error under } H_0}{\\text{standard error}}\\right)\n$$\n:::\n\n::: {.column width=\"40%\"}\nWe say $H_0$ is implausible at level $\\alpha$ if either:\n\n- $|T| > q$ for the $\\alpha$-critical value $q$\n\n    + $q$ is the $1 - \\frac{\\alpha}{2}$ quantile of the $t_{n - 1}$ model\n\n- $\\underbrace{P(|T| > |T_\\text{observed}|)}_\\text{p-value} < \\alpha$\n:::\n\n:::\n\nThis procedure controls the error rate $\\alpha$: the proportion of samples for which we'd make a false rejection.\n\n## From last time\n\n*Practice problem: test the hypothesis that the average U.S. adult sleeps 8 hours.*\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-100-1.png)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculations\nsleep.mean <- mean(sleep) \nsleep.mean.se <- sd(sleep)/sqrt(length(sleep))\ntstat <- (sleep.mean - 8)/sleep.mean.se \ncrit.val <- qt(0.975, df = 3178) \np.val <- 2*pt(abs(tstat), df = 3178, lower.tail = F)\nci <- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------------------------\n estimate   std.err   tstat    cval       pval    \n---------- --------- -------- ------- ------------\n  6.959     0.02447   -42.53   1.961   2.622e-313 \n--------------------------------------------------\n\n\n:::\n:::\n\n\n\n95% confidence interval: (6.91, 7.01)\n:::\n\n::: {.column width=\"40%\"}\nA complete narrative summary:\n\n> The data provide evidence that the average U.S. adult does not sleep 8 hours per night (*T* = -42.53 on 3178 degrees of freedom, *p* < 0.0001). With 95% confidence, the mean nightly hours of sleep among U.S. adults is estimated to be between 6.91 and 7.01 hours, with a point estimate of 6.59 hours (SE: 0.0245).\n:::\n\n:::\n\n## Tests and intervals\n\n> Tests and intervals are usually reported together\n\nConsider how the test and interval provide complementary information:\n\n- the test tells you U.S. adults don't sleep 8 hours\n- the interval tells you how much they *do* sleep\n\nIt is reasonable that they should be consistent, and in fact they are.\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculations\nsleep.mean <- mean(sleep) \nsleep.mean.se <- sd(sleep)/sqrt(length(sleep))\ntstat <- (sleep.mean - 8)/sleep.mean.se \ncrit.val <- qt(0.975, df = 3178) \np.val <- 2*pt(abs(tstat), df = 3178, lower.tail = F)\nci <- sleep.mean + c(-1, 1)*crit.val*sleep.mean.se\n```\n:::\n\n\n:::\n\n::: {.column width=\"45%\"}\n> Notice that the critical value in the 5% significance level test is exactly the same as that used in the 95% confidence interval.\n:::\n\n:::\n\n## Tests and intervals\n\nThe critical value used in a $\\alpha$ significance level test is identical to the critical value used in a $(1 - \\alpha)$ interval. Consequently:\n\n$$\n\\underbrace{\\bar{x} - c\\times SE(\\bar{x}) < \\mu_0 < \\bar{x} + c\\times SE(\\bar{x})}_\\text{hypothesized value is in the interval}\n\\quad\\Longleftrightarrow\\quad\n\\underbrace{-c < \\frac{\\bar{x} - \\mu_{0}}{SE(\\bar{x})} < c}_{|T| < c}\n$$\n\nMeaning: the interval includes exactly those values that the test fails to reject.\n\n- Sensible considering both use the same information: the distance between the point estimate and population mean, relative to the variability of the estimate\n\n## Tests and intervals\n\n> The level-$\\alpha$ test rejects $H_0: \\mu = \\mu_0$ exactly when $\\mu_0$ is outside the $(1 - \\alpha)\\times 100$% confidence interval for $\\mu$.\n\n\n::: {.columns}\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-104-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column}\nLeft, $p$-values for a sequence of tests:\n\n- $p > 0.05$ precisely for $\\mu_0$ in 95% CI\n- $p > 0.01$ precisely for $\\mu_0$ in 99% CI\n\nIn other words:\n\n$$\\text{level $\\alpha$ test rejects} \\Longleftrightarrow \\text{$1 - \\alpha$ CI excludes}$$\n\n::: \n\n:::\n\n\n## The `t.test(...)` function\n\nSince tests and intervals go together, there is a single R function that computes both.\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(sleep, mu = 8, conf.level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  sleep\nt = -42.533, df = 3178, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 8\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n```\n\n\n:::\n:::\n\n\n\n::: \n\n::: {.column width=\"50%\"}\nNo critical value is reported, so you have to make the decision using the $p$ value:\n\n- $p < \\alpha$: reject\n- $p > \\alpha$: fail to reject\n:::\n\n:::\n\nTake a moment to locate each component of the test and estimates from the output.\n\n## Interpreting $p$-values\n\n> $p$-values measure the strength of evidence against $H_0$ and favoring $H_A$: smaller $p$-values indicate stronger evidence; larger $p$-values indicate weaker evidence.\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\nThe mathematical definition is:\n$$p = P(|T| > |T_\\text{observed}|)$$ \n\n- technically, the probability under $H_0$ that $T$ exceeds the observed value in magnitude\n- informally, how unusual/rare your data are\n\n:::\n\n::: {.column width=\"45%\"}\nAs a measure of the strength of evidence favoring the alternative:\n\nvalue | strength of evidence\n---|---\n$p < 0.001$ | very strong\n$0.001 < p < 0.01$ | strong\n$0.01 < p < 0.05$ | moderate\n$0.05 < p < 0.1$ | suggestive\n$0.1 < p$ | no evidence\n:::\n\n:::\n\n## Your turn: interpret these $p$-values\n\n> Don't just match the value to the table; add context, and state the test outcome.\n\n::: {.columns}\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# example 1\nt.test(sleep, mu = 6.8)$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.205865e-11\n```\n\n\n:::\n\n```{.r .cell-code}\n# example 2\nt.test(sleep, mu = 6.9)$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01578191\n```\n\n\n:::\n\n```{.r .cell-code}\n# example 3\nt.test(sleep, mu = 7)$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09482291\n```\n\n\n:::\n\n```{.r .cell-code}\n# example 4\nt.test(sleep, mu = 7.1)$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.366935e-09\n```\n\n\n:::\n\n```{.r .cell-code}\n# example 5\nt.test(sleep, mu = 7.2)$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.532371e-22\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\nvalue | strength of evidence\n---|---\n$p < 0.001$ | very strong\n$0.001 < p < 0.01$ | strong\n$0.01 < p < 0.05$ | moderate\n$0.05 < p < 0.1$ | suggestive\n$0.1 < p$ | no evidence\n:::\n\n:::\n\nExample: \"**the data** [DO/DO NOT] **provide** [STRENGTH] **evidence that** [ALTERNATIVE]\"\n\n## A directional test\n\n> Does the average U.S. adult sleep *less than* 7 hours?\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\nThis example leads to a *directional* test:\n\n$$\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu < 7\n\\end{cases}\n$$\n\nThe test statistic is the same as before:\n\n$$\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n$$\n:::\n\n::: {.column width=\"45%\"}\nThe *lower-sided* $p$-value is 0.0474:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-107-1.png){fig-align='center'}\n:::\n:::\n\n\n\n:::\n\n:::\n\nFor the $p$-value, we look at how often $T$ is larger *in the direction of the alternative*.\n\n- in this case, how often $T$ is smaller (underestimate by more)\n\n## The other direction\n\n> Does the average U.S. adult sleep ***more** than* 7 hours?\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\nNow the alternative is the opposite direction:\n\n$$\n\\begin{cases}\nH_0: &\\mu = 7 \\\\\nH_A: &\\mu > 7\n\\end{cases}\n$$\n\nThe test statistic is the same as before:\n\n$$\nT = \\frac{\\bar{x} - 7}{SE(\\bar{x})} = -1.671\n$$\n:::\n\n::: {.column width=\"45%\"}\nThe ***upper**-sided* $p$-value is 0.9526:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-108-1.png){fig-align='center'}\n:::\n:::\n\n\n\n:::\n\n:::\n\nFor the $p$-value, we look at how often $T$ is larger *in the direction of the alternative*.\n\n- in this case, how often $T$ is **larger** (**over**estimate by more)\n\n## Directional hypotheses\n\nTests for the mean can involve directional or non-directional alternatives. We refer to these as one-sided and two-sided tests, respectively.\n\n| Test type   | Alternative      | Direction favoring alternative |\n|-------------|------------------|--------------------|\n| Upper-sided | $\\mu > \\mu_0$    | larger $T$       |\n| Lower-sided | $\\mu < \\mu_0$    | smaller $T$       |\n| Two-sided   | $\\mu \\neq \\mu_0$ | larger $|T|$        |\n\nThe direction of the test affects the $p$-value calculation (and thus decision), but *won't* change the test statistic.\n\nConceptually tricky, but easy in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# upper-sided\nt.test(ddt, mu = mu_0, alternative = 'greater')\n\n# lower-sided\nt.test(ddt, mu = mu_0, alternative = 'less')\n\n# two-sided (default)\nt.test(ddt, mu = mu_0, alternative = 'two.sided')\n```\n:::\n\n\n\n\n## Three $t$-tests\n\n::: {.columns}\n\n::: {.column width=\"33%\"}\nDo U.S. adults sleep 7 hours per night?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# two sided test\nt.test(sleep, \n       mu = 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.09482\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.911123 7.007090\nsample estimates:\nmean of x \n 6.959107 \n```\n\n\n:::\n:::\n\n\n\n> Suggestive but insufficient evidence that U.S. adults don't sleep 7 hours\n\n:::\n\n::: {.column width=\"33%\"}\nDo U.S. adults sleep less than 7 hours per night?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# lower-sided test\nt.test(sleep, \n       mu = 7, \n       alternative = 'less')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.04741\nalternative hypothesis: true mean is less than 7\n95 percent confidence interval:\n     -Inf 6.999372\nsample estimates:\nmean of x \n 6.959107 \n```\n\n\n:::\n:::\n\n\n\n> Moderate evidence that U.S. adults sleep less than 7 hours\n\n:::\n\n::: {.column width=\"34%\"}\nDo U.S. adults sleep more than 7 hours per night?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# upper-sided test\nt.test(sleep, \n       mu = 7, \n       alternative = 'greater')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  sleep\nt = -1.671, df = 3178, p-value = 0.9526\nalternative hypothesis: true mean is greater than 7\n95 percent confidence interval:\n 6.918841      Inf\nsample estimates:\nmean of x \n 6.959107 \n```\n\n\n:::\n:::\n\n\n\n> No evidence that U.S. adults sleep more than 7 hours\n:::\n\n:::\n\n\n## Another example: DDT data\n\nThe following are 15 measurements of the pesticide DDT in kale in parts per million (ppm).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n_2.79_, _2.93_, _3.22_, _3.78_, _3.22_, _3.38_, _3.18_, _3.33_, _3.34_, _3.06_, _3.07_, _3.56_, _3.08_, _4.64_ and _3.34_\n:::\n:::\n\n\n\n> C. E. Finsterwalder (1976) Collaborative study of an extension of the Mills et al method for the determination of pesticide residues in food. J. Off. Anal. Chem. 59, 169–171.\n\nImagine the target level for safety considerations is 3ppm or less, and you want to use this data to determine whether the mean DDT level is within safe limits.\n\n$$\n\\begin{cases}\nH_0: &\\mu = 3 \\quad(\\text{null hypothesis})\\\\\nH_A: &\\mu > 3 \\quad(\\text{alternative hypothesis})\n\\end{cases}\n$$\n\n*We choose this direction because we're concerned with evidence that mean DDT **exceeds** the threshold.* \n\n## Another example: DDT data\n\n::: columns\n::: {.column width=\"45%\"}\n**If in fact $\\mu = 3$**, then according to the $t$ model 0.58% of samples would produce an error of this magnitude or more in the direction of the alternative:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-114-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(ddt, mu = 3, alternative = 'greater')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.005753\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.129197      Inf\nsample estimates:\nmean of x \n    3.328 \n```\n\n\n:::\n:::\n\n\n\n> The data provide strong evidence that mean DDT in kale exceeds 3ppm (*T* = 2.9059 on 14 degrees of freedom, *p* = 0.0058). With 95% confidence, the mean DDT is estimated to be at least 3.129, with a point estimate of 3.32 (SE: 0.1168).\n:::\n:::\n\n*Notice the one-sided interval! (`Inf` = $\\infty$.) This is called a \"lower confidence bound\".*\n\n\n## Your turn: which alternative?\n\n> Write the hypotheses in notation and identify which test (upper/lower/two sided) should be used.\n\nUsing the temperature/heartrate data:\n\n1. Is mean body temperature less than 98.6°F?\n2. Is mean heart rate greater than 60 bpm? \n3. Is mean heart rate 65 bpm?\n\nUsing the NC births data:\n\n1. Is the mean number of weeks at birth 40?\n2. Is the mean birth weight at least 7 lbs?\n3. Is the mean birth weight under 8 lbs?\n\n# Lecture 9: Two-sample inference\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. [lecture] two sample inference for means\n2. [lab] two-sample $t$ tests in R\n3. [test prep] practice problems\n\n## From last time\n\nPractice problem: *test whether actual body weight exceeds desired body weight*.\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------------------------------\n subject   actual   desired   difference \n--------- -------- --------- ------------\n    1       265       225         40     \n\n    2       150       150         0      \n\n    3       137       150        -13     \n\n    4       159       125         34     \n\n    5       145       125         20     \n-----------------------------------------\n\n\n:::\n:::\n\n\n::: \n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweight.diffs <- brfss$weight - brfss$wtdesire\nt.test(weight.diffs, \n       mu = 0, \n       alternative = 'greater')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  weight.diffs\nt = 4.2172, df = 59, p-value = 4.311e-05\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 10.99824      Inf\nsample estimates:\nmean of x \n 18.21667 \n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n> The data provide very strong evidence that the average U.S. adult's actual weight exceeds their desired weight (*T* = 4.2172 on 59 degrees of freedom, *p* < 0.0001).\n\nInference is on the mean difference: $H_0: \\delta = 0$ vs. $H_A: \\delta > 0$.\n\n*Can we also do inference on a difference in means?*\n\n## Evolution of Darwin's finches\n\n::: columns\n::: {.column width=\"70%\"}\nPeter and Rosemary Grant caught and measured birds from more than 20 generations of finches on Daphne Major.\n\n-   severe drought in 1977 limited food to large tough seeds\n\n-   selection pressure favoring larger and stronger beaks\n\n-   hypothesis: beak depth increased in 1978 relative to 1976\n\n:::\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------\n year   depth \n------ -------\n 1976   10.8  \n\n 1976    7.4  \n\n 1978   11.4  \n\n 1978   10.6  \n--------------\n\n\n:::\n:::\n\n\n:::\n:::\n\nTo answer this, we need to test a hypothesis involving two means:\n\n$$\n\\begin{cases}\nH_0: &\\mu_{1976} = \\mu_{1978} \\\\\nH_A: &\\mu_{1976} < \\mu_{1978}\n\\end{cases}\n$$\n\n- can't do inference on a mean difference here (no pairing of observations)\n- treat each year as an independent sample\n\n## Two-sample inference\n\n\n\n::: {.cell}\n\n:::\n\n\n\nIf $x_1, \\dots, x_{58}$ are the 1976 observations and $y_1, \\dots, y_{65}$ are the 1978 observations:\n\n- $\\bar{x}$ is a point estimate for $\\mu_{1976}$ with standard error $SE(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}$\n- $\\bar{y}$ is a point estimate for $\\mu_{1978}$ with standard error $SE(\\bar{y}) = \\frac{s_y}{\\sqrt{n}}$\n\n::: {.columns}\n\n::: {.column}\n\nInference uses a new $T$ statistic:\n\n$$\nT = \\frac{\\bar{x} - \\bar{y} - \\delta_0}{SE(\\bar{x} - \\bar{y})}\n$$\n\n- $\\delta_0$ is the hypothesized difference in means\n-   $SE(\\bar{x} - \\bar{y}) = \\sqrt{SE(\\bar{x})^2 + SE(\\bar{y})^2}$\n-   $t_\\nu$ model approximates the sampling distribution when each sample meets assumptions for one-sample inference\n\n:::\n\n::: {.column}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-121-1.png){fig-align='center'}\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Checking assumptions\n\n> The two-sample test is appropriate whenever two one-sample tests would be.\n\n::: {.columns}\n\n::: {.column}\nIn other words, the test assumes that *both* samples are either:\n\n- sufficiently large; or\n- have little skew and few outliers\n\nTo check, simply inspect each histogram.\n\n- both distributions unimodal\n- both a bit left skewed \n- no extreme outliers\n- large sample sizes (58, 65)\n:::\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-122-1.png)\n:::\n:::\n\n\n:::\n\n:::\n\n\n## Checking assumptions (alternative)\n\n> The two-sample test is appropriate whenever two one-sample tests would be.\n\n::: {.columns}\n\n::: {.column}\nIn other words, the test assumes that *both* samples are either:\n\n- sufficiently large; or\n- have little skew and few outliers\n\nCould also check side-by-side boxplots for:\n\n- approximate symmetry of boxes\n- outliers far from whiskers\n\nThis is also a nice visualization of differences between samples.\n:::\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-123-1.png)\n:::\n:::\n\n\n:::\n\n:::\n\n## Interpreting outputs and results\n\n::: columns\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(depth ~ year, data = finch,\n       mu = 0, alternative = 'less')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  depth by year\nt = -4.5727, df = 111.79, p-value = 6.255e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is less than 0\n95 percent confidence interval:\n       -Inf -0.4698812\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.453448          10.190769 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"55%\"}\n> The data provide very strong evidence that mean beak depth increased following the drought (*T* = -4.5727 on 111.79 degrees of freedom, *p* < 0.0001). With 95% confidence, the mean increase is estimated to be at least 0.4699 mm, with a point estimate of 0.7373 (SE 0.1612).\n:::\n:::\n\nHighly similar, but notice:\n\n- input is a formula `depth ~ year` (\"depth depends on year\") and data frame `finch`\n- `mu` now indicates hypothesized difference in means\n- decimal degrees of freedom\n- alternative is relative to the order in which groups appear\n\n## Cloud data\n\n> Does seeding clouds with silver iodide increase mean rainfall?\n\n::: columns\n::: {.column width=\"60%\"}\nData are rainfall measurements in a target area from 26 days when clouds were seeded and 26 days when clouds were not seeded.\n\n-   `rainfall` gives volume of rainfall in acre-feet\n-   `treatment` indicates whether clouds were seeded\n\nHypotheses to test: \n$$\n\\begin{cases}\nH_0: &\\mu_\\text{seeded} = \\mu_\\text{unseeded} \\\\\nH_A: &\\mu_\\text{seeded} > \\mu_\\text{unseeded}\n\\end{cases}\n$$\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n----------------------\n rainfall   treatment \n---------- -----------\n  334.1      seeded   \n\n  489.1      seeded   \n\n  200.7      seeded   \n\n   40.6      seeded   \n\n   21.7     unseeded  \n\n   17.3     unseeded  \n\n   68.5     unseeded  \n\n  830.1     unseeded  \n----------------------\n\n\n:::\n:::\n\n\n:::\n:::\n\n## Cloud data: which alternative?\n\n> Does seeding clouds with silver iodide increase mean rainfall?\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'less')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.9731\nalternative hypothesis: true difference in means between group seeded and group unseeded is less than 0\n95 percent confidence interval:\n     -Inf 512.1582\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\nYou can tell which group R considers first based on which estimate is printed first.\n\n\n- `'greater'` is interpreted as [FIRST GROUP] > [SECOND GROUP]\n- `'less'` is interpreted as [FIRST GROUP] < [SECOND GROUP]\n\n## Cloud data: interpretation\n\n> Does seeding clouds with silver iodide increase mean rainfall?\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n> The data provide moderate evidence that cloud seeding increases mean rainfall (*T* = 1.9982 on 33.855 degrees of freedom, *p* = 0.02689). With 95% confidence, seeding is estimated to increase mean rainfall by at least 42.63 acre-feet, with a point estimate of 277.4 (SE 138.8199).\n:::\n\n:::\n\n## Body temperatures (again)\n\n> Does mean body temperature differ between men and women?\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-130-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column}\nTest $H_0: \\mu_F = \\mu_M$ against $H_A: \\mu_F \\neq \\mu_M$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(body.temp ~ sex, data = temps, \n       mu = 0, alternative = 'two.sided')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  body.temp by sex\nt = 1.7118, df = 34.329, p-value = 0.09595\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.09204497  1.07783444\nsample estimates:\nmean in group female   mean in group male \n            98.65789             98.16500 \n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\nSuggestive but insufficient evidence that mean body temperature differs by sex.\n\nNotice: estimated difference (F - M) is 0.493 °F (SE 0.2879)\n\n## What if we had more data?\n\n\n\n::: {.cell}\n\n:::\n\n\n\nHere are estimates from two larger samples of 65 individuals each (compared with 19, 20):\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------------------------\n  sex     mean.temp     se      n  \n-------- ----------- --------- ----\n female     98.39     0.09222   65 \n\n  male      98.1      0.08667   65 \n-----------------------------------\n\n\n:::\n:::\n\n\n\n- estimated difference (F - M) is smaller 0.2892 °F\n- but so is the standard error SE 0.1266 (recall more data $\\longleftrightarrow$ better precision)\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(body.temp ~ sex, data = temps.aug, \n       mu = 0, alternative = 'two.sided')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  body.temp by sex\nt = 2.2854, df = 127.51, p-value = 0.02394\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n 0.03881298 0.53964856\nsample estimates:\nmean in group female   mean in group male \n            98.39385             98.10462 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n> The data provide moderate evidence that mean body temperature differs by sex (*T* = 2.29 on 127.51 degrees of freedom, *p* = 0.02394).\n:::\n\n:::\n\n## Power calculations\n\n> How much data do you need to collect in order to detect a difference of $\\delta$?\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\nThe statistical **power** of a test captures how often it detects a specified alternative.\n\n- measures how often the test correctly rejects (proportion of samples)\n- value depends on...\n\n    a. magnitude of difference between null value and true value of parameter\n    b. significance level\n    c. sample size\n\n:::\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(power = 0.95, \n             delta = 0.5, \n             sig.level = 0.05, \n             type = 'two.sample',\n             alternative = 'two.sided')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 104.928\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n$\\Rightarrow$ need 105 observations in each group to detect a difference of 0.5 standard deviations for 95% of samples with a 5% significance level test\n:::\n\n:::\n\n## A statistical trap\n\n> If you collect enough data, you can detect an arbitrarily small difference in means almost always.\n\n::: {.columns}\n\n::: {.column width=\"70%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-136-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"30%\"}\nSo keep in mind:\n\n- statistical significance $\\neq$ practical significance\n- always check your point estimates\n:::\n\n:::\n\n# Extras\n\n## The equal-variance $t$-test\n\nIf it is reasonable to assume the (population) standard deviations are the same in each group, one can gain a bit of power by using a different standard error:\n\n$$SE_\\text{pooled}(\\bar{x} - \\bar{y}) = \\sqrt{\\frac{\\color{red}{s_p^2}}{n_x} + \\frac{\\color{red}{s_p^2}}{n_y}}\n\\quad\\text{where}\\quad \\color{red}{s_p} = \\underbrace{\\sqrt{\\frac{(n_x - 1)s_x^2 + (n_y - 1)s_y^2}{n_x + n_y - 2}}}_{\\text{weighted average of } s_x^2 \\;\\&\\; s_y^2}$$\n\nImplement by adding `var.equal = T` as an argument to `t.test()`.\n\n- larger df is used, hence more frequent rejections\n- avoid unless you have a small sample\n\n# Lecture 10: Power analyses\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. [lecture] Statistical power; post-hoc and sample size power analyses.\n2. [review/lab] Test 2 practice problems.\n\n## $p$-values and false rejections\n\n> A $p$-value captures how often you'd make a mistake **if $H_0$ were true**.\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n```\n\n\n:::\n:::\n\n\n\nIf there is no effect of cloud seeding, then we would see $T > 1.9982$ for 2.689% of samples.\n\n:::\n\n::: {.column}\nThe test rejects at the 5% significance level ($p < 0.05$), but that doesn't completely rule out $H_0$.\n\n- while unlikely, our sample could have been one of the 26 in 1000 where $T$ exceeds 1.9982 despite no effect\n- by rejecting here (when $T = 1.9982$) we are willing to be wrong 2.689% of the time\n\n*By rejecting when $p < \\alpha$ we are willing to be wrong $\\alpha\\times 100$% of the time.*\n\n:::\n\n:::\n\n## A different kind of error?\n\n> But you can also make a mistake when $H_0$ is false!\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'two.sided')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.05377\nalternative hypothesis: true difference in means between group seeded and group unseeded is not equal to 0\n95 percent confidence interval:\n  -4.764295 559.556603\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n```\n\n\n:::\n:::\n\n\n\nWe'd see $|T| > 1.9982$ for 5.377% of samples if there's no effect. But what if there is an effect?\n:::\n\n::: {.column}\nThe two-sided test fails to reject at the 5% significance level ($p > 0.05$), but that doesn't completely rule out $H_A$.\n\n- the estimated effect -- increase of 277.4 acre-feet -- could be too small relative to the variability in rainfall\n\n- hard to say how often we'd make this kind of mistake without knowing the real difference\n\n*The rate of fail-to-reject errors depends on the (unknown) true parameter value.*\n:::\n\n:::\n\n## Decision errors\n\n> There are two ways to make a mistake in a hypothesis test -- two \"*error types*\".\n\n| | Reject $H_0$ | Fail to reject $H_0$\n---|---|---\n**True $H_0$** | type I error | [correct decision]{style=\"color:lightgrey\"}\n**False $H_0$** | [correct decision]{style='color:lightgrey'} | type II error\n\n::: {.columns}\n\n::: {.column}\n\nAny statistical test will have certain error rates:\n\n- type I error rate is denoted $\\alpha$\n- type II error rate is denoted $1 - \\beta$\n\n:::\n\n::: {.column}\n\nThe significance level of a test is its type I error rate.\n\n- reject when $p < \\alpha$ $\\Longleftrightarrow$ mistakenly reject $\\alpha\\times 100$% of the time\n\nBut we don't know the type II error rate!\n\n- depends on which alternative parameter value is true\n\n:::\n\n:::\n\n\n## Simulating type II errors\n\n::: columns\n::: {.column width=\"55%\"}\nSummary stats for cloud data:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------\n treatment   mean     sd     n  \n----------- ------- ------- ----\n  seeded      442    650.8   26 \n\n unseeded    164.6   278.4   26 \n--------------------------------\n\n\n:::\n:::\n\n\n\nWe can approximate the type II error rate by:\n\n1.  simulating datasets with matching statistics\n2.  performing two-sided tests of no difference\n3.  computing the proportion of fail-to-reject decisions\n\n\n:::\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntype2sim(delta = 277, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-142-1.png)\n:::\n:::\n\n\n:::\n:::\n\n> If in fact the effect size is exactly 277, a level 5% test with similar data will fail to reject ~70% of the time!\n\n## Larger effect size\n\n::: columns\n::: {.column width=\"55%\"}\nSummary stats for cloud data:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------\n treatment   mean     sd     n  \n----------- ------- ------- ----\n  seeded      442    650.8   26 \n\n unseeded    164.6   278.4   26 \n--------------------------------\n\n\n:::\n:::\n\n\n\nWe can approximate the type II error rate by:\n\n1.  simulating datasets with matching statistics\n2.  performing two-sided tests of no difference\n3.  computing the proportion of fail-to-reject decisions\n\n\n:::\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntype2sim(delta = 350, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-145-1.png)\n:::\n:::\n\n\n:::\n:::\n\n> If in fact the effect size is exactly 400, a level 5% test with similar data will fail to reject ~40% of the time.\n\n## Smaller effect size\n\n::: columns\n::: {.column width=\"55%\"}\nSummary stats for cloud data:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------\n treatment   mean     sd     n  \n----------- ------- ------- ----\n  seeded      442    650.8   26 \n\n unseeded    164.6   278.4   26 \n--------------------------------\n\n\n:::\n:::\n\n\n\nWe can approximate the type II error rate by:\n\n1.  simulating datasets with matching statistics\n2.  performing two-sided tests of no difference\n3.  computing the proportion of fail-to-reject decisions\n\n\n:::\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntype2sim(delta = 100, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-148-1.png)\n:::\n:::\n\n\n:::\n:::\n\n> If in fact the effect size is exactly 100, a level 5% test with similar data will fail to reject ~90% of the time.\n\n\n\n## Statistical power\n\nThe **power** of a test refers to its **true rejection rate** across alternatives and is defined as: $$\\beta = \\underbrace{(1 - \\text{type II error rate})}_\\text{correct decision rate when null is false}$$\n\nPower is often interpreted as a detection rate:\n\n-   high type II error $\\longrightarrow$ low power $\\longrightarrow$ low detection rate\n-   low type II error $\\longrightarrow$ high power $\\longrightarrow$ high detection rate\n\n> In general tests have low power for alternatives close to the null value (where \"close\" is relative to sampling variability).\n\n## Power curves\n\n> Power is usually construed as a *curve* depending on the true difference. \n\n::: {.columns}\n\n::: {.column width=\"60%\"}\n\nPower curve for the test exactly as performed with the cloud seeding data:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-149-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\nAll other attributes of the test are fixed to approximate the test performed:\n\n- sample size $n = 26$\n- significance level $\\alpha = 0.05$\n- population standard deviation $\\sigma = 650$ (larger of two group estimates)\n\n:::\n\n:::\n\n\n## Two common power analyses\n\n::: columns\n::: column\n**Post hoc analysis**: how much power does the test I conducted have if the true difference is exactly equal to my estimate?\n\nHelps to interpret negative results:\n\n-   low power $\\rightarrow$ failure to reject was likely\n-   high power $\\rightarrow$ failure to reject was not likely\n\n::: callout-important\n## Don't over-interpret post-hoc analyses\n\nFailure to reject using a well-powered test *does not confirm the null hypothesis*.\n:::\n\n:::\n\n::: column\n**Sample size determination**: how much data do I need to collect to detect a difference of $\\delta$ using a particular test?\n\nHelps avoid two potential issues:\n\n-   too little data $\\rightarrow$ study not likely to yield significant results\n-   too much data $\\rightarrow$ study is too likely to yield significant results\n:::\n:::\n\n## Post-hoc analysis\n\n> Can we estimate the power of a test we already performed?\n\n::: columns\n::: {.column width=\"55%\"}\nFeasible if we assume (a) a population standard deviation and (b) test conditions are met.\n\nFor the cloud seeding test:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(delta = 250, # magnitude of difference\n             sd = 650, # largest population SD\n             n = 26, # smallest sample size\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 26\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.2743235\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"45%\"}\nFor a conservative estimate, use:\n\n-   *smallest* of the two sample sizes\n-   *largest* of the two standard deviations\n-   *smaller* difference than observed\n\n> $\\Longrightarrow$ our test would only reject in favor of a difference of the observed magnitude about 27% of the time\n\nFailure to reject doesn't strongly rule out the alternative.\n:::\n:::\n\n## Sample size calculation\n\n> If you were (re)designing the study, how much data should you collect to detect a specified effect size?\n\n::: columns\n::: {.column width=\"55%\"}\nTo detect a difference of 250 or more due to cloud seeding with power 0.9:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(power = 0.9, # target power level\n             delta = 250, # smallest difference\n             sd = 650, # largest population SD\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 143.0276\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"45%\"}\nFor a conservative estimate, use:\n\n-   *overestimate* of the larger of the two standard deviations\n-   *minimum* difference of interest\n\n> $\\Longrightarrow$ we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time\n:::\n:::\n\n\n\n## Practical constraints\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\n\nMinimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-152-1.png)\n:::\n:::\n\n\n\nAssumes $\\sigma = 650$ for a conservative estimate.\n\n:::\n\n::: {.column width=\"40%\"}\nIt may not be affordable to obtain data for 144 days per treatment group (pilots and planes are expensive). What is achievable within constraints?\n\n- power of 0.8 will require *n* = 59 per group\n\n    + 138 days total\n\n- decreasing to 0.7 will require *n* = 45 per group\n\n    + 90 days total\n\n:::\n\n:::\n\n# Lecture 11: Analysis of variance\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## Today's agenda\n\n1. \\[lecture\\] inference comparing several population means\n2. \\[lab\\] fitting ANOVA models in R\n\n## More than two means?\n\n::: columns\n::: column\nYou previously considered this data on chick weights at 20 days of age by diet:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-154-1.png)\n:::\n:::\n\n\n\n:::\n\n::: column\nHere we have *four means* to compare rather than just two. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------------------------\n diet   mean     se      sd     n  \n------ ------- ------- ------- ----\n  1     170.4   13.45   55.44   17 \n\n  2     205.6   22.22   70.25   10 \n\n  3     258.9   20.63   65.24   10 \n\n  4     233.9   12.52   37.57   9  \n-----------------------------------\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n> Does mean weight at 20 days differ by diet? How do you test this?\n\n## Hypotheses for a difference in means\n\nLet $\\mu_i = \\text{mean weight on diet } i = 1, 2, 3, 4$.\n\nThe hypothesis that there are **no differences** in means by diet is:\n\n$$\nH_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\quad (\\text{no difference in means})\n$$\n\nThe alternative, if this is false, is that there is **at least one difference**:\n\n$$\nH_A: \\mu_i \\neq \\mu_j \\quad (\\text{at least one difference})\n$$\n\n## How much difference is enough?\n\nHere are two made-up examples of four sample means.\n\n::: columns\n::: {.column width=\"70%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-156-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"30%\"}\n\n> Why does it look like there's a difference at right but not at left?\n\n:::\n:::\n\nThink about the $t$-test: we say there's a difference if $T = \\frac{\\text{estimate} - \\text{hypothesis}}{\\text{variability}}$ is large.\n\nSame idea here: we see differences if they are big *relative to the variability in estimates*.\n\n## Partitioning variation\n\n> Partitioning variation into two or more components is called \"analysis of variance\"\n\n::: columns\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-157-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"55%\"}\nFor the chick data, two sources of variability:\n\n-   [group]{style=\"color:red\"} variability between diets\n\n-   [error]{style=\"color:blue\"} variability among chicks\n\nThe analysis of variance (ANOVA) model:\n\n$$\\color{grey}{\\text{total variation}} = \\color{red}{\\text{group variation}} + \\color{blue}{\\text{error variation}}$$\n:::\n:::\n\n\nWe'll base the test on the ratio $F = \\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}$.\n\n\n## The $F$ statistic: a variance ratio\n\n> The $F$ statistic measures variability attributable to group differences relative to variability attributable to individual differences.\n\n\n::: columns\n::: column\nNotation:\n\n-   $\\bar{x}$: \"grand\" mean of all observations\n-   $\\bar{x}_i$: mean of observations in group $i$\n-   $s_i$: SD of observations in group $i$\n-   $k$ groups\n-   $n$ total observations\n-   $n_i$ observations per group\n\n:::\n\n::: column\nMeasures of variability:\n\n$$\\color{red}{MSG} = \\frac{1}{k - 1}\\sum_i n_i(\\bar{x}_i - \\bar{x})^2 \\quad(\\color{red}{\\text{group}})$$ \n$$\\color{blue}{MSE} = \\frac{1}{n - k}\\sum_i (n_i - 1)s_i^2 \\quad(\\color{blue}{\\text{error}})$$ Ratio:\n\n$$F = \\frac{\\color{red}{MSG}}{\\color{blue}{MSE}} \\quad\\left(\\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}}\\right)$$\n:::\n:::\n\n## Sampling distribution for $F$\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\nIf the data satisfy these conditions:\n\n1. the distribution of values is symmetric and unimodal within each group\n2. the variability (standard deviation) is roughly the same across groups\n\nThen the $F$ statistic has a sampling distribution well-approximated by an $F_{k - 1, n - k}$ model.\n\n- numerator degrees of freedom $k - 1$\n- denominator degrees of freedom $n - k$\n:::\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![$F$ models for several different numerator degrees of freedom $k - 1$ with fixed $n = 30$.](all-slides_files/figure-docx/unnamed-chunk-158-1.png)\n:::\n:::\n\n\n:::\n\n:::\n\n## $p$-values for the $F$ test\n\n::: {.columns}\n\n::: {.column}\nTo test the hypotheses:\n\n$$\n\\begin{cases}\nH_0: &\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\\\\nH_0: &\\mu_i \\neq \\mu_j \\quad\\text{for some}\\quad i \\neq j\n\\end{cases}\n$$\nCalculate the $F$ statistic:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ingredients of mean squares\nk <- nrow(chicks.summary)\nn <- nrow(chicks)\nn.i <- chicks.summary$n\nxbar.i <- chicks.summary$mean\ns.i <- chicks.summary$sd\nxbar <- mean(chicks$weight)\n\n# mean squares\nmsg <- sum(n.i*(xbar.i - xbar)^2)/(k - 1)\nmse <- sum((n.i - 1)*s.i^2)/(n - k)\n\n# f statistic\nfstat <- msg/mse\nfstat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.463598\n```\n\n\n:::\n:::\n\n\n\nAnd reject $H_0$ when $F$ is large. \n:::\n\n::: {.column}\n\nFor a significance level $\\alpha$ test, reject $H_0$ when $\\underbrace{P(F > F_\\text{obs})}_\\text{p-value} < \\alpha$.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-160-1.png)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npf(fstat, 4 - 1, 46 - 4, lower.tail = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002909054\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n## Interpreting $F$ statistics and $p$-values\n\n::: {.columns}\n\n::: {.column}\n$$\n\\begin{cases}\nH_0: &\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\\\\nH_0: &\\mu_i \\neq \\mu_j \\quad\\text{for some}\\quad i \\neq j\n\\end{cases}\n$$\n\n$F = \\frac{\\color{red}{\\text{group variation}}}{\\color{blue}{\\text{error variation}}} = \\frac{MSG}{MSE} = 5.4636$.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-162-1.png)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npf(fstat, 4 - 1, 46 - 4, lower.tail = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002909054\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n\n> *F = 5.4636* means the proportion of variation in weight attributable to diets is 5.46 times greater than the proportion of variation attributable to chicks.\n\nThe statistical significance of this result is measured by the $p$-value:\n\n- if there is in fact no difference in means, then only 0.29% of samples (*i.e.*, 2 in 1000) would produce at least as much diet-to-diet variability as we observed.\n\n- so in this case we reject $H_0$ at the 1% level\n\n:::\n\n:::\n\n## ANOVA in R\n\nThe `aov(...)` function fits ANOVA models using a formula/dataframe specification:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit anova model\nfit <- aov(weight ~ diet, data = chicks)\n\n# generate table\nsummary(fit)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------------------------------------\n    &nbsp;       Df   Sum Sq   Mean Sq   F value    Pr(>F)  \n--------------- ---- -------- --------- --------- ----------\n   **diet**      3    55881     18627     5.464    0.002909 \n\n **Residuals**   42   143190    3409       NA         NA    \n------------------------------------------------------------\n\nTable: Analysis of Variance Model\n\n\n:::\n:::\n\n\n\nThe typical style for interpretation closely follows that of previous inferences for the mean:\n\n> The data provide strong evidence of an effect of diet on mean weight (*F = 5.464* on *3* and *42* df, *p = 0.0029*).\n\n## Analysis of variance table\n\nThe results of an analysis of variance are traditionally displayed in a table.\n\nSource | degrees of freedom | Sum of squares | Mean square | F statistic | *p*-value\n---|---|---|---|---|---\nGroup | $k - 1$ | SSG | $MSG = \\frac{SSG}{k - 1}$ | $\\frac{MSG}{MSE}$  | $P(F > F_\\text{obs})$\nError | $n - k$ | SSE | $MSE = \\frac{SSE}{n - k}$ | \n\n- the sum of square terms are 'raw' measures of variability\n- the mean square terms are averages adjusted for the amount of data available to estimate variability due to each source\n\nFormally, the ANOVA model says $(n - 1)s^2 = SSG + SSE$.\n\n## Checking assumptions\n\n::: {.columns}\n\n::: {.column}\nThe ANOVA test assumes:\n\n1. the distribution of values is symmetric and unimodal within each group\n2. the variability (standard deviation) is roughly the same across groups\n\nTo check these assumptions:\n\n- compare group standard deviations for similarity\n- visually inspect distributions within each group for approximate symmetry\n\nSimilar to the $t$ test, greater departures from these assumptions are allowable for larger sample sizes.\n:::\n\n::: {.column}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-166-1.png)\n:::\n\n::: {.cell-output-display}\n\n-----------------------------------\n diet   mean     se      sd     n  \n------ ------- ------- ------- ----\n  1     170.4   13.45   55.44   17 \n\n  2     205.6   22.22   70.25   10 \n\n  3     258.9   20.63   65.24   10 \n\n  4     233.9   12.52   37.57   9  \n-----------------------------------\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Another example: treating anorexia\n\n::: {.columns}\n\n::: {.column}\nWeight change was measured for 72 young female anorexia patients randomly allocated to three treatment groups:\n\n- cognitive behavioral therapy (CBT)\n- family treatment (FT)\n- a control (Cont)\n\nGrouped summary statistics:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------------------\n treat   post - pre    sd     n  \n------- ------------ ------- ----\n  CBT      3.007      7.309   29 \n\n Cont      -0.45      7.989   26 \n\n  FT       7.265      7.157   17 \n---------------------------------\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-168-1.png)\n:::\n:::\n\n\n\n> Were any of the treatments more effective than others?\n:::\n\n:::\n\n## Another example: treating anorexia\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit anova model\nfit <- aov(change ~ treat, data = anorexia)\n\n# generate table\nsummary(fit)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------------------------------------\n    &nbsp;       Df   Sum Sq   Mean Sq   F value    Pr(>F)  \n--------------- ---- -------- --------- --------- ----------\n   **treat**     2    614.6     307.3     5.422    0.006499 \n\n **Residuals**   69    3911     56.68      NA         NA    \n------------------------------------------------------------\n\nTable: Analysis of Variance Model\n\n\n:::\n:::\n\n\n\n> The data provide strong evidence of an effect of therapeutic treatment on mean weight change among young women with anorexia (*F = 5.422* on *2* and *69* degrees of freedom, *p = 0.0065*).\n\n# Lecture 12: Post-hoc inference\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. [lecture] inference on group means and contrasts after performing ANOVA\n2. [lab] estimates, tests, and intervals using `emmeans`\n\n## From before: diet restriction\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.columns}\n\n::: {.column width=\"40%\"}\n**Data summaries**:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-173-1.png)\n:::\n\n::: {.cell-output-display}\n\n----------------------------\n diet    mean     sd     n  \n------- ------- ------- ----\n  NP     27.4    6.134   49 \n\n N/N85   32.69   5.125   57 \n\n N/R50   42.3    7.768   71 \n\n N/R40   45.12   6.703   60 \n----------------------------\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"60%\"}\n**Inference using ANOVA**:\n\n$$\n\\begin{align*}\n&H_0: \\mu_\\text{NP} = \\mu_\\text{N/N85} = \\mu_\\text{N/R50} = \\mu_\\text{N/R40} \\\\\n&H_A: \\text{at least two means differ}\n\\end{align*}\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- aov(lifetime ~ diet, data = longevity)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \ndiet          3  11426    3809   87.41 <2e-16 ***\nResiduals   233  10152      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n> The data provide strong evidence that diet restriction has an effect on mean lifetime among mice (*F = 87.41* on *3* and *233* degrees of freedom, *p < 0.0001*).\n:::\n\n:::\n\n## Follow-up questions\n\nThe ANOVA tells us there's evidence of an effect of diet restriction on lifespan. \n\nSo now we'd want to know:\n\n1. What are the mean lifespans for each level of restriction?\n2. For which levels of dietary restriction do mean lifespans differ?\n3. What are the gains in mean lifespan for each level of restriction relative to an unrestricted diet?\n4. For which levels of restriction are gains in mean lifespan significant?\n\nAnswers require *post-hoc* inferences (done after-the-fact) on:\n\n- group means $\\mu_i$ (question 1)\n- \"contrasts\" $\\mu_i - \\mu_j$ (questions 2-4)\n\n## Estimates for group means\n\n> Interval estimates for group means in ANOVA use a model-based standard error.\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(object = fit, spec = ~ diet) |> \n  confint(level = 0.95)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------------------\n diet    estimate     SE         95% CI     \n------- ---------- -------- ----------------\n  NP       27.4     0.943    (25.54, 29.26) \n\n N/N85    32.69     0.8743   (30.97, 34.41) \n\n N/R50     42.3     0.7834   (40.75, 43.84) \n\n N/R40    45.12     0.8522   (43.44, 46.8)  \n--------------------------------------------\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\nInterval estimates use a \"pooled\" standard deviation:\n\n$$SE_i = \\frac{s_\\text{pooled}}{\\sqrt{n_i}} = \\frac{\\sqrt{MSE}}{\\sqrt{n_i}}$$\n\nOtherwise identical to $t_{n - k}$ confidence intervals.\n:::\n\n:::\n\nRationale: \n\n- the ANOVA model assumes equal variability (standard deviations) across groups\n- better precision (for variability estimates, not means) when assumption holds\n\n## Bonferroni adjustment\n\n> Problem: several 95% intervals don't have simultaneous 95% coverage.\n\n- *individual* coverage: how often *one* interval covers the population mean\n- *simultaneous* coverage: how often *all* intervals cover population means *at the same time*\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-177-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column}\nThe **Bonferroni correction** for $k$ intervals consists in changing the individual coverage level to $\\left(1 - \\frac{\\alpha}{k}\\right)\\%$.\n\n- Effectively a width increase\n- Guarantees joint coverage $(1 - \\alpha)\\%$\n- Tends to be over conservative with many means (large $k$)\n:::\n\n:::\n\n## Implementation in R\n\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# table\nemmeans(object = fit, spec = ~ diet) |>\n  confint(level = 0.95, adjust = 'bonferroni')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n diet  emmean    SE  df lower.CL upper.CL\n NP      27.4 0.943 233     25.0     29.8\n N/N85   32.7 0.874 233     30.5     34.9\n N/R50   42.3 0.783 233     40.3     44.3\n N/R40   45.1 0.852 233     43.0     47.3\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \n```\n\n\n:::\n:::\n\n\n\n- note Bonferroni adjustment\n- these are model-based estimates that depend on the fitted ANOVA model\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualization\nemmeans(object = fit, spec = ~ diet) |>\n  confint(level = 0.95, adjust = 'bonferroni') |>\n  plot(xlab = 'mean lifespan (months)', \n       ylab = 'diet')\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-180-1.png)\n:::\n:::\n\n\n\n:::\n\n:::\n\n*Caution: these intervals do NOT indicate which means differ significantly.*\n\n## Pairwise comparisons\n\n> Pairwise comparisons are inferences made on \"contrasts\" between pairs of means.\n\nThe difference $\\mu_{NP} - \\mu_{N/N85}$ is an example of a contrast. It is common to perform inference on all pairwise contrasts to determine which means differ and by how much.\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(object = fit, specs = ~ diet) |> \n  contrast('pairwise')\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n    difference       estimate    SE   \n------------------- ---------- -------\n   NP - (N/N85)       -5.289    1.286 \n   NP - (N/R50)       -14.9     1.226 \n   NP - (N/R40)       -17.71    1.271 \n (N/N85) - (N/R50)    -9.606    1.174 \n (N/N85) - (N/R40)    -12.43    1.221 \n (N/R50) - (N/R40)    -2.819    1.158 \n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.column width=\"45%\"}\n\n- estimates are $\\bar{x}_i - \\bar{x}_j$\n- $SE_{ij} = SE(\\bar{x}_i - \\bar{x}_j) = s_\\text{pooled}\\sqrt{\\frac{1}{n_i} + \\frac{1}{n_j}}$\n- inference based on a $t_{n - k}$ model\n    \n    + degrees of freedom: $n - k$\n    + tests: $T_{ij} = \\frac{\\bar{x}_i - \\bar{x}_j}{SE_{ij}}$\n    + intervals: $\\bar{x}_i - \\bar{x}_j \\pm c\\times SE_{ij}$\n:::\n\n:::\n\n*Multiplicity corrections must adjust for the number of contrasts (6), not means (4).*\n\n\n## Pairwise comparisons: tests\n\n> Which means differ significantly? All except R50 and R40.\n\n::: {.columns}\n\n::: {.column}\n`emmeans` *then* `contrast` *then* `test`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(object = fit, specs = ~ diet) |>\n  contrast('pairwise') |>\n  test(adjust = 'bonferroni')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  <.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  <.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  <.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  <.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n```\n\n\n:::\n:::\n\n\n- *p*-values are adjusted for multiplicity\n- reject if *adjusted* *p*-value is below the significance threshold\n:::\n\n::: {.column}\nHypotheses for pairwise tests:\n\n$$\\begin{cases} H_0: \\mu_i - \\mu_j = 0 \\\\ H_A: \\mu_i - \\mu_j \\neq 0 \\end{cases}$$\nTest statistic:\n\n$$T_{ij} = \\frac{\\bar{x}_i - \\bar{x}_j}{SE_{ij}}$$\n\n$p$-values are obtained from a $t_{n - k}$ model for the sampling distribution of $T_{ij}$.\n\n:::\n\n:::\n\n## Pairwise comparisons: tests\n\n> Which means differ significantly? All except R50 and R40.\n\n::: {.columns}\n\n::: {.column}\n`emmeans` *then* `contrast` *then* `test`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(object = fit, specs = ~ diet) |>\n  contrast('pairwise') |>\n  test(adjust = 'bonferroni')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0003\n NP - (N/R50)        -14.90 1.23 233 -12.150  <.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  <.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  <.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  <.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0937\n\nP value adjustment: bonferroni method for 6 tests \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\nInterpretation:\n\n> The data provide evidence at the 5% significance level that mean lifespan differs among all levels of diet restriction except the N/R40 and N/R50 groups (*p* = 0.0937), for which the evidence is suggestive but inconclusive.\n:::\n\n:::\n\n## Multiple testing correction matters\n\n> Using unadjusted $p$-values will inflate type I error rates.\n\n::: {.columns}\n\n::: {.column}\nsetting `adjust = 'none'`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(object = fit, specs = ~ diet) |>\n  contrast('pairwise') |>\n  test(adjust = 'none')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast          estimate   SE  df t.ratio p.value\n NP - (N/N85)         -5.29 1.29 233  -4.113  0.0001\n NP - (N/R50)        -14.90 1.23 233 -12.150  <.0001\n NP - (N/R40)        -17.71 1.27 233 -13.938  <.0001\n (N/N85) - (N/R50)    -9.61 1.17 233  -8.183  <.0001\n (N/N85) - (N/R40)   -12.43 1.22 233 -10.177  <.0001\n (N/R50) - (N/R40)    -2.82 1.16 233  -2.436  0.0156\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\nFailure to adjust for multiple inferences leads to a different conclusion:\n\n> The data provide evidence at the 5% significance levelthat mean lifespan differs among all levels of diet restriction.\n\n:::\n\n:::\n\nThis is *incorrect*, because the joint significance level is not 5%.\n\nWithout adjustment, type I error could be as high as $k\\times\\alpha = 6\\times 0.05 = 0.3$!\n\n## Pairwise comparisons: intervals\n\n> How much do means differ? Anywhere from 2 - 21 months, depending.\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\n`emmeans` *then* `contrast` *then* `confint`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(object = fit, specs = ~ diet) |> \n  contrast('pairwise') |>\n  confint(level = 0.95, adjust = 'bonferroni')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n```\n\n\n:::\n:::\n\n\n\n- `level` specifies *joint* coverage after adjustment\n:::\n\n::: {.column width=\"45%\"}\nIntervals are for the parameter $\\mu_i - \\mu_j$:\n\n$$\\bar{x}_i - \\bar{x}_j \\pm c\\times SE_{ij}$$\n\nThe critical value $c$ is obtained from the $t_{n - k}$ model.\n\nFor a $(1 - \\alpha)\\times 100\\%$ confidence interval with Bonferroni correction: \n\n$$c = \\left(1 - \\frac{\\alpha}{2k}\\right) \\;\\text{quantile}$$\n\n:::\n\n:::\n\n## Pairwise comparisons: intervals\n\n> How much do means differ? Anywhere from 2 - 21 months, depending.\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\n`emmeans` *then* `contrast` *then* `confint`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(object = fit, specs = ~ diet) |> \n  contrast('pairwise') |>\n  confint(level = 0.95, adjust = 'bonferroni')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast          estimate   SE  df lower.CL upper.CL\n NP - (N/N85)         -5.29 1.29 233    -8.71   -1.867\n NP - (N/R50)        -14.90 1.23 233   -18.16  -11.633\n NP - (N/R40)        -17.71 1.27 233   -21.10  -14.333\n (N/N85) - (N/R50)    -9.61 1.17 233   -12.73   -6.482\n (N/N85) - (N/R40)   -12.43 1.22 233   -15.67   -9.177\n (N/R50) - (N/R40)    -2.82 1.16 233    -5.90    0.261\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 6 estimates \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"45%\"}\nInterpretations are the same as usual:\n\n> With 95% confidence, mean lifespan on a normal diet is estimated to exceed mean lifespan on an unrestricted diet by between 1.87 and 8.71 months, with a point estimate of 5.29 months difference (SE 1.29).\n:::\n\n:::\n\n## Pairwise comparisons: visualizations\n\n::: {.columns}\n\n::: {.column width=\"45%\"}\nAnother option is to visualize the pairwise comparison inferences by displaying simultaneous 95% intervals.\n\nEasy to spot significant contrasts:\n\n- intervals exclude 0 $\\Leftrightarrow$ tests reject\n:::\n\n::: {.column width=\"55%\"}\n`emmeans` *then* `contrast` *then* `confint` *then* `plot`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(object = fit, specs = ~ diet) |>\n  contrast('pairwise') |>\n  confint(level = 0.95, adjust = 'bonferroni') |>\n  plot(xlab = 'difference in mean lifetime (months)', \n       ylab = 'contrast')\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-190-1.png)\n:::\n:::\n\n\n\n:::\n\n\n:::\n\n\n\n## Comparisons with a control\n\n> Does diet restriction increase mean lifespan, and if so by how much?\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\nSpecify `contrast('trt.vs.ctrl')`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(object = fit, specs = ~ diet) |> \n  contrast('trt.vs.ctrl') |>\n  confint(level = 0.95, adjust = 'dunnett')\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n   contrast     estimate    SE         95% CI     \n-------------- ---------- ------- ----------------\n (N/N85) - NP    5.289     1.286    (2.23, 8.34)  \n (N/R50) - NP     14.9     1.226   (11.98, 17.81) \n (N/R40) - NP    17.71     1.271   (14.7, 20.73)  \n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n- Multiple inference adjustment uses Dunnett's method \n    \n    + specialized correction for comparisons with a control\n    + `adjust = 'dunnett'`\n    \n- Comparisons will be relative to first group in R\n:::\n\n:::\n\nWith 95% confidence, relative to an unrestricted diet...\n\n- a 85kcal/day diet increases lifespan by an estimated 2.23 to 8.34 months\n- a 50kcal/day diet increases lifespan by an estimated 11.98 to 17.81 months\n- a 40kcal/day diet increases lifespan by an estimated 14.70 to 20.73 months\n\n\n# Extras\n\n## Log contrasts: relative change\n\n> Can we instead estimate a *percent change* in lifespan relative to the control?\n\n::: {.columns}\n\n::: {.column width=\"44%\"}\nThe contrast in log-lifetimes would be:\n\n$$\n\\log(\\mu_{N85}) - \\log(\\mu_{NP}) = \\log\\left(\\frac{\\mu_{N85}}{\\mu_{NP}}\\right)\n$$\nSo to answer the question:\n\n1. refit the ANOVA model with *log* lifetimes\n2. compute contrasts with control group using Dunnett's method\n3. exponentiate estimates to obtain ratios (and hence percentages)\n:::\n\n::: {.column width=\"56%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit.log <- aov(log(lifetime) ~ diet, data = longevity)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n  contrast    estimate      95% CI    \n------------ ---------- --------------\n (N/R50)/NP    1.572     (1.42, 1.73) \n (N/R40)/NP    1.688     (1.52, 1.87) \n\n\n:::\n:::\n\n\n\nFact: mean log lifetime = log median lifetime.\n\n> With 95% confidence, diet restriction to 40kcal (a 52.9% reduction) is estimated to increase median lifespan in mice by 52% to 87%, with a point estimate of 68.8%.\n\n*Extra credit: work out and interpret the interval estimate for the contrast not shown above.*\n:::\n\n:::\n\n\n## Power calculations for ANOVA\n\n> How much data should we collect to detect a difference in mean lifespan of 1 month?\n\n::: {.columns}\n\n::: {.column}\nTo perform sample size power calculations, one needs:\n\n- number of groups\n- target significance level ($\\alpha$)\n- target power level\n- guess or prior estimate of variance ratio $\\frac{\\text{group variation}}{\\text{error variation}}$\n\n\n\n::: {.cell}\n\n:::\n\n\n\nFrom the existing study, $\\sqrt{MSE} = 6.6$\n:::\n\n::: {.column}\nSo to detect effects on the order of one month at 90% power:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.anova.test(groups = 4, \n                 sig.level = 0.05, \n                 power = 0.9, \n                 within.var = 6.633, \n                 between.var = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Balanced one-way analysis of variance power calculation \n\n         groups = 4\n              n = 32.32851\n    between.var = 1\n     within.var = 6.633\n      sig.level = 0.05\n          power = 0.9\n\nNOTE: n is number in each group\n```\n\n\n:::\n:::\n\n\n... we need 33 mice per treatment group.\n:::\n\n:::\n\n# Lecture 13: Inference for proportions\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. [lecture] Inference for binomial proportions\n2. [lab] Tests for proportions in R\n\n## Binomial proportions\n\n> A binomial variable is a nominal categorical variable with two unique values.\n\n::: columns\n::: {.column width=\"65%\"}\nUsually, binomial data record the presence/absence of an event, trait, or property of interest.\n\nInference for binomial data has a different flavor:\n\n-   non-numeric values $\\Rightarrow$ can't compute usual statistics (mean, variance, etc.)\n-   focus on proportions instead\n\nExample: prevalence of diabetes among US adults?\n\n-   estimate and standard error?\n-   confidence interval?\n-   hypothesis test?\n:::\n\n::: {.column width=\"35%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-198-1.png)\n:::\n:::\n\n\n:::\n:::\n\n## Estimating proportions\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n----------------------------------------\n     &nbsp;        Yes     No     total \n---------------- ------- ------- -------\n   **count**       57      443     500  \n\n **proportion**   0.114   0.886     1   \n----------------------------------------\n\nTable: *Diabetes data summary*\n\n\n:::\n:::\n\n\n\nEstimated diabetes prevalence: 11.4%.\n\n-   NHANES data are a random sample of the U.S. adult population\n-   sample statistics should approximate population statistics\n:::\n\n::: {.column width=\"50%\"}\nWe'll formalize this as estimating the **population proportion** $$p = \\frac{\\# \\text{ individuals with diabetes}}{\\text{total population size } N}$$ Using the **sample proportion** $$\\hat{p} = \\frac{\\# \\text{ respondents with diabetes}}{\\text{sample size } n}$$\n:::\n:::\n\nThe first step towards inference is a measure of precision for $\\hat{p}$. What is $SE(\\hat{p})$?\n\n## SE for a sample proportion\n\n> Binomial data are most variable when $p = 0.5$ and least variable when $p \\approx 0$ or $1$\n\n::: columns\n::: {.column width=\"45%\"}\nMeasure of spread for binomial data: $$\\sqrt{\\hat{p}(1 - \\hat{p})}$$\n\n-   highest when $\\hat{p} \\approx 0.5$\n-   lowest when $\\hat{p} \\approx 0 \\text{ or } 1$\n\nAnalogous to estimating a mean: $$\nSE\\left(\\hat{p}\\right) \n= \\frac{\\text{spread}}{\\sqrt{\\text{sample size}}} \n= \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n$$\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-200-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n## Sampling distribution of $\\hat{p}$\n\n::: columns\n::: column\nThe sample proportion $\\hat{p}$ has a sampling distribution that can be approximated by a normal model, provided:\n\n-   $\\hat{p}$ isn't too close to 0 or 1\n-   $n$ is sufficiently large\n\nA common condition to check:\n\n$$n\\hat{p} \\geq 10\\text{ and }n(1 - \\hat{p}) \\geq 10$$\n:::\n\n::: column\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-201-1.png)\n:::\n:::\n\n\n:::\n:::\n\nThis model can be used to construct hypothesis tests and confidence intervals for $p$.\n\n## Confidence interval for $p$\n\nA confidence interval for a binomial proportion $p$ is:\n\n$$\\hat{p} \\pm c \\times SE(\\hat{p})$$\n\nThe critical value $c$ comes from the normal model.\n\n-   empirical rule:\n\n    -   $c = 1$ gives a 68% interval\n    -   $c = 2$ gives a 95% interval\n    -   $c = 3$ gives a 99.7% interval\n\n-   for a $(1 - \\alpha)\\times 100 \\%$ confidence interval use the $1 - \\frac{\\alpha}{2}$ quantile of the normal model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(1 - 0.1/2) # c for 90% interval\nqnorm(1 - 0.05/2) # c for 95% interval\nqnorm(1 - 0.01/2) # c for 99% interval\n```\n:::\n\n\n\n## Example: diabetes prevalence\n\n::: columns\n::: column\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------------\n p.hat     se       n  \n------- --------- -----\n 0.114   0.01421   500 \n-----------------------\n\nTable: Point estimate for diabetes prevalence\n\n\n:::\n:::\n\n\n:::\n\n::: column\n> It is estimated that the proportion of the U.S. adult population with diagnosed diabetes is 11.4% (*SE = 1.42%*).\n:::\n:::\n\nCheck assumptions for the normal model:\n\n$$\n500\\times 0.114 = 57 \\geq 10\n\\quad\\text{and}\\quad 500\\times 0.886 = 443 \\geq 10\n$$\n\n::: columns\n::: column\n95% confidence interval for diabetes prevalence:\n\n$$\n0.114 \\pm 2\\times 0.01421 = (0.0881, 0.1459)\n$$\n:::\n\n::: column\n> With 95% confidence, the proportuion of U.S. adults with diagnosed diabetes is estimated to be between 8.81% and 14.59%.\n:::\n:::\n\n## Hypothesis tests for $p$\n\n::: columns\n::: {.column width=\"45%\"}\nTo test whether true prevalence is 10%: \n$$\n\\begin{cases}\nH_0: &p = 0.1 \\\\ \nH_A: &p \\neq 0.1\n\\end{cases}\n$$\n\nWe can use the test statistic:\n\n$$\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n= \\frac{\\hat{p} - 0.1}{\\sqrt{\\frac{0.1 (0.9)}{500}}}\n$$\nUnder $H_0$, the sampling distribution of $Z$ is approximated by a normal model, provided:\n\n- $np_0 \\geq 10$\n- $n(1 - p_0) \\geq 10$\n:::\n\n::: {.column width=\"55%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-204-1.png)\n:::\n:::\n\n\n\nHere $p = P(|Z| > 1.043) = 0.2967$, so:\n\n> the data provide no evidence that prevalence differs from 10%.\n\n\n\n:::\n:::\n\n## Hypothesis tests for $p$\n\n::: columns\n::: {.column width=\"45%\"}\nTo test whether true prevalence is 15%: \n$$\n\\begin{cases}\nH_0: &p = 0.15 \\\\ \nH_A: &p \\neq 0.15\n\\end{cases}\n$$\n\nWe can use the test statistic:\n\n$$\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n= \\frac{\\hat{p} - 0.15}{\\sqrt{\\frac{0.15 (0.85)}{500}}}\n$$\nUnder $H_0$, the sampling distribution of $Z$ is approximated by a normal model, provided:\n\n- $np_0 \\geq 10$\n- $n(1 - p_0) \\geq 10$\n:::\n\n::: {.column width=\"55%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-205-1.png)\n:::\n:::\n\n\n\nHere $p = P(|Z| > 2.254) = 0.0242$, so:\n\n> the data provide moderate evidence that prevalence differs from 15%.\n\n:::\n:::\n\n## Hypothesis tests for $p$\n\n::: columns\n::: {.column width=\"45%\"}\nTo test if prevalence is below 14%: \n$$\n\\begin{cases}\nH_0: &p = 0.14 \\\\ \nH_A: &p < 0.14\n\\end{cases}\n$$\n\nWe can use the test statistic:\n\n$$\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n= \\frac{\\hat{p} - 0.14}{\\sqrt{\\frac{0.14 (0.86)}{500}}}\n$$\n\nUnder $H_0$, the sampling distribution of $Z$ is approximated by a normal model, provided:\n\n- $np_0 \\geq 10$\n- $n(1 - p_0) \\geq 10$\n:::\n\n::: {.column width=\"55%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-206-1.png)\n:::\n:::\n\n\n\nHere $p = P(Z < 1.676) = 0.0469$, so:\n\n> the data provide moderate evidence that prevalence is less than 14%.\n\n\n\n:::\n:::\n\n## Inference for a proportion in R\n\n::: columns\n::: {.column width=\"45%\"}\nInference using the normal model in R:\n\n1.  Construct a table of the frequency distribution\n2.  Pass the table to `prop.test()`\n\nRemarks about output:\n\n-   `X-squared` gives $Z^2$\n-   `correct = F` performs the test without continuity correction\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5-7\"}\n# variable of interest\ndia <- nhanes$diabetes\n\n# pass table to prop.test\ntable(dia) |> \n  prop.test(p = 0.1, alternative = 'two.sided',\n            conf.level = 0.95, correct = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 1.0889, df = 1, p-value = 0.2967\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.0890369 0.1448491\nsample estimates:\n    p \n0.114 \n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n> The data provide no evidence that diabetes prevalence among U.S. adults differs from 10%. With 95% confidence, prevalence is estimated to be between 8.90% and 14.48%, with a point estimate of 11.4% (SE = 1.42%).\n\n## `correct = F`?\n\n> A \"continuity correction\" reduces approximation error for the normal model.\n\n::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\ntable(dia) |> \n  prop.test(p = 0.1, \n            alternative = 'two.sided',\n            conf.level = 0.95, \n            correct = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t1-sample proportions test without continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 1.0889, df = 1, p-value = 0.2967\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.0890369 0.1448491\nsample estimates:\n    p \n0.114 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\ntable(dia) |> \n  prop.test(p = 0.1, \n            alternative = 'two.sided',\n            conf.level = 0.95, \n            correct = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  table(dia), null probability 0.1\nX-squared = 0.93889, df = 1, p-value = 0.3326\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.08814952 0.14594579\nsample estimates:\n    p \n0.114 \n```\n\n\n:::\n:::\n\n\n:::\n:::\n\nOmitting the `correct` argument implements the correction by default.\n\n## Exact inference for a proportion\n\nThe test can also be performed using the *exact* sampling distribution obtained from a binomial probability model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinom.test(x = 57, n = 500, p = 0.1, alternative = 'two.sided')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tExact binomial test\n\ndata:  57 and 500\nnumber of successes = 57, number of trials = 500, p-value = 0.2964\nalternative hypothesis: true probability of success is not equal to 0.1\n95 percent confidence interval:\n 0.0874949 0.1451685\nsample estimates:\nprobability of success \n                 0.114 \n```\n\n\n:::\n:::\n\n\n\nInputs:\n\n-   `x` gives the number of occurrences of the category of interest\n-   `n` gives the sample size\n\n\n\n<!-- ## Your turn: sleep trouble -->\n\n<!-- > Use the NHANES data to estimate the proportion of U.S. adults reporting sleep trouble. Test whether at least 20% of U.S. adults report sleep trouble. -->\n\n<!-- ::: columns -->\n<!-- ::: column -->\n<!-- 1.  Compute point estimate and standard error -->\n\n<!-- 2.  Perform the hypothesis test -->\n\n<!--     -   write the hypotheses -->\n<!--     -   calculate the value of the test statistic -->\n<!--     -   record the $p$-value -->\n\n<!-- 3.  Construct an interval estimate -->\n<!-- ::: -->\n\n<!-- ::: column -->\n<!-- ```{r, echo = T} -->\n<!-- # to get you started -->\n<!-- trouble <- factor(nhanes$SleepTrouble,  -->\n<!--                   levels = c('Yes', 'No')) -->\n<!-- trouble.tbl <- table(trouble) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- trouble.tbl |>  -->\n<!--   pander(caption = 'Having sleep trouble?') -->\n<!-- ``` -->\n<!-- ::: -->\n<!-- ::: -->\n\n## Two-way tables\n\n> Two-way tables or \"contingency\" tables compare two categorical variables.\n\n::: columns\n::: column\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------------------------\n   &nbsp;      Cold   NoCold    n  \n------------- ------ -------- -----\n **Placebo**   335      76     411 \n\n  **VitC**     302     105     407 \n-----------------------------------\n\nTable: Vitamin C experiment\n\n\n:::\n:::\n\n\n\n:::\n\n::: column\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-212-1.png)\n:::\n:::\n\n\n\n:::\n:::\n\n-   vitamin C and placebo treatments were randomly allocated to 818 volunteers\n-   volunteers took treatments daily for a cold season\n-   study recorded how many volunteers came down with a cold\n\n*Is vitamin C effective at preventing common cold?*\n\n## Inference for two proportions\n\nWe can first consider inferences on the difference in proportions:\n\n$$\\delta = p_\\text{placebo} - p_\\text{vitC}$$\n\n<!-- $$\\begin{cases}H_0: p_\\text{placebo} - p_\\text{vitamin C} \\leq 0 \\\\ H_A: p_\\text{placebo} - p_\\text{vitamin C} > 0 \\end{cases}$$ -->\n\nInferences are based on groupwise estimates:\n\n-   point estimate: $\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC}$\n\n-   standard error: $\\sqrt{SE^2(\\hat{p}_\\text{placebo}) + SE^2(\\hat{p}_\\text{vitC})}$\n\nWhen both groups meet the conditions for inference for one proportion, the statistic\n\n$$\nZ = \\frac{\\hat{p}_1 - \\hat{p}_2 - \\delta}{SE(\\hat{p}_1 - \\hat{p}_2)}\n$$\nhas a sampling distribution well-approximated by a normal model.\n\n## Confidence interval for the difference\n\n\n$$\n\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC} \\pm c\\times SE(\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC})\n$$\nFor a $(1 - \\alpha)\\times 100\\%$ confidence interval the critical value $c$ is chosen to be the $\\left(1 - \\frac{\\alpha}{2}\\right)$ quantile of the normal model.\n\n-   point estimate: $\\hat{p}_\\text{placebo} - \\hat{p}_\\text{vitC} = 0.0731$\n\n-   standard error: $\\sqrt{SE^2(\\hat{p}_\\text{placebo}) + SE^2(\\hat{p}_\\text{vitC})} = 0.0289$\n\n- critical value for 95% interval: `qnorm(1 - 0.05/2) = 1.959964`\n\n95% confidence interval: (0.0164, 0.1298)\n\n> With 95% confidence, the prevalence of common cold is estimated to be between 1.64% and 12.98% lower among adults who take daily vitamin C supplements.\n\n## Tests for a difference in proportions\n\n::: columns\n::: column\nWe can also test whether vitamin C prevents common cold:\n\n$$\n\\begin{cases} \nH_0: &p_\\text{placebo} - p_\\text{vitC} = 0\\\\ \nH_A: &p_\\text{placebo} - p_\\text{vitC} > 0 \n\\end{cases}\n$$\n\nHypothesis tests use the test statistic:\n\n$$Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}$$\n\nWith a slightly different SE where: \n$$\\hat{p} = \\frac{n_1\\hat{p}_1 + n_2\\hat{p}_2}{n_1 + n_2}$$\n:::\n\n::: column\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-213-1.png)\n:::\n:::\n\n\n\nHere $p = P(Z > 2.517) = 0.0059$, so:\n\n> the data provide strong evidence that vitamin C prevents common cold.\n\n:::\n:::\n\n\n## Inference in R\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: columns\n::: column\nThree steps:\n\n1.  Construct a table of the frequency distribution by group\n\n    -   outcomes should be columns\n    -   groups should be rows\n\n2.  Pass to `prop.test()`\n\nThe alternative reads the same way as in `t.test`.\n:::\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6-8\"}\n# variables of interest\ntreatment <- vitamin$treatment\noutcome <- vitamin$outcome\n\n# pass table to prop.test\ntable(treatment, outcome) |>\n  prop.test(alternative = 'greater', \n            correct = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t2-sample test for equality of proportions without continuity correction\n\ndata:  table(treatment, outcome)\nX-squared = 6.3366, df = 1, p-value = 0.005914\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.02548153 1.00000000\nsample estimates:\n   prop 1    prop 2 \n0.8150852 0.7420147 \n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n> The data provide strong evidence that vitamin C prevents common cold (Z = 2.517, *p* = 0.0059). With 95% confidence, the reduction in probability is estimated to be at least 0.0255, with a point estimate of 0.0731 (SE = 0.0289).\n\n\n## Sampling and two-way tables\n\n::: {.columns}\n\n::: {.column width=\"55%\"}\nConsider this case-control study:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------------------------------\n   &nbsp;      Smokers   NonSmokers   n  \n------------- --------- ------------ ----\n **Cancer**      83          3        86 \n\n **Control**     72          14       86 \n-----------------------------------------\n\n\n:::\n:::\n\n\nThis is an example of *outcome-based* sampling:\n\n- 86 lung cancer patients and 86 controls \n- can't estimate cancer prevalence\n\n::: \n\n::: {.column width=\"45%\"}\nA different approach to inference is needed to analyze this data. Next time:\n\n- tests of association in two-way tables\n- inference for risk and odds ratios\n\n:::\n\n:::\n\n\n<!-- ## Your turn: two cases -->\n\n<!-- ```{r} -->\n<!-- obesity <- case1801 |> column_to_rownames('Obesity') |> as.matrix() |> as.table() -->\n<!-- smoking <- case1803 |> column_to_rownames('Smoking') |> as.matrix() |> as.table() |> t() -->\n\n<!-- save(list = c('obesity', 'smoking', 'cold'), file = 'data/prop-cases.RData') -->\n<!-- ``` -->\n\n<!-- ::: columns -->\n<!-- ::: column -->\n<!-- Researchers categorized 3,112 individuals in American Samoa according to whether they were obese and recorded whether subjects had cardiovascular disease (CVD). -->\n\n<!-- ```{r} -->\n<!-- obesity |> pander() -->\n<!-- ``` -->\n\n<!-- *Test for a difference in disease rates between obese and non-obese populations, and produce an interval estimate for the difference in proportions.* -->\n<!-- ::: -->\n\n<!-- ::: column -->\n<!-- Researchers identified 86 lung cancer patients and 86 controls (without lung cancer), and categorized them according to whether they were smokers or non-smokers. -->\n\n<!-- ```{r} -->\n<!-- smoking |> pander() -->\n<!-- ``` -->\n\n<!-- *Test for a difference in the proportion of smokers among cancer patients compared with controls, and produce an interval estimate for the difference.* -->\n<!-- ::: -->\n<!-- ::: -->\n\n# Lecture 14: Tests of association\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. [lecture] tests of association in contingency tables\n2. [lab] $\\chi^2$ tests and residual analysis in R\n\n## Do asthma rates differ by sex?\n\n::: {.columns}\n\n::: {.column width=\"45%\"}\nFrom a subsample of NHANES data:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------------------\n   &nbsp;     asthma   no asthma \n------------ -------- -----------\n **female**     49        781    \n\n  **male**      30        769    \n---------------------------------\n\n\n:::\n:::\n\n\n\nInference for the difference in proportions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(asthma$sex, asthma$asthma) |>\n  prop.test(alternative = 'two.sided', \n            conf.level = 0.95,\n            correct = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t2-sample test for equality of proportions without continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 4.0741, df = 1, p-value = 0.04355\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.0007323913 0.0422460305\nsample estimates:\n    prop 1     prop 2 \n0.05903614 0.03754693 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"55%\"}\nInterpretation:\n\n> There is moderate evidence that asthma prevalence differs between men and women (*Z* = 2.108, *p* = 0.0436). With 95% confidence, the difference in prevalence (F - M) is estimated to be between 0.07% and 4.22%, with a point estimate of 2.15%.\n\nThis inference relies on a specific measure of association (difference in prevalence) that we can't always estimate.\n\n*Could we test for association between sex and asthma without relying on a specific measure?*\n:::\n\n:::\n\n## Association and independence\n\n::: {.columns}\n\n::: {.column width=\"59%\"}\nConsider the hypotheses:\n\n$$\n\\begin{cases}\nH_0: &\\text{asthma}\\perp\\text{sex} \\; &(\\text{independence})\\\\\nH_A: &\\neg(\\text{asthma}\\perp\\text{sex}) \\; &(\\text{association})\n\\end{cases}\n$$\n\nConsider also the proportions:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------------------------\n   &nbsp;     asthma    no asthma      total    \n------------ --------- ------------ ------------\n **female**   0.03008     0.4794       0.5095   \n\n  **male**    0.01842   **0.4721**   **0.4905** \n\n **total**    0.0485    **0.9515**       1      \n------------------------------------------------\n\n\n:::\n:::\n\n\n\n- $p_{ij}$: proportion of observations in cell $ij$ \n- $p_i$, $p_j$: marginal proportions in row $i$ or column $j$\n\n\n:::\n\n::: {.column width=\"41%\"}\n\nIf sex and asthma are independent:\n\n$$\np_{ij} \\approx p_i \\times p_j\n$$\n\nFor example, we'd expect: \n\n$$\n0.4721 \\approx 0.4905 \\times 0.9515\n$$\n\nIn other words:\n\n- 49% of respondents are men\n- 95% of respondents don't have asthma\n- so roughly 49% of 95% would be men without asthma\n\n\n:::\n\n:::\n\n## Basis for a test: expected counts\n\nExpected proportions translate directly to expected counts:\n$$p_{ij} = p_i \\times p_j \n\\quad\\Longleftrightarrow\\quad n_{ij} = \\frac{n_{i\\cdot} \\times n_{\\cdot j}}{n}$$\n\n::: {.columns}\n\n::: {.column width=\"40%\"}\nActual counts:\n\n&nbsp; | O1 | O2 | total\n---+---+---+---\n**G1** | $n_{11}$ | $n_{12}$ | $\\color{red}{n_{1\\cdot}}$\n**G2** | $n_{21}$ | $n_{22}$ | $\\color{orange}{n_{2\\cdot}}$\n**total** | $\\color{blue}{n_{\\cdot 1}}$ | $\\color{green}{n_{\\cdot 2}}$ | $n$\n:::\n\n::: {.column width=\"60%\"}\nExpected counts under independence:\n\n&nbsp; | O1 | O2 | total\n---+---+---+---\n**G1** | $\\hat{n}_{11} = \\frac{\\color{red}{n_{1\\cdot}} \\color{black}{\\times} \\color{blue}{n_{\\cdot 1}}}{n}$ | $\\hat{n}_{12} = \\frac{\\color{red}{n_{1\\cdot}} \\color{black}{\\times} \\color{green}{n_{\\cdot 2}}}{n}$ | $\\color{red}{n_{1\\cdot}}$\n**G2** | $\\hat{n}_{21} = \\frac{\\color{orange}{n_{2\\cdot}} \\color{black}{\\times} \\color{blue}{n_{\\cdot 1}}}{n}$ | $\\hat{n}_{22} = \\frac{\\color{orange}{n_{2\\cdot}} \\color{black}{\\times} \\color{green}{n_{\\cdot 2}}}{n}$ | $\\color{orange}{n_{2\\cdot}}$\n**total** | $\\color{blue}{n_{\\cdot 1}}$ | $\\color{green}{n_{\\cdot 2}}$ | $n$\n\n:::\n\n:::\nIdea for a test of independence: \n\n- reject $H_0$ if actual and expected counts differ enough across the table\n- *i.e.*, reject $H_0$ when $n_{ij} - \\hat{n}_{ij}$ is large across $i, j$\n\n## Computing expected counts\n\n::: {.columns}\n\n::: {.column width=\"40%\"}\nActual counts:\n\n&nbsp; | O1 | O2 | total\n---+---+---+---\n**G1** | $n_{11}$ | $n_{12}$ | $\\color{red}{n_{1\\cdot}}$\n**G2** | $n_{21}$ | $n_{22}$ | $\\color{orange}{n_{2\\cdot}}$\n**total** | $\\color{blue}{n_{\\cdot 1}}$ | $\\color{green}{n_{\\cdot 2}}$ | $n$\n:::\n\n::: {.column width=\"60%\"}\nExpected counts under independence:\n\n&nbsp; | O1 | O2 | total\n---+---+---+---\n**G1** | $\\hat{n}_{11} = \\frac{\\color{red}{n_{1\\cdot}} \\color{black}{\\times} \\color{blue}{n_{\\cdot 1}}}{n}$ | $\\hat{n}_{12} = \\frac{\\color{red}{n_{1\\cdot}} \\color{black}{\\times} \\color{green}{n_{\\cdot 2}}}{n}$ | $\\color{red}{n_{1\\cdot}}$\n**G2** | $\\hat{n}_{21} = \\frac{\\color{orange}{n_{2\\cdot}} \\color{black}{\\times} \\color{blue}{n_{\\cdot 1}}}{n}$ | $\\hat{n}_{22} = \\frac{\\color{orange}{n_{2\\cdot}} \\color{black}{\\times} \\color{green}{n_{\\cdot 2}}}{n}$ | $\\color{orange}{n_{2\\cdot}}$\n**total** | $\\color{blue}{n_{\\cdot 1}}$ | $\\color{green}{n_{\\cdot 2}}$ | $n$\n\n:::\n\n:::\n\nFor the asthma example:\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------------------------------\n   &nbsp;     asthma   no asthma   total \n------------ -------- ----------- -------\n **female**     49        781       830  \n\n  **male**      30        769       799  \n\n **total**      79       1550      1629  \n-----------------------------------------\n\nTable: Actual\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n-----------------------------------------\n   &nbsp;     asthma   no asthma   total \n------------ -------- ----------- -------\n **female**   40.25      789.7      830  \n\n  **male**    38.75      760.3      799  \n\n **total**      79       1550      1629  \n-----------------------------------------\n\nTable: Expected\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n\n## The chi-square ($\\chi^2$) statistic\n\nA measure of the amount by which actual counts differ from expected counts under independence is the **chi** (pronounced /ˈkaɪ ) **square statistic**:\n\n$$\n\\chi^2 = \\sum_{ij} \\frac{\\left(n_{ij} - \\hat{n}_{ij}\\right)^2}{\\hat{n}_{ij}} \n\\qquad\\left(\\sum_\\text{all cells} \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\\right)\n$$\n\n::: {.columns}\n\n::: {.column}\nCell-wise calculation:\n\n------------------------------------------------------------------------------\n   &nbsp;     asthma                           no asthma \n------------ -------------------------------- --------------------------------\n **female**   $\\frac{(49 - 40.25)^2}{40.25}$  $\\frac{(781 - 789.7)^2}{789.7}$  \n\n  **male**    $\\frac{(30 - 38.75)^2}{38.75}$  $\\frac{(769 - 760.3)^2}{760.3}$   \n------------------------------------------------------------------------------\n:::\n\n::: {.column}\nResult:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------------------\n   &nbsp;     asthma   no asthma \n------------ -------- -----------\n **female**   1.901     0.09691  \n\n  **male**    1.975     0.1007   \n---------------------------------\n\n\n:::\n:::\n\n\n:::\n\n:::\n\nChi-square statistic: \n$$\n\\chi^2 \n= 1.9014 + 1.9751 + 0.0969 + 0.1007 \n= 4.0741\n$$\n\n## Sampling distribution for $\\chi^2$\n\n::: {.columns}\n\n::: {.column}\nUnder $H_0$, the $\\chi^2$ statistic has a sampling distribution that can be approximated by a $\\chi^2_1$ model.\n\n- subscript indicates degrees of freedom parameter\n\nThe model assumes no expected counts are too small.\n\n- rule of thumb: at least 10 ($\\hat{n}_{ij} \\geq 10$)\n- consequences: if $\\hat{n}_{ij}$ are too small, the statistic is inflated relative to the model, leading to a higher type I error rate\n:::\n\n::: {.column}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-223-1.png)\n:::\n:::\n\n\n\n:::\n\n:::\n\n\n## Computing $p$ values\n\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\n\n$$\n\\begin{cases}\nH_0: &\\text{asthma}\\perp\\text{sex} \\; &(\\text{independence})\\\\\nH_A: &\\neg(\\text{asthma}\\perp\\text{sex}) \\; &(\\text{association})\n\\end{cases}\n$$\n\nTo determine the test outcome, find the $p$-value: \n\n$$\nP(\\chi^2_1 > \\chi^2_\\text{obs}) = P(\\chi^2_1 > 4.074) = 0.0435\n$$\n\nSo if asthma and sex were independent, only 4% of random samples would produce a table that deviates from expected counts by more than what we observed.\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-224-1.png)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npchisq(4.074, df = 1, lower.tail = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04354804\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Implementation in R\n\nThe R implementation is `chisq.test(...)`.\n\n- input: contingency table\n- no constraints on row/column arrangement\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# construct table and pass to chisq.test\ntable(asthma$sex, asthma$asthma) |> \n  chisq.test(correct = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 4.0741, df = 1, p-value = 0.04355\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n> The data provide moderate evidence that asthma prevalence is associated with sex ($\\chi^2$ = 4.074 on 1 degree of freedom, *p* = 0.0435).\n:::\n\n:::\n\n## Residuals in $\\chi^2$ tests\n\n::: {.columns}\n\n::: {.column}\nThe **residual** for each cell is defined as a standardized difference between the observed and expected count:\n\n$$r_{ij} = \\frac{n_{ij} - \\hat{n}_{ij}}{\\sqrt{\\hat{n}_{ij}}} $$\n\nExamining residuals can indicate the source(s) of an inferred association. \n\n- $r_{ij} > 0$: observation exceeds expectation\n- $r_{ij} < 0$: observation is under expectation\n- large $|r_{ij}|$ explain the association\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# store test result\nrslt <- chisq.test(asthma.tbl, correct = F)\n\n# examine residuals\nrslt$residuals\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------------------\n   &nbsp;     asthma   no asthma \n------------ -------- -----------\n **female**   1.379     -0.3113  \n\n  **male**    -1.405    0.3173   \n---------------------------------\n\n\n:::\n:::\n\n\n\nLook for the largest residuals:\n\n> Asthma prevalence is higher-than-expected among women and lower-than-expected among men.\n:::\n\n:::\n\n## Continuity correction\n\nThe $\\chi^2$ test for independence is typically applied with Yates' continuity correction.\n\n::: {.columns}\n\n::: {.column}\nThis consists in using a modified version of the test statistic:\n\n$$\n\\chi^2_\\text{Yates} = \\sum_{ij} \\frac{\\left(|n_{ij} - \\hat{n}_{ij}| - 0.5\\right)^2}{\\hat{n}_{ij}}\n$$\n\n- every other detail of the test is the same\n- doesn't change expected counts\n- residuals are still computed as $\\frac{n_{ij} - \\hat{n}_{ij}}{\\sqrt{\\hat{n}_{ij}}}$\n:::\n\n::: {.column}\nImplementation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# construct table and pass to chisq.test\ntable(asthma$sex, asthma$asthma) |> \n  chisq.test(correct = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 3.6217, df = 1, p-value = 0.05703\n```\n\n\n:::\n:::\n\n\n\nNote the larger $p$-value -- this changes the conclusion!\n:::\n\n:::\n\n## Spot any similarities?\n\nCompare the $\\chi^2$ test with inference on the difference in proportions.\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# chi square test\ntable(asthma$sex, asthma$asthma) |> \n  chisq.test(correct = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 3.6217, df = 1, p-value = 0.05703\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# difference in proportions\ntable(asthma$sex, asthma$asthma) |> \n  prop.test(alternative = 'two.sided', \n            conf.level = 0.95, \n            correct = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t2-sample test for equality of proportions with continuity correction\n\ndata:  table(asthma$sex, asthma$asthma)\nX-squared = 3.6217, df = 1, p-value = 0.05703\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.0004958005  0.0434742223\nsample estimates:\n    prop 1     prop 2 \n0.05903614 0.03754693 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n- the tests are identical!\n\n- the difference in proportions $\\hat{p}_F - \\hat{p}_M$ is one specific measure of association\n\n- next time we'll learn about other measures, which also have the same inference attached\n\n:::\n\n:::\n\n\n## Extending to $I \\times J$ tables\n\n::: {.columns}\n\n::: {.column}\nFAMuSS data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------------------\n     &nbsp;       CC    CT    TT    total \n---------------- ----- ----- ----- -------\n **African Am**   16     6     5     27   \n\n   **Asian**      21    18    16     55   \n\n **Caucasian**    125   216   126    467  \n\n  **Hispanic**     4    10     9     23   \n\n   **Other**       7    11     5     23   \n\n   **total**      173   261   161    595  \n------------------------------------------\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\nExpected counts:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------------------------\n     &nbsp;        CC      CT      TT     total \n---------------- ------- ------- ------- -------\n **African Am**   7.85    11.84   7.31     27   \n\n   **Asian**      15.99   24.13   14.88    55   \n\n **Caucasian**    135.8   204.8   126.4    467  \n\n  **Hispanic**    6.69    10.09   6.22     23   \n\n   **Other**      6.69    10.09   6.22     23   \n\n   **total**       173     261     161     595  \n------------------------------------------------\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n- expected counts and chi-square statistic are calculated exactly the same way\n- degrees of freedom are now $(I - 1)\\times(J - 1)$\n- appropriate provided all $\\hat{n}_{ij} > 1$ and most (~80%) $\\hat{n}_{ij} \\geq 5$\n\n## Extending to $I\\times J$ tables\n\nIn detail:\n\n---------------------------------------------------------------------------------------------------------------------\n     &nbsp;       CC                                CT                                TT  \n---------------- --------------------------------  --------------------------------  --------------------------------\n **African Am**   $\\frac{(16 - 7.85)^2}{7.85}$     $\\frac{(6 - 11.84)^2}{11.84}$     $\\frac{(5 - 7.306)^2}{7.306}$  \n\n   **Asian**      $\\frac{(21 - 15.99)^2}{15.99}$    $\\frac{(18 - 24.13)^2}{24.13}$    $\\frac{(16 -14.88)^2}{14.88}$  \n\n **Caucasian**    $\\frac{(125 - 135.8)^2}{135.8}$   $\\frac{(216 - 204.9)^2}{204.9}$   $\\frac{(126 - 126.4)^2}{126.4}$ \n\n  **Hispanic**     $\\frac{(4 - 6.687)^2}{6.687}$    $\\frac{(10 - 10.09)^2}{10.09}$     $\\frac{(9 - 6.224)^2}{6.224}$  \n\n   **Other**       $\\frac{(7 - 6.687)^2}{6.687}$    $\\frac{(11 - 10.09)^2}{10.09}$     $\\frac{(5 - 6.224)^2}{6.224}$  \n---------------------------------------------------------------------------------------------------------------------\n\nThen:\n\n$$\\begin{cases} &\\chi^2 = \\sum \\text{all cells above} = 19.4 \\\\\n&P(\\chi^2_{8} > 19.4) = 0.01286 \\end{cases}\n\\quad\\Longrightarrow\\quad \\text{reject hypothesis of no association}$$\n\n## Inference for $I\\times J$ tables in R\n\n::: {.columns}\n\n::: {.column}\nThe implementation is the same as for a $2\\times 2$ table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# construct table and pass to chisq.test\ntable(famuss$race, famuss$actn3.r577x) |>\n  chisq.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(famuss$race, famuss$actn3.r577x)\nX-squared = 19.4, df = 8, p-value = 0.01286\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n> The data provide evidence of an association between race and genotype ($\\chi^2$ = 19.4 on 8 degrees of freedom, *p = 0.01286*).\n\n:::\n\n:::\n\n*Which genotype/race combinations are contributing most to this inferred association?*\n\n## Residual analysis\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# store result of test; display residuals\nrslt <- chisq.test(tbl)\nrslt$residuals\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------------------------\n     &nbsp;         CC         CT         TT    \n---------------- --------- ---------- ----------\n **African Am**    2.909     -1.698    -0.8531  \n\n   **Asian**       1.252     -1.247     0.2897  \n\n **Caucasian**    -0.9254    0.7789    -0.03244 \n\n  **Hispanic**    -1.039    -0.02804    1.113   \n\n   **Other**      0.1209     0.2868    -0.4905  \n------------------------------------------------\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column}\nAgain look for the largest absolute residuals to explain inferred association.\n\n> African American and Asian populations have higher CC and lower CT frequencies than would be expected if genotype were independent of race.\n:::\n\n:::\n\n# Lecture 15: Measures of association\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today's agenda\n\n1. [lecture] relative risk and odds ratios\n2. [lab] using `epitools` to estimate risk and odds ratios in R\n3. [discussion] final project guidelines\n\n## From last time: smoking\n\n::: {.columns}\n\n::: {.column}\nResearchers sampled 86 lung cancer patients (cases) and 86 healthy individuals (controls) and recorded smoking status:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------------\n   &nbsp;      Smokers   NonSmokers \n------------- --------- ------------\n **cancer**      83          3      \n\n **control**     72          14     \n------------------------------------\n\n\n:::\n:::\n\n\n\nAt the 5% level, association is significant:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmoking.tbl <- table(smoking$group, \n                     smoking$smoking) \nsmoking.tbl |> chisq.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  smoking.tbl\nX-squared = 6.5275, df = 1, p-value = 0.01062\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column}\nBut there is a difficulty for measuring the association:\n\n- data are really *two* independent samples\n- so can't estimate cancer prevalence\n\n:::\n\n:::\n\n## Case-control studies\n\n> Outcome-based sampling limits which proportions are estimable\n\n![](img/casecontrol.png)\n\n- cases and controls are two independent samples\n- exposure prevalence is estimable (within case and control populations)\n- case prevalence is **not** estimable\n\n## Smoking example\n\n::: {.columns}\n\n::: {.column}\nSo due to study design, the only estimable difference in proportions is:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmoking.prop.test <- smoking.tbl |> prop.test()\nsmoking.prop.test$conf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.029149 0.226665\nattr(,\"conf.level\")\n[1] 0.95\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n\n> With 95% confidence, the proportion of smokers among cancer patients is estimated to be betwteen 2.9 and 22.7 percentage points higher than among controls.\n\n:::\n\n:::\n\nThis makes for an awkward conclusion:\n\n- we really want to know how smoking affects cancer risk\n- ...not how cancer affects smoking risk\n\nSo for this kind of study, we need a different measure of association: ***odds ratios***.\n\n## Odds \n\n> The odds of an outcome measure its relative likelihood\n\nIf $p$ is the true cancer prevalence (a population proportion), then the **odds** of cancer are defined as the ratio:\n$$\n\\text{odds} = \\frac{p}{1 - p}\n$$\nThe odds represent the factor by which cancer is more likely than not, *e.g.*:\n\n- $\\text{odds} = 2$ indicates the outcome (cancer) is twice as likely as not\n- $\\text{odds} = 1/2$ indicates the outcome (cancer) is half as likely as not\n\n## Odds ratios\n\n> An odds ratio compares the relative likelihood of an outcome between two groups\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\nIf $a, b, c, d$ are population proportions:\n\n$\\;$ | outcome 1 (O1) | outcome 2 (O2)\n---|:---:|:---:\ngroup 1 (G1) | [a]{style=\"color:red\"} | [b]{style=\"color:blue\"} \ngroup 2 (G2) | [c]{style=\"color:orange\"} | [d]{style=\"color:purple\"}\n\n\n- the odds of outcome 1 in group 1 is $\\frac{\\textcolor{red}{a}}{\\textcolor{blue}{b}}$\n- the odds of outcome 1 in group 2 is $\\frac{\\textcolor{orange}{c}}{\\textcolor{purple}{d}}$\n\n\n:::\n\n::: {.column width=\"40%\"}\nThe **odds ratio** of outcome 1 comparing group 1 with group 2 is:\n\n$$\n\\frac{\\text{odds}_{G1}(O1)}{\\text{odds}_{G2}(O1)}\n= \\frac{\\textcolor{red}{a}/\\textcolor{blue}{b}}{\\textcolor{orange}{c}/\\textcolor{purple}{d}}\n= \\frac{\\textcolor{red}{a}\\textcolor{purple}{d}}{\\textcolor{blue}{b}\\textcolor{orange}{c}}\n$$\nA surprising algebraic fact is that:\n\n$$\n\\frac{\\text{odds}_{G1}(O1)}{\\text{odds}_{G2}(O1)} \n=\\frac{\\text{odds}_{O1}(G1)}{\\text{odds}_{O2}(G1)}\n$$\n\n:::\n\n:::\n\n*This means that the ratio of odds of cancer between smokers and nonsmokers can be estimated from the ratio of odds of smoking between cases and controls.*\n\n## Estimating odds ratios\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\nFor notation let:\n\n- $\\hat{p}_\\text{case}$ : proportion of smokers among cases\n- $\\hat{p}_\\text{control}$ : proportion of smokers among controls\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------------\n   &nbsp;      Smokers   NonSmokers \n------------- --------- ------------\n **cancer**      83          3      \n\n **control**     72          14     \n------------------------------------\n\n\n:::\n:::\n\n\n:::\n\n:::\n\nAn estimate of ratio of odds of cancer among smokers compared with nonsmokers ($\\omega$) is the estimated ratio of odds of smoking among cancer patients compared with controls:\n$$\n\\hat{\\omega} \n=\\left(\\frac{\\hat{p}_\\text{case}}{1 - \\hat{p}_\\text{case}}\\right)\\Bigg/\\left(\\frac{\\hat{p}_\\text{control}}{1 - \\hat{p}_\\text{control}}\\right)\n= \\frac{83/3}{72/14} = 5.38\n$$\n\n> It is estimated that the odds of lung cancer are 5.38 times greater for smokers compared with nonsmokers.\n\n## Confidence intervals for odds ratios\n\nThe sampling distribution of the log odds ratio can be approximated by a normal model.\n\n$$\n\\log\\left(\\hat{\\omega}\\right) \\pm c \\times SE\\left(\\log\\left(\\hat{\\omega}\\right)\\right) \n\\quad\\text{where}\\quad\nSE\\left(\\log\\left(\\hat{\\omega}\\right)\\right) = \\sqrt{\\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}}\n$$\n\n\n::: {.columns}\n\n::: {.column}\nThe `oddsratio(...)` function in the `epitools` package will compute and back-transform the interval for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noddsratio(smoking.tbl, rev = 'both', \n          method = 'wald', correction = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n         odds ratio with 95% C.I.\n          estimate    lower    upper\n  control  1.00000       NA       NA\n  cancer   5.37963 1.486376 19.47045\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column}\n- critical value $c$ from the normal model\n- exponentiate to obtain an interval for $\\omega$\n\n> With 95% confidence, the odds of lung cancer are estimated to be between 1.49 and 19.47 times greater for smokers compared with nonsmokers.\n:::\n\n:::\n\n## Implementation details\n\n::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\noddsratio(smoking.tbl, rev = 'both',\n          method = 'wald', correction = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n$data\n         \n          NonSmokers Smokers Total\n  control         14      72    86\n  cancer           3      83    86\n  Total           17     155   172\n\n$measure\n         odds ratio with 95% C.I.\n          estimate    lower    upper\n  control  1.00000       NA       NA\n  cancer   5.37963 1.486376 19.47045\n\n$p.value\n         two-sided\n           midp.exact fisher.exact chi.square\n  control          NA           NA         NA\n  cancer  0.005116319  0.008822805 0.01062183\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"55%\"}\n\n`oddsratio` is picky about data inputs:\n\n- outcome of interest should be *second* column\n- group of interest should be *second* row\n- the `rev` argument will reverse orders\n    \n    + `rev = neither` keeps original orientation (default)\n    + `rev = rows` reverses order of rows\n    + `rev = columns` reverses order of columns\n    + `rev = both` reverses both\n\n:::\n\n:::\n\n## Interpretation\n\n::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\noddsratio(smoking.tbl, rev = 'both',\n          method = 'wald', correction = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n$data\n         \n          NonSmokers Smokers Total\n  control         14      72    86\n  cancer           3      83    86\n  Total           17     155   172\n\n$measure\n         odds ratio with 95% C.I.\n          estimate    lower    upper\n  control  1.00000       NA       NA\n  cancer   5.37963 1.486376 19.47045\n\n$p.value\n         two-sided\n           midp.exact fisher.exact chi.square\n  control          NA           NA         NA\n  cancer  0.005116319  0.008822805 0.01062183\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"55%\"}\nFirst report the test result, then the measure of association:\n\n> The data provide evidence of an association between smoking and lung cancer ($\\chi^2$ = 6.53 on 1 degree of freedom, *p* = 0.1062). With 95% confidence, the odds of cancer are estimated to be between 1.49 amd 19.47 times greater among smokers compared with nonsmokers, with a point estimate of 5.38.\n\n\n:::\n\n:::\n\nComments:\n\n- the `chi.square` $p$-value is from the test of association/independence\n- if assumptions aren't met, can use the `fisher.exact` instead (see V&H 8.3.5)\n\n## Asthma data\n\n::: {.columns}\n\n::: {.column}\nConsider estimating the difference in proportions:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------------------\n   &nbsp;     asthma   no asthma \n------------ -------- -----------\n **female**     49        781    \n\n  **male**      30        769    \n---------------------------------\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nasthma.tbl <- table(asthma$sex, asthma$asthma)\nprop.test(asthma.tbl, conf.level = 0.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t2-sample test for equality of proportions with continuity correction\n\ndata:  asthma.tbl\nX-squared = 3.6217, df = 1, p-value = 0.05703\nalternative hypothesis: two.sided\n90 percent confidence interval:\n 0.002841347 0.040137075\nsample estimates:\n    prop 1     prop 2 \n0.05903614 0.03754693 \n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n> With 90% confidence, asthma prevalence is estimated to be between 0.28 and 4.01 percentage points higher among women than among men.\n\nIs a difference of up to 4 percentage points practically meaningful? Well, it depends:\n\n- yes if prevalence is very low\n- no if prevalence is very high\n\n\n## Relative risk\n\nIf $p_F, p_M$ are the (population) proportions of women and men with asthma, then the **relative risk** of asthma among women compared with men is defined as:\n\n$$\nRR = \\frac{p_F}{p_M}\n\\qquad\n\\left(\\frac{\\text{risk among women}}{\\text{risk among men}}\\right)\n$$\n\nAn estimate of the relative risk is simply the ratio of estimated proportions. For the asthma data, an estimate is:\n$$\n\\widehat{RR} = \\frac{\\hat{p}_F}{\\hat{p}_M} = \\frac{0.059}{0.038} = 1.57 \n$$\n\n> It is estimated that the risk of asthma among women is 1.57 times greater than among men.\n\n## Confidence intervals for relative risk\n\nA normal model can be used to approximate the sampling distribution of $\\log(RR)$ and construct a confidence interval. If $\\hat{p}_1$ and $\\hat{p}_2$ are the two estimated proportions:\n\n$$\\log\\left(\\widehat{RR}\\right) \\pm c \\times SE\\left(\\log\\left(\\widehat{RR}\\right)\\right)\n\\quad\\text{where}\\quad SE\\left(\\log\\left(\\widehat{RR}\\right)\\right) = \\sqrt{\\frac{1 - p_1}{p_1n_1} + \\frac{1 - p_2}{p_2n_2}}$$\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\nThe `riskratio(...)` function from the `epitools` package will compute and back-transform the interval for you:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nriskratio(asthma.tbl, \n          rev = 'both', conf.level = 0.9,\n          method = 'wald', correction = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n        risk ratio with 90% C.I.\n         estimate    lower    upper\n  male   1.000000       NA       NA\n  female 1.572329 1.083353 2.282007\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n- critical value $c$ from normal model\n- exponentiate for an interval for $RR$\n\n> With 90% confidence, the risk of asthma is estimated to be betwen 1.08 and 2.28 times greater for women than for men, with a point estimate of 1.57.\n:::\n\n:::\n\n\n## Implementation details\n\n::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nriskratio(asthma.tbl, \n          rev = 'both', conf.level = 0.9,\n          method = 'wald', correction = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n$data\n        \n         no asthma asthma Total\n  male         769     30   799\n  female       781     49   830\n  Total       1550     79  1629\n\n$measure\n        risk ratio with 90% C.I.\n         estimate    lower    upper\n  male   1.000000       NA       NA\n  female 1.572329 1.083353 2.282007\n\n$p.value\n        two-sided\n         midp.exact fisher.exact chi.square\n  male           NA           NA         NA\n  female 0.04412095   0.04961711 0.05703135\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"55%\"}\n\nImplementation is identical to `oddsratio`:\n\n- outcome of interest should be *second* column\n- group of interest should be *second* row\n- rearrange the input data using `rev`\n\nAlso similarly:\n\n- `chi.square` gives the $p$-value for the $\\chi^2$ test of association\n- `fisher.exact` gives an exact p-value you can use if assumptions aren't met\n\n:::\n\n:::\n\n## Interpretation\n\n::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nriskratio(asthma.tbl, \n          rev = 'both', conf.level = 0.9,\n          method = 'wald', correction = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n$data\n        \n         no asthma asthma Total\n  male         769     30   799\n  female       781     49   830\n  Total       1550     79  1629\n\n$measure\n        risk ratio with 90% C.I.\n         estimate    lower    upper\n  male   1.000000       NA       NA\n  female 1.572329 1.083353 2.282007\n\n$p.value\n        two-sided\n         midp.exact fisher.exact chi.square\n  male           NA           NA         NA\n  female 0.04412095   0.04961711 0.05703135\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"55%\"}\nFirst report the test result, then the measure of association:\n\n> The data provide evidence at the 10% significance level of an association between asthma and sex ($\\chi^2$ = 3.62 on 1 degree of freedom, *p* = 0.057). With 90% confidence, the risk of asthma is estimated to be betwen 1.08 and 2.28 times greater for women than for men, with a point estimate of 1.57.\n\n\n:::\n\n:::\n\nComments:\n\n- the `chi.square` $p$-value is from the test of association/independence\n- if assumptions aren't met, can use the `fisher.exact` instead\n\n## Further examples: cyclosporiasis {.scrollable}\n\n::: {.columns}\n\n::: {.column}\nAn outbreak of cyclosporiasis was detected among residents of New Jersey. In a case-control study, investigators found that 21 of 30 case-patients and 4 of 60 controls had eaten raspberries.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------------------\n   &nbsp;      raspberries   no raspberries \n------------- ------------- ----------------\n  **case**         21              9        \n\n **control**        4              56       \n--------------------------------------------\n\n\n:::\n:::\n\n\n\n- outcome-based sampling\n- odds ratio should be used\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check test assumptions\nchisq.test(cyclo.tbl)$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                cyclosporiasis\nraspberries           case  control\n  raspberries     8.333333 16.66667\n  no raspberries 21.666667 43.33333\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute measure of association and p value\noddsratio(cyclo.tbl, rev = 'both',\n          method = 'wald', correction = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n$measure\n                odds ratio with 95% C.I.\nraspberries      estimate    lower    upper\n  no raspberries  1.00000       NA       NA\n  raspberries    32.66667 9.081425 117.5048\n\n$p.value\n                two-sided\nraspberries        midp.exact fisher.exact   chi.square\n  no raspberries           NA           NA           NA\n  raspberries    6.358611e-10 6.183017e-10 1.247883e-09\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n> The data provide very strong evidence of an association between eating raspberries during the outbreak and case incidence ($\\chi^2$ = 36.89 on 1 degree of freedom, *p* < 0.0001). With 95% confidence, the odds of indcidence are estimated to be between 9.08 and 117.5 times higher among NJ residents who ate raspberries during the outbreak.\n\n## Further examples: smoking and CHD\n\n::: {.columns}\n\n::: {.column}\nA cohort study of 3,000 smokers and 5,000 nonsmokers investigated the link between smoking and the development of coronary heart disease (CHD) over 1 year.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n------------------------------\n    &nbsp;       CHD   no CHD \n--------------- ----- --------\n  **smoker**     84     2916  \n\n **nonsmoker**   87     4913  \n------------------------------\n\n\n:::\n:::\n\n\n\n- two independent samples, but **not** outcome-based sampling\n:::\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check test assumptions\nchisq.test(chd.tbl)$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           chd\nsmoker          CHD   no CHD\n  smoker     64.125 2935.875\n  nonsmoker 106.875 4893.125\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nriskratio(chd.tbl, rev = 'both',\n          method = 'wald', correction = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n$measure\n           risk ratio with 95% C.I.\nsmoker      estimate    lower    upper\n  nonsmoker 1.000000       NA       NA\n  smoker    1.609195 1.196452 2.164325\n\n$p.value\n           two-sided\nsmoker       midp.exact fisher.exact  chi.square\n  nonsmoker          NA           NA          NA\n  smoker    0.001799736  0.001800482 0.001976694\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n> The data provide very strong evidence that smoking is associated with coronary heart disease ($\\chi^2$ = 9.5711 on 1 degree of freedom, *p* = 0.00198). With 95% confidence, the risk of CHD is estimated to be between 1.196 and 2.164 times greater among smokers compared with nonsmokers, with a point estimate of 1.609.\n\n## Further examples: malaria vaccine {.scrollable}\n\n::: {.columns}\n\n::: {.column}\nIn a randomized trial for a malaria vaccine, 20 individuals were randomly allocated to receive a dose of the vaccine or a placebo.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n----------------------------------------\n   &nbsp;      infection   no infection \n------------- ----------- --------------\n **placebo**       6            0       \n\n **vaccine**       5            9       \n----------------------------------------\n\n\n:::\n:::\n\n\n\n- small sample size, so expected counts are likely too small to meet $\\chi^2$ test assumptions\n:::\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check test assumptions\nchisq.test(malaria.tbl)$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         outcome\ntreatment infection no infection\n  placebo       3.3          2.7\n  vaccine       7.7          6.3\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nriskratio(malaria.tbl, rev = 'columns',\n          method = 'wald', correction = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n$measure\n         risk ratio with 95% C.I.\ntreatment  estimate     lower     upper\n  placebo 1.0000000        NA        NA\n  vaccine 0.3571429 0.1768593 0.7212006\n\n$p.value\n         two-sided\ntreatment midp.exact fisher.exact chi.square\n  placebo         NA           NA         NA\n  vaccine  0.0119195   0.01408669 0.03094368\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n> The data provide moderate evidence that the malaria vaccine is effective (Fisher's exact test, *p* = 0.0141). \n\nA twist: how to interpret the CI? Answer: $\\text{efficacy = 1 - RR}$\n\n> With 95% confidence, the risk of infection is estimated to be between 27.88% and 82.31% lower among vaccinated individuals, with a point estimate of 64.29% efficacy.\n\n# Lecture 16: Simple linear regression\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## Today's agenda\n\n1. [lecture] estimation and inference for simple linear regression\n2. course evaluations\n3. final scheduling\n4. [lab] fitting SLR models in R\n\n## PREVEND data\n\n::: {.columns}\n\n::: {.column width=\"70%\"}\nRuff Figural Fluency Test (RFFT) is a cognitive assessment.\n\n- measures nonverbal capacity for initiation, planning, and divergent reasoning\n- scale: 0 (worst) to 175 (best)\n    \n\n:::\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n---------------------\n casenr   age   rfft \n-------- ----- ------\n  126     37    136  \n\n   33     36     80  \n\n  145     37    102  \n---------------------\n\n\n:::\n:::\n\n\n\n:::\n\n::: \n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-270-1.png){fig-align='center'}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n> How much does cognitive ability as measured by RFFT decline with age on average?\n\n:::\n\n:::\n\n## Best-fitting line\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-271-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column}\nPreviously you found the best-fitting line:\n\n$$\n\\text{RFFT} = 134.098 - 1.191 \\times \\text{age}\n$$\n\n> With each year of age, RFFT decreases by 1.191 points on average.\n:::\n\n:::\n\n\n\n$$\n\\begin{align}\n\\text{slope}: \n\\quad-1.191 &= \\text{cor}(\\text{age}, \\text{RFFT})\\times\\frac{SD(\\text{RFFT})}{SD(\\text{age})} \n\\\\\n\\text{intercept}:\n\\quad134.098 &= \\text{mean}(\\text{RFFT}) - (-1.191)\\times\\text{mean}(\\text{age})\n\\end{align}\n$$\n\n## Bias and error\n\n::: {.columns}\n\n::: {.column}\nRecall how you found this line:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-272-1.png)\n:::\n:::\n\n\n\n:::\n\n::: {.column}\nBias and error are measured via residuals:\n$$\n\\textcolor{red}{e_i} = y_i - \\textcolor{blue}{\\hat{y}_i}\n$$\n\n- $\\text{bias} = -\\frac{1}{n}\\sum_i \\textcolor{red}{e_i}$\n- $\\text{SSE} = \\sum_i \\textcolor{red}{e_i}^2$\n\nWe said that the best-fitting line achieved two conditions:\n\n- **no bias**: underestimates and overestimates equally often\n- **minimal error**: as close as possible to as many data points as possible\n\n:::\n\n:::\n\n\n\n\n\n## The SLR model\n\n::: {.columns}\n\n::: {.column}\nThe simple linear regression model is:\n\n$$\nY \n= \\textcolor{blue}{\\underbrace{\\beta_0 + \\beta_1 x}_\\text{mean}} + \n\\textcolor{red}{\\underbrace{\\epsilon}_\\text{error}}\n$$\n:::\n\n::: {.column}\n- continuous response $Y$\n- explanatory variable $x$\n- regression coefficients $\\beta_0, \\beta_1$\n- model error $\\epsilon$\n:::\n\n:::\n\nThe values that minimize error subject to the model being unbiased are:\n\n$$\\begin{align*}\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} &\\quad(\\text{unbiased}) \\\\\n\\hat{\\beta}_1 &= \\frac{s_y}{s_x}\\times r  &\\quad(\\text{minimizes SSE})\n\\end{align*}$$\n\nThese are called the **least squares estimates**.\n\n\n## Least squares estimates in R\n\nAccording to the model, a one-unit increment in $x$ corresponds to a $\\beta_1$-unit change in mean $Y$:\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit model\nfit <- lm(formula = rfft ~ age, data = prevend)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = rfft ~ age, data = prevend)\n\nCoefficients:\n(Intercept)          age  \n    134.098       -1.191  \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\n> With each additional year of age, mean RFFT score decreases by an estimated 1.191 points.\n\n:::\n\n:::\n\n- `formula = <RESPONSE> ~ <EXPLANATORY>` specifies the model\n- `data = <DATAFRAME>` specifies the observations\n\n## Error variability and model fit\n\nThe residual standard deviation provides an estimate of error variability:\n\n$$\\textcolor{\\red}{\\hat{\\sigma}} = \\sqrt{\\frac{1}{n - 2} \\sum_i e_i^2} \\qquad\\text{(estimated error variability)}$$\n\n::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-274-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"55%\"}\nThe proportion of variability explained by the model is:\n$$\nR^2 = 1 - \\frac{(n - 2)\\textcolor{red}{\\hat{\\sigma}^2}}{(n - 1)\\textcolor{darkgrey}{s_y^2}}\n\\quad\\left(1 - \\frac{\\text{error variability}}{\\text{total variability}}\\right)\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - (n - 2)*sigma(fit)^2/((n - 1)*var(rfft))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4043103\n```\n\n\n:::\n:::\n\n\n\n> Age explains 40.43% of variability in RFFT.\n:::\n\n:::\n\n## Standard errors for the coefficients\n\nStandard errors for the coefficients are:\n\n$$SE\\left(\\hat{\\beta}_0\\right) = \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{(n - 1)s_x^2}} \\qquad\\text{and}\\qquad\nSE\\left(\\hat{\\beta}_1\\right) = \\hat{\\sigma}\\sqrt{\\frac{1}{(n - 1)s_x^2}}$$\n\nWhile you won't need to know these formulae, do notice that:\n\n- more data $\\longrightarrow$ less sampling variability\n- more spread in $x$ $\\longrightarrow$ less sampling variability\n\n## Inference for the coefficients \n\n::: {.columns}\n\n::: {.column}\nIf the errors are symmetric and unimodal, then the sampling distribution of\n$$\nT = \\frac{\\hat{\\beta}_1 - \\beta_1}{SE(\\beta_1)}\n$$\nis well-approximated by a $t_{n - 2}$ model.\n\n1. Significance test:\n$\\begin{cases} H_0: \\beta_1 = 0 \\\\ H_A: \\beta_1 \\neq 0 \\end{cases}$\n\n2. Confidence interval: \n$\\hat{\\beta}_1 \\pm c\\times SE\\left(\\hat{\\beta}_1\\right)$\n:::\n\n::: {.column}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-276-1.png){fig-align='center'}\n:::\n:::\n\n\n\n- $P(T > |T_\\text{obs}|) \\approx 0$: very strong evidence of an association (true slope is not zero)\n- confidence interval using $t_{206}$ critical value: (-1.389, -0.992)\n:::\n\n:::\n\n\n\n## Inference for the PREVEND study\n\n::: {.columns}\n\n::: {.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(rfft ~ age, data = prevend)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = rfft ~ age, data = prevend)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.085 -14.690  -2.937  12.744  77.975 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 134.0981     6.0701   22.09   <2e-16 ***\nage          -1.1908     0.1007  -11.82   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.52 on 206 degrees of freedom\nMultiple R-squared:  0.4043,\tAdjusted R-squared:  0.4014 \nF-statistic: 139.8 on 1 and 206 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nconfint(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %      97.5 %\n(Intercept) 122.130647 146.0654574\nage          -1.389341  -0.9922471\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column}\nFitted model:\n$$\n\\text{RFFT} = 134.098 - 1.191 \\times \\text{age}\n$$\n\n\n- Age explains an estimated 40.43% of variation in RFFT.\n\n- With each year of age mean RFFT declines by an estimated 1.19 points (SE 0.10).\n\n- There is a significant association between age and mean RFFT score (*T* = -11.82 on 206 degrees of freedom, *p* < 0.0001). \n\n\n- With 95% confidence, each additional year of age is associated with an estimated decline in mean RFFT between 0.99 and 1.39 points.\n\n:::\n\n:::\n\n## Kleiber's law\n\n[Kleiber's law](https://en.wikipedia.org/wiki/Kleiber%27s_law) refers to the relationship between metabolic rate and body mass.\n\n::: columns\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-278-1.png)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"55%\"}\nWe can estimate it via the SLR model: \n$$\n\\log(\\text{metabolism}) = \\beta_0 + \\beta_1 \\log(\\text{mass}) + \\epsilon\n$$ \n\nFitted model:\n$$\n\\log(\\text{metabolism}) = 5.64 + \n0.74 \\times \\log(\\text{mass})\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(log.metab ~ log.mass, data = kleiber)\n```\n:::\n\n\n\n:::\n:::\n\n## Kleiber's law: inference\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(log.metab ~ log.mass, data = kleiber)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log.metab ~ log.mass, data = kleiber)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.14216 -0.26466 -0.04889  0.25308  1.37616 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.63833    0.04709  119.73   <2e-16 ***\nlog.mass     0.73874    0.01462   50.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4572 on 93 degrees of freedom\nMultiple R-squared:  0.9649,\tAdjusted R-squared:  0.9645 \nF-statistic:  2553 on 1 and 93 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-281-1.png)\n:::\n:::\n\n\n\n- $\\hat{\\beta}_0 = 5.638$\n- $\\hat{\\beta}_1 = 0.739$\n- $\\hat{\\sigma} = 0.457$\n\n:::\n\n:::\n\n> There is a significant association between body mass and metabolism (*p* < 0.0001): body mass explains 96.49% of variation in metabolism; with 95% confidence, a unit increment in log mass is associated with an estimated increase in mean log metabolism between 0.7097 and 0.7678, with a point estimate of 0.7387.\n\n## Kleiber's law: model interpretation\n\nExponentiating both sides of the fitted SLR model equation:\n\n$$\n\\underbrace{\\text{metabolism}}_{e^{\\log(\\text{metabolism})}} = \\underbrace{280.99}_{e^{5.64}} \\times \\underbrace{\\text{mass}^{0.74}}_{e^{0.74 \\log(\\text{mass})}}\n$$\n\nSo we've really estimated what's known as a *power law* relationship: $y = ax^b$.\n\n- multiplicative, not additive, relationship\n- doubling $x$ corresponds to changing $y$ by a factor of $2^b$\n\nThe estimate and interval for $\\beta_1$ in the SLR model can be transformed appropriately for a more direct interpretation:\n\n> With 95% confidence, every doubling of body mass is associated with an estimated 63.55-70.26% increase in median metabolism.\n\n# Activity 1: Nonparametric inference for location\n\n*Self-paced activity in place of one lecture*\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\nIn this activity you'll learn about nonparametric alternatives to the $t$ test for one and two means. The activity is organized much like a lab, but with extra narrative. You should read through the narrative at your own pace and try the exercises provided as you go. At the end there are two practice problems that you'll be expected to complete before next class.\n\n### Background: parametric and nonparametric inference\n\nConsider the basis for the inferences developed so far: under certain conditions (typically regularity of the underlying population distribution, assessed by checking histograms for unimodality and approximate symmetry) and with sufficient sample sizes, a model is specified for the sampling distribution of a test statistic. \n\n- inference for one mean: $t_{n - 1}$ model for the sampling distribution of $T = \\frac{\\bar{x} - \\mu}{SE(\\bar{x})}$\n- inference comparing two means: $t_{\\nu}$ model for the sampling distribution of $T = \\frac{\\bar{x} - \\bar{y} - \\delta}{SE(\\bar{x} - \\bar{y})}$\n- inference comparing several means: $F_{k - 1, n - k}$ model for the sampling distribution of $F = \\frac{MSG}{MSE}$\n\nThese are all what are known as *parametric models*, because they are specified through one or two parameters that determine their exact shape. The parameters in this case are the degrees of freedom terms -- $n - 1$ for the one-sample $t$ test, $\\nu$ (usually estimated) for the two-sample $t$ test, and $k - 1$ and $n - k$ for the $F$ test.\n\nAs such, these procedures are examples of *parametric inference* -- inferences that utilize a parametric model for the data and/or test statistic.\n\nWhen assumptions for parametric inference aren't tenable, or when a parametric model is not available, there are so-called **nonparametric** methods of inference: **methods that don't depend on a parametric model** such as the $t$ or $F$ models we've learned about in class.\n\nWe will consider specifically nonparametric procedures based on ranks, *i.e.*, ordering observations from smallest to largest. \n\n### Motivating examples\n\nIn practice, the situation that most often leads an analyst to consider rank-based nonparametric methods is that the assumptions for the $t$ test either don't hold or are difficult to assess.\n\nBelow are two such situations you've already encountered in this class.\n\n#### Small sample sizes\n\nWhen sample sizes are small, it's awkward to assess assumptions for parametric inference, because with few observations histograms can lack any discernible shape. For example, the most we can say about the following data on heart rates for 19 women and 20 men is that there are no evident outliers.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/motivation 1- small sample sizes-1.png)\n:::\n:::\n\n\n\nIn practical terms, the $t$ test is likely still fine under these circumstances; however, some may wish to consider an inference for comparing heart rates between groups that doesn't depend on distributional assumptions.\n\n#### Assumptions don't hold\n\nOn occasion you may go to check assumptions and find that they're clearly violated. For example, the pairwise differences between actual and desired weights from BRFSS respondents showed clear skewness and several large outliers. In that case, the sample size was big enough that we overlooked the issue, but it's not hard to imagine a similar situation cropping up with fewer observations.\n\nSuppose you only had 12 observations that showed the same skew and had one big outlier:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/motivation 2- assumptions fail-1.png)\n:::\n:::\n\n\n\nHere the $t$ test isn't appropriate, and using it anyway would likely result in a true significance level, coverage, and power quite different from the nominal levels specified in the test, so it would be hard to trust the result. This is a great situation to use a rank-based nonparametric alternative.\n\n### Inference on \"location\" (not mean)\n\nThe usual parametric inferences pertain to the population mean; not so with rank-based nonparametrics. Instead, these inferences pertain simply to \"location\". \n\nOften \"location\" is characterized in terms of the \"center\" of a distribution so that inferences can be interpreted in a manner similar to parametric tests and intervals. \n\nWe will follow this convention and consider the hypotheses to be about the center(s) of the population model(s), denoted $c$. For example, the two-sided test of center would test the hypotheses:\n\n$$\n\\begin{cases}\nH_0: &c = c_0 \\\\\nH_A: &c \\neq c_0\n\\end{cases}\n$$\n\nFor the two-sided test of difference in centers, we will test the hypotheses:\n\n$$\n\\begin{cases}\nH_0: &c_1 = c_2 \\\\\nH_A: &c_1 \\neq c_2\n\\end{cases}\n$$\n\nHowever, you should keep in mind that \"location\" is a more general notion that encompasses all measures of location of a distribution.\n\n### One-sample inference: signed rank test\n\nThe basic premise of this test is that, if the population model is symmetric, its center should evenly divide the data.\n\n::: callout-tip\n\n## Warm up\n\nConsider the following 15 measurements of DDT in kale in ppm in order from smallest to largest:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 2.79 2.93 3.06 3.07 3.08 3.18 3.22 3.22 3.33 3.34 3.34 3.38 3.56 3.78 4.64\n```\n\n\n:::\n:::\n\n\n\nSuppose you wish to test whether the center $c$ of the population model is $c_0 = 3$ ppm and assume a symmetric population model.\n\n1.  How many observations would you expect to be smaller than $c_0 = 3$ if 3ppm is in fact the center?\n2.  How many observations are actually smaller than 3 ppm?\n3.  Based on your answers to 1-2, do you think it is likely that in fact $c = 3$?\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## Solution\n\nIf the population is symmetric about $c_0$, you'd expect roughly half of observations (7.5) to be smaller than 3ppm.\n\nThe actual number of observations smaller than 3ppm is:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n\nThis is much less than half, so it seems unlikely that the center is actually 3ppm.\n:::\n\nThe signed rank test is a nonparametric alternative to the one-sample $t$ test applicable to any symmetric population model. The particular form of symmetry and presence of outliers do not affect the test.\n\n#### Hypotheses\n\nThe test can be directional or two-sided, just like the $t$ test. Thus, the possible hypotheses are: \n\n$$\nH_0: c = c_0 \\quad\\text{vs.}\\quad H_A: c \\mathrel{\\substack{<\\\\\\neq\\\\ >}} c_0\n$$ \n\n#### Test procedure\n\nWhile the intuition of the test is that half of observations should be smaller than the true center under population symmetry, the test statistic is not quite as direct as a tally of how many observations are below the hypothetical value. Instead, the procedure is as follows:\n\n1.  \\[center\\] Calculate deviations $d_i = x_i - c_0$\n\n2.  \\[rank\\] Sort and rank the absolute deviations $|d_i|$\n\n    -   average ranks in case of ties\n    -   drop zeros\n\n3.  \\[sum\\] Add up the positive 'signed ranks' $\\sum_{\\text{sign}(d_i) > 0} r_i$\n\nThis produces the test statistic:\n\n$$V = \\sum_{i = 1}^n \\text{sign}(d_i) \\times R_i$$\n\n::: callout-tip\n\n## Check your understanding\n\nTry working out the rank sum procedure manually using the DDT data -- it's small enough that you could jot down the steps on some scratch paper. See if you can calculate $V$.\n\nTo help, here are the deviations sorted smallest to largest:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] -0.21 -0.07  0.06  0.07  0.08  0.18  0.22  0.22  0.33  0.34  0.34  0.38\n[13]  0.56  0.78  1.64\n```\n\n\n:::\n:::\n\n\n\n1. Start by writing the deviations in order of absolute value in a column. \n2. Then rank them 1-15 in an adjacent column. If there is a tie -- *e.g.*, two absolute deviations of 0.05 -- then assign them both the average of the ranks. For example, if 0.05 occurs twice in positions 2 and 3, then give them both rank 2.5.\n3. Write down the sign of the deviation in a new column.\n4. Then write down the \"signed rank\", or product of the sign and the rank, in a fourth column.\n5. Add up the positive signed ranks. This is the signed rank statistic.\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nThis shows the steps in R, but don't worry about the codes; the output is meant to illustrate what you would do on paper to find the rank sum statistic.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 5\n        di abs.di  rank  sign signed.rank\n     <dbl>  <dbl> <dbl> <dbl>       <dbl>\n 1  0.0600 0.0600   1       1         1  \n 2 -0.0700 0.0700   2.5    -1        -2.5\n 3  0.0700 0.0700   2.5     1         2.5\n 4  0.0800 0.0800   4       1         4  \n 5  0.180  0.180    5       1         5  \n 6 -0.21   0.21     6      -1        -6  \n 7  0.220  0.220    7.5     1         7.5\n 8  0.220  0.220    7.5     1         7.5\n 9  0.33   0.33     9       1         9  \n10  0.34   0.34    10.5     1        10.5\n11  0.34   0.34    10.5     1        10.5\n12  0.38   0.38    12       1        12  \n13  0.56   0.56    13       1        13  \n14  0.78   0.78    14       1        14  \n15  1.64   1.64    15       1        15  \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 111.5\n```\n\n\n:::\n:::\n\n\n:::\n\n#### $p$-values for the test\n\nJust like other hypothesis tests, the signed rank test rejects if $V$ is sufficiently large in the direction of the alternative. \n\nA sampling distribution for $V$ can be found exactly using combinatorics, or approximated using probability theory. In this caseWe won't go into details about either approach, except to indicate that there is a sampling distribution for $V$ that we can use to obtain $p$-values in the same fashion that the $t_{n - 1}$ model was used to obtain $p$-values for the $t$ test.\n\nIn this case, there are 3.2768\\times 10^{4} possible sign combinations; of these, only about 0.375% give a larger value of $V$. That provides a $p$-value for the test, and since $p = 0.0018 < 0.05$ we would reject $H_0$ at the 5% significance level. This result is interpreted as:\n\n> The data provide strong evidence that the typical DDT concentration in kale is not 3ppm (signed rank statistic *V* = 111.5, *p* = 0.00375).\n\n#### Implementation with `wilcox.test(...)`\n\nThe implementation in R looks and functions much like `t.test`:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  ddt\nV = 111.5, p-value = 0.003751\nalternative hypothesis: true location is not equal to 3\n99 percent confidence interval:\n 3.060070 3.780032\nsample estimates:\n(pseudo)median \n       3.26001 \n```\n\n\n:::\n:::\n\n\n\nSome remarks:\n- `exact = F` produces approximate $p$-values and confidence intervals; you may see warning messages if this is excluded\n-   pseudo-median is a measure of center, but not the same as a median or mean (check!)\n\n::: callout-note\n## Your turn 1\n\nPerform the analogous inference using the $t$ test and compare the results. Do the tests agree at the 1% significance level?\n\n\n\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Solution\nThe tests do not agree: the signed rank test rejects at the 1% level, but the $t$ test does not.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  ddt\nt = 2.9059, df = 14, p-value = 0.01151\nalternative hypothesis: true mean is not equal to 3\n99 percent confidence interval:\n 2.991996 3.664004\nsample estimates:\nmean of x \n    3.328 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: callout-note\n## Your turn 2\n\nPerform the signed rank test to determine whether actual weight exceeds desired weight by more than 10lbs at the 5% significance level. Report the result in the usual narrative format. Compare your result with the inference obtained from a $t$ test.\n\n\n\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Solution\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  weight.diff\nV = 61, p-value = 0.04559\nalternative hypothesis: true location is greater than 10\n95 percent confidence interval:\n 10.00003      Inf\nsample estimates:\n(pseudo)median \n       22.8617 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  weight.diff\nt = 1.6127, df = 11, p-value = 0.06755\nalternative hypothesis: true mean is greater than 10\n95 percent confidence interval:\n 7.019188      Inf\nsample estimates:\nmean of x \n    36.25 \n```\n\n\n:::\n:::\n\n\n\nInterpretation of the signed rank test test:\n\n> The data provide evidence that actual weight exceeds desired weight by more than 10lbs (signed rank statistic *V* = 61, *p* = 0.0456).\n\nThe signed rank test finds evidence that actual weight exceeds desired weight by more than 10lbs, where the $t$ test does not.\n:::\n\n### Two-sample inference: rank sum test\n\nThe rank-sum test is a nonparametric alternative to the two-sample $t$ test based on ranks. The key idea for the test is that if observations in both groups come from the same population distribution then they should be exchangeable (*i.e.*, groupings don't matter).\n\nThus, the only assumption for this test is that data are independent. The test can be directional, and thus it is possible to test the following hypotheses comparing centers:\n\n$$\nH_0: c_1 = c_2\\quad\\text{vs}\\quad H_A: c_1 \\mathrel{\\substack{<\\\\\\neq\\\\>}} c_2\n$$\n\n#### The alternative hypothesis\n\nThis test is a bit funny in that the null hypothesis is really that *the distributions are exactly the same*. The alternative to this possibility can come about in a number of ways. The alternative is usually interpreted as a *difference in location*, primarily because this is the situation that the test has power to detect. \n\nSo, it is often said that the test also assumes that the samples differ only in location. In other words, the test is most appropriate when the histograms look \"shifted\", but not fundamentally different, as illustrated below.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-286-1.png)\n:::\n:::\n\n\n\nIn all of these cases, there is a (true) difference in location, but if shape differs too much, the rank sum test should *not* be used. That said, it can be hard to tell with small samples, so it may not really be practical to to check this assumption.\n\n\n**Example 1.** Out-of-state tuition costs from 26 public and 26 private universities. These data differ primarily in spread, not location; so the rank-sum test might not work well here. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-287-1.png)\n:::\n:::\n\n\n\n\n**Example 2.** Deviations from expected cancer rates in CT in years with high and low sunspot activity. The shape is a bit hard to discern here, but it seems plausible that the distribution for high sunspot years is shifted to the right of that for low sunspot years. So, the rank sum test would be appropriate here.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-288-1.png)\n:::\n:::\n\n\n\n\n\n#### Rank sum test procedure\n\nThe rank sum procedure, though a bit opaque, is rather simple:\n\n1.  \\[pool\\] Combine observations from both groups\n2.  \\[rank\\] Sort and rank pooled observations\n3.  \\[sum\\] Add up ranks in the first group\n4.  \\[adjust\\] Subtract $\\frac{n_1(n_1 + 1)}{2}$, where $n_1$ is the sample size of the first group\n\nThe test rejects if the sum is larger than expected in the direction of the alternative. As in the signed rank test, combinatorics are used to determine a $p$-value for the test.\n\nThe rationale for this procedure is that if the distributions are the same, then the ranks should be evenly distributed among the two groups; this induces a particular sampling distribution on the sum of the ranks in each group. The adjustment facilitates computation of $p$-values.\n\nLet's illustrate using data from an experiment in which participants were randomly assigned to receive a fish oil supplement or a regular oil supplement. For each subject, the reduction in blood pressure was measured after a period of time on the treatments.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/fish oil data-1.png)\n:::\n:::\n\n\n\nThere's a bit of difference in spread, but enough of a shift in location that the rank sum test is reasonable to apply.\n\n::: callout-tip\n## Check your understanding\n\nCarry out the rank sum procedure outlined above and compute the rank sum statistic by summing up the ranks in the fish oil group.\n\nTo facilitate calculations, here are the data in order of increasing blood pressure:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   bp       diet\n1  -6 RegularOil\n2  -4 RegularOil\n3  -3 RegularOil\n4   0    FishOil\n5   0    FishOil\n6   0 RegularOil\n7   1 RegularOil\n8   2    FishOil\n9   2 RegularOil\n10  2 RegularOil\n11  8    FishOil\n12 10    FishOil\n13 12    FishOil\n14 14    FishOil\n```\n\n\n:::\n:::\n\n\n\nAdd a column of ranks by hand (or in R if you can figure out how!), averaging ranks for any ties. Then add up the ranks in the `FishOil` group, and subtract $\\frac{n_1(n_1 + 1)}{2} = \\frac{7\\times 8}{2} = 28$ to obtain the rank sum statistic.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nThe output below illustrates the rankings and selection of which ones to add up. Don't worry about the codes\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   bp       diet rank ranks.fish\n1  -6 RegularOil    1          0\n2  -4 RegularOil    2          0\n3  -3 RegularOil    3          0\n4   0    FishOil    5          5\n5   0    FishOil    5          5\n6   0 RegularOil    5          0\n7   1 RegularOil    7          0\n8   2    FishOil    9          9\n9   2 RegularOil    9          0\n10  2 RegularOil    9          0\n11  8    FishOil   11         11\n12 10    FishOil   12         12\n13 12    FishOil   13         13\n14 14    FishOil   14         14\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 41\n```\n\n\n:::\n:::\n\n\n:::\n\n#### Implementation using `wilcox.test(...)`\n\nThe `wilcox.test` function also implements the rank sum test, using the same syntax as `t.test(...)`. For example, using the fish oil data, we might test at the 1% significance level whether the fish oil supplement caused a greater reduction in blood pressure:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(bp ~ diet, data = fish.oil, \n            mu = 0, alternative = 'greater', \n            exact = F, conf.int = T, conf.level = 0.99)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  bp by diet\nW = 41, p-value = 0.01957\nalternative hypothesis: true location shift is greater than 0\n99 percent confidence interval:\n -0.9999444        Inf\nsample estimates:\ndifference in location \n              7.999962 \n```\n\n\n:::\n:::\n\n\n\nFor this test, the $p$-value gives the percentage of possible rank allocations among the groups for which the rank sum is at least as favorable to $H_A$; again this is computed using combinatorics or approximation methods.\n\nIn this instance, the $p$-value indicates that only about 1.96% of all possible rank allocations among the two groups would produce a rank sum statistic at least as large. Thus, testing at the 1% significance level:\n\n> The data provide do not provide sufficient evidence at the 1% significance level that the fish oil supplement caused a greater reduction in blood pressure than the regular oil supplement (rank sum statistic *W* = 41, *p* = 0.01957).\n\n::: callout-note\n## Your turn 3\n\nPerform the corresponding $t$ test for comparison at the 1% significance level. Do the tests agree?\n\n\n\n\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Solution\n\nThe tests do not agree; the $t$ test supports evidence of an effect at the 1% level where the rank sum test does not.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  bp by diet\nt = 3.0621, df = 9.2643, p-value = 0.006542\nalternative hypothesis: true difference in means between group FishOil and group RegularOil is greater than 0\n95 percent confidence interval:\n 3.111056      Inf\nsample estimates:\n   mean in group FishOil mean in group RegularOil \n                6.571429                -1.142857 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: callout-note\n## Your turn 4\n\nUse the `cancer` dataset (specifically the `delta` variable) to test whether the change in cancer rate is higher in years with high sunspot activity. Carry out the test at the 5% level, and report the result in the usual narrative style.\n\n\n\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Solution\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  delta by sunspot\nW = 157.5, p-value = 0.3072\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n -0.1000484        Inf\nsample estimates:\ndifference in location \n            0.09999013 \n```\n\n\n:::\n:::\n\n\n\nInterpretation:\n\n> The data provide no evidence that the change in cancer rate is higher in years with high sunspot activity (rank sum statistic *W* = 157.5, *p* = 0.3072). \n\n:::\n\n### Practice problem\n\n1. Is there a difference in cholesterol associated with consuming oat bran compared with consuming corn flakes? Use the `cholesterol` dataset to test for a difference at the 5% level using a nonparametric test.\n\n    a. Construct boxplots to compare the distributions for location shift. Does the nonparametric test seem appropriate?\n    b. Carry out the test.\n    c. Report the test result in the usual narrative style.\n\n\n\n\n\n\n\n# Activity 2: Line fitting\n\n*Self-paced activity in place of one lecture*\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nThis activity is a self-paced warmup in preparation for learning about linear regression. Simply put, linear regression consists of fitting a line to data in order to describe the relationship between two numeric variables. \n\nBy 'fitting' we do mean something a bit more specific than drawing any old line through a scatterplot, but it's worth emphasizing that the criteria we'll develop later in class, though formal, do reflect several common intuitions about what a 'well-fitting' line looks like. So, the goal of this activity is to explore some of those intuitions. \n\nThe over-arching question I'd like you to consider as you work through this activity is: ***what makes a line \"good\" as a representation of the relationship between two variables?***\n\nWe'll use two datasets:\n\n- `prevend`, which contains measurements of cognitive assessment score and age for 208 adults\n- `mammals`, which contains measurements of brain weight (g) and body weight (kg) for 62 mammal species\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n### Scatterplots\n\nTo start, let's review scatterplots. These are straightforward to construct using `plot(...)`; in addition to what you have seen before, the commands below show you an alternate syntax for making scatterplots using a formula specification of the form `y ~ x`, as well as how to adjust the labels.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/constructing scatterplots-1.png)\n:::\n\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/constructing scatterplots-2.png)\n:::\n\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/constructing scatterplots-3.png)\n:::\n:::\n\n\n\n> There is a negative linear relationship between RFFT score and age, suggesting cognitive function decreases with age.\n\nScatterplots allow for visual assessment of trend. A trend is *linear* if it seems to follow a straight line; linear trends are *positive* if they increase from left to right and *negative* if they decrease from left to right. Most of the time, you won't see a perfectly clear straight-line relationship, so what we really mean by this is that (a) there's a trend and (b) a line would describe it about as well as any other path drawn through the scatter.\n\n::: callout-note\n## Your turn 1\n\nUsing the `mammals` dataset, construct a scatterplot of `log.brain` (y) against `log.body` (x) using the formula syntax. Label the axes \"log body weight (kg)\" and \"log brain weight (g)\". Describe the trend you see in the data.\n\n\n\n\n\n\n:::\n\n::: {.callout-note icon=false collapse=true appearance=\"minimal\"}\n## Solution\n\nThere is a positive linear relationship between log brain weight and log body weight, suggesting that larger mammals in general have larger brains. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-295-1.png)\n:::\n:::\n\n\n:::\n\nIn most contexts one variable is of direct interest, and the other is considered as potentially explaining the variable of interest. For example:\n\n- in the `prevend` data, age might explain cognitive functioning\n- in the `mammals` data, body size might explain brain size\n\nPlots are typically oriented so that the variable of interest, or *response*, is on the vertical (y) axis and the *explanatory variable* is on the horizontal (x) axis. Throughout the rest of this activity, you'll see these terms and the notation $x, y$ used correspondingly.\n\n### Correlation\n\nBoth of the above examples show linear trends of differing strength. The `prevend` data are not as close to falling on a straight line as the `mammals` data, so there is a stronger linear relationship between log brain and log body weights than there is between RFFT score and age. \n\nFurther, the two examples show trends of differing direction: the trend in the `prevend` data is negative (RFFT declines with age) whereas the trend in the `mammals` data is positive (brain size increases with body size).\n\nThe direction and strength of a linear relationship can be measured directly by the correlation coefficient, defined as:\n\n$$\nr = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\n$$\nThe correlation coefficient is always between -1 and 1:\n\n- $r \\rightarrow 0$ indicates no relationship\n- $r \\rightarrow 1$ indicates a perfect linear relationship\n- $r > 0$ indicates a positive linear relationship\n- $r < 0$ indicates a negative linear relationship\n\nCorrelations are simple to compute in R:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.635854\n```\n\n\n:::\n:::\n\n\n\nInterpretations should note the direction and strength. Strength is a little subjective, but as a rule of thumb:\n\n- $|r| < 0.3$: no relationship\n- $0.3 \\leq |r| < 0.6$: weak to moderate relationship\n- $0.6 \\leq |r| < 1$: moderate to strong relationship\n\nIn this case:\n\n> There is a moderate negative linear relationship between age and RFFT.\n\n::: callout-note\n## Your turn 2\n\nCompute the correlation between log brain weight and log body weight; interpret the correlation in context.\n\n\n\n::: {.cell}\n\n:::\n\n\n:::\n\n::: {.callout-note icon=false collapse=true appearance=\"minimal\"}\n## Solution\n\nThere is a strong positive linear relationship between log body weight and log brain weight.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9595748\n```\n\n\n:::\n:::\n\n\n:::\n\nGenerally, the stronger the correlation, the easier it will be to visualize a line passing through the data.\n\n::: callout-tip\n## Exploration: visualizing correlations\n\nThe `sim.by.cor` function simulates observations of two numeric variables that have a specified correlation `r`. It returns a plot of the simulated data showing the scatter, the $y = x$ line, and the sample correlation. The sample correlation will differ a little bit from the true value, so don't be surprised if the sample correlation doesn't exactly match the value of `r` that you input.\n\nUse this to explore the correspondence between the sample correlation and the visual appearance of scatterplots:\n\n- try several negative correlations\n- try several positive correlations\n- try both large (near $\\pm$ 1) and small (near 0) values\n\nFor each value of `r` you try, run the command a few times to see several simulated datasets.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/visualize data with different correlations-1.png)\n:::\n:::\n\n\n:::\n\n\n### Hand-fitting lines\n\nNow we'll turn to actually drawing lines through data scatter to approximate linear relationships. We'll do this by specifying a slope and (sometimes) intercept, so a quick reminder of the slope-intercept form for the equation of a line may be handy:\n\n$$\ny = a + bx\n$$\n\n\n- the intercept $a$ is the value at which the line passes through the vertical axis (the value of $y$ when $x = 0$)\n- the slope $b$ is the amount by which $y$ increases for each increment in $x$ (sometimes called 'rise over run')\n\nI've written a function called `hand.fit` that allows you to construct a scatterplot and easily add a line by specifying the slope and intercept. \n\nFirst we'll consider \"eye-fitting\" a line by trying out different slopes and intercepts until a combination is found that looks good. For example, for the `prevend` dataset a slope of -1.3 and an intercept of 145 look reasonable:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/hand fitting a line to the prevend data-1.png)\n:::\n:::\n\n\n\nConsider what this line says about the relationship between RFFT and age:\n\n> With each year of age, RFFT score decreases by 1.3 points.\n\nThe intercept doesn't have a clear interpretation, because $x = 0$ is not meaningful for this dataset: technically, it says that RFFT for a 0-year-old would be 145, which of course is nonsensical.\n\n::: callout-note\n## Your turn 3\n\nFor the log brain weight and log body weight variables in the `mammals` dataset, experiment with the intercept `a` and the slope `b` until you find a pair of values that seem to reflect the trend well. Interpret the relationship described by the line.\n\nI suggest starting with `a = 0` and `b = 0`. Note that if the values are too extreme, the line may not appear at all on the plot.\n\n\n\n\n\n\n:::\n\n::: {.callout-note icon=false collapse=true appearance=\"minimal\"}\n## Solution\n\nThere are many possible lines that visually fit the data well. One possible solution:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-298-1.png)\n:::\n:::\n\n\n\nThis line says that each 1-unit increment in the log of a mammal's body weight is associated with a 0.85-unit increase in log brain weight. (There is a better interpretation, but it's a bit more mathematically complicated.)\n:::\n\nNow we'll develop some ideas that will help to assess how good these lines are as representations of the $x-y$ relationship.\n\n### Residuals\n\nA residual is something left over; in this context, the difference between the line and a data point. There is one residual for every data point. If we denote the value of the response variable on the line as:\n\n$$\n\\hat{y}_i = a + bx\n$$\n\nThen the residual for the $i$th observation is:\n\n$$\ne_i = y_i - \\hat{y}_i\n$$\n\nResiduals help to capture the fit of the model, because taken together, they convey how close the line is to each data point.\n\n#### Visualizing residuals\n\nAdding the argument `res = T` to the `hand.fit(...)` function will show the residuals visually as vertical red line segments. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/visualize residuals-1.png)\n:::\n:::\n\n\n\nData points below the line have negative residuals, and data points above the line have positive residuals. \n\n::: callout-note \n## Your turn 4\n\nPlot the residuals for the line you identified in the last exercise.\n\n\n\n\n\n\n\n:::\n\n::: {.callout-note icon=false collapse=true appearance=\"minimal\"}\n## Solution\n\nFor the values I identified, the residuals look like this:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-300-1.png)\n:::\n:::\n\n\n::: \n\nTake a moment to consider what the residuals in your plot and the example above convey about line fit:\n\n- What would the residuals look like if fit is good? \n- What would they look like if fit is good?\n\n#### Bias\n\nIn this context, bias refers to how often the line overestimates and underestimates data values:\n\n- if the line tends to overestimate and underestimate equally often, it is unbiased\n- if the line tends to overestimate more often or underestimate more often, then it is biased\n\nThe average residual captures whether the line is biased:\n\n- positive average residual $\\longrightarrow$ underestimates more often $\\longrightarrow$ negative bias\n- negative average residual $\\longrightarrow$ overestimates more often $\\longrightarrow$ positive bias\n- average residual near zero $\\longrightarrow$ unbiased\n\nSo, define as a measure of bias the negative average residual:\n$$\n\\text{bias} = -\\frac{1}{n}\\sum_{i = 1}^n e_i\n$$\n\nIn the example above, the bias of 4.5 means that on average, the line overestimates RFFT score by 4.5 points. If we increase the intercept, we will overestimate even more often, and thus increase the bias:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/more positive bias-1.png)\n:::\n:::\n\n\n\n::: callout-note\n## Your turn 5\n\nUsing the log brain and log body weights in the `mammals` dataset, see if you can fit lines with positive, negative, and no bias.\n\n1. Find a line that has obvious positive bias.\n2. Find a line that has obvious negative bias.\n3. Find a line that has low bias.\n\nFor simplicity, keep your slope the same, and just change the intercept.\n\n\n\n\n\n\n:::\n\n::: {.callout-note icon=false collapse=true appearance=\"minimal\"}\n## Solution\n\nIndividual results will vary a lot. Here are a few scenarios:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-302-1.png)\n:::\n\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-302-2.png)\n:::\n\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-302-3.png)\n:::\n:::\n\n\n\n\n:::\n\nClearly, low bias is a desirable property of a fitted line. However, it is not the only desirable property.\n\n#### Error\n\nThe total magnitude of the residuals conveys how close the line is to the data scatter in general and thus, error. One way to measure this is by the sum of squared residuals:\n\n$$\n\\text{SSE} = \\sum_{i = 1}^n e_i^2\n$$\n\nThis quantity will be smallest whenever the line is as close as it can be to as many data points as possible at once. As a result, a good fit will generally have low error.\n\nWhile high error can arise from bias, it is also possible for a line with no bias to have high error. For example, a horizontal line through the mean RFFT will have zero bias, but still be a bad estimate:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/no bias but high error-1.png)\n:::\n:::\n\n\n\nThe reason the bias is zero is that the residuals balance each other out in the average. It is a mathematical fact that whenever the line passes exactly through the point defined by the sample mean of each variable -- *i.e.*, the point $(\\bar{x}, \\bar{y})$ -- the bias is exactly zero. \n\n::: callout-note\n## Your turn 6\n\nMimic the example above using the log brain and log body weights in the `mammals` dataset: find a line that has low bias but high error.\n\n\n\n\n\n\n\n:::\n\n::: {.callout-note icon=false collapse=true appearance=\"minimal\"}\n## Solution\n\nIndividual results will vary. A horizontal line through the mean log brain weight will work. Here's a more extreme example:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-304-1.png)\n:::\n:::\n\n\n:::\n\n\n\n#### Best unbiased line\n\nConsidering that there is an easy way to ensure the bias of the line is zero -- constrain it to pass through the center $(\\bar{x}, \\bar{y})$ -- we should concern ourselves with finding the unbiased line with lowest error. Finding the best unbiased line reduces, essentially, to finding a slope.\n\nOmitting the intercept altogether in the `hand.fit(...)` function will constrain it to pass through the center, add a point to visualize the center of the data, and print the intercept resulting from the constraint:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/constrain the line to pass through center-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nintercept     slope \n 134.6375   -1.2000 \n```\n\n\n:::\n:::\n\n\n\nOnce we have a slope in hand, the intercept for the fitted line can be found by direct arithmetic:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 134.6375\n```\n\n\n:::\n:::\n\n\n\nSo, finding the best unbiased line amounts to fine-tuning the slope until SSE reaches a minimum. For the `prevend` data, the best unbiased line is about this:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/approximate best unbiased line-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nintercept     slope \n 134.0515   -1.1900 \n```\n\n\n:::\n:::\n\n\n\nSo the best unbiased line results around $a = 134.05$ and $b = -1.19$.\n\nThere is an analytic solution for the slope of the best unbiased line:\n\n$$\na = r\\times \\frac{s_y}{s_x}\n$$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/exact best unbiased line-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n intercept      slope \n134.098052  -1.190794 \n```\n\n\n:::\n:::\n\n\n\nThe exact best unbiased line is therefore:\n$$\ny = 134.0981  + -1.1908x\n$$\n\n::: callout-note\n## Your turn 7\n\nFind the best unbiased line for the `mammals` data.\n\n\n\n\n\n\n:::\n\n::: {.callout-note icon=false collapse=true appearance=\"minimal\"}\n## Solution\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](all-slides_files/figure-docx/unnamed-chunk-306-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nintercept     slope \n2.1347887 0.7516859 \n```\n\n\n:::\n:::\n\n\n:::\n\n### Practice problem\n\n1. [L10] The `kleiber` dataset contains observations of log-transformed average mass (kg) and log-transformed metabolic rate (kJ/day). Kleiber's law refers to the relationship by which metabolism depends on body mass.\n\n    a. Construct a scatterplot of the data, and be sure to orient the response variable and explanatory variable properly on the plot. Is there a trend, and if so, is it linear?\n    b. Compute the correlation and comment on the strength and direction of linear relationship between log-mass and log-metabolism.\n    c. Find the equation of the best unbiased line in slope-intercept form.\n    d. [extra credit] Find an expression of the form $y = a x^b$ for the relationship in (c) on the original (*i.e.*, not log-transformed) scale. What is the exponent $b$? \n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}