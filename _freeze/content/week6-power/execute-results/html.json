{
  "hash": "8f303e0b9981355028d29359dd66bba3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Power analyses\"\nsubtitle: \"Post hoc power analyses and determination of sample sizes for study design\"\nformat: \n  revealjs:\n    logo: img/poly-logo-2.jpg\n    footer: \"STAT218\"\n    smaller: true\n    mermaid:\n      theme: neutral\nexecute: \n  echo: false\n  warning: false\n  message: false\n---\n\n::: {.cell}\n\n:::\n\n\n## Today's agenda\n\n1. [lecture] Statistical power; post-hoc and sample size power analyses.\n2. [review/lab] Test 2 practice problems.\n\n## $p$-values and false rejections\n\n> A $p$-value captures how often you'd make a mistake **if $H_0$ were true**.\n\n::: {.columns}\n\n::: {.column}\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'greater')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.02689\nalternative hypothesis: true difference in means between group seeded and group unseeded is greater than 0\n95 percent confidence interval:\n 42.63408      Inf\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n```\n\n\n:::\n:::\n\n\nIf there is no effect of cloud seeding, then we would see $T > 1.9982$ for 2.689% of samples.\n\n:::\n\n::: {.column}\nThe test rejects at the 5% significance level ($p < 0.05$), but that doesn't completely rule out $H_0$.\n\n- while unlikely, our sample could have been one of the 26 in 1000 where $T$ exceeds 1.9982 despite no effect\n- by rejecting here (when $T = 1.9982$) we are willing to be wrong 2.689% of the time\n\n*By rejecting when $p < \\alpha$ we are willing to be wrong $\\alpha\\times 100$% of the time.*\n\n:::\n\n:::\n\n## A different kind of error?\n\n> But you can also make a mistake when $H_0$ is false!\n\n::: {.columns}\n\n::: {.column}\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(rainfall ~ treatment, data = cloud, \n       mu = 0, alternative = 'two.sided')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  rainfall by treatment\nt = 1.9982, df = 33.855, p-value = 0.05377\nalternative hypothesis: true difference in means between group seeded and group unseeded is not equal to 0\n95 percent confidence interval:\n  -4.764295 559.556603\nsample estimates:\n  mean in group seeded mean in group unseeded \n              441.9846               164.5885 \n```\n\n\n:::\n:::\n\n\nWe'd see $|T| > 1.9982$ for 5.377% of samples if there's no effect. But what if there is an effect?\n:::\n\n::: {.column}\nThe two-sided test fails to reject at the 5% significance level ($p > 0.05$), but that doesn't completely rule out $H_A$.\n\n- the estimated effect -- increase of 277.4 acre-feet -- could be too small relative to the variability in rainfall\n\n- hard to say how often we'd make this kind of mistake without knowing the real difference\n\n*The rate of fail-to-reject errors depends on the (unknown) true parameter value.*\n:::\n\n:::\n\n## Decision errors\n\n> There are two ways to make a mistake in a hypothesis test -- two \"*error types*\".\n\n| | Reject $H_0$ | Fail to reject $H_0$\n---|---|---\n**True $H_0$** | type I error | [correct decision]{style=\"color:lightgrey\"}\n**False $H_0$** | [correct decision]{style='color:lightgrey'} | type II error\n\n::: {.columns}\n\n::: {.column}\n\nAny statistical test will have certain error rates:\n\n- type I error rate is denoted $\\alpha$\n- type II error rate is denoted $1 - \\beta$\n\n:::\n\n::: {.column}\n\nThe significance level of a test is its type I error rate.\n\n- reject when $p < \\alpha$ $\\Longleftrightarrow$ mistakenly reject $\\alpha\\times 100$% of the time\n\nBut we don't know the type II error rate!\n\n- depends on which alternative parameter value is true\n\n:::\n\n:::\n\n\n## Simulating type II errors\n\n::: columns\n::: {.column width=\"55%\"}\nSummary stats for cloud data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------\n treatment   mean     sd     n  \n----------- ------- ------- ----\n  seeded      442    650.8   26 \n\n unseeded    164.6   278.4   26 \n--------------------------------\n\n\n:::\n:::\n\n\nWe can approximate the type II error rate by:\n\n1.  simulating datasets with matching statistics\n2.  performing two-sided tests of no difference\n3.  computing the proportion of fail-to-reject decisions\n\n\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ntype2sim(delta = 277, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](week6-power_files/figure-revealjs/unnamed-chunk-6-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\n> If in fact the effect size is exactly 277, a level 5% test with similar data will fail to reject ~70% of the time!\n\n## Larger effect size\n\n::: columns\n::: {.column width=\"55%\"}\nSummary stats for cloud data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------\n treatment   mean     sd     n  \n----------- ------- ------- ----\n  seeded      442    650.8   26 \n\n unseeded    164.6   278.4   26 \n--------------------------------\n\n\n:::\n:::\n\n\nWe can approximate the type II error rate by:\n\n1.  simulating datasets with matching statistics\n2.  performing two-sided tests of no difference\n3.  computing the proportion of fail-to-reject decisions\n\n\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ntype2sim(delta = 350, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](week6-power_files/figure-revealjs/unnamed-chunk-9-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\n> If in fact the effect size is exactly 400, a level 5% test with similar data will fail to reject ~40% of the time.\n\n## Smaller effect size\n\n::: columns\n::: {.column width=\"55%\"}\nSummary stats for cloud data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n--------------------------------\n treatment   mean     sd     n  \n----------- ------- ------- ----\n  seeded      442    650.8   26 \n\n unseeded    164.6   278.4   26 \n--------------------------------\n\n\n:::\n:::\n\n\nWe can approximate the type II error rate by:\n\n1.  simulating datasets with matching statistics\n2.  performing two-sided tests of no difference\n3.  computing the proportion of fail-to-reject decisions\n\n\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ntype2sim(delta = 100, n = 26, sd = 650.8, \n         alpha = 0.05, nsim = 1000)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](week6-power_files/figure-revealjs/unnamed-chunk-12-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\n> If in fact the effect size is exactly 100, a level 5% test with similar data will fail to reject ~90% of the time.\n\n\n\n## Statistical power\n\nThe **power** of a test refers to its **true rejection rate** across alternatives and is defined as: $$\\beta = \\underbrace{(1 - \\text{type II error rate})}_\\text{correct decision rate when null is false}$$\n\nPower is often interpreted as a detection rate:\n\n-   high type II error $\\longrightarrow$ low power $\\longrightarrow$ low detection rate\n-   low type II error $\\longrightarrow$ high power $\\longrightarrow$ high detection rate\n\n> In general tests have low power for alternatives close to the null value (where \"close\" is relative to sampling variability).\n\n## Power curves\n\n> Power is usually construed as a *curve* depending on the true difference. \n\n::: {.columns}\n\n::: {.column width=\"60%\"}\n\nPower curve for the test exactly as performed with the cloud seeding data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week6-power_files/figure-revealjs/unnamed-chunk-13-1.png){width=576}\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\nAll other attributes of the test are fixed to approximate the test performed:\n\n- sample size $n = 26$\n- significance level $\\alpha = 0.05$\n- population standard deviation $\\sigma = 650$ (larger of two group estimates)\n\n:::\n\n:::\n\n\n## Two common power analyses\n\n::: columns\n::: column\n**Post hoc analysis**: how much power does the test I conducted have if the true difference is exactly equal to my estimate?\n\nHelps to interpret negative results:\n\n-   low power $\\rightarrow$ failure to reject was likely\n-   high power $\\rightarrow$ failure to reject was not likely\n\n::: callout-important\n## Don't over-interpret post-hoc analyses\n\nFailure to reject using a well-powered test *does not confirm the null hypothesis*.\n:::\n\n:::\n\n::: column\n**Sample size determination**: how much data do I need to collect to detect a difference of $\\delta$ using a particular test?\n\nHelps avoid two potential issues:\n\n-   too little data $\\rightarrow$ study not likely to yield significant results\n-   too much data $\\rightarrow$ study is too likely to yield significant results\n:::\n:::\n\n## Post-hoc analysis\n\n> Can we estimate the power of a test we already performed?\n\n::: columns\n::: {.column width=\"55%\"}\nFeasible if we assume (a) a population standard deviation and (b) test conditions are met.\n\nFor the cloud seeding test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(delta = 250, # magnitude of difference\n             sd = 650, # largest population SD\n             n = 26, # smallest sample size\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 26\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.2743235\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"45%\"}\nFor a conservative estimate, use:\n\n-   *smallest* of the two sample sizes\n-   *largest* of the two standard deviations\n-   *smaller* difference than observed\n\n> $\\Longrightarrow$ our test would only reject in favor of a difference of the observed magnitude about 27% of the time\n\nFailure to reject doesn't strongly rule out the alternative.\n:::\n:::\n\n## Sample size calculation\n\n> If you were (re)designing the study, how much data should you collect to detect a specified effect size?\n\n::: columns\n::: {.column width=\"55%\"}\nTo detect a difference of 250 or more due to cloud seeding with power 0.9:\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(power = 0.9, # target power level\n             delta = 250, # smallest difference\n             sd = 650, # largest population SD\n             sig.level = 0.05, \n             type = 'two.sample', \n             alternative = 'two.sided') \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 143.0276\n          delta = 250\n             sd = 650\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"45%\"}\nFor a conservative estimate, use:\n\n-   *overestimate* of the larger of the two standard deviations\n-   *minimum* difference of interest\n\n> $\\Longrightarrow$ we need at least 144 observations in each group to detect a difference of 250 or more at least 90% of the time\n:::\n:::\n\n\n\n## Practical constraints\n\n::: {.columns}\n\n::: {.column width=\"60%\"}\n\nMinimum detectable difference at 5 levels of power as a function of sample size for a one-sided test:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week6-power_files/figure-revealjs/unnamed-chunk-16-1.png){width=576}\n:::\n:::\n\n\nAssumes $\\sigma = 650$ for a conservative estimate.\n\n:::\n\n::: {.column width=\"40%\"}\nIt may not be affordable to obtain data for 144 days per treatment group (pilots and planes are expensive). What is achievable within constraints?\n\n- power of 0.8 will require *n* = 59 per group\n\n    + 138 days total\n\n- decreasing to 0.7 will require *n* = 45 per group\n\n    + 90 days total\n\n:::\n\n:::",
    "supporting": [
      "week6-power_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}